{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":"<p>Prodigy is an AI-powered workflow orchestration tool that enables development teams to automate complex tasks using Claude AI through structured YAML workflows.</p>"},{"location":"#what-is-prodigy","title":"What is Prodigy?","text":"<p>Prodigy combines the power of Claude AI with workflow orchestration to:</p> <ul> <li>Automate repetitive development tasks - Code reviews, refactoring, testing</li> <li>Process work in parallel - MapReduce-style parallel execution across git worktrees</li> <li>Resume long-running operations - Checkpoint and resume capabilities for workflows that span hours or days</li> <li>Handle failures gracefully - Dead Letter Queue (DLQ) for automated retry of failed items</li> <li>Maintain quality - Built-in validation, error handling, and retry mechanisms</li> <li>Track changes - Full git integration with automatic commits and merge workflows</li> <li>Generate living documentation - Keep docs synchronized with code automatically</li> <li>Compose complex workflows - Import, extend, and template reusable workflow components</li> </ul> <pre><code>graph TD\n    User[User defines workflow.yml] --&gt; Prodigy[Prodigy Orchestrator]\n    Prodigy --&gt; Parse[Parse &amp; Validate YAML]\n    Parse --&gt; Worktree[Create Git Worktree]\n    Worktree --&gt; Execute[Execute Commands]\n\n    Execute --&gt; Shell[Shell Commands]\n    Execute --&gt; Claude[Claude AI Commands]\n    Execute --&gt; Control[Control Flow]\n\n    Shell --&gt; Commit[Git Commit]\n    Claude --&gt; Commit\n    Control --&gt; Commit\n\n    Commit --&gt; Checkpoint{Checkpoint?}\n    Checkpoint --&gt;|Save State| Resume[Resume Later]\n    Checkpoint --&gt;|Continue| Next{More Steps?}\n\n    Next --&gt;|Yes| Execute\n    Next --&gt;|No| Merge[Merge to Main Branch]\n    Merge --&gt; Complete[Complete]\n\n    Resume -.-&gt;|prodigy resume| Execute\n\n    style User fill:#e8f5e9\n    style Prodigy fill:#e1f5ff\n    style Claude fill:#fff3e0\n    style Merge fill:#f3e5f5</code></pre> <p>Figure: Prodigy workflow execution showing orchestration, isolation, and checkpoint/resume capability.</p> <p>Production-Ready Features</p> <p>Prodigy includes enterprise features like checkpoints for resuming interrupted workflows, a Dead Letter Queue for automatic failure recovery, and git worktree isolation to keep your main repository clean during execution.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Your First Workflow</p> <p>Start with a simple build-test cycle to see Prodigy in action. This example shows the simplified array syntax for straightforward workflows.</p> <p>Create a simple workflow in <code>workflow.yml</code>:</p> <pre><code># Source: workflows/complex-build-pipeline.yml (simplified)\n# Simplified array syntax - use this for straightforward workflows\n- shell: \"cargo build\"                    # (1)!\n- shell: \"cargo test\"                     # (2)!\n  on_failure:\n    claude: \"/fix-failing-tests\"          # (3)!\n- shell: \"cargo clippy\"                   # (4)!\n\n1. Build the project (commits on success)\n2. Run tests (commits on success)\n3. If tests fail, Claude automatically attempts fixes\n4. Run linting checks\n</code></pre> <p>Syntax Options</p> <p>This example uses the simplified array syntax for simple workflows. For more complex workflows with parallel execution, MapReduce, or advanced features, use the full workflow structure with <code>name</code>, <code>mode</code>, and other configuration fields.</p> <p>Run it:</p> <pre><code>prodigy run workflow.yml\n</code></pre>"},{"location":"#documentation-features","title":"Documentation Features","text":"<p>This book itself is maintained using Prodigy's automated documentation system! Learn how to set up automated, always-up-to-date documentation for your own project:</p> <ul> <li>Automated Documentation Overview - How it works</li> <li>Quick Start (15 minutes) - Fast setup</li> <li>Tutorial (30 minutes) - Comprehensive guide</li> </ul>"},{"location":"#key-concepts","title":"Key Concepts","text":"<ul> <li>Workflows: YAML files defining sequences of commands</li> <li>Commands: Shell commands, Claude AI invocations, or control flow</li> <li>Variables: Capture output from commands and use throughout workflow with <code>${VAR}</code> syntax. Supports nested fields, defaults, and environment variables. See Environment Variables for details</li> <li>Environment: Configuration with secrets management and profile-based values</li> <li>MapReduce: Parallel processing across multiple git worktrees</li> <li>Checkpoints: Save and resume workflow state for long-running operations</li> <li>Validation: Workflow structure and syntax validation, implementation completeness checking with the validate command, and runtime validation. See Command Types for details</li> <li>Observability: Event tracking, Claude execution logs, and comprehensive debugging tools</li> </ul> <p>Start Simple, Scale Up</p> <p>Begin with simple sequential workflows to learn the basics, then progress to MapReduce for parallel processing when you need to handle 10+ similar tasks concurrently.</p>"},{"location":"#next-steps","title":"Next Steps","text":"<p>Getting Started:</p> <ul> <li>Workflow Basics - Learn workflow fundamentals</li> <li>Command Types - Explore available command types</li> <li>Examples - See real-world workflows</li> </ul> <p>Advanced Features:</p> <ul> <li>MapReduce Workflows - Parallel processing at scale</li> <li>Environment Variables - Configuration and secrets management</li> <li>Error Handling - Graceful failure handling strategies</li> </ul> <p>Operations:</p> <ul> <li>Troubleshooting - Common issues and solutions</li> </ul>"},{"location":"commands/","title":"Command Types","text":""},{"location":"commands/#1-shell-commands","title":"1. Shell Commands","text":"<pre><code># Simple shell command\n- shell: \"cargo test\"\n\n# With output capture\n- shell: \"ls -la | wc -l\"\n  capture_output: \"file_count\"\n\n# With failure handling\n- shell: \"cargo clippy\"\n  on_failure:\n    claude: \"/fix-warnings ${shell.output}\"\n\n# With nested failure handlers (multi-level error recovery)\n- shell: \"cargo test\"\n  on_failure:\n    claude: \"/debug-test-failures ${shell.output}\"\n    on_failure:\n      shell: \"notify-team.sh 'Tests failed and debugging unsuccessful'\"\n\n# With timeout\n- shell: \"cargo bench\"\n  timeout: 600  # seconds\n\n# With conditional execution\n- shell: \"cargo build --release\"\n  when: \"${tests_passed}\"\n\n# Working directory control (use shell cd command)\n- shell: \"cd crates/prodigy-core &amp;&amp; cargo test\"\n\n# Environment variables (use shell syntax)\n- shell: \"PATH=/custom/bin:$PATH rustfmt --check src/**/*.rs\"\n</code></pre>"},{"location":"commands/#2-claude-commands","title":"2. Claude Commands","text":"<pre><code># Simple Claude command\n- claude: \"/prodigy-analyze\"\n\n# With arguments\n- claude: \"/prodigy-implement-spec ${spec_file}\"\n\n# With commit requirement\n- claude: \"/prodigy-fix-bugs\"\n  commit_required: true\n\n# With output capture\n- claude: \"/prodigy-generate-plan\"\n  capture_output: \"implementation_plan\"\n</code></pre>"},{"location":"commands/#3-goal-seeking-commands","title":"3. Goal-Seeking Commands","text":"<p>Iteratively refine code until a validation threshold is met.</p> <pre><code>- goal_seek:\n    goal: \"Achieve 90% test coverage\"\n    claude: \"/prodigy-coverage --improve\"\n    validate: \"cargo tarpaulin --print-summary | grep 'Coverage' | sed 's/.*Coverage=\\\\([0-9]*\\\\).*/score: \\\\1/'\"\n    threshold: 90\n    max_attempts: 5\n    timeout_seconds: 300\n    fail_on_incomplete: true\n  commit_required: true\n</code></pre> <p>Fields: - <code>goal</code>: Human-readable description - <code>claude</code> or <code>shell</code>: Command to execute for refinement - <code>validate</code>: Command that outputs <code>score: N</code> (0-100) - <code>threshold</code>: Minimum score to consider complete - <code>max_attempts</code>: Maximum refinement iterations - <code>timeout_seconds</code>: Optional timeout per attempt - <code>fail_on_incomplete</code>: Whether to fail workflow if threshold not met (default: true)</p> <p>Troubleshooting: - Threshold not met: Check that validate command outputs exactly <code>score: N</code> format (0-100) - Not converging: Use <code>fail_on_incomplete: false</code> for optional quality gates - Debug scores: Run workflow with verbose mode (<code>-v</code>) to see validation scores each iteration - Max attempts reached: Increase <code>max_attempts</code> or lower <code>threshold</code> if goal is too ambitious</p>"},{"location":"commands/#4-foreach-commands","title":"4. Foreach Commands","text":"<p>Iterate over a list with optional parallelism.</p> <pre><code>- foreach:\n    input: \"find . -name '*.rs' -type f\"  # Command\n    # OR\n    # input: [\"file1.rs\", \"file2.rs\"]    # List\n\n    parallel: 5  # Number of parallel executions (or true/false)\n\n    do:\n      - claude: \"/analyze-file ${item}\"\n      - shell: \"cargo check ${item}\"\n\n    continue_on_error: true\n    max_items: 50\n</code></pre> <p>Variables and Error Handling: - ${item}: Current item value available in loop body - continue_on_error: true (default): Failed items don't stop the loop - Parallel execution caveat: Output order is not guaranteed when using <code>parallel</code> - No built-in result aggregation: Use <code>write_file</code> commands to collect results if needed</p> <p>Example with result collection: <pre><code>- foreach:\n    input: [\"module1\", \"module2\", \"module3\"]\n    parallel: 3\n    do:\n      - shell: \"cargo test --package ${item}\"\n      - write_file:\n          path: \"results/${item}.txt\"\n          content: \"Test result: ${shell.output}\"\n          create_dirs: true\n</code></pre></p>"},{"location":"commands/#5-write-file-commands","title":"5. Write File Commands","text":"<p>Create or overwrite files with content from variables or literals. Supports text, JSON, and YAML formats with automatic validation and formatting.</p> <pre><code># Write plain text file\n- write_file:\n    path: \"output/result.txt\"\n    content: \"Build completed at ${shell.output}\"\n    format: text\n    mode: \"0644\"\n    create_dirs: true\n\n# Write JSON file with validation\n- write_file:\n    path: \"config/generated.json\"\n    content: |\n      {\n        \"version\": \"${version}\",\n        \"timestamp\": \"${timestamp}\",\n        \"items\": ${items_json}\n      }\n    format: json\n\n# Write YAML file with formatting\n- write_file:\n    path: \".prodigy/metadata.yml\"\n    content: |\n      workflow: ${workflow.name}\n      iteration: ${workflow.iteration}\n      results:\n        success: ${map.successful}\n        total: ${map.total}\n    format: yaml\n</code></pre> <p>WriteFileConfig Fields: - <code>path</code> - File path to write (supports variable interpolation) - <code>content</code> - Content to write (supports variable interpolation) - <code>format</code> - Output format: <code>text</code> (default), <code>json</code>, <code>yaml</code> - <code>mode</code> - File permissions in octal (default: \"0644\") - <code>create_dirs</code> - Create parent directories if they don't exist (default: false)</p> <p>Format Validation: - <code>json</code> - Validates JSON syntax and pretty-prints output - <code>yaml</code> - Validates YAML syntax and formats output - <code>text</code> - Writes content as-is without validation</p> <p>Best Practices: - Use format validation for config files: Set <code>format: json</code> or <code>format: yaml</code> when generating configuration files to catch syntax errors early - Set appropriate permissions: Use <code>mode</code> field to control file permissions (e.g., <code>\"0600\"</code> for sensitive files) - Handle nested paths: Set <code>create_dirs: true</code> when writing to paths that may not exist - Combine with validation: Use <code>validate</code> field to ensure generated files meet requirements before proceeding - For logs and documentation: Use <code>format: text</code> to write content as-is without validation overhead</p>"},{"location":"commands/#6-validation-commands","title":"6. Validation Commands","text":"<p>Validate implementation completeness with automatic retry.</p> <p>Warning: The <code>command</code> field in ValidationConfig is deprecated. Use <code>shell</code> instead for shell commands or <code>claude</code> for Claude commands. The <code>command</code> field is still supported for backward compatibility but will be removed in a future version.</p> <pre><code>- claude: \"/implement-auth-spec\"\n  validate:\n    shell: \"debtmap validate --spec auth.md --output result.json\"\n    result_file: \"result.json\"\n    threshold: 95  # Percentage completion required (default: 100.0)\n    timeout: 60\n    expected_schema: \"validation-schema.json\"  # Optional JSON schema\n\n    # What to do if incomplete\n    on_incomplete:\n      claude: \"/complete-implementation ${validation.gaps}\"\n      max_attempts: 3\n      fail_workflow: true\n      commit_required: true\n      prompt: \"Implementation incomplete. Continue?\"  # Optional interactive prompt\n</code></pre> <p>ValidationConfig Fields: - <code>shell</code> or <code>claude</code> - Single validation command (use <code>shell</code>, not deprecated <code>command</code>) - <code>commands</code> - Array of commands for multi-step validation - <code>result_file</code> - Path to JSON file with validation results - <code>threshold</code> - Minimum completion percentage (default: 100.0) - <code>timeout</code> - Timeout in seconds - <code>expected_schema</code> - JSON schema for validation output structure</p> <p>OnIncompleteConfig Fields: - <code>shell</code> or <code>claude</code> - Single gap-filling command - <code>commands</code> - Array of commands for multi-step gap filling - <code>max_attempts</code> - Maximum retry attempts (default: 2) - <code>fail_workflow</code> - Whether to fail workflow if validation incomplete (default: true) - <code>commit_required</code> - Whether to require commit after gap filling (default: false) - <code>prompt</code> - Optional interactive prompt for user guidance</p> <p>Alternative: Array format for multi-step validation</p> <pre><code>- claude: \"/implement-feature\"\n  validate:\n    # When using array format, ValidationConfig uses default threshold (100.0)\n    # and creates a commands array\n    - shell: \"run-tests.sh\"\n    - shell: \"check-coverage.sh\"\n    - claude: \"/validate-implementation --output validation.json\"\n      result_file: \"validation.json\"\n</code></pre> <p>Alternative: Multi-step gap filling</p> <pre><code>- claude: \"/implement-feature\"\n  validate:\n    shell: \"validate.sh\"\n    result_file: \"result.json\"\n    on_incomplete:\n      commands:\n        - claude: \"/analyze-gaps ${validation.gaps}\"\n        - shell: \"run-fix-script.sh\"\n        - claude: \"/verify-fixes\"\n      max_attempts: 2\n</code></pre>"},{"location":"commands/#command-reference","title":"Command Reference","text":""},{"location":"commands/#command-fields","title":"Command Fields","text":"<p>All command types support these common fields:</p> <p>Source: src/config/command.rs:320-401</p> Field Type Description <code>id</code> string Unique identifier for referencing outputs <code>timeout</code> number Command timeout in seconds <code>commit_required</code> boolean Whether command should create a git commit <code>when</code> string Conditional execution expression <code>capture_output</code> boolean/string Capture command output to a variable (boolean for default \"output\" var, string for custom name) <code>capture_format</code> enum Output parsing format: <code>string</code> (default), <code>number</code>, <code>json</code>, <code>lines</code>, <code>boolean</code> <code>on_success</code> object Command to run on success <code>on_failure</code> object Error handling configuration (see OnFailure Handler section below) <code>validate</code> object Validation configuration for implementation completeness <code>output_file</code> string Redirect command output to a file path <p>Note on Field Availability: All fields in this table are available in the user-facing YAML WorkflowStepCommand format (src/config/command.rs:320-401). Additional fields like <code>working_dir</code>, <code>env</code>, <code>on_exit_code</code>, and <code>capture</code> exist in the internal WorkflowStep representation (src/cook/workflow/executor/data_structures.rs:35-157) but are NOT exposed in YAML. See the Technical Notes section below for workarounds.</p>"},{"location":"commands/#onfailure-handler-configuration","title":"OnFailure Handler Configuration","text":"<p>The <code>on_failure</code> field accepts an <code>OnFailureConfig</code> which supports multiple configuration formats:</p> <p>Source: src/cook/workflow/on_failure.rs:67-115</p> <p>Simple Format - Single Command: <pre><code>- shell: \"cargo clippy\"\n  on_failure:\n    claude: \"/fix-warnings ${shell.output}\"\n</code></pre></p> <p>Advanced Format - With Retry Control: <pre><code>- shell: \"cargo test\"\n  on_failure:\n    claude: \"/debug-test-failures ${shell.output}\"\n    max_retries: 3              # Maximum retry attempts (default: 1)\n    fail_workflow: false        # Continue workflow even after failure handling\n    retry_original: true        # Retry original command after handler (default: false)\n</code></pre></p> <p>Detailed Format - With Handler Strategy:</p> <p>For fine-grained control over failure handling behavior, use the Detailed format with <code>FailureHandlerConfig</code> (src/cook/workflow/on_failure.rs:9-49):</p> <pre><code>- shell: \"cargo build\"\n  on_failure:\n    commands:\n      - claude: \"/analyze-build-errors ${shell.output}\"\n      - shell: \"cargo clean &amp;&amp; cargo build\"\n    strategy: recovery          # recovery, fallback, cleanup, or custom\n    timeout: 300                # Handler timeout in seconds\n    fail_workflow: false        # Whether to fail workflow after handling\n    handler_failure_fatal: true # Whether handler failure should be fatal\n</code></pre> <p>Handler Strategies (src/cook/workflow/on_failure.rs:9-22): - <code>recovery</code> (default) - Attempt to fix the problem and retry the original operation. Use this when failures are transient or can be resolved programmatically. - <code>fallback</code> - Use an alternative approach when the primary method fails. Best for scenarios with multiple valid execution paths. - <code>cleanup</code> - Perform resource cleanup before failing. Use for releasing locks, closing connections, or removing temporary files. - <code>custom</code> - Custom handler logic for specialized error recovery patterns not covered by other strategies.</p> <p>Real-World Examples:</p> <p>From <code>workflows/coverage.yml</code> (lines 13-17): <pre><code>- shell: \"just test\"\n  on_failure:\n    claude: \"/prodigy-debug-test-failure --spec ${coverage.spec} --output ${shell.output}\"\n    max_attempts: 3\n    fail_workflow: false\n</code></pre></p> <p>From <code>workflows/implement.yml</code> (lines 20-30): <pre><code>- shell: \"just test\"\n  on_failure:\n    claude: \"/prodigy-debug-test-failure --spec $ARG --output ${shell.output}\"\n    max_attempts: 5\n    fail_workflow: false\n</code></pre></p> <p>Nested Failure Handlers: <pre><code>- shell: \"cargo test\"\n  on_failure:\n    claude: \"/debug-test-failures ${shell.output}\"\n    on_failure:\n      shell: \"notify-team.sh 'Tests failed and debugging unsuccessful'\"\n</code></pre></p>"},{"location":"commands/#planned-feature-capturestreams","title":"Planned Feature: CaptureStreams","text":"<p>Note: The <code>capture_streams</code> field is defined in WorkflowStepCommand (src/config/command.rs:394-396) but is reserved for future use and not yet functional in YAML workflows.</p> <p>Why it's not available yet: The field exists as a <code>String</code> placeholder in the YAML parser, but the execution engine doesn't yet support fine-grained stream capture control. This feature is planned to provide selective capture of stdout, stderr, exit codes, success status, and execution duration.</p> <p>Current Approach: Use the <code>capture_output</code> and <code>capture_format</code> fields to control output capture, which cover most common use cases:</p> <pre><code># Capture stdout as string (most common use case)\n- shell: \"cargo test\"\n  capture_output: \"test_output\"\n  capture_format: \"string\"\n\n# Capture exit status as boolean\n- shell: \"cargo test\"\n  capture_output: \"test_passed\"\n  capture_format: \"boolean\"\n\n# Capture and parse JSON output\n- shell: \"cargo metadata --format-version 1\"\n  capture_output: \"project_info\"\n  capture_format: \"json\"\n</code></pre> <p>When will this be available? This feature requires execution engine changes to support the internal <code>CaptureStreams</code> type (src/cook/workflow/executor/data_structures.rs:83). Until then, use the <code>capture_output</code> and <code>capture_format</code> fields which cover most common use cases.</p>"},{"location":"commands/#capture-format-examples","title":"Capture Format Examples","text":"<p>The <code>capture_format</code> field controls how captured output is parsed:</p> <pre><code># String format (default) - raw text output\n- shell: \"git rev-parse HEAD\"\n  capture: \"commit_hash\"\n  capture_format: \"string\"\n\n# Number format - parses numeric output\n- shell: \"wc -l &lt; file.txt\"\n  capture: \"line_count\"\n  capture_format: \"number\"\n\n# JSON format - parses JSON output\n- shell: \"cargo metadata --format-version 1\"\n  capture: \"project_metadata\"\n  capture_format: \"json\"\n\n# Lines format - splits output into array of lines\n- shell: \"git diff --name-only\"\n  capture: \"changed_files\"\n  capture_format: \"lines\"\n\n# Boolean format - true if command succeeds, false otherwise\n- shell: \"grep -q 'pattern' file.txt\"\n  capture: \"pattern_found\"\n  capture_format: \"boolean\"\n</code></pre>"},{"location":"commands/#deprecated-fields","title":"Deprecated Fields","text":"<p>Warning: The following fields are deprecated but still supported for backward compatibility. They will be removed in a future version. Please migrate to the recommended alternatives.</p> <p>These fields are deprecated:</p> <ul> <li><code>test:</code> - Use <code>shell:</code> with <code>on_failure:</code> instead</li> <li><code>command:</code> in ValidationConfig - Use <code>shell:</code> instead</li> <li>Nested <code>commands:</code> in <code>agent_template</code> and <code>reduce</code> - Use direct array format instead</li> <li>Legacy variable aliases (<code>$ARG</code>, <code>$ARGUMENT</code>, <code>$FILE</code>, <code>$FILE_PATH</code>) - Use modern <code>${item.*}</code> syntax</li> </ul> <p>Using capture_output Field</p> <p>The <code>capture_output</code> field supports both Boolean and String variants for backward compatibility:</p> <p>Source: src/config/command.rs:366-368 (CaptureOutputConfig enum: src/config/command.rs:403-411)</p> <pre><code># String variant - captures to named variable (recommended)\n- shell: \"git rev-parse HEAD\"\n  capture_output: \"commit_hash\"\n\n# Boolean variant - captures to default \"output\" variable\n- shell: \"ls -la | wc -l\"\n  capture_output: true\n</code></pre> <p>Migration Guide - capture_output Variants:</p> <p>The <code>capture_output</code> field accepts both boolean and string values for backward compatibility:</p> Syntax Variable Name When to Use <code>capture_output: true</code> <code>${output}</code> Quick capture to default variable <code>capture_output: \"myvar\"</code> <code>${myvar}</code> Recommended: explicit variable name <p>Before and After Examples:</p> <pre><code># Boolean variant - captures to ${output}\n# BEFORE:\n- shell: \"git rev-parse HEAD\"\n  capture_output: true\n# Access with: ${output}\n\n# AFTER (recommended):\n- shell: \"git rev-parse HEAD\"\n  capture_output: \"commit_hash\"\n# Access with: ${commit_hash}\n\n# String variant - already correct\n- shell: \"ls -la | wc -l\"\n  capture_output: \"file_count\"\n# Access with: ${file_count}\n</code></pre> <p>Note: The internal WorkflowStep struct (src/cook/workflow/executor/data_structures.rs:75) uses a <code>capture</code> field internally, but this is NOT exposed in the user-facing YAML WorkflowStepCommand struct. Always use <code>capture_output</code> for capturing command output in YAML workflows.</p> <p>You can then reference captured values using <code>${commit_hash}</code> or <code>${output}</code> in subsequent commands.</p>"},{"location":"commands/#technical-notes","title":"Technical Notes","text":"Internal Implementation Fields (for contributors)  The following fields exist in the internal WorkflowStep struct (src/cook/workflow/executor/data_structures.rs:35-157) but are NOT available in the user-facing WorkflowStepCommand YAML syntax (src/config/command.rs:320-401). These are implementation details managed by Prodigy's execution engine during workflow execution:  **Execution Control Fields (Internal Only):** - `handler` - Internal HandlerStep for execution routing - `retry` - Internal RetryConfig for automatic retry logic - `auto_commit` - Internal commit tracking - `commit_config` - Internal commit configuration - `step_validate` - Internal validation state - `skip_validation` - Internal validation control - `validation_timeout` - Internal validation timing - `ignore_validation_failure` - Internal validation handling  **Environment and Context Fields (Planned/Not Yet Exposed in YAML):** - `working_dir` - Working directory control (WorkflowStep:99, NOT in WorkflowStepCommand) - `env` - Environment variable overrides (WorkflowStep:103, NOT in WorkflowStepCommand) - `on_exit_code` - Exit code specific handlers (WorkflowStep:119, NOT in WorkflowStepCommand) - `capture` - Variable name for output (WorkflowStep:75, use `capture_output` in YAML instead)  **Why the separation?** WorkflowStepCommand defines what users can write in YAML. WorkflowStep is the internal runtime representation with additional fields populated during execution. Some features exist in the execution layer but aren't yet exposed in the YAML syntax.  **Workarounds for missing YAML fields:**  Since `cwd`, `env`, and `on_exit_code` are NOT available in WorkflowStepCommand, use these shell-level alternatives:  <pre><code># Instead of cwd field (NOT available):\n# \u274c - shell: \"cargo test\"\n#      cwd: \"crates/prodigy-core\"\n\n# \u2713 Use shell cd command:\n- shell: \"cd crates/prodigy-core &amp;&amp; cargo test\"\n\n# Instead of env field (NOT available):\n# \u274c - shell: \"rustfmt --check src/**/*.rs\"\n#      env:\n#        PATH: \"/custom/bin:${PATH}\"\n\n# \u2713 Use shell environment syntax:\n- shell: \"PATH=/custom/bin:$PATH rustfmt --check src/**/*.rs\"\n\n# Instead of on_exit_code field (NOT available):\n# \u274c - shell: \"cargo build\"\n#      on_exit_code:\n#        1:\n#          claude: \"/fix-compile-errors\"\n\n# \u2713 Use on_failure with conditional logic:\n- shell: \"cargo build\"\n  on_failure:\n    claude: \"/fix-compile-errors ${shell.output}\"\n</code></pre>  These planned features may be exposed in future versions of Prodigy."},{"location":"commands/#cross-references","title":"Cross-References","text":"<p>For more information on related topics: - Variable Interpolation: See the Variables chapter for details on using captured outputs like <code>${variable_name}</code> in subsequent commands - Environment Variables: See the Environment Variables chapter for global env, secrets, and profiles - Error Handling: See the Error Handling chapter for advanced <code>on_failure</code> strategies and retry patterns - MapReduce Workflows: See the MapReduce chapter for large-scale parallel command execution with agent templates</p> <p>Example: Using Captured Output in Subsequent Commands</p> <pre><code># Capture build output and use it in later commands\n- shell: \"cargo build --release 2&gt;&amp;1\"\n  capture: \"build_output\"\n  capture_format: \"string\"\n\n# Use the captured output in Claude command\n- claude: \"/analyze-warnings '${build_output}'\"\n  when: \"${build_output contains 'warning'}\"\n\n# Store output to file for later analysis\n- write_file:\n    path: \"logs/build-${workflow.iteration}.log\"\n    content: \"${build_output}\"\n    create_dirs: true\n</code></pre>"},{"location":"examples/","title":"Examples","text":"<p>Last Verified: 2025-01-11 against codebase commit 753f90e0</p> <p>All examples in this chapter have been validated against the current implementation. Field names, syntax, and configuration options are verified against source code definitions.</p> <p>This chapter demonstrates practical Prodigy workflows with real-world examples. Examples progress from simple to advanced, covering standard workflows, MapReduce parallel processing, error handling, and advanced features.</p>"},{"location":"examples/#quick-reference","title":"Quick Reference","text":"<p>Find the right example for your use case:</p> Use Case Example Key Features Simple build/test pipeline Example 1 Basic commands, error handling Iterative optimization Example 2 Goal seeking, validation feedback Loop over configurations Example 3 Foreach iteration, parallel processing Parallel code processing Example 4, 8 MapReduce, distributed work Conditional logic Example 5 Capture output, when clauses Multi-step validation Example 6 Validation with gap filling Environment configuration Example 7 Env vars, secrets, profiles Dead Letter Queue (DLQ) Example 8 Error handling, retry failed items Generate config files Example 9 write_file with JSON/YAML/text Advanced git tracking Example 10 Git context variables, working_dir External service resilience Example 11 Circuit breakers, fail fast Retry with backoff Example 12 Exponential/linear/custom backoff Reusable workflows Example 13 Composition (preview feature) Custom merge process Example 14 Merge workflows, pre-merge validation"},{"location":"examples/#example-1-simple-build-and-test","title":"Example 1: Simple Build and Test","text":"<p>Basic Workflow</p> <p>This example shows a simple linear workflow with error handling. The <code>on_failure</code> handler automatically invokes Claude to fix failing tests.</p> <pre><code>- shell: \"cargo build\"\n- shell: \"cargo test\"\n  on_failure:\n    claude: \"/fix-failing-tests\"\n- shell: \"cargo clippy\"\n</code></pre>"},{"location":"examples/#example-2-coverage-improvement-with-goal-seeking","title":"Example 2: Coverage Improvement with Goal Seeking","text":"<p>When to Use Goal Seeking</p> <p>Use <code>goal_seek</code> when you need iterative improvement toward a measurable goal (e.g., test coverage, performance metrics). For one-time validation with gap filling, use <code>validate</code> instead (see Example 6).</p> <pre><code>- goal_seek:\n    goal: \"Achieve 80% test coverage\"\n    claude: \"/improve-coverage\"  # Can also use 'shell' for shell commands\n    validate: |\n      coverage=$(cargo tarpaulin | grep 'Coverage' | sed 's/.*: \\([0-9.]*\\)%.*/\\1/')\n      echo \"score: ${coverage%.*}\"\n    threshold: 80\n    max_attempts: 5\n    timeout_seconds: 1800  # Optional: 30 minute timeout for entire goal-seeking process\n    fail_on_incomplete: false  # Optional: Continue workflow even if goal not reached\n</code></pre> <p>Source: GoalSeekConfig from src/cook/goal_seek/mod.rs:14-41, execution engine from src/cook/goal_seek/engine.rs:23-116</p> <p>How Goal Seeking Works:</p> <p>Goal seeking provides iterative refinement with automatic convergence detection: 1. Execute the improvement command (Claude or shell) 2. Run validation script to get a score (0-100) 3. Pass environment variables to next iteration:    - <code>PRODIGY_VALIDATION_SCORE</code> - Current score    - <code>PRODIGY_VALIDATION_OUTPUT</code> - Full validation output    - <code>PRODIGY_VALIDATION_GAPS</code> - Identified improvement areas 4. Repeat until threshold reached, max attempts hit, or convergence detected 5. Auto-stops when no improvement in last 3 attempts (convergence)</p> <pre><code>flowchart TD\n    Start[Start Goal Seek] --&gt; Execute[Execute Improvement&lt;br/&gt;Claude or Shell Command]\n    Execute --&gt; Validate[Run Validation Script]\n    Validate --&gt; Score{Score &gt;= Threshold?}\n\n    Score --&gt;|Yes| Success[Complete - Goal Achieved]\n    Score --&gt;|No| CheckAttempts{Max Attempts&lt;br/&gt;Reached?}\n\n    CheckAttempts --&gt;|Yes| Incomplete[Complete - Goal Not Reached]\n    CheckAttempts --&gt;|No| CheckConvergence{No Improvement&lt;br/&gt;in 3 Attempts?}\n\n    CheckConvergence --&gt;|Yes| Converged[Complete - Converged]\n    CheckConvergence --&gt;|No| SetEnv[Set Environment Vars&lt;br/&gt;PRODIGY_VALIDATION_SCORE&lt;br/&gt;PRODIGY_VALIDATION_OUTPUT&lt;br/&gt;PRODIGY_VALIDATION_GAPS]\n\n    SetEnv --&gt; Execute\n\n    style Success fill:#e8f5e9\n    style Incomplete fill:#fff3e0\n    style Converged fill:#e1f5ff</code></pre> <p>Figure: Goal-seeking execution flow showing iterative refinement with convergence detection.</p> <p>Validation Script Format: <pre><code># Must output \"score: N\" where N is 0-100\necho \"score: 75\"\n# Optional: output \"gaps: description\" for targeted improvements\necho \"gaps: Missing tests for auth module\"\n</code></pre></p> <p>Goal-Seek vs Validate: - <code>goal_seek</code>: Iterative optimization with feedback loop - <code>validate</code>: One-time completion check (see Example 6)</p> <p>Note: Changes are automatically committed during goal-seeking iterations. Use <code>commit_required: true</code> on the outer goal_seek step to control commit behavior.</p>"},{"location":"examples/#example-3-foreach-iteration","title":"Example 3: Foreach Iteration","text":"<pre><code># Test multiple configurations in sequence\n- foreach:\n    - rust-version: \"1.70\"\n      profile: debug\n    - rust-version: \"1.71\"\n      profile: release\n    - rust-version: \"stable\"\n      profile: release\n  do:\n    - shell: \"rustup install ${foreach.item.rust-version}\"\n    - shell: \"cargo +${foreach.item.rust-version} build --profile ${foreach.item.profile}\"\n    - shell: \"cargo +${foreach.item.rust-version} test\"\n\n# Parallel foreach with error handling\n- foreach:\n    - \"web-service\"\n    - \"api-gateway\"\n    - \"worker-service\"\n  parallel: 3  # Options: false (sequential), true (default parallelism), or number (specific count)\n  continue_on_error: true\n  do:\n    - shell: \"cd services/${foreach.item} &amp;&amp; cargo build\"\n    - shell: \"cd services/${foreach.item} &amp;&amp; cargo test\"\n      on_failure:\n        claude: \"/fix-service-tests ${foreach.item}\"\n</code></pre>"},{"location":"examples/#example-4-parallel-code-review","title":"Example 4: Parallel Code Review","text":"<p>MapReduce Pattern</p> <p>This example demonstrates parallel processing using the MapReduce pattern: setup generates work items, map processes them in parallel agents, and reduce aggregates results.</p> <pre><code># Source: Based on examples/mapreduce-json-input.yml and examples/mapreduce-command-input.yml\nname: parallel-code-review\nmode: mapreduce\n\nsetup:\n  - shell: \"find src -name '*.rs' &gt; files.txt\"\n  - shell: \"jq -R -s -c 'split(\\\"\\n\\\") | map(select(length &gt; 0) | {path: .})' files.txt &gt; items.json\"\n\nmap:\n  input: items.json            # (1)!\n  json_path: \"$[*]\"            # (2)!\n  agent_template:\n    - claude: \"/review-file ${item.path}\"\n      id: \"review\"\n      capture_output: \"review_result\"\n      capture_format: \"json\"\n    - shell: \"cargo check ${item.path}\"\n  max_parallel: 5              # (3)!\n\nreduce:\n  - claude: \"/summarize-reviews ${map.results}\"\n\n1. JSON file produced by setup phase containing work items\n2. JSONPath expression to extract items from the JSON structure\n3. Number of concurrent agents processing work items in parallel\n</code></pre> <pre><code>graph TD\n    Start[Start Workflow] --&gt; Setup[Setup Phase&lt;br/&gt;Generate items.json]\n    Setup --&gt; Map[Map Phase&lt;br/&gt;Parallel Processing]\n\n    Map --&gt; A1[Agent 1&lt;br/&gt;Review file 1]\n    Map --&gt; A2[Agent 2&lt;br/&gt;Review file 2]\n    Map --&gt; A3[Agent 3&lt;br/&gt;Review file 3]\n    Map --&gt; A4[Agent 4&lt;br/&gt;Review file 4]\n    Map --&gt; A5[Agent 5&lt;br/&gt;Review file 5]\n\n    A1 --&gt; Merge[Aggregate Results]\n    A2 --&gt; Merge\n    A3 --&gt; Merge\n    A4 --&gt; Merge\n    A5 --&gt; Merge\n\n    Merge --&gt; Reduce[Reduce Phase&lt;br/&gt;Summarize Reviews]\n    Reduce --&gt; End[Complete]\n\n    style Setup fill:#e1f5ff\n    style Map fill:#fff3e0\n    style Reduce fill:#f3e5f5\n    style A1 fill:#e8f5e9\n    style A2 fill:#e8f5e9\n    style A3 fill:#e8f5e9\n    style A4 fill:#e8f5e9\n    style A5 fill:#e8f5e9</code></pre> <p>Figure: MapReduce execution flow showing setup, parallel map agents, and reduce aggregation.</p> <p>Note: JSONPath <code>\"$[*]\"</code> matches all items in the root array. Since the setup phase creates an array of <code>{path: ...}</code> objects, each map agent receives an <code>item</code> object with <code>item.path</code> available for use in commands.</p> <p>Advanced JSONPath Patterns: - <code>$.items[*]</code> - Extract items from nested object - <code>$.items[*].files[*]</code> - Extract from nested arrays (flattens results) - <code>$.items[?(@.priority &gt; 5)]</code> - Filter items by condition - <code>$[?(@.severity == 'critical')]</code> - Filter array by field value</p>"},{"location":"examples/#example-5-conditional-deployment","title":"Example 5: Conditional Deployment","text":"<pre><code>- shell: \"cargo test --quiet &amp;&amp; echo true || echo false\"\n  id: \"test\"\n  capture_output: \"test_result\"  # Canonical field name (alias: 'capture')\n  capture_format: \"boolean\"  # Supported formats explained below\n  timeout: 300  # Timeout in seconds (5 minutes)\n\n- shell: \"cargo build --release\"\n  when: \"${test_result} == true\"\n\n- shell: \"docker build -t myapp .\"\n  when: \"${test_result} == true\"\n  on_success:\n    shell: \"docker push myapp:latest\"\n</code></pre> <p>Note: <code>capture_format</code> options: - <code>string</code> - Raw text output (default) - <code>json</code> - Parse output as JSON object - <code>lines</code> - Split output into array of lines - <code>number</code> - Parse output as numeric value - <code>boolean</code> - Parse as true/false based on exit code or output text</p> <p>Advanced capture options: <pre><code># Capture specific streams (stdout, stderr, exit_code, success, duration)\n- shell: \"cargo build 2&gt;&amp;1\"\n  capture_output: \"build_output\"\n  capture_streams: \"stdout,stderr,exit_code\"  # Capture multiple streams\n\n# Access captured values\n- shell: \"echo 'Exit code was ${build_output.exit_code}'\"\n</code></pre></p>"},{"location":"examples/#example-6-multi-step-validation","title":"Example 6: Multi-Step Validation","text":"<p>Validation vs Goal Seeking</p> <p>Use <code>validate</code> for one-time completion checks with targeted gap filling. Unlike <code>goal_seek</code> which iteratively improves a metric, <code>validate</code> checks if work meets criteria and fixes specific gaps if not.</p> <pre><code># Source: Validation pattern from src/cook/goal_seek/mod.rs and features.json\n- claude: \"/implement-feature auth\"\n  commit_required: true\n  validate:\n    commands:\n      - shell: \"cargo test auth\"\n      - shell: \"cargo clippy -- -D warnings\"\n      - claude: \"/validate-implementation --output validation.json\"\n    result_file: \"validation.json\"\n    threshold: 90\n    on_incomplete:\n      claude: \"/complete-gaps ${validation.gaps}\"\n      commit_required: true\n      max_attempts: 2\n</code></pre> <p>Validation Lifecycle Explanation:</p> <p>The validation system follows this flow: 1. Execute validation commands - Run tests, linting, and custom validation scripts 2. Parse result file - Read <code>validation.json</code> to extract score and gaps 3. Check threshold - Compare score against threshold (90 in this example) 4. Populate <code>validation.gaps</code> - If score &lt; threshold, extract gaps from result file 5. Execute <code>on_incomplete</code> - Pass gaps to Claude for targeted fixes</p> <p>Result File Format:</p> <p>The validation result file (<code>validation.json</code>) should contain: <pre><code>{\n  \"score\": 75,\n  \"gaps\": [\n    \"Missing tests for login endpoint\",\n    \"No error handling for invalid tokens\",\n    \"Documentation incomplete for auth module\"\n  ]\n}\n</code></pre></p> <p>The <code>${validation.gaps}</code> variable is populated from the <code>gaps</code> array in the result file. If the result file doesn't contain a <code>gaps</code> field, validation will fail with an error.</p> <p>Alternative: Shell Script Validation</p> <p>You can also use shell scripts that output structured data: <pre><code>validate:\n  commands:\n    - shell: |\n        # Run tests and extract missing coverage\n        cargo tarpaulin --output-format json &gt; coverage.json\n        # Parse coverage and create validation result\n        jq '{score: .coverage, gaps: .uncovered_files}' coverage.json &gt; validation.json\n  result_file: \"validation.json\"\n  threshold: 80\n</code></pre></p> <p>Note: Validation is a one-time completion check, distinct from <code>goal_seek</code> which iteratively improves until a threshold is met. Use validation when you want to verify completeness and have Claude fill specific gaps.</p>"},{"location":"examples/#example-7-environment-aware-workflow","title":"Example 7: Environment-Aware Workflow","text":"<pre><code># Global environment variables (including secrets with masking)\nenv:\n  # Regular variables\n  NODE_ENV: production\n  API_URL: https://api.production.com\n\n  # Secrets (automatically masked in logs)\n  # Use secret: true and value fields for sensitive data\n  API_KEY:\n    secret: true\n    value: \"${SECRET_API_KEY}\"\n\n  # Secret with external provider\n  DB_PASSWORD:\n    secret: true\n    value: \"${DB_PASSWORD}\"\n    # provider: \"vault\"  # Optional: external secret store (not yet implemented)\n\n# Environment profiles for different contexts\nprofiles:\n  production:\n    env:\n      API_URL: https://api.production.com\n      LOG_LEVEL: error\n    description: \"Production environment with error-level logging\"\n\n  staging:\n    env:\n      API_URL: https://api.staging.com\n      LOG_LEVEL: warn\n    description: \"Staging environment with warning-level logging\"\n\n# Load additional variables from .env files\n# Note: Paths are relative to workflow file location\nenv_files:\n  - .env\n  - .env.production\n\n# Workflow steps\n- shell: \"cargo build --release\"\n\n# Use environment variables in commands\n- shell: \"echo 'Deploying to ${NODE_ENV} at ${API_URL}'\"\n\n# Override environment for specific step using env field\n- shell: \"./deploy.sh\"\n  env:\n    LOG_LEVEL: debug\n</code></pre> <p>Source: Environment configuration from src/cook/environment/config.rs:12-36, secret masking from src/cook/environment/config.rs:84-96</p> <p>Note: Profiles are activated using the <code>--profile &lt;name&gt;</code> CLI flag when running workflows. For example: <pre><code># Use production profile\nprodigy run workflow.yml --profile production\n\n# Use staging profile\nprodigy run workflow.yml --profile staging\n</code></pre></p> <p>Secrets Masking: Variables with <code>secret: true</code> are automatically masked in: - Command output logs - Error messages - Event logs - Checkpoint files</p> <p>Example masked output: <pre><code>$ echo 'API key is ***'\n</code></pre></p> <p>Alternative Secrets Syntax (Legacy):</p> <p>Both modern and legacy secret syntaxes are supported:</p> <pre><code># Modern approach (recommended)\nenv:\n  API_KEY:\n    secret: true\n    value: \"${SECRET_KEY}\"\n\n# Legacy approach (still supported)\nsecrets:\n  API_KEY:\n    provider: env\n    key: \"SECRET_KEY\"\n</code></pre> <p>The modern <code>env</code>-based approach is recommended for consistency, but legacy workflows using the top-level <code>secrets:</code> field continue to work.</p> <p>Source: Environment configuration from src/cook/environment/config.rs:12-36, secret support from src/cook/environment/config.rs:84-96, example workflow from workflows/mapreduce-env-example.yml:7-25, profile structure from tests/environment_workflow_test.rs:68-88</p>"},{"location":"examples/#example-8-complex-mapreduce-with-error-handling","title":"Example 8: Complex MapReduce with Error Handling","text":"<p>Resource Management</p> <p>Setting <code>max_parallel</code> too high can exhaust system resources (CPU, memory, file handles). Start with 5-10 concurrent agents and monitor resource usage before increasing.</p> <pre><code># Source: Combines patterns from src/config/mapreduce.rs and workflows/debtmap-reduce.yml\nname: tech-debt-elimination\nmode: mapreduce\n\nsetup:\n  - shell: \"debtmap analyze . --output debt.json\"\n\nmap:\n  input: debt.json                          # (1)!\n  json_path: \"$.items[*]\"                   # (2)!\n  filter: \"item.severity == 'critical'\"     # (3)!\n  sort_by: \"item.priority DESC\"             # (4)!\n  max_items: 20                             # (5)!\n  max_parallel: 5                           # (6)!\n  distinct: \"item.id\"                       # (7)!\n\n  # Timeout configuration (optional - default is 600 seconds / 10 minutes)\n  timeout_config:\n    agent_timeout_secs: 600                 # (8)!\n    cleanup_grace_period_secs: 30           # (9)!\n\n  agent_template:\n    - claude: \"/fix-debt-item '${item.description}'\"\n      commit_required: true\n    - shell: \"cargo test\"\n      on_failure:\n        claude: \"/debug-and-fix\"\n\nreduce:\n  - shell: \"debtmap analyze . --output debt-after.json\"\n  - claude: \"/compare-debt-reports --before debt.json --after debt-after.json\"\n\nerror_policy:\n  on_item_failure: dlq                      # (10)!\n  continue_on_failure: true                 # (11)!\n  max_failures: 5                           # (12)!\n  failure_threshold: 0.3                    # (13)!\n  error_collection: aggregate               # (14)!\n\n1. JSON file containing work items from setup phase\n2. JSONPath to extract work items from the JSON structure\n3. Only process items matching this condition (severity == 'critical')\n4. Process high-priority items first\n5. Limit to first 20 items (after filtering and sorting)\n6. Run up to 5 agents concurrently\n7. Prevent duplicate processing based on item.id field\n8. Maximum time per agent (10 minutes default)\n9. Extra time allowed for cleanup operations after timeout\n10. Send failed items to Dead Letter Queue for retry\n11. Continue processing remaining items when one fails\n12. Stop workflow after 5 total failures\n13. Stop workflow if &gt;30% of items fail\n14. Aggregate errors and report at end\n</code></pre> <p>on_failure Syntax Note: The <code>on_failure</code> field accepts a single command (either <code>shell:</code> or <code>claude:</code>), not an array of commands. The command structure is: <pre><code>on_failure:\n  claude: \"/fix-command\"\n  # or\n  shell: \"echo 'Fixing...'\"\n  max_attempts: 3       # Optional: retry the on_failure handler\n  commit_required: true # Optional: require commit after fixing\n</code></pre></p> <p>For multiple failure recovery steps, nest handlers: <pre><code># Source: Pattern from workflows/implement-with-tests.yml:36-39\n- shell: \"cargo test\"\n  on_failure:\n    claude: \"/fix-tests\"\n    commit_required: true\n    on_failure:\n      # Nested handler for second-level failure\n      claude: \"/fix-tests --deep-analysis\"\n      commit_required: true\n</code></pre></p> <p>Note: The entire <code>error_policy</code> block is optional with sensible defaults. If not specified, failed items go to the Dead Letter Queue (<code>on_item_failure: dlq</code>), workflow continues despite failures (<code>continue_on_failure: true</code>), and errors are aggregated at the end (<code>error_collection: aggregate</code>). Use <code>max_failures</code> or <code>failure_threshold</code> to fail fast if too many items fail.</p> <p>Deduplication with <code>distinct</code>: The <code>distinct</code> field enables idempotent processing by preventing duplicate work items. When specified, Prodigy extracts the value of the given field (e.g., <code>item.id</code>) from each work item and ensures only unique values are processed. This is useful when: - Input data may contain duplicates - Resuming a workflow after adding new items - Preventing wasted work on identical items - Ensuring exactly-once processing semantics</p> <p>Example: With <code>distinct: \"item.id\"</code>, if your input contains <code>[{id: \"1\", ...}, {id: \"2\", ...}, {id: \"1\", ...}]</code>, only items with IDs \"1\" and \"2\" will be processed (the duplicate \"1\" is skipped).</p> <p>Resuming MapReduce Workflows: MapReduce jobs can be resumed using either the session ID or job ID: <pre><code># Resume using session ID\nprodigy resume session-mapreduce-1234567890\n\n# Resume using job ID\nprodigy resume-job mapreduce-1234567890\n\n# Unified resume command (auto-detects ID type)\nprodigy resume mapreduce-1234567890\n</code></pre></p> <p>Session-Job ID Mapping: The bidirectional mapping between session IDs and job IDs is stored in <code>~/.prodigy/state/{repo_name}/mappings/</code> and created automatically when the MapReduce workflow starts. This allows you to resume using either the session ID (e.g., <code>session-mapreduce-1234567890</code>) or the job ID (e.g., <code>mapreduce-1234567890</code>), and Prodigy will automatically find the correct checkpoint data.</p> <p>Source: Session-job mapping implementation from MapReduce checkpoint and resume (Spec 134)</p> <p>Dead Letter Queue (DLQ) for Failed Items</p> <p>Failed work items are automatically sent to the DLQ for retry. Use <code>prodigy dlq retry &lt;job_id&gt;</code> to reprocess failed items with the same agent configuration, or <code>prodigy dlq show &lt;job_id&gt;</code> to inspect failure details.</p> <p>Debugging Failed Agents: When agents fail, DLQ entries include a <code>json_log_location</code> field pointing to the Claude JSON log file for debugging: <pre><code># View failed items and their log locations\nprodigy dlq show &lt;job_id&gt; | jq '.items[].failure_history[].json_log_location'\n\n# Inspect the Claude interaction for a failed agent\ncat &lt;json_log_location&gt; | jq\n</code></pre> This allows you to see exactly what tools Claude invoked and why the agent failed.</p>"},{"location":"examples/#example-9-generating-configuration-files","title":"Example 9: Generating Configuration Files","text":"<pre><code># Generate a JSON configuration file\n- write_file:\n    path: \"config/deployment.json\"\n    format: json  # Options: text, json, yaml\n    create_dirs: true  # Create parent directories if they don't exist\n    content:\n      environment: production\n      api_url: \"${API_URL}\"\n      features:\n        - auth\n        - analytics\n        - notifications\n      timeout: 30\n\n# Generate a YAML configuration file\n- write_file:\n    path: \"config/services.yml\"\n    format: yaml\n    content:\n      services:\n        web:\n          image: \"myapp:latest\"\n          ports:\n            - \"8080:8080\"\n        database:\n          image: \"postgres:15\"\n          environment:\n            POSTGRES_DB: \"${DB_NAME}\"\n\n# Generate a plain text report\n- write_file:\n    path: \"reports/summary.txt\"\n    format: text\n    mode: \"0644\"  # File permissions (optional)\n    content: |\n      Deployment Summary\n      ==================\n      Environment: ${NODE_ENV}\n      API URL: ${API_URL}\n      Timestamp: $(date)\n</code></pre>"},{"location":"examples/#example-10-advanced-features","title":"Example 10: Advanced Features","text":"<pre><code># Nested error handling with retry configuration\n- shell: \"cargo build --release\"\n  on_failure:\n    shell: \"cargo clean\"\n    on_success:\n      shell: \"cargo build --release\"\n      max_attempts: 2\n  on_success:\n    shell: \"cargo test --release\"\n\n# Complex conditional execution with max_attempts\n- shell: \"cargo test\"\n  id: \"test\"\n  capture_output: \"test_output\"\n\n- claude: \"/fix-tests\"\n  when: \"${test_output} contains 'FAILED'\"\n  max_attempts: 3\n\n# Conditional deployment based on test results\n- shell: \"cargo build --release\"\n  when: \"${test.exit_code} == 0\"\n\n# Multi-condition logic\n- shell: \"./deploy.sh\"\n  when: \"${test_output} contains 'passed' and ${build_output} contains 'Finished'\"\n</code></pre> <p>Note: Advanced features currently supported: - Nested handlers: Chain <code>on_failure</code> and <code>on_success</code> handlers for complex error recovery - Max attempts: Combine with conditional execution for automatic retry logic - Conditional execution: Use <code>when</code> clauses with captured output or variables - Complex conditionals: Combine multiple conditions with <code>and</code>/<code>or</code> operators - Working directory: Per-command directory control using <code>working_dir</code> field - Git context variables: Automatic tracking of file changes during workflow execution</p> <p>Example of working_dir usage:</p> <p>Source: Field definition from src/commands/handlers/shell.rs:40, examples from workflows/environment-example.yml:52-64</p> <pre><code># Run command in specific directory\n- name: \"Build frontend\"\n  shell: \"npm run build\"\n  working_dir: ./frontend      # Execute in frontend/ directory\n\n# Combine with environment variables\n- name: \"Run backend tests\"\n  shell: \"pytest\"\n  env:\n    PYTHONPATH: \"./src:./tests\"\n  working_dir: ./backend\n\n# Use variable interpolation for dynamic paths\n- name: \"Deploy to environment\"\n  shell: \"echo 'Deploying...'\"\n  working_dir: \"${env.DEPLOY_DIR}\"  # Path from environment variable\n</code></pre> <p>Note: The <code>working_dir</code> field is fully implemented and production-ready: - Accepts relative or absolute paths - Supports variable interpolation (e.g., <code>\"${env.PROJECT_DIR}\"</code>) - Falls back to current execution context if not specified - Paths are resolved to absolute paths automatically - Relative paths are resolved via the workflow execution context</p> <p>Git Context Variables (Spec 122): Prodigy automatically tracks file changes during workflow execution and exposes them as variables:</p> <pre><code># Access files changed in current step\n- shell: \"echo 'Modified files: ${step.files_modified}'\"\n- shell: \"echo 'Added files: ${step.files_added}'\"\n- shell: \"echo 'Deleted files: ${step.files_deleted}'\"\n\n# Format as JSON array\n- shell: \"echo '${step.files_modified:json}'\"\n\n# Filter by glob pattern (only Rust files)\n- shell: \"echo 'Rust files changed: ${step.files_modified:*.rs}'\"\n\n# Access workflow-level aggregations\n- shell: \"echo 'Total commits: ${workflow.commit_count}'\"\n- shell: \"echo 'All modified files: ${workflow.files_modified}'\"\n\n# Access uncommitted changes\n- shell: \"echo 'Staged files: ${git.staged_files}'\"\n- shell: \"echo 'Unstaged files: ${git.unstaged_files}'\"\n\n# Pattern filtering for git context\n- claude: \"/review-changes ${git.modified_files|pattern:**/*.rs}\"\n- shell: \"echo 'Changed Rust files: ${git.staged_files|pattern:**/*.rs}'\"\n\n# Basename-only output (no paths)\n- shell: \"echo 'File names: ${git.modified_files|basename}'\"\n</code></pre> <p>Available Git Context Variables: - <code>${step.files_added}</code> - Files added in current step - <code>${step.files_modified}</code> - Files modified in current step - <code>${step.files_deleted}</code> - Files deleted in current step - <code>${step.commits}</code> - Commit SHAs created in this step - <code>${step.insertions}</code> - Lines inserted in this step - <code>${step.deletions}</code> - Lines deleted in this step - <code>${workflow.commit_count}</code> - Total commits in workflow - <code>${workflow.files_modified}</code> - All files modified across workflow - <code>${git.staged_files}</code> - Currently staged files (uncommitted) - <code>${git.unstaged_files}</code> - Modified but unstaged files - <code>${git.modified_files}</code> - All uncommitted modifications</p> <p>Supported Formats and Filters: - <code>:json</code> - Format as JSON array - <code>:*.rs</code> - Filter by glob pattern (e.g., <code>*.rs</code>, <code>src/**/*.py</code>) - <code>|pattern:**/*.rs</code> - Alternative syntax for glob pattern filtering (equivalent to <code>:pattern</code>) - <code>|basename</code> - Extract just file names without paths</p> <p>Note: Both <code>:pattern</code> and <code>|pattern:</code> syntaxes are valid and equivalent. Use whichever is more readable in your context: - <code>${git.modified_files:*.rs}</code> - Colon syntax (more concise) - <code>${git.modified_files|pattern:**/*.rs}</code> - Pipe syntax (more explicit)</p> <p>Source: Git context tracking from src/cook/workflow/git_context.rs:1-120, variable resolution from src/cook/workflow/git_context.rs:36-42</p> <p>Troubleshooting MapReduce Cleanup: If agent worktree cleanup fails (due to disk full, permission errors, etc.), use the orphaned worktree cleanup command: <pre><code># List and clean orphaned worktrees for a specific job\nprodigy worktree clean-orphaned &lt;job_id&gt;\n\n# Dry run to preview what would be cleaned\nprodigy worktree clean-orphaned &lt;job_id&gt; --dry-run\n\n# Force cleanup without confirmation\nprodigy worktree clean-orphaned &lt;job_id&gt; --force\n</code></pre> Note: Agent execution status is independent of cleanup status. If an agent completes successfully but cleanup fails, the agent is still marked as successful and results are preserved.</p> <p>Source: Orphaned worktree cleanup from MapReduce cleanup failure handling (Spec 136)</p>"},{"location":"examples/#example-11-circuit-breaker-for-resilient-error-handling","title":"Example 11: Circuit Breaker for Resilient Error Handling","text":"<pre><code>name: api-processing-with-circuit-breaker\nmode: mapreduce\n\nsetup:\n  - shell: \"curl https://api.example.com/items &gt; items.json\"\n\nmap:\n  input: items.json\n  json_path: \"$[*]\"\n  max_parallel: 10\n\n  agent_template:\n    - shell: \"curl -X POST https://api.example.com/process -d '${item}'\"\n      max_attempts: 3\n    - claude: \"/validate-processing ${item.id}\"\n\n  # Circuit breaker prevents cascading failures from external service issues\n  error_policy:\n    on_item_failure: dlq\n    continue_on_failure: true\n    circuit_breaker:\n      failure_threshold: 5      # Open circuit after 5 consecutive failures\n      success_threshold: 3      # Close circuit after 3 consecutive successes\n      timeout: 30s             # Wait 30s before testing recovery (half-open state)\n      half_open_requests: 3    # Allow 3 test requests in half-open state\n\nreduce:\n  - claude: \"/summarize-results ${map.results}\"\n</code></pre> <p>Source: CircuitBreakerConfig from src/cook/workflow/error_policy.rs:46-88, state machine from error_policy.rs:251-343</p> <p>Circuit Breaker State Transitions:</p> <pre><code>stateDiagram-v2\n    [*] --&gt; Closed: Initial State\n    Closed --&gt; Open: failure_threshold&lt;br/&gt;consecutive failures\n    Open --&gt; HalfOpen: timeout expires\n    HalfOpen --&gt; Closed: success_threshold&lt;br/&gt;successes\n    HalfOpen --&gt; Open: any failure in&lt;br/&gt;half_open_requests\n\n    note right of Closed\n        Requests flow normally\n        Track failure count\n    end note\n\n    note right of Open\n        Reject all requests\n        Wait for timeout\n    end note\n\n    note right of HalfOpen\n        Allow limited test requests\n        Evaluate recovery\n    end note</code></pre> <p>Figure: Circuit breaker state machine showing transitions based on failure and success thresholds.</p> <p>When to Use Circuit Breakers: - External API calls that may become unavailable - Database operations during maintenance windows - Network-dependent operations with potential outages - Any operation where cascading failures should be prevented</p> <p>Circuit Breaker Benefits: - Fail Fast: Stop wasting resources on doomed requests - Self-Healing: Automatically test recovery after timeout - Prevent Overload: Give external services time to recover - Clear Signals: Circuit state indicates system health</p> <p>Default Values (from error_policy.rs:63-76): - <code>failure_threshold: 5</code> - Balance between sensitivity and stability - <code>success_threshold: 3</code> - Require consistent success before full recovery - <code>timeout: 30s</code> - Typical recovery time for transient issues - <code>half_open_requests: 3</code> - Minimal testing before full recovery</p> <p>See Also: Error Handling Guide for comprehensive error handling patterns</p>"},{"location":"examples/#example-12-retry-configuration-with-backoff-strategies","title":"Example 12: Retry Configuration with Backoff Strategies","text":"<pre><code>name: resilient-deployment\nmode: standard\n\n# Global retry defaults for all commands\nerror_policy:\n  retry_config:\n    max_attempts: 3          # Maximum retry attempts\n    backoff: exponential      # Backoff strategy (see variants below)\n\n# Workflow steps\n- shell: \"curl https://api.example.com/deploy\"\n  # Inherits global retry_config\n\n- shell: \"docker push myapp:latest\"\n  # Override with custom retry config\n  retry_config:\n    max_attempts: 5\n    backoff:\n      exponential:\n        initial: 2s          # Start with 2 second delay\n        multiplier: 2.0      # Double delay each retry (2s, 4s, 8s, 16s, 32s)\n</code></pre> <p>Source: BackoffStrategy enum from src/cook/retry_v2.rs:70-98, RetryConfig from retry_v2.rs:14-52</p> <p>Backoff Strategy Variants:</p> Exponential (Default)LinearFibonacciFixedCustom <p>Delay doubles each retry - best for most scenarios: <pre><code>backoff:\n  exponential:\n    initial: 1s         # First retry after 1s\n    multiplier: 2.0     # Second after 2s, third after 4s, fourth after 8s\n</code></pre> Use when: Fast backoff needed, transient errors expected</p> <p>Delay increases by fixed increment - steady load reduction: <pre><code>backoff:\n  linear:\n    initial: 1s         # First retry after 1s\n    increment: 2s       # Second after 3s, third after 5s, fourth after 7s\n</code></pre> Use when: Gradual backoff preferred, predictable retry timing</p> <p>Delay follows Fibonacci sequence - gradual then aggressive: <pre><code>backoff:\n  fibonacci:\n    initial: 1s         # Delays: 1s, 1s, 2s, 3s, 5s, 8s, 13s...\n</code></pre> Use when: Balance between exponential and linear</p> <p>Same delay for all retries - consistent timing: <pre><code>backoff:\n  fixed:\n    delay: 5s           # All retries wait exactly 5s\n</code></pre> Use when: Rate limiting, polling operations</p> <p>Specify exact delays - full control: <pre><code>backoff:\n  custom:\n    delays: [1s, 5s, 15s, 30s, 60s]\n</code></pre> Use when: Complex SLA requirements, specific retry patterns</p> <p>Advanced Retry Configuration:</p> <pre><code>- shell: \"curl https://api.example.com/data\"\n  retry_config:\n    max_attempts: 5                         # (1)!\n    backoff:\n      exponential:\n        initial: 1s                         # (2)!\n        multiplier: 2.0                     # (3)!\n    initial_delay: 1s                       # (4)!\n    max_delay: 60s                          # (5)!\n    jitter: true                            # (6)!\n    jitter_factor: 0.3                      # (7)!\n    retry_budget: 300s                      # (8)!\n    retry_on:                               # (9)!\n      - network\n      - timeout\n      - server_error\n      - rate_limit\n    on_failure: continue                    # (10)!\n\n1. Maximum number of retry attempts before giving up\n2. Starting delay for exponential backoff (1s, 2s, 4s, 8s, ...)\n3. Multiplier for exponential growth (2.0 = double each retry)\n4. Base delay added before backoff calculation\n5. Maximum delay cap to prevent excessive waiting\n6. Add randomness to prevent thundering herd problem\n7. Random variance range (0.3 = \u00b130% of calculated delay)\n8. Total time budget for all retries (5 minutes)\n9. Only retry on specific error types (fail fast on others)\n10. Action after all retries exhausted (continue, fail, dlq)\n</code></pre> <p>Source: Test examples from src/cook/retry_v2.rs:582-659, backoff calculation from retry_v2.rs:283-305</p> <p>Backoff Strategy Comparison:</p> Strategy Use Case Delay Pattern Example Exponential Most scenarios, fast backoff 2^n \u00d7 initial 1s, 2s, 4s, 8s Linear Steady load reduction initial + (n \u00d7 increment) 1s, 3s, 5s, 7s Fibonacci Gradual backoff Fibonacci(n) \u00d7 initial 1s, 1s, 2s, 3s, 5s Fixed Rate limiting, polling constant 5s, 5s, 5s, 5s Custom Complex SLA requirements user-defined 1s, 5s, 15s, 60s <p>Jitter Benefits (from retry_v2.rs:644-659): - Prevents thundering herd when multiple agents retry simultaneously - Adds \u00b130% random variance by default (configurable via <code>jitter_factor</code>) - Example: 10s delay becomes 7-13s with 0.3 jitter factor</p> <p>See Also: Retry State Tracking for persistence and recovery</p>"},{"location":"examples/#example-13-workflow-composition-preview-feature","title":"Example 13: Workflow Composition (Preview Feature)","text":"<p>Note: Workflow composition features are partially implemented. Core composition logic exists but CLI integration is pending (Spec 131-133). This example shows the planned syntax.</p> <pre><code># Import reusable workflow fragments\nimports:\n  - \"./workflows/common/test-suite.yml\"\n  - \"./workflows/common/deploy.yml\"\n\n# Extend base workflow\nextends: \"./workflows/base-ci.yml\"\n\nname: extended-ci-workflow\nmode: standard\n\n# Template for reusable command sets\ntemplates:\n  rust_test:\n    - shell: \"cargo build\"\n    - shell: \"cargo test\"\n    - shell: \"cargo clippy\"\n\n  deploy_to_env:\n    parameters:\n      - env_name\n      - target_url\n    commands:\n      - shell: \"echo 'Deploying to ${env_name}'\"\n      - shell: \"curl -X POST ${target_url}/deploy\"\n\n# Use templates in workflow\nsteps:\n  - template: rust_test\n  - template: deploy_to_env\n    with:\n      env_name: \"production\"\n      target_url: \"${API_URL}\"\n</code></pre> <p>Source: Composition architecture from features.json:workflow_composition, implementation status note from drift analysis</p> <p>Planned Composition Features: - Imports: Reuse workflow fragments across projects - Extends: Inherit from base workflows with overrides - Templates: Parameterized command sets for DRY workflows - Parameters: Type-safe template parameterization</p> <p>Current Status: - Core composition logic: \u2713 Implemented - Configuration parsing: \u2713 Implemented - CLI integration: \u23f3 Pending (Spec 131-133) - Template rendering: \u23f3 Pending</p> <p>Workaround Until CLI Integration: Use YAML anchors and aliases for basic composition:</p> <pre><code># Define reusable blocks with anchors\n.rust_test: &amp;rust_test\n  - shell: \"cargo build\"\n  - shell: \"cargo test\"\n\n.deploy: &amp;deploy\n  - shell: \"echo 'Deploying...'\"\n\n# Reference with aliases\nworkflow:\n  - *rust_test\n  - *deploy\n</code></pre>"},{"location":"examples/#example-14-custom-merge-workflows","title":"Example 14: Custom Merge Workflows","text":"<p>MapReduce workflows execute in isolated git worktrees. When the workflow completes, you can define a custom merge workflow to control how changes are merged back to your original branch.</p> <pre><code>name: code-review-with-merge\nmode: mapreduce\n\n# Environment variables available to merge commands\nenv:\n  PROJECT_NAME: \"my-project\"\n  NOTIFICATION_URL: \"https://api.slack.com/webhooks/...\"\n\nsetup:\n  - shell: \"find src -name '*.rs' &gt; files.json\"\n  - shell: \"jq -R -s -c 'split(\\\"\\n\\\") | map(select(length &gt; 0) | {path: .})' files.json &gt; items.json\"\n\nmap:\n  input: \"items.json\"\n  json_path: \"$[*]\"\n  agent_template:\n    - claude: \"/review-code ${item.path}\"\n      commit_required: true\n  max_parallel: 5\n\nreduce:\n  - claude: \"/summarize-reviews ${map.results}\"\n\n# Custom merge workflow (executed when merging worktree back to original branch)\nmerge:\n  commands:\n    # Merge-specific variables are available:\n    # ${merge.worktree} - Worktree name (e.g., \"session-abc123\")\n    # ${merge.source_branch} - Source branch in worktree\n    # ${merge.target_branch} - Target branch (where you started workflow)\n    # ${merge.session_id} - Session ID for correlation\n\n    # Pre-merge validation\n    - shell: \"echo 'Preparing to merge ${merge.worktree}'\"\n    - shell: \"echo 'Source: ${merge.source_branch} \u2192 Target: ${merge.target_branch}'\"\n\n    # Run tests before merging\n    - shell: \"cargo test --all\"\n      on_failure:\n        claude: \"/fix-failing-tests before merge\"\n        commit_required: true\n        max_attempts: 2\n\n    # Run linting\n    - shell: \"cargo clippy -- -D warnings\"\n      on_failure:\n        claude: \"/fix-clippy-warnings\"\n        commit_required: true\n\n    # Optional: Custom validation via Claude\n    - claude: \"/validate-merge-readiness ${merge.source_branch} ${merge.target_branch}\"\n\n    # Actually perform the merge using prodigy-merge-worktree\n    # IMPORTANT: Always pass both source and target branches\n    - claude: \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"\n\n    # Post-merge notifications (using env vars)\n    - shell: \"echo 'Successfully merged ${PROJECT_NAME} changes from ${merge.worktree}'\"\n    # - shell: \"curl -X POST ${NOTIFICATION_URL} -d 'Merge completed for ${PROJECT_NAME}'\"\n\n  # Optional: Timeout for entire merge phase (seconds)\n  timeout: 600  # 10 minutes\n</code></pre> <p>Source: Merge workflow configuration from src/config/mapreduce.rs:84-94, merge variables from worktree merge orchestrator, example from workflows/mapreduce-env-example.yml:83-94, test from tests/merge_workflow_integration.rs:64-121</p> <p>Merge Workflow Features:</p> <ol> <li>Merge-Specific Variables (automatically provided):</li> <li><code>${merge.worktree}</code> - Name of the worktree being merged</li> <li><code>${merge.source_branch}</code> - Branch in worktree (usually <code>prodigy-mapreduce-...</code>)</li> <li><code>${merge.target_branch}</code> - Your original branch (main, master, feature-xyz, etc.)</li> <li> <p><code>${merge.session_id}</code> - Session ID for tracking</p> </li> <li> <p>Pre-Merge Validation:</p> </li> <li>Run tests, linting, or custom checks before merging</li> <li>Use Claude commands for intelligent validation</li> <li> <p>Use <code>on_failure</code> handlers to fix issues automatically</p> </li> <li> <p>Environment Variables:</p> </li> <li>Global <code>env</code> variables are available in merge commands</li> <li>Useful for notifications, project-specific settings</li> <li> <p>Secrets are masked in merge command output</p> </li> <li> <p>Timeout Control:</p> </li> <li>Optional <code>timeout</code> field (in seconds) for the merge phase</li> <li>Prevents merge workflows from hanging indefinitely</li> </ol> <p>Important Notes: - Always pass both <code>${merge.source_branch}</code> and <code>${merge.target_branch}</code> to <code>/prodigy-merge-worktree</code> - This ensures the merge targets your original branch, not a hardcoded main/master - Without a custom merge workflow, you'll be prompted interactively to merge</p> <p>Handling Merge Failures: If merge validation fails (e.g., tests fail, linting fails), the <code>on_failure</code> handlers will attempt to fix the issues. If fixes cannot be applied automatically, the merge workflow will fail, and changes remain in the worktree for manual review:</p> <pre><code># Source: Pattern from workflows/mapreduce-env-example.yml:83-94\n- shell: \"cargo test --all\"\n  on_failure:\n    claude: \"/fix-failing-tests before merge\"\n    commit_required: true\n    max_attempts: 2\n    # If tests still fail after 2 attempts, workflow stops\n    # Changes remain in worktree at ~/.prodigy/worktrees/{repo_name}/{session_id}/\n</code></pre> <p>Recovery from Failed Merge: 1. Navigate to the worktree: <code>cd ~/.prodigy/worktrees/{repo_name}/{session_id}/</code> 2. Fix issues manually and commit changes 3. Resume the merge workflow: <code>prodigy resume {session_id}</code> 4. Or manually merge: <code>git checkout {target_branch} &amp;&amp; git merge {source_branch}</code></p> <p>Simplified Format: If you don't need timeout configuration, you can use the simplified format:</p> <pre><code>merge:\n  - shell: \"cargo test\"\n  - claude: \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"\n</code></pre> <p>This is equivalent to <code>merge.commands</code> but more concise.</p>"},{"location":"git-context-advanced/","title":"Advanced Git Context","text":"<p>This chapter covers automatic git tracking and git context variables in Prodigy workflows. Learn how to access file changes, commits, and modification statistics, and how to filter and format this data using shell commands.</p> <p>\u26a0\ufe0f Current Implementation Status</p> <p>Git context variables are currently provided as space-separated strings only. Advanced features like pattern filtering (<code>:*.rs</code>) and format modifiers (<code>:json</code>, <code>:lines</code>) are not yet implemented in the variable interpolation system, though the underlying infrastructure exists.</p> <p>For filtering and formatting, use shell post-processing commands like <code>grep</code>, <code>tr</code>, <code>jq</code>, and <code>xargs</code>. See Shell-Based Filtering and Formatting for practical examples.</p>"},{"location":"git-context-advanced/#overview","title":"Overview","text":"<p>Prodigy automatically tracks git changes throughout workflow execution and exposes them through variables. No configuration is needed\u2014git context variables are available out-of-the-box in any git repository. You can access file changes, commits, and modification statistics at both the step and workflow level.</p> <p>What you get: - Automatic tracking of all git changes during workflow execution - Variables for step-level changes (current command) and workflow-level changes (cumulative) - Simple space-separated format ready for shell commands - Full integration with MapReduce workflows</p>"},{"location":"git-context-advanced/#how-git-tracking-works","title":"How Git Tracking Works","text":""},{"location":"git-context-advanced/#automatic-tracking","title":"Automatic Tracking","text":"<p>Git context is automatically tracked when you run workflows in a git repository:</p> <ul> <li>GitChangeTracker is initialized at workflow start (src/cook/workflow/git_context.rs)</li> <li>Each step's changes are tracked between <code>begin_step</code> and <code>complete_step</code> calls</li> <li>Variables are pre-formatted as space-separated strings and added to the interpolation context</li> <li>No YAML configuration needed\u2014tracking happens transparently</li> </ul> <p>Technical Details (src/cook/workflow/executor/context.rs:96-172):</p> <p>When preparing the interpolation context for each command, git variables are added like this:</p> <pre><code>// Variables are pre-formatted as space-separated strings\ncontext.set(\"step.files_added\", Value::String(changes.files_added.join(\" \")));\ncontext.set(\"step.files_modified\", Value::String(changes.files_modified.join(\" \")));\n// ... etc for all git context variables\n</code></pre> <p>This means custom formatting must be done using shell commands after variable interpolation.</p>"},{"location":"git-context-advanced/#when-tracking-is-active","title":"When Tracking is Active","text":"<p>Git tracking is active in: - Regular workflows running in git repositories - MapReduce setup, map, and reduce phases - Child worktrees created for map agents</p> <p>Git tracking is not active in: - Non-git repositories - Workflows without git integration</p>"},{"location":"git-context-advanced/#git-context-variables","title":"Git Context Variables","text":""},{"location":"git-context-advanced/#step-level-variables","title":"Step-Level Variables","text":"<p>Track changes made during the current step:</p> <pre><code># Access files changed in this step\n- shell: \"echo Changed: ${step.files_changed}\"\n- shell: \"echo Added: ${step.files_added}\"\n- shell: \"echo Modified: ${step.files_modified}\"\n- shell: \"echo Deleted: ${step.files_deleted}\"\n\n# Access commit information\n- shell: \"echo Commits: ${step.commits}\"\n- shell: \"echo Commit count: ${step.commit_count}\"\n\n# Access modification statistics\n- shell: \"echo Insertions: ${step.insertions}\"\n- shell: \"echo Deletions: ${step.deletions}\"\n</code></pre>"},{"location":"git-context-advanced/#workflow-level-variables","title":"Workflow-Level Variables","text":"<p>Track cumulative changes across all steps:</p> <pre><code># Access all files changed in workflow\n- shell: \"echo Changed: ${workflow.files_changed}\"\n- shell: \"echo Added: ${workflow.files_added}\"\n- shell: \"echo Modified: ${workflow.files_modified}\"\n- shell: \"echo Deleted: ${workflow.files_deleted}\"\n\n# Access all commits\n- shell: \"echo Commits: ${workflow.commits}\"\n- shell: \"echo Commit count: ${workflow.commit_count}\"\n\n# Access total modifications\n- shell: \"echo Insertions: ${workflow.insertions}\"\n- shell: \"echo Deletions: ${workflow.deletions}\"\n</code></pre>"},{"location":"git-context-advanced/#variable-reference","title":"Variable Reference","text":"Variable Scope Description <code>step.files_added</code> Step Files added in current step <code>step.files_modified</code> Step Files modified in current step <code>step.files_deleted</code> Step Files deleted in current step <code>step.files_changed</code> Step All files changed (added + modified + deleted) <code>step.commits</code> Step Commit SHAs from current step <code>step.commit_count</code> Step Number of commits in current step <code>step.insertions</code> Step Lines added in current step <code>step.deletions</code> Step Lines deleted in current step <code>workflow.files_added</code> Workflow All files added in workflow <code>workflow.files_modified</code> Workflow All files modified in workflow <code>workflow.files_deleted</code> Workflow All files deleted in workflow <code>workflow.files_changed</code> Workflow All files changed in workflow <code>workflow.commits</code> Workflow All commit SHAs in workflow <code>workflow.commit_count</code> Workflow Total commits in workflow <code>workflow.insertions</code> Workflow Total lines added in workflow <code>workflow.deletions</code> Workflow Total lines deleted in workflow"},{"location":"git-context-advanced/#shell-based-filtering-and-formatting","title":"Shell-Based Filtering and Formatting","text":"<p>Since git context variables are provided as space-separated strings, all filtering and formatting must be done using shell commands. This section shows practical patterns for common tasks.</p>"},{"location":"git-context-advanced/#default-format-space-separated","title":"Default Format (Space-Separated)","text":"<p>Git context variables are always formatted as space-separated strings:</p> <pre><code>- shell: \"echo ${step.files_changed}\"\n# Output: src/main.rs src/lib.rs tests/test.rs\n</code></pre> <p>This format works well with most shell commands:</p> <pre><code># Pass directly to commands\n- shell: \"cargo fmt ${step.files_changed}\"\n- shell: \"git add ${workflow.files_modified}\"\n\n# Use in loops\n- shell: |\n    for file in ${step.files_added}; do\n      echo \"Processing $file\"\n    done\n</code></pre>"},{"location":"git-context-advanced/#filtering-by-file-extension","title":"Filtering by File Extension","text":"<p>Use <code>grep</code> to filter files by extension or pattern:</p> <pre><code># Only Rust files\n- shell: |\n    rust_files=$(echo \"${step.files_changed}\" | tr ' ' '\\n' | grep '\\.rs$')\n    echo \"$rust_files\"\n# Output:\n# src/main.rs\n# src/lib.rs\n\n# Only files in src/ directory\n- shell: |\n    src_files=$(echo \"${step.files_changed}\" | tr ' ' '\\n' | grep '^src/')\n    echo \"$src_files\"\n\n# Multiple extensions (Rust or TOML)\n- shell: |\n    filtered=$(echo \"${step.files_modified}\" | tr ' ' '\\n' | grep -E '\\.(rs|toml)$')\n    echo \"$filtered\"\n\n# Pass filtered files to a command\n- shell: |\n    rust_files=$(echo \"${step.files_changed}\" | tr ' ' '\\n' | grep '\\.rs$' | tr '\\n' ' ')\n    if [ -n \"$rust_files\" ]; then\n      cargo fmt $rust_files\n    fi\n</code></pre>"},{"location":"git-context-advanced/#converting-to-json-format","title":"Converting to JSON Format","text":"<p>Use <code>jq</code> to convert space-separated files to JSON arrays:</p> <pre><code># Convert to JSON array\n- shell: \"echo ${step.files_added} | tr ' ' '\\n' | jq -R | jq -s\"\n# Output: [\"src/main.rs\",\"src/lib.rs\",\"tests/test.rs\"]\n\n# Filter AND convert to JSON\n- shell: |\n    echo \"${step.files_changed}\" | tr ' ' '\\n' | grep '\\.rs$' | jq -R | jq -s\n# Output: [\"src/main.rs\",\"src/lib.rs\"]\n\n# Pretty-print JSON\n- shell: |\n    echo \"${workflow.files_modified}\" | tr ' ' '\\n' | jq -R | jq -s '.'\n</code></pre>"},{"location":"git-context-advanced/#converting-to-newline-separated-format","title":"Converting to Newline-Separated Format","text":"<p>Use <code>tr</code> to convert space-separated to newline-separated:</p> <pre><code># One file per line\n- shell: \"echo ${step.files_changed} | tr ' ' '\\n'\"\n# Output:\n# src/main.rs\n# src/lib.rs\n# tests/test.rs\n\n# Useful with xargs for parallel processing\n- shell: |\n    echo \"${workflow.files_modified}\" | tr ' ' '\\n' | xargs -I {} cp {} backup/\n\n# Count files\n- shell: \"echo ${step.files_added} | tr ' ' '\\n' | wc -l\"\n</code></pre>"},{"location":"git-context-advanced/#converting-to-csv-format","title":"Converting to CSV Format","text":"<p>Use <code>tr</code> to convert to comma-separated values:</p> <pre><code># Comma-separated\n- shell: \"echo ${step.files_added} | tr ' ' ','\"\n# Output: src/main.rs,src/lib.rs,tests/test.rs\n\n# CSV with filtering\n- shell: |\n    echo \"${step.files_changed}\" | tr ' ' '\\n' | grep '\\.md$' | tr '\\n' ',' | sed 's/,$//'\n# Output: README.md,CHANGELOG.md\n</code></pre>"},{"location":"git-context-advanced/#combining-filtering-and-formatting","title":"Combining Filtering and Formatting","text":"<p>Practical examples combining multiple operations:</p> <pre><code># Get Rust files as JSON\n- shell: |\n    echo \"${step.files_changed}\" | tr ' ' '\\n' | grep '\\.rs$' | jq -R | jq -s\n\n# Get source files as comma-separated list\n- shell: |\n    echo \"${workflow.files_modified}\" | tr ' ' '\\n' | grep '^src/' | tr '\\n' ',' | sed 's/,$//'\n\n# Count files by extension\n- shell: |\n    echo \"${workflow.files_changed}\" | tr ' ' '\\n' | sed 's/.*\\.//' | sort | uniq -c\n# Output:\n#    5 md\n#    12 rs\n#    3 toml\n</code></pre>"},{"location":"git-context-advanced/#use-cases","title":"Use Cases","text":""},{"location":"git-context-advanced/#code-review-workflows","title":"Code Review Workflows","text":"<p>Review only source code changes using shell filtering:</p> <pre><code># Filter to only Rust source files before review\n- shell: |\n    rust_files=$(echo \"${step.files_changed}\" | tr ' ' '\\n' | grep '^src/.*\\.rs$' | tr '\\n' ' ')\n    if [ -n \"$rust_files\" ]; then\n      echo \"Rust files changed: $rust_files\"\n    fi\n\n# Pass filtered files to Claude for review\n- shell: |\n    src_changes=$(echo \"${step.files_changed}\" | tr ' ' '\\n' | grep '^src/')\n    if [ -n \"$src_changes\" ]; then\n      echo \"$src_changes\" &gt; /tmp/review-files.txt\n      # Then use /tmp/review-files.txt in your review command\n    fi\n\n- shell: \"echo Reviewing ${step.commit_count} commits\"\n</code></pre>"},{"location":"git-context-advanced/#documentation-updates","title":"Documentation Updates","text":"<p>Work with documentation changes using filtering:</p> <pre><code># Find markdown files that changed\n- shell: |\n    md_files=$(echo \"${workflow.files_changed}\" | tr ' ' '\\n' | grep '\\.md$' | tr '\\n' ' ')\n    if [ -n \"$md_files\" ]; then\n      echo \"Documentation files changed: $md_files\"\n      markdownlint $md_files\n    fi\n\n# List changed docs in newline format\n- shell: |\n    echo \"${workflow.files_modified}\" | tr ' ' '\\n' | grep '\\.md$'\n\n# Check if any docs were updated\n- shell: |\n    doc_count=$(echo \"${workflow.files_changed}\" | tr ' ' '\\n' | grep '\\.md$' | wc -l)\n    if [ \"$doc_count\" -gt 0 ]; then\n      echo \"Documentation was updated ($doc_count files)\"\n    fi\n</code></pre>"},{"location":"git-context-advanced/#test-verification","title":"Test Verification","text":"<p>Focus on test-related changes:</p> <pre><code># Run tests for changed test files\n- shell: |\n    test_files=$(echo \"${step.files_changed}\" | tr ' ' '\\n' | grep '_test\\.rs$' | tr '\\n' ' ')\n    if [ -n \"$test_files\" ]; then\n      cargo test $test_files\n    fi\n\n# Verify test coverage for new files in tests/\n- shell: |\n    new_tests=$(echo \"${step.files_added}\" | tr ' ' '\\n' | grep '^tests/')\n    if [ -n \"$new_tests\" ]; then\n      echo \"New test files added:\"\n      echo \"$new_tests\"\n      # Run coverage analysis\n    fi\n</code></pre>"},{"location":"git-context-advanced/#conditional-execution","title":"Conditional Execution","text":"<p>Use git context with shell conditions:</p> <pre><code># Only run clippy if Rust files changed\n- shell: |\n    has_rust=$(echo \"${step.files_changed}\" | tr ' ' '\\n' | grep '\\.rs$')\n    if [ -n \"$has_rust\" ]; then\n      cargo clippy\n    fi\n\n# Run different linters based on file types\n- shell: |\n    if echo \"${workflow.files_changed}\" | tr ' ' '\\n' | grep -q '\\.rs$'; then\n      cargo fmt --check\n    fi\n\n    if echo \"${workflow.files_changed}\" | tr ' ' '\\n' | grep -q '\\.md$'; then\n      markdownlint **/*.md\n    fi\n\n# Check commit count\n- shell: |\n    if [ \"${step.commit_count}\" -gt 1 ]; then\n      echo \"Multiple commits detected (${step.commit_count})\"\n    fi\n</code></pre>"},{"location":"git-context-advanced/#mapreduce-workflows","title":"MapReduce Workflows","text":"<p>Git context works across MapReduce phases:</p> <pre><code>name: review-changes\nmode: mapreduce\n\nsetup:\n  # Workflow-level tracking starts here\n  - shell: \"git diff main --name-only &gt; changed-files.txt\"\n  - shell: \"echo Setup modified: ${step.files_changed}\"\n\nmap:\n  input: \"changed-files.txt\"\n  agent_template:\n    # Each agent has its own step tracking\n    - claude: \"/review ${item}\"\n    - shell: \"echo Agent changed: ${step.files_changed}\"\n\nreduce:\n  # Access workflow-level changes from all agents\n  - shell: \"echo Total changes: ${workflow.files_changed}\"\n  - shell: \"echo Total commits: ${workflow.commit_count}\"\n</code></pre>"},{"location":"git-context-advanced/#best-practices","title":"Best Practices","text":"<ul> <li>Use Shell Filtering: Filter variables to only relevant files using <code>grep</code>, <code>tr</code>, and other shell utilities</li> <li>Choose Appropriate Format: Convert to JSON with <code>jq</code>, newlines with <code>tr</code>, or CSV for different use cases</li> <li>Scope Appropriately: Use <code>step.*</code> for current changes, <code>workflow.*</code> for cumulative tracking</li> <li>Handle Empty Results: Always check if filtered results are non-empty before using them</li> <li>Test Your Filters: Debug with <code>echo</code> commands to verify filtering works as expected</li> <li>Document Intent: Add comments explaining complex shell filtering pipelines</li> <li>Combine Operations: Chain <code>tr</code>, <code>grep</code>, and <code>jq</code> for powerful filtering and formatting</li> </ul>"},{"location":"git-context-advanced/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Git operations are performed once per step and cached (src/cook/workflow/git_context.rs)</li> <li>Variables are pre-formatted when added to the interpolation context</li> <li>Shell filtering happens at runtime, so complex filters may add overhead</li> <li>Workflow-level tracking maintains cumulative state without re-scanning git history</li> <li>Variable resolution is fast since values are pre-computed strings</li> </ul>"},{"location":"git-context-advanced/#troubleshooting","title":"Troubleshooting","text":""},{"location":"git-context-advanced/#filter-not-matching-any-files","title":"Filter Not Matching Any Files","text":"<p>Issue: Your grep filter doesn't match any files</p> <pre><code># Debug: Echo the unfiltered variable first\n- shell: \"echo All files: ${step.files_changed}\"\n- shell: |\n    filtered=$(echo \"${step.files_changed}\" | tr ' ' '\\n' | grep '\\.rs$')\n    echo \"Filtered: $filtered\"\n</code></pre> <p>What happens: When a filter matches no files, the variable is empty. This is expected behavior.</p> <p>Solution: Always check if filtered results are non-empty:</p> <pre><code>- shell: |\n    rust_files=$(echo \"${step.files_changed}\" | tr ' ' '\\n' | grep '\\.rs$' | tr '\\n' ' ')\n    if [ -n \"$rust_files\" ]; then\n      cargo fmt $rust_files\n    else\n      echo \"No Rust files changed\"\n    fi\n</code></pre>"},{"location":"git-context-advanced/#empty-git-context-variables","title":"Empty Git Context Variables","text":"<p>Issue: Git context variables are empty</p> <p>Possible causes: - Not running in a git repository - No commits have been made in the current step - Git tracking not initialized</p> <p>Solution: Verify git tracking is active:</p> <pre><code># Check if variables are populated\n- shell: |\n    echo \"Step files changed: ${step.files_changed}\"\n    echo \"Workflow files changed: ${workflow.files_changed}\"\n    echo \"Commit count: ${step.commit_count}\"\n</code></pre> <p>If all are empty, check: 1. Are you in a git repository? (<code>git status</code>) 2. Has the step made any commits yet? 3. Is git tracking active for this workflow type?</p>"},{"location":"git-context-advanced/#pattern-syntax-not-working","title":"Pattern Syntax Not Working","text":"<p>Issue: Trying to use <code>:*.rs</code> or <code>:json</code> modifiers produces errors or unexpected results</p> <p>Cause: Pattern filtering and format modifiers are not implemented in variable interpolation. Git context variables are always space-separated strings.</p> <p>What you tried (doesn't work): <pre><code># These do NOT work - modifiers not implemented\n- shell: \"echo ${step.files_changed:*.rs}\"\n- shell: \"echo ${step.files_added:json}\"\n- shell: \"echo ${workflow.files_modified:lines}\"\n</code></pre></p> <p>Solution: Use shell commands for all filtering and formatting:</p> <pre><code># Filter with grep\n- shell: \"echo ${step.files_changed} | tr ' ' '\\n' | grep '\\.rs$'\"\n\n# Format as JSON\n- shell: \"echo ${step.files_added} | tr ' ' '\\n' | jq -R | jq -s\"\n\n# Format as newlines\n- shell: \"echo ${workflow.files_modified} | tr ' ' '\\n'\"\n</code></pre> <p>See Shell-Based Filtering and Formatting for complete examples.</p>"},{"location":"git-context-advanced/#variables-not-interpolating","title":"Variables Not Interpolating","text":"<p>Issue: Variables appear as literal strings like <code>${step.files_changed}</code></p> <p>Possible causes: - Variable name misspelled - Using unsupported variable - YAML quoting issues</p> <p>Solution: Verify the variable name and use proper quoting:</p> <pre><code># Correct syntax\n- shell: \"echo ${step.files_changed}\"\n- shell: |\n    echo \"${workflow.files_modified}\"\n</code></pre>"},{"location":"git-context-advanced/#shell-filtering-complexity","title":"Shell Filtering Complexity","text":"<p>Issue: Shell filtering pipelines are getting too complex</p> <p>Solution: Extract complex filtering to separate shell scripts:</p> <pre><code># Create a helper script\n- shell: |\n    cat &gt; /tmp/filter-rust.sh &lt;&lt;'EOF'\n    #!/bin/bash\n    echo \"$1\" | tr ' ' '\\n' | grep '\\.rs$' | tr '\\n' ' '\n    EOF\n    chmod +x /tmp/filter-rust.sh\n\n# Use the helper\n- shell: |\n    rust_files=$(/tmp/filter-rust.sh \"${step.files_changed}\")\n    if [ -n \"$rust_files\" ]; then\n      cargo clippy $rust_files\n    fi\n</code></pre>"},{"location":"git-context-advanced/#future-features","title":"Future Features","text":"<p>The git context infrastructure includes methods that are not yet exposed to workflows. These are planned for future releases:</p>"},{"location":"git-context-advanced/#pattern-filtering-planned","title":"Pattern Filtering (Planned)","text":"<p>The <code>GitChangeTracker::resolve_variable()</code> method (src/cook/workflow/git_context.rs:489-505) supports pattern filtering, but it's not currently called during workflow execution.</p> <p>Planned syntax: <pre><code># Not yet implemented - planned for future release\n- shell: \"echo ${step.files_changed:*.rs}\"\n- shell: \"echo ${workflow.files_modified:src/**/*.rs}\"\n</code></pre></p> <p>Currently variables are pre-formatted as space-separated strings during interpolation context creation (src/cook/workflow/executor/context.rs:106-172).</p>"},{"location":"git-context-advanced/#format-modifiers-planned","title":"Format Modifiers (Planned)","text":"<p>The <code>GitChangeTracker::format_file_list()</code> method (src/cook/workflow/git_context.rs:477-486) supports JSON, newline, and CSV formats, but it's not used during variable resolution.</p> <p>Planned syntax: <pre><code># Not yet implemented - planned for future release\n- shell: \"echo ${step.files_added:json}\"\n- shell: \"echo ${workflow.files_changed:lines}\"\n- shell: \"echo ${step.files_modified:csv}\"\n</code></pre></p>"},{"location":"git-context-advanced/#implementation-note","title":"Implementation Note","text":"<p>To enable these features, the interpolation engine would need to support custom resolvers that call <code>git_tracker.resolve_variable()</code> instead of using pre-formatted string values. This would allow runtime formatting and filtering based on variable modifier syntax.</p> <p>Until then, use shell post-processing as documented in Shell-Based Filtering and Formatting.</p>"},{"location":"git-context-advanced/#see-also","title":"See Also","text":"<ul> <li>Variables and Interpolation - Basic variable usage and interpolation syntax</li> <li>Workflow Basics - Git integration fundamentals and workflow structure</li> <li>MapReduce Workflows - Using git context in parallel jobs</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":"<p>This guide covers installing Prodigy and the prerequisite tools needed to run AI-powered workflow automation.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing Prodigy, ensure you have:</p> <p>Required: - Claude Code CLI - Prodigy executes Claude commands via the Claude Code CLI   - Install from: https://github.com/anthropics/claude-code   - Verify: <code>claude --version</code> - Rust 1.70+ - Required for building Prodigy from source   - Install from: https://rustup.rs/   - Verify: <code>rustc --version</code>   - Source: Cargo.toml:4 (edition = \"2021\" requires Rust 1.56+, recommended 1.70+)</p> <p>Required for MapReduce Workflows: - Git 2.25+ - Required for worktree isolation and parallel execution   - Install from: https://git-scm.com/   - Verify: <code>git --version</code>   - Note: Git worktrees are used to isolate parallel work in MapReduce workflows</p> <p>Optional: - mdBook - Required only for documentation workflows   - Install: <code>cargo install mdbook</code>   - Verify: <code>mdbook --version</code> - jq - Useful for inspecting JSON outputs and DLQ items   - Install from: https://jqlang.github.io/jq/</p>"},{"location":"installation/#using-cargo-recommended","title":"Using Cargo (Recommended)","text":"<p>The simplest way to install Prodigy is via Cargo, Rust's package manager:</p> <pre><code>cargo install prodigy\n</code></pre> <p>This command: 1. Downloads the latest version from crates.io 2. Compiles the binary with optimizations 3. Installs to <code>~/.cargo/bin/prodigy</code> (ensure this is in your PATH)</p> <p>Verify installation: <pre><code>prodigy --version\n# Expected output: prodigy 0.2.7\n</code></pre></p> <p>Source: README.md:51-55</p>"},{"location":"installation/#from-source","title":"From Source","text":"<p>To install the latest development version or contribute to Prodigy, build from source:</p> <pre><code># Clone the repository\ngit clone https://github.com/iepathos/prodigy\ncd prodigy\n\n# Build and install\ncargo build --release\ncargo install --path .\n</code></pre> <p>Build process: 1. <code>cargo build --release</code> compiles with optimizations (takes 3-5 minutes) 2. Binary is created at <code>target/release/prodigy</code> 3. <code>cargo install --path .</code> copies binary to <code>~/.cargo/bin/</code></p> <p>Verify installation: <pre><code>prodigy --version\n</code></pre></p> <p>Source: README.md:57-66</p>"},{"location":"installation/#optional-man-pages","title":"Optional: Man Pages","text":"<p>Prodigy includes comprehensive man pages for CLI reference. Install them with:</p> <pre><code>./scripts/install-man-pages.sh\n</code></pre> <p>This installs man pages to <code>/usr/local/share/man/man1/</code>. After installation:</p> <pre><code># View main Prodigy documentation\nman prodigy\n\n# View specific command documentation\nman prodigy-run\nman prodigy-resume\nman prodigy-dlq\n</code></pre> <p>Source: README.md:68-69, scripts/install-man-pages.sh</p>"},{"location":"installation/#verification","title":"Verification","text":"<p>After installation, verify Prodigy is working correctly:</p> <p>Check version: <pre><code>prodigy --version\n</code></pre></p> <p>View available commands: <pre><code>prodigy --help\n</code></pre></p> <p>Test basic functionality: <pre><code># Initialize Claude commands\nprodigy init\n\n# Should create .claude/commands/ directory\nls -la .claude/commands/\n</code></pre></p>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#command-not-found-prodigy","title":"Command not found: prodigy","text":"<p>Cause: <code>~/.cargo/bin</code> is not in your PATH</p> <p>Fix: Add to your shell profile (~/.bashrc, ~/.zshrc, etc.): <pre><code>export PATH=\"$HOME/.cargo/bin:$PATH\"\n</code></pre></p> <p>Then reload: <code>source ~/.bashrc</code> (or restart terminal)</p>"},{"location":"installation/#cargo-command-not-found","title":"cargo: command not found","text":"<p>Cause: Rust toolchain not installed</p> <p>Fix: Install Rust via rustup: <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n</code></pre></p>"},{"location":"installation/#build-fails-with-linker-not-found","title":"Build fails with \"linker not found\"","text":"<p>Cause: Missing C compiler/linker (required by some Rust dependencies)</p> <p>Fix: - macOS: Install Xcode Command Line Tools: <code>xcode-select --install</code> - Linux: Install build essentials: <code>sudo apt-get install build-essential</code> (Debian/Ubuntu) - Windows: Install Visual Studio Build Tools</p>"},{"location":"installation/#permission-denied-when-installing-man-pages","title":"Permission denied when installing man pages","text":"<p>Cause: <code>/usr/local/share/man/man1/</code> requires elevated permissions</p> <p>Fix: Run with sudo: <pre><code>sudo ./scripts/install-man-pages.sh\n</code></pre></p>"},{"location":"installation/#platform-specific-notes","title":"Platform-Specific Notes","text":"<p>macOS: - Xcode Command Line Tools recommended for best compatibility - Man pages install to <code>/usr/local/share/man/man1/</code> by default - Homebrew users: Cargo is included with <code>brew install rust</code></p> <p>Linux: - Build essentials package required for compilation - Man pages may require sudo for installation - Consider using system package manager if available</p> <p>Windows: - Visual Studio Build Tools required for Rust compilation - Consider using WSL2 for better compatibility with git worktrees - Man pages not supported on Windows (use <code>prodigy --help</code> instead)</p>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>After installation, explore Prodigy's features:</p> <p>Getting Started: - Introduction - Overview of Prodigy's capabilities - Workflow Basics - Learn how to write workflows</p> <p>Popular Use Cases: - Automated Documentation - Keep docs synchronized with code - MapReduce Workflows - Parallel processing at scale - Examples - See real-world workflow examples</p>"},{"location":"mapreduce-worktree-architecture/","title":"MapReduce Worktree Architecture","text":"<p>MapReduce workflows in Prodigy use an isolated git worktree architecture that ensures the main repository remains untouched during workflow execution. This chapter explains the worktree hierarchy, branch naming conventions, merge flows, and debugging strategies.</p>"},{"location":"mapreduce-worktree-architecture/#overview","title":"Overview","text":"<p>When you run a MapReduce workflow, Prodigy creates a hierarchical worktree structure:</p> <pre><code>Main Repository (untouched during execution)\n    \u2193\nParent Worktree (session-mapreduce-{id})\n    \u251c\u2500\u2500 Setup Phase \u2192 Executes here\n    \u251c\u2500\u2500 Reduce Phase \u2192 Executes here\n    \u2514\u2500\u2500 Map Phase \u2192 Each agent in child worktree\n        \u251c\u2500\u2500 Child Worktree (mapreduce-agent-{id})\n        \u251c\u2500\u2500 Child Worktree (mapreduce-agent-{id})\n        \u2514\u2500\u2500 Child Worktree (mapreduce-agent-{id})\n</code></pre> <p>This architecture provides complete isolation, allowing parallel agents to work independently while preserving a clean main repository.</p>"},{"location":"mapreduce-worktree-architecture/#worktree-hierarchy","title":"Worktree Hierarchy","text":""},{"location":"mapreduce-worktree-architecture/#parent-worktree","title":"Parent Worktree","text":"<p>Created at the start of MapReduce workflow execution:</p> <p>Location: <code>~/.prodigy/worktrees/{project}/session-mapreduce-{timestamp}</code></p> <p>Purpose: - Isolates all workflow execution from main repository - Hosts setup phase execution - Hosts reduce phase execution - Serves as merge target for agent results</p> <p>Branch: Follows <code>prodigy-{session-id}</code> pattern. The session ID includes a timestamp (e.g., <code>session-mapreduce-20250112_143052</code>), so the full branch name becomes <code>prodigy-session-mapreduce-20250112_143052</code> (source: src/worktree/builder.rs:176-178)</p> <p>Worktree Allocation Strategies:</p> <p>All worktrees in Prodigy have names, paths, and git branches - the distinction is in how they're allocated:</p> <ul> <li> <p>Directly-Created Worktrees: MapReduce coordinators create session worktrees with explicit, predictable names (e.g., <code>session-mapreduce-20250112_143052</code>). These have deterministic paths and are easy to locate.</p> </li> <li> <p>Pool-Allocated Worktrees: When agents request worktrees via <code>WorktreeRequest::Anonymous</code> (source: src/cook/execution/mapreduce/resources/worktree.rs:42), they receive pre-allocated worktrees from a shared pool. These worktrees have pool-assigned names rather than request-specific names. The pool allocation strategy enables efficient resource reuse across multiple agents.</p> </li> </ul> <p>Important: Both allocation strategies produce worktrees with full identity (name, path, branch). The difference is in naming predictability and resource management approach.</p>"},{"location":"mapreduce-worktree-architecture/#child-worktrees","title":"Child Worktrees","text":"<p>Created for each map agent:</p> <p>Location: <code>~/.prodigy/worktrees/{project}/mapreduce-agent-{agent_id}</code></p> <p>Purpose: - Complete isolation per agent - Independent failure handling - Parallel execution safety</p> <p>Branch: Follows <code>prodigy-{worktree-name}</code> pattern (branched from parent worktree)</p> <p>Resource Management: Agent worktrees can be acquired through two strategies:</p> <ol> <li> <p>Worktree Pool (preferred): Agents first attempt to acquire pre-allocated worktrees from a <code>WorktreePool</code>. This reduces creation overhead and enables efficient resource reuse.</p> </li> <li> <p>Direct Creation (fallback): If the pool is exhausted or unavailable, agents fall back to creating dedicated worktrees via <code>WorktreeManager</code>.</p> </li> </ol> <p>The <code>acquire_session</code> method implements this pool-first strategy, ensuring optimal resource utilization while maintaining isolation guarantees.</p> <p>Note: The <code>agent_id</code> in the location path encodes the work item information. Agent worktrees are created dynamically as map agents execute.</p>"},{"location":"mapreduce-worktree-architecture/#branch-naming-conventions","title":"Branch Naming Conventions","text":"<p>Prodigy uses consistent branch naming to track worktree relationships:</p>"},{"location":"mapreduce-worktree-architecture/#parent-worktree-branch","title":"Parent Worktree Branch","text":"<p>Format: <code>prodigy-{session-id}</code></p> <p>The branch name follows the universal worktree pattern where all worktrees use <code>prodigy-{name}</code>. For MapReduce workflows, the session ID itself includes the timestamp, so the full branch name looks like:</p> <p>Example: <code>prodigy-session-mapreduce-20250112_143052</code></p> <p>This is <code>prodigy-</code> + the session ID <code>session-mapreduce-20250112_143052</code></p>"},{"location":"mapreduce-worktree-architecture/#agent-worktree-branch","title":"Agent Worktree Branch","text":"<p>Format: <code>prodigy-{worktree-name}</code></p> <p>All worktrees in Prodigy follow the universal <code>prodigy-{name}</code> branch naming pattern (source: src/worktree/builder.rs:178). The worktree name itself varies based on the allocation strategy:</p> <p>Pool-Allocated Worktrees: When agents acquire worktrees from the pre-allocated pool, the worktree name is generated by the pool and may not follow a predictable pattern. These are still tracked by the consistent <code>prodigy-{name}</code> branch format.</p> <p>Directly-Created Worktrees: When agents create dedicated worktrees (fallback when pool is exhausted), the name typically encodes job and agent information.</p> <p>Example: <code>prodigy-mapreduce-agent-mapreduce-20251109_193734_agent_22</code></p> <p>This is <code>prodigy-</code> + the worktree name <code>mapreduce-agent-mapreduce-20251109_193734_agent_22</code></p> <p>Directly-Created Worktree Name Components: - <code>mapreduce-agent-</code>: Indicates this is a MapReduce agent worktree - <code>{job_id}</code>: The MapReduce job identifier (includes timestamp) - <code>_agent_{n}</code>: Sequential agent number within the job</p> <p>Note: The branch naming is always consistent (<code>prodigy-{name}</code>), but worktree naming varies based on allocation strategy.</p>"},{"location":"mapreduce-worktree-architecture/#merge-flow","title":"Merge Flow","text":"<p>MapReduce workflows involve multiple merge operations to aggregate results:</p>"},{"location":"mapreduce-worktree-architecture/#1-agent-merge-child-parent","title":"1. Agent Merge (Child \u2192 Parent)","text":"<p>When an agent completes successfully:</p> <pre><code>Child Worktree (agent branch)\n    \u2193 merge\nParent Worktree (session branch)\n</code></pre> <p>Process: 1. Agent completes all commands successfully 2. Agent commits changes to its branch 3. Merge coordinator adds agent to merge queue 4. Sequential merge into parent worktree branch 5. Child worktree cleanup</p>"},{"location":"mapreduce-worktree-architecture/#2-mapreduce-to-parent-merge","title":"2. MapReduce to Parent Merge","text":"<p>After all map agents complete and reduce phase finishes:</p> <pre><code>Parent Worktree (session branch)\n    \u2193 merge\nMain Repository (original branch)\n</code></pre> <p>Process: 1. All agents merged into parent worktree 2. Reduce phase executes in parent worktree 3. User confirms merge to main repository 4. Sequential merge with conflict detection 5. Parent worktree cleanup</p>"},{"location":"mapreduce-worktree-architecture/#merge-strategies","title":"Merge Strategies","text":"<p>Fast-Forward When Possible: If no divergence, use fast-forward merge</p> <p>Three-Way Merge: When branches have diverged, perform three-way merge</p> <p>Conflict Handling: Stop and report conflicts for manual resolution</p>"},{"location":"mapreduce-worktree-architecture/#agent-merge-details","title":"Agent Merge Details","text":""},{"location":"mapreduce-worktree-architecture/#merge-queue","title":"Merge Queue","text":"<p>Agents are added to a merge queue as they complete:</p> <p>Queue Architecture: Merge queue is managed in-memory by a background worker task using a tokio unbounded mpsc channel (<code>mpsc::unbounded_channel::&lt;MergeRequest&gt;()</code>). Merge requests are processed sequentially via this channel, eliminating MERGE_HEAD race conditions. Queue state is not persisted - merge operations are atomic (source: src/cook/execution/mapreduce/merge_queue.rs:70).</p> <p>Resume and Recovery: The merge queue state is reconstructed on resume from checkpoint data (source: src/cook/execution/mapreduce/merge_queue.rs:153). When a MapReduce workflow is interrupted and resumed, the queue is rebuilt based on: - Completed agents: Already merged, skip re-merging - Failed agents: Tracked in DLQ, can be retried separately - In-progress agents: Moved back to pending status, will be re-executed - Pending agents: Continue processing from where left off</p> <p>Any in-progress merges at the time of interruption are retried from the agent worktree state. This ensures no agent results are lost during resume.</p> <p>Queue Processing: Queue processes <code>MergeRequest</code> objects containing: - <code>agent_id</code>: Unique agent identifier - <code>branch_name</code>: Agent's git branch to merge - <code>item_id</code>: Work item identifier for correlation - <code>env</code>: Execution environment context (variables, secrets)</p> <p>Merge requests are processed FIFO with automatic conflict detection.</p>"},{"location":"mapreduce-worktree-architecture/#sequential-merge-processing","title":"Sequential Merge Processing","text":"<p>Merges are processed sequentially to prevent conflicts:</p> <ol> <li>Lock merge queue</li> <li>Take next agent from pending queue</li> <li>Perform merge into parent worktree</li> <li>Update queue (move to merged or failed)</li> <li>Release lock</li> </ol>"},{"location":"mapreduce-worktree-architecture/#automatic-conflict-resolution","title":"Automatic Conflict Resolution","text":"<p>If a standard git merge fails with conflicts, the merge queue automatically invokes Claude using the <code>/prodigy-merge-worktree</code> command to resolve conflicts intelligently:</p> <p>Conflict Resolution Flow: 1. Standard git merge attempted 2. If conflicts detected, invoke Claude with <code>/prodigy-merge-worktree {branch_name}</code> 3. Claude is executed with <code>PRODIGY_AUTOMATION=true</code> environment variable (source: src/cook/execution/mapreduce/merge_queue.rs:98-99) 4. Claude analyzes conflicts and attempts resolution 5. If Claude succeeds, merge completes automatically 6. If Claude fails, agent is marked as failed and added to DLQ</p> <p>PRODIGY_AUTOMATION Environment Variable: When set to <code>true</code>, this signals to Claude Code that it's operating in automated workflow mode and should use appropriate merge strategies without requiring user interaction. Claude will attempt to resolve conflicts autonomously using standard git merge strategies and code analysis.</p> <p>Benefits: - Reduces manual merge conflict resolution overhead - Handles common conflict patterns automatically - Preserves full context for debugging via Claude logs - Falls back gracefully to DLQ for complex conflicts - Automated execution mode ensures non-interactive conflict resolution</p> <p>This automatic conflict resolution is especially useful when multiple agents modify overlapping code areas.</p>"},{"location":"mapreduce-worktree-architecture/#parent-to-master-merge","title":"Parent to Master Merge","text":""},{"location":"mapreduce-worktree-architecture/#merge-confirmation","title":"Merge Confirmation","text":"<p>After reduce phase completes, Prodigy prompts for merge confirmation:</p> <pre><code>\u2713 MapReduce workflow completed successfully\n\nMerge session-mapreduce-20250112_143052 to master? [y/N]\n</code></pre>"},{"location":"mapreduce-worktree-architecture/#custom-merge-workflows","title":"Custom Merge Workflows","text":"<p>Configure custom merge validation:</p> <pre><code>merge:\n  - shell: \"git fetch origin\"\n  - shell: \"cargo test\"\n  - shell: \"cargo clippy\"\n  - claude: \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"\n</code></pre> <p>Important: Always pass both <code>${merge.source_branch}</code> and <code>${merge.target_branch}</code> to the <code>/prodigy-merge-worktree</code> command (source: .claude/commands/prodigy-merge-worktree.md). This ensures the merge targets the branch you were on when you started the workflow, not a hardcoded main/master branch.</p>"},{"location":"mapreduce-worktree-architecture/#merge-variables","title":"Merge Variables","text":"<p>Available during merge workflows:</p> <ul> <li><code>${merge.worktree}</code> - Worktree name</li> <li><code>${merge.source_branch}</code> - Session branch name</li> <li><code>${merge.target_branch}</code> - Main repository branch (usually master/main)</li> <li><code>${merge.session_id}</code> - Session ID for correlation</li> </ul>"},{"location":"mapreduce-worktree-architecture/#debugging-mapreduce-worktrees","title":"Debugging MapReduce Worktrees","text":""},{"location":"mapreduce-worktree-architecture/#inspecting-worktree-state","title":"Inspecting Worktree State","text":"<pre><code># List all worktrees\ngit worktree list\n\n# View worktree details\ncd ~/.prodigy/worktrees/{project}/session-mapreduce-*\ngit status\ngit log\n\n# View agent worktree\ncd ~/.prodigy/worktrees/{project}/agent-*\ngit log --oneline\n</code></pre>"},{"location":"mapreduce-worktree-architecture/#finding-agent-worktree-paths","title":"Finding Agent Worktree Paths","text":"<p>Agent worktrees may be directly-created (with predictable names) or pool-allocated (with pool-assigned names). To correlate agent IDs to worktree paths:</p> <p>Directly-Created Worktrees (deterministic paths): <pre><code># Pattern: ~/.prodigy/worktrees/{project}/mapreduce-agent-{job_id}_agent_{n}\ncd ~/.prodigy/worktrees/{project}/mapreduce-agent-*\n</code></pre></p> <p>Pool-Allocated Worktrees (pool-assigned paths): <pre><code># List all worktrees and correlate by branch name\ngit worktree list\n\n# Look for branches matching agent pattern\ngit branch -a | grep prodigy-mapreduce-agent\n</code></pre></p> <p>Note: Both allocation strategies produce fully-identified worktrees with names, paths, and branches. Pool allocation assigns names from the pool's naming scheme, while direct creation uses request-specific naming patterns. Use <code>WorktreeInfo</code> tracking (described below) to correlate agent IDs to actual worktree locations.</p> <p>WorktreeInfo Tracking: Prodigy captures worktree metadata in <code>WorktreeInfo</code> structs containing: - <code>name</code>: Worktree identifier - <code>path</code>: Full filesystem path - <code>branch</code>: Git branch name</p> <p>This information is logged in MapReduce events and can be inspected via <code>prodigy events {job_id}</code> to correlate agent IDs to worktree paths.</p>"},{"location":"mapreduce-worktree-architecture/#common-debugging-scenarios","title":"Common Debugging Scenarios","text":"<p>Agent Failed to Merge:</p> <ol> <li>Check DLQ for failure details: <code>prodigy dlq show {job_id}</code></li> <li>Inspect failed agent worktree: <code>cd ~/.prodigy/worktrees/{project}/mapreduce-agent-*</code></li> <li>Review agent changes: <code>git diff master</code></li> <li>Check for conflicts: <code>git status</code></li> <li>Review Claude merge logs if conflict resolution was attempted</li> </ol> <p>Parent Worktree Not Merging:</p> <ol> <li>Check parent worktree: <code>cd ~/.prodigy/worktrees/{project}/session-mapreduce-*</code></li> <li>Verify all agents merged: <code>git log --oneline</code></li> <li>Check for uncommitted changes: <code>git status</code></li> <li>Review merge history: <code>git log --graph --oneline --all</code></li> </ol>"},{"location":"mapreduce-worktree-architecture/#merge-conflict-resolution","title":"Merge Conflict Resolution","text":"<p>If merge conflicts occur:</p> <pre><code># Navigate to parent worktree\ncd ~/.prodigy/worktrees/{project}/session-mapreduce-*\n\n# View conflicts\ngit status\n\n# Resolve manually\nvim &lt;conflicted-file&gt;\n\n# Complete merge\ngit add &lt;conflicted-file&gt;\ngit commit\n</code></pre>"},{"location":"mapreduce-worktree-architecture/#verification-commands","title":"Verification Commands","text":""},{"location":"mapreduce-worktree-architecture/#verify-main-repository-is-clean","title":"Verify Main Repository is Clean","text":"<pre><code># Main repository should have no changes from MapReduce execution\ngit status\n# Expected: nothing to commit, working tree clean\n</code></pre>"},{"location":"mapreduce-worktree-architecture/#verify-worktree-isolation","title":"Verify Worktree Isolation","text":"<pre><code># Check that parent worktree has changes\ncd ~/.prodigy/worktrees/{project}/session-mapreduce-*\ngit status\ngit log --oneline\n\n# Main repository should still be clean\ncd /path/to/main/repo\ngit status\n</code></pre>"},{"location":"mapreduce-worktree-architecture/#verify-agent-merges","title":"Verify Agent Merges","text":"<pre><code># Check for merge events\nprodigy events {job_id}\n\n# Verify merged agents in parent worktree\ncd ~/.prodigy/worktrees/{project}/session-mapreduce-*\ngit log --oneline | grep \"Merge\"\n</code></pre>"},{"location":"mapreduce-worktree-architecture/#best-practices","title":"Best Practices","text":""},{"location":"mapreduce-worktree-architecture/#worktree-management","title":"Worktree Management","text":"<ul> <li>Cleanup: Remove old worktrees after successful merge: <code>prodigy worktree clean</code></li> <li>Monitoring: Check worktree disk usage periodically</li> <li>Inspection: Review worktrees before deleting to verify results</li> </ul>"},{"location":"mapreduce-worktree-architecture/#merge-workflows","title":"Merge Workflows","text":"<ul> <li>Test Before Merge: Run tests in merge workflow to catch issues</li> <li>Sync Upstream: Fetch and merge origin/main before merging to main</li> <li>Conflict Prevention: Keep MapReduce jobs focused to minimize conflicts</li> </ul>"},{"location":"mapreduce-worktree-architecture/#debugging","title":"Debugging","text":"<ul> <li>Preserve Worktrees: Don't delete worktrees until debugging is complete</li> <li>Event Logs: Review event logs for merge failures: <code>prodigy events {job_id}</code></li> <li>DLQ Review: Check failed items that might indicate merge issues</li> </ul>"},{"location":"mapreduce-worktree-architecture/#troubleshooting","title":"Troubleshooting","text":""},{"location":"mapreduce-worktree-architecture/#worktree-creation-fails","title":"Worktree Creation Fails","text":"<p>Issue: Cannot create parent or child worktree Solution: Check disk space, verify git repository is valid, ensure no existing worktree with same name</p>"},{"location":"mapreduce-worktree-architecture/#agent-merge-fails","title":"Agent Merge Fails","text":"<p>Issue: Agent results fail to merge into parent Solution: Check merge queue, inspect agent worktree for conflicts, review agent changes</p>"},{"location":"mapreduce-worktree-architecture/#parent-merge-conflicts","title":"Parent Merge Conflicts","text":"<p>Issue: Merging parent worktree to main causes conflicts Solution: Resolve conflicts manually, consider rebasing parent worktree on latest main</p>"},{"location":"mapreduce-worktree-architecture/#orphaned-worktrees","title":"Orphaned Worktrees","text":"<p>Issue: Worktrees remain after workflow completion Solution: Use <code>prodigy worktree clean</code> to remove old worktrees, or manually remove with <code>git worktree remove</code></p>"},{"location":"mapreduce-worktree-architecture/#see-also","title":"See Also","text":"<ul> <li>MapReduce Workflows - MapReduce workflow basics</li> <li>Error Handling - Handling merge failures</li> <li>Troubleshooting - General troubleshooting guide</li> </ul>"},{"location":"advanced/","title":"Advanced Features","text":"<p>This chapter covers advanced workflow features for building sophisticated automation pipelines. These features enable conditional execution, parallel processing, validation, and complex control flow.</p>"},{"location":"advanced/#conditional-execution","title":"Conditional Execution","text":"<p>Control when commands execute based on expressions or previous command results.</p>"},{"location":"advanced/#expression-based-conditions","title":"Expression-Based Conditions","text":"<p>Use the <code>when</code> field to conditionally execute commands based on variable values:</p> <pre><code># Execute only when variable is true\n- shell: \"cargo build --release\"\n  when: \"${tests_passed}\"\n\n# Execute based on complex expression\n- shell: \"deploy.sh\"\n  when: \"${environment == 'production' &amp;&amp; tests_passed}\"\n</code></pre>"},{"location":"advanced/#expression-syntax-for-when-clauses","title":"Expression Syntax for When Clauses","text":"<p>The <code>when</code> clause supports a flexible expression syntax for conditional logic:</p> <p>Variable Interpolation: - Use <code>${variable}</code> to reference captured outputs or environment variables - Variables are evaluated in the context of previous command results - Boolean variables are evaluated as truthy/falsy values</p> <p>Comparison Operators: - <code>==</code> - Equality comparison (e.g., <code>${status == 'success'}</code>) - <code>!=</code> - Inequality comparison (e.g., <code>${exit_code != 0}</code>) - <code>&gt;</code> - Greater than (e.g., <code>${score &gt; 80}</code>) - <code>&lt;</code> - Less than (e.g., <code>${errors &lt; 5}</code>) - <code>&gt;=</code> - Greater than or equal to (e.g., <code>${coverage &gt;= 90}</code>) - <code>&lt;=</code> - Less than or equal to (e.g., <code>${warnings &lt;= 10}</code>) - <code>contains</code> - String matching (e.g., <code>${output contains 'success'}</code>)</p> <p>Logical Operators: - <code>&amp;&amp;</code> - Logical AND (e.g., <code>${tests_passed &amp;&amp; build_succeeded}</code>) - <code>||</code> - Logical OR (e.g., <code>${is_dev || is_staging}</code>)</p> <p>Type Coercion: - String values: Non-empty strings are truthy, empty strings are falsy - Numeric values: Non-zero numbers are truthy, zero is falsy - Boolean values: <code>true</code> is truthy, <code>false</code> is falsy</p> <p>Complex Expressions: <pre><code># Multiple conditions with logical operators\n- shell: \"deploy.sh\"\n  when: \"${environment == 'production' &amp;&amp; tests_passed &amp;&amp; coverage &gt;= 80}\"\n\n# Nested logic with parentheses\n- shell: \"run-checks.sh\"\n  when: \"${(is_pr || is_main) &amp;&amp; tests_passed}\"\n\n# Comparing captured outputs\n- shell: \"notify-team.sh\"\n  when: \"${test-step.exit_code == 0 &amp;&amp; build-step.success}\"\n</code></pre></p>"},{"location":"advanced/#on-success-handlers","title":"On Success Handlers","text":"<p>Execute follow-up commands when a command succeeds:</p> <pre><code>- shell: \"cargo test\"\n  on_success:\n    shell: \"cargo bench\"\n</code></pre> <p>Note: The <code>on_success</code> field supports any workflow step command with all its features, including nested conditionals, output capture, validation, and error handlers. You can create complex success workflows by combining multiple handlers or using <code>when</code> clauses for sophisticated control flow.</p> <p>Complex On Success Example:</p> <p>The <code>on_success</code> field accepts a complete workflow step command with all its features:</p> <pre><code>- shell: \"cargo build --release\"\n  on_success:\n    shell: \"check-binary-size.sh\"\n    validate:\n      threshold: 100\n    on_failure:\n      claude: \"/optimize-binary-size\"\n      max_attempts: 2\n</code></pre> <p>Source: Based on WorkflowStepCommand structure (src/config/command.rs:376)</p>"},{"location":"advanced/#on-failure-handlers","title":"On Failure Handlers","text":"<p>Handle failures with automatic remediation:</p> <pre><code>- shell: \"cargo clippy\"\n  on_failure:\n    claude: \"/fix-warnings\"\n    max_attempts: 3\n    fail_workflow: false\n    commit_required: true\n</code></pre> <p>The <code>on_failure</code> configuration supports: - <code>max_attempts</code>: Maximum retry attempts (default: 3) - <code>fail_workflow</code>: Whether to fail entire workflow on final failure (default: false) - <code>commit_required</code>: Whether the remediation command should create a git commit (default: true)</p> <p>Note: These defaults come from the <code>TestDebugConfig</code> which provides sensible defaults for error recovery workflows.</p> <p>Source: TestDebugConfig struct definition (src/config/command.rs:168-183)</p>"},{"location":"advanced/#nested-conditionals","title":"Nested Conditionals","text":"<p>Chain multiple levels of conditional execution:</p> <pre><code>- shell: \"cargo check\"\n  on_success:\n    shell: \"cargo build --release\"\n    on_success:\n      shell: \"cargo test --release\"\n      on_failure:\n        claude: \"/debug-failures '${shell.output}'\"\n</code></pre> <p>Note: For multi-step error recovery, nest individual <code>on_failure</code> handlers at each step rather than using a commands array. The <code>TestDebugConfig</code> supports only a single <code>claude</code> command per handler.</p>"},{"location":"advanced/#output-capture-and-variable-management","title":"Output Capture and Variable Management","text":"<p>Capture command output in different formats for use in subsequent steps.</p>"},{"location":"advanced/#capture-variable","title":"Capture Variable","text":"<p>Capture output to a named variable using the <code>capture_output</code> field:</p> <pre><code># Capture as string (backward compatible)\n- shell: \"git rev-parse HEAD\"\n  capture_output: \"commit_hash\"\n\n# Reference in later steps\n- shell: \"echo 'Commit: ${commit_hash}'\"\n</code></pre>"},{"location":"advanced/#command-agnostic-capture","title":"Command-Agnostic Capture","text":"<p>The <code>last.*</code> variables capture output from any command type without needing explicit <code>capture_output</code>:</p> <pre><code># Shell command output\n- shell: \"cargo test\"\n  # Output automatically available as ${last.output} and ${last.exit_code}\n\n# Use in next command (any type)\n- claude: \"/analyze ${last.output}\"\n\n# Or reference in conditional\n- shell: \"notify-failure.sh\"\n  when: \"${last.exit_code != 0}\"\n</code></pre> <p>Available Variables: - <code>${last.output}</code> - Output from the last command of any type (shell, claude, etc.) - <code>${last.exit_code}</code> - Exit code from the last command</p> <p>These variables work across all command types, making them ideal for generic workflows where you don't want to hard-code command-specific variables like <code>${shell.output}</code> or <code>${claude.output}</code>.</p> <p>Source: Variable constants defined in src/cook/workflow/variables.rs:35-36</p>"},{"location":"advanced/#capture-formats","title":"Capture Formats","text":"<p>Control how output is parsed with <code>capture_format</code>:</p> <pre><code># String (default) - trimmed output as single string\n- shell: \"git rev-parse HEAD\"\n  capture_output: \"commit_hash\"\n  capture_format: \"string\"\n\n# Number - parse output as number\n- shell: \"wc -l &lt; file.txt\"\n  capture_output: \"line_count\"\n  capture_format: \"number\"\n\n# JSON - parse output as JSON object\n- shell: \"cargo metadata --format-version 1\"\n  capture_output: \"metadata\"\n  capture_format: \"json\"\n\n# Lines - split output into array of lines\n- shell: \"find . -name '*.rs'\"\n  capture_output: \"rust_files\"\n  capture_format: \"lines\"\n\n# Boolean - parse \"true\"/\"false\" as boolean\n- shell: \"test -f README.md &amp;&amp; echo true || echo false\"\n  capture_output: \"has_readme\"\n  capture_format: \"boolean\"\n</code></pre> <p>Error Handling: If parsing fails (e.g., non-numeric output with <code>capture_format: number</code>), the command will fail with a descriptive error. Use <code>capture_format: string</code> (default) when output format is unreliable.</p> <p>Source: CaptureFormat enum (src/cook/workflow/variables.rs:260-265)</p>"},{"location":"advanced/#stream-capture-control","title":"Stream Capture Control","text":"<p>The <code>capture_streams</code> field supports two formats for flexible output capture.</p> <p>Simple String Format - For basic stream selection:</p> <pre><code># Capture only stdout (default)\n- shell: \"cargo build\"\n  capture_output: \"build_log\"\n  capture_streams: \"stdout\"\n\n# Capture only stderr\n- shell: \"cargo test\"\n  capture_output: \"error_log\"\n  capture_streams: \"stderr\"\n\n# Capture both streams merged\n- shell: \"npm install\"\n  capture_output: \"install_log\"\n  capture_streams: \"both\"\n</code></pre> <p>Structured Object Format - For advanced control with exit code, success status, and duration:</p> <pre><code># Structured format with all fields\n- shell: \"cargo test\"\n  capture_output: \"test_result\"\n  capture_streams:\n    stdout: true      # Capture stdout stream (default: true)\n    stderr: true      # Capture stderr stream (default: false)\n    exit_code: true   # Capture exit code (default: true)\n    success: true     # Capture success status (default: true)\n    duration: true    # Capture execution duration in seconds (default: true)\n\n# Access individual fields\n- shell: \"echo 'Test exit code: ${test_result.exit_code}'\"\n- shell: \"echo 'Test passed: ${test_result.success}'\"\n- shell: \"echo 'Duration: ${test_result.duration}s'\"\n</code></pre> <p>Source: CaptureStreams struct definition (src/cook/workflow/variables.rs:268-292)</p> <p>Format Flexibility:</p> <p>The <code>capture_streams</code> field accepts two formats:</p> <ol> <li>Simple string (<code>\"stdout\"</code>, <code>\"stderr\"</code>, <code>\"both\"</code>) - Stored as <code>Option&lt;String&gt;</code> in YAML config, best for basic stream selection</li> <li>Structured object - Parsed into <code>CaptureStreams</code> struct during execution, enables fine-grained control over <code>exit_code</code>, <code>success</code>, and <code>duration</code> capture</li> </ol> <p>Use simple format for basic cases, structured format when you need detailed execution metadata.</p> <p>Source: WorkflowStepCommand.capture_streams field (src/config/command.rs:396), CaptureStreams struct (src/cook/workflow/variables.rs:268-292)</p>"},{"location":"advanced/#output-file-redirection","title":"Output File Redirection","text":"<p>Write output directly to a file instead of capturing it:</p> <pre><code># Redirect stdout to file\n- shell: \"cargo doc --no-deps\"\n  output_file: \"docs/build.log\"\n\n# Combine with capture for dual output\n- shell: \"cargo test\"\n  capture_output: \"test_status\"\n  output_file: \"test-results.log\"\n</code></pre>"},{"location":"advanced/#execution-context","title":"Execution Context","text":"<p>Configure where and how commands execute in your workflow.</p> <p>Note: Step-level environment variable configuration (<code>env</code>, <code>clear_env</code>, <code>inherit</code>) and working directory (<code>working_dir</code>, <code>cwd</code>) are internal features available in the execution layer but not currently exposed in the YAML configuration layer (WorkflowStepCommand). These features exist in the runtime <code>WorkflowStep</code> type for internal use.</p> <p>For workflow-level environment configuration, see the Environment Variables section in Workflow Basics.</p>"},{"location":"advanced/#additional-topics","title":"Additional Topics","text":"<p>See also: - Step Identification - Timeout Configuration - Implementation Validation - Parallel Iteration with Foreach - Goal-Seeking Operations</p>"},{"location":"advanced/composition/","title":"Workflow Composition","text":"<p>Prodigy enables building complex workflows from reusable components through imports, inheritance, templates, and parameters.</p>"},{"location":"advanced/composition/#overview","title":"Overview","text":"<p>Workflow composition features: - Imports: Include external workflow definitions - Inheritance: Extend base workflows with <code>extends</code> - Templates: Reusable workflow templates with parameters - Parameters: Type-safe workflow parameterization - Sub-workflows: Nested workflow execution with result passing</p> <pre><code>graph TD\n    Base[Base Workflow&lt;br/&gt;base-ci.yml] --&gt; Child[Child Workflow&lt;br/&gt;specialized-ci.yml]\n\n    Import1[Common Steps&lt;br/&gt;common/ci-steps.yml] --&gt; Main[Main Workflow&lt;br/&gt;main.yml]\n    Import2[Deploy Steps&lt;br/&gt;common/deploy.yml] --&gt; Main\n\n    Template[Template&lt;br/&gt;rust-ci-template] --&gt; Instance1[Instance 1&lt;br/&gt;project-a-ci]\n    Template --&gt; Instance2[Instance 2&lt;br/&gt;project-b-ci]\n\n    Main --&gt; Sub1[Sub-workflow&lt;br/&gt;build]\n    Main --&gt; Sub2[Sub-workflow&lt;br/&gt;deploy]\n\n    style Base fill:#e1f5ff\n    style Template fill:#fff3e0\n    style Main fill:#f3e5f5\n    style Sub1 fill:#e8f5e9\n    style Sub2 fill:#e8f5e9</code></pre> <p>Figure: Workflow composition architecture showing imports, inheritance, templates, and sub-workflows.</p>"},{"location":"advanced/composition/#imports","title":"Imports","text":"<p>Import workflow definitions from external files:</p> <p>Best Practice</p> <p>Use imports to share common steps across multiple workflows. This reduces duplication and ensures consistency across your CI/CD pipelines.</p> <pre><code># main-workflow.yml\nname: main-workflow\n\nimports:\n  - path: \"common/ci-steps.yml\"    # (1)!\n    alias: ci                        # (2)!\n  - path: \"common/deploy-steps.yml\"\n    alias: deploy\n\n- shell: \"cargo build\"\n- workflow: ci.test-suite           # (3)!\n- workflow: deploy.to-staging       # (4)!\n\n1. Path relative to workflow file location\n2. Alias for referencing imported workflows\n3. Execute imported workflow using alias\n4. Access nested workflows with dot notation\n</code></pre>"},{"location":"advanced/composition/#import-syntax","title":"Import Syntax","text":"<pre><code>imports:\n  - path: \"relative/path/to/workflow.yml\"\n    alias: workflow_name  # Optional alias for referencing\n</code></pre>"},{"location":"advanced/composition/#import-caching","title":"Import Caching","text":"<p>Imported workflows are cached in memory to avoid reloading the same file multiple times during composition. This improves performance when multiple workflows reference the same imports.</p> <pre><code># Source: src/cook/workflow/composition/composer.rs:661-698\n# Both workflows below share the same cached import\nname: workflow-1\nimports:\n  - path: \"common/steps.yml\"\n    alias: common\n\n---\nname: workflow-2\nimports:\n  - path: \"common/steps.yml\"  # Loaded from cache\n    alias: common\n</code></pre>"},{"location":"advanced/composition/#inheritance","title":"Inheritance","text":"<p>Extend base workflows to customize behavior:</p> <pre><code># base-workflow.yml\nname: base-ci\nenv:\n  RUST_BACKTRACE: \"1\"\n\n- shell: \"cargo build\"\n- shell: \"cargo test\"\n\n---\n# specialized-workflow.yml\nname: specialized-ci\nextends: \"base-workflow.yml\"\n\nenv:\n  EXTRA_FLAG: \"true\"\n\n# Adds additional steps to base workflow\n- shell: \"cargo clippy\"\n- shell: \"cargo doc\"\n</code></pre>"},{"location":"advanced/composition/#override-behavior","title":"Override Behavior","text":"<ul> <li>Environment variables are merged (child overrides parent)</li> <li>Steps from parent executed first</li> <li>Child can override specific fields</li> </ul>"},{"location":"advanced/composition/#base-workflow-resolution","title":"Base Workflow Resolution","text":"<p>When using <code>extends</code>, base workflows are searched in priority order:</p> <pre><code># Source: src/cook/workflow/composition/composer.rs:625-642\n# Search order:\n# 1. bases/{name}.yml\n# 2. templates/{name}.yml\n# 3. workflows/{name}.yml\n# 4. {name}.yml (current directory)\n\nname: my-workflow\nextends: \"common-steps\"  # Searches in order above\n</code></pre> <p>This allows organizing base workflows in dedicated directories while maintaining backward compatibility with workflows in the project root.</p> <pre><code>flowchart TD\n    Start[extends: common-steps] --&gt; Check1{bases/&lt;br/&gt;common-steps.yml?}\n    Check1 --&gt;|Found| Load1[Load Base Workflow]\n    Check1 --&gt;|Not Found| Check2{templates/&lt;br/&gt;common-steps.yml?}\n\n    Check2 --&gt;|Found| Load2[Load Base Workflow]\n    Check2 --&gt;|Not Found| Check3{workflows/&lt;br/&gt;common-steps.yml?}\n\n    Check3 --&gt;|Found| Load3[Load Base Workflow]\n    Check3 --&gt;|Not Found| Check4{common-steps.yml&lt;br/&gt;current dir?}\n\n    Check4 --&gt;|Found| Load4[Load Base Workflow]\n    Check4 --&gt;|Not Found| Error[Error: Base Not Found]\n\n    Load1 --&gt; Merge[Merge with Child]\n    Load2 --&gt; Merge\n    Load3 --&gt; Merge\n    Load4 --&gt; Merge\n\n    Merge --&gt; Result[Composed Workflow]\n\n    style Check1 fill:#e1f5ff\n    style Check2 fill:#e1f5ff\n    style Check3 fill:#e1f5ff\n    style Check4 fill:#e1f5ff\n    style Error fill:#ffebee\n    style Result fill:#e8f5e9</code></pre> <p>Figure: Base workflow resolution flow showing priority search order.</p>"},{"location":"advanced/composition/#templates","title":"Templates","text":"<p>Create reusable workflow templates:</p> <pre><code># Source: workflows/example-template.yml\n# template: rust-ci-template\nname: rust-ci\ndescription: \"Standard Rust CI workflow\"\n\nparameters:\n  target:\n    type: string\n    required: true\n    description: \"Build target\"\n  coverage:\n    type: boolean\n    default: false\n    description: \"Run coverage analysis\"\n\n- shell: \"cargo build --target ${target}\"\n- shell: \"cargo test --target ${target}\"\n\n- shell: \"cargo tarpaulin\"\n  when: \"${coverage} == true\"\n</code></pre> <p>Template Detection</p> <p>Workflow files using composition features (template, imports, extends, workflows, or parameters keywords) are automatically detected by Prodigy during workflow parsing via the <code>is_composable_workflow()</code> function and composed before execution.</p> <p>Source: <code>src/cook/workflow/composer_integration.rs:44-50</code></p>"},{"location":"advanced/composition/#using-templates","title":"Using Templates","text":"<pre><code>name: my-project-ci\ntemplate: \"rust-ci-template\"\n\nparameters:\n  target: \"x86_64-unknown-linux-gnu\"\n  coverage: true\n</code></pre>"},{"location":"advanced/composition/#template-storage","title":"Template Storage","text":"<p>Templates are searched in priority order:</p> <ol> <li>Global (<code>~/.prodigy/templates/</code>): Shared across all repositories</li> <li>Project-local (<code>.prodigy/templates/</code>): Repository-specific templates</li> <li>Legacy (<code>templates/</code>): Older project-local templates</li> </ol> <p>Automatic Directory Creation</p> <p>Template directories are automatically created by the template registry if they don't exist, so you can start using templates immediately.</p> <p>Source: <code>src/cook/workflow/composer_integration.rs:93-136</code></p> Global TemplatesProject TemplatesLegacy Location <pre><code># Shared across all projects\n~/.prodigy/templates/\n\u251c\u2500\u2500 rust-ci.yml\n\u251c\u2500\u2500 python-ci.yml\n\u2514\u2500\u2500 deploy-k8s.yml\n</code></pre> <pre><code># Repository-specific templates\n.prodigy/templates/\n\u251c\u2500\u2500 custom-ci.yml\n\u2514\u2500\u2500 integration-tests.yml\n</code></pre> <pre><code># Older project-local templates\ntemplates/\n\u2514\u2500\u2500 old-workflow.yml\n</code></pre>"},{"location":"advanced/composition/#parameters","title":"Parameters","text":"<p>Define workflow parameters with type checking:</p> <pre><code>parameters:\n  environment:\n    type: string\n    required: true\n    description: \"Deployment environment\"\n    validation: \"environment in ['dev', 'staging', 'prod']\"\n\n  replicas:\n    type: number\n    default: 3\n    description: \"Number of replicas\"\n\n  features:\n    type: array\n    default: []\n    description: \"Feature flags to enable\"\n\n  config:\n    type: object\n    required: false\n    description: \"Additional configuration\"\n</code></pre>"},{"location":"advanced/composition/#parameter-types","title":"Parameter Types","text":"<ul> <li>string: Text value</li> <li>number: Numeric value (integer or float)</li> <li>boolean: true/false</li> <li>array: List of values</li> <li>object: Nested key-value pairs</li> </ul>"},{"location":"advanced/composition/#parameter-validation","title":"Parameter Validation","text":"<p>Validation Expressions</p> <p>Parameter validation uses boolean expressions. If validation fails, the workflow will not execute. Always test validation rules with expected inputs.</p> <pre><code>parameters:\n  port:\n    type: number\n    validation: \"port &gt;= 1024 &amp;&amp; port &lt;= 65535\"  # (1)!\n\n  environment:\n    type: string\n    validation: \"environment in ['dev', 'staging', 'prod']\"  # (2)!\n\n1. Numeric range validation for port numbers\n2. String enum validation for allowed environments\n</code></pre>"},{"location":"advanced/composition/#using-parameters","title":"Using Parameters","text":"<p>Parameters can be used in commands, environment variables, and merge workflows:</p> <pre><code># Source: src/cook/workflow/composition/composer.rs:438-453\nparameters:\n  environment:\n    type: string\n    required: true\n  api_key:\n    type: string\n    required: true\n\n# In commands\n- shell: \"deploy.sh --env ${environment}\"\n- shell: \"kubectl apply -f k8s/${environment}/\"\n\n# In environment variables\nenv:\n  DEPLOY_ENV: \"${environment}\"\n  API_KEY: \"${api_key}\"\n\n# In merge workflows\nmerge:\n  commands:\n    - shell: \"git merge origin/${environment}-branch\"\n    - claude: \"/validate --env ${environment}\"\n</code></pre>"},{"location":"advanced/composition/#sub-workflows","title":"Sub-Workflows","text":"<p>Execute workflows within workflows:</p> <p>Modular Execution</p> <p>Sub-workflows provide modular execution with independent contexts. Use them to break complex workflows into manageable, reusable pieces.</p> <pre><code>name: main-workflow\n\nsub_workflows:\n  build:\n    name: build-subworkflow\n    - shell: \"cargo build --release\"\n    - shell: \"cargo test --release\"\n\n  deploy:\n    name: deploy-subworkflow\n    parameters:\n      environment: string               # (1)!\n    - shell: \"kubectl apply -f k8s/${environment}/\"\n\n# Execute sub-workflows\n- workflow: build                       # (2)!\n\n- workflow: deploy                      # (3)!\n  parameters:\n    environment: \"staging\"\n\n# Access sub-workflow results\n- shell: \"echo Build completed: ${build.success}\"  # (4)!\n\n1. Sub-workflow parameters define expected inputs\n2. Execute sub-workflow without parameters\n3. Pass parameters to parameterized sub-workflow\n4. Access sub-workflow results in parent workflow\n</code></pre>"},{"location":"advanced/composition/#sub-workflow-features","title":"Sub-Workflow Features","text":"<ul> <li>Independent execution contexts</li> <li>Parameter passing between workflows</li> <li>Result capture and access</li> <li>Conditional execution</li> </ul> <p>Implementation Status</p> <p>Sub-workflow execution is currently in development. The data structures and configuration parsing are complete, but the execution integration is still being finalized.</p> <p>Source: <code>src/cook/workflow/composition/sub_workflow.rs</code></p>"},{"location":"advanced/composition/#template-registry","title":"Template Registry","text":"<p>Manage reusable templates centrally:</p> <pre><code># Source: src/cli/args.rs:831-851, src/cli/router.rs:210-233\n\n# Register template\nprodigy template register ./templates/rust-ci.yml --name rust-ci\n\n# List available templates\nprodigy template list\n</code></pre> <p>Template Usage</p> <p>Templates are used by referencing them in workflow YAML files with the <code>template:</code> field, not via CLI flags. CLI-based parameter passing to templates is planned for a future release.</p>"},{"location":"advanced/composition/#registry-structure","title":"Registry Structure","text":"<pre><code>~/.prodigy/template-registry/\n\u251c\u2500\u2500 rust-ci/\n\u2502   \u251c\u2500\u2500 template.yml\n\u2502   \u2514\u2500\u2500 metadata.json\n\u251c\u2500\u2500 python-ci/\n\u2502   \u2514\u2500\u2500 template.yml\n\u2514\u2500\u2500 deploy/\n    \u2514\u2500\u2500 template.yml\n</code></pre>"},{"location":"advanced/composition/#template-metadata","title":"Template Metadata","text":"<p>Templates can include metadata:</p> <pre><code>name: rust-ci-template\nversion: \"1.0.0\"\ndescription: \"Standard Rust CI workflow with testing and linting\"\nauthor: \"team@example.com\"\ntags: [\"rust\", \"ci\", \"testing\"]\n\nparameters:\n  # ... parameter definitions\n</code></pre>"},{"location":"advanced/composition/#composition-in-action","title":"Composition in Action","text":"<p>Here's how the composition features work together in real workflows:</p> <pre><code>sequenceDiagram\n    participant User\n    participant Composer\n    participant Registry\n    participant Executor\n\n    User-&gt;&gt;Composer: Load workflow.yml\n    Composer-&gt;&gt;Registry: Resolve imports\n    Registry--&gt;&gt;Composer: Imported workflows\n    Composer-&gt;&gt;Registry: Resolve base (extends)\n    Registry--&gt;&gt;Composer: Base workflow\n    Composer-&gt;&gt;Composer: Merge base + child\n    Composer-&gt;&gt;Composer: Interpolate parameters\n    Composer-&gt;&gt;Composer: Compose sub-workflows\n    Composer--&gt;&gt;Executor: Composed workflow\n    Executor-&gt;&gt;Executor: Execute steps\n    Executor-&gt;&gt;Executor: Execute sub-workflows\n    Executor--&gt;&gt;User: Results\n\n    Note over Composer,Registry: Template caching&lt;br/&gt;optimization\n    Note over Executor: Sub-workflow&lt;br/&gt;isolation</code></pre> <p>Figure: Workflow composition execution sequence showing resolution, merging, and execution phases.</p>"},{"location":"advanced/composition/#examples","title":"Examples","text":""},{"location":"advanced/composition/#modular-ci-pipeline","title":"Modular CI Pipeline","text":"<pre><code># .prodigy/templates/common-ci.yml\nname: common-ci-steps\n\n- shell: \"git fetch origin\"\n- shell: \"git diff --name-only origin/main\"\n  capture_output: changed_files\n\n---\n# rust-ci.yml\nname: rust-project-ci\nextends: \".prodigy/templates/common-ci.yml\"\n\nimports:\n  - path: \".prodigy/templates/rust-lint.yml\"\n    alias: lint\n\n- shell: \"cargo build\"\n- workflow: lint.clippy\n- shell: \"cargo test\"\n</code></pre>"},{"location":"advanced/composition/#parameterized-deployment","title":"Parameterized Deployment","text":"<pre><code>name: deploy-workflow\n\nparameters:\n  environment:\n    type: string\n    required: true\n    validation: \"environment in ['dev', 'staging', 'prod']\"\n\n  image_tag:\n    type: string\n    required: true\n    description: \"Docker image tag to deploy\"\n\n  replicas:\n    type: number\n    default: 3\n    validation: \"replicas &gt;= 1 &amp;&amp; replicas &lt;= 20\"\n\nenv:\n  DEPLOY_ENV: \"${environment}\"\n  IMAGE_TAG: \"${image_tag}\"\n  REPLICAS: \"${replicas}\"\n\n- shell: \"kubectl set image deployment/app app=${IMAGE_TAG}\"\n- shell: \"kubectl scale deployment/app --replicas=${REPLICAS}\"\n- shell: \"kubectl rollout status deployment/app\"\n</code></pre>"},{"location":"advanced/composition/#template-with-sub-workflows","title":"Template with Sub-Workflows","text":"<pre><code>name: comprehensive-ci\ndescription: \"Full CI/CD pipeline with build, test, and deploy\"\n\nparameters:\n  deploy_enabled:\n    type: boolean\n    default: false\n\nsub_workflows:\n  build:\n    - shell: \"cargo build --release\"\n    - shell: \"docker build -t app:latest .\"\n\n  test:\n    - shell: \"cargo test\"\n    - shell: \"cargo clippy\"\n\n  deploy:\n    parameters:\n      environment: string\n    - shell: \"kubectl apply -f k8s/${environment}/\"\n\n# Main workflow\n- workflow: build\n- workflow: test\n\n- workflow: deploy\n  when: \"${deploy_enabled} == true\"\n  parameters:\n    environment: \"staging\"\n</code></pre>"},{"location":"advanced/composition/#see-also","title":"See Also","text":"<ul> <li>Workflow Structure - Basic workflow syntax</li> <li>Variables - Variable system for parameters</li> <li>Template System - Creating and using templates</li> </ul>"},{"location":"advanced/git-integration/","title":"Git Integration","text":"<p>Prodigy provides deep git integration with worktree isolation, automatic commit tracking, and customizable merge workflows.</p>"},{"location":"advanced/git-integration/#overview","title":"Overview","text":"<p>Git integration features: - Worktree isolation: Each session runs in isolated git worktree - Automatic commits: Track all changes with automatic git commits - Commit tracking: Full audit trail of modifications - Smart merging: Customizable merge workflows with validation - Branch tracking: Intelligent merge target detection</p>"},{"location":"advanced/git-integration/#worktree-management","title":"Worktree Management","text":"<p>Every Prodigy session executes in an isolated git worktree:</p> <pre><code>~/.prodigy/worktrees/{repo_name}/\n\u2514\u2500\u2500 session-{session_id}/   # Isolated worktree for this session\n</code></pre>"},{"location":"advanced/git-integration/#benefits","title":"Benefits","text":"<ul> <li>Isolation: Main repository remains untouched during execution</li> <li>Parallelism: Multiple sessions can run concurrently</li> <li>Safety: Failed workflows don't pollute main repo</li> <li>Debugging: Worktrees preserve full execution history</li> </ul>"},{"location":"advanced/git-integration/#worktree-lifecycle","title":"Worktree Lifecycle","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Created: Workflow Start\n    Created --&gt; Executing: Commands Run\n    Executing --&gt; Modified: Changes Made\n    Modified --&gt; Executing: More Commands\n    Modified --&gt; Completed: Workflow Done\n    Completed --&gt; MergePrompt: User Decision\n    MergePrompt --&gt; Merged: Approve Merge\n    MergePrompt --&gt; Preserved: Decline Merge\n    Merged --&gt; Cleaned: Cleanup Success\n    Merged --&gt; Orphaned: Cleanup Failed\n    Cleaned --&gt; [*]\n    Orphaned --&gt; Cleaned: Manual Cleanup\n    Preserved --&gt; [*]: Worktree Kept\n\n    note right of Modified\n        Automatic commits\n        Full audit trail\n    end note\n\n    note right of Orphaned\n        Registry created\n        Use clean-orphaned\n    end note</code></pre> <p>Figure: Worktree lifecycle showing creation, execution, merge decision, and cleanup states.</p> <ol> <li>Creation: Prodigy creates worktree when workflow starts</li> <li>Execution: All commands run in worktree context</li> <li>Changes: Modifications committed automatically</li> <li>Completion: User prompted to merge back to original branch</li> <li>Cleanup: Worktree removed after successful merge</li> </ol>"},{"location":"advanced/git-integration/#manual-worktree-management","title":"Manual Worktree Management","text":"<pre><code># List worktrees\nprodigy worktree ls\n\n# Clean completed worktrees\nprodigy worktree clean\n\n# Force cleanup\nprodigy worktree clean -f\n\n# Clean orphaned worktrees (failed cleanup)\nprodigy worktree clean-orphaned &lt;job_id&gt;\n</code></pre> <p>Worktree Hygiene</p> <p>Regularly clean up completed worktrees with <code>prodigy worktree clean</code> to free disk space. Each worktree is a full copy of your repository, so they can accumulate quickly during development.</p>"},{"location":"advanced/git-integration/#commit-tracking","title":"Commit Tracking","text":"<p>Prodigy automatically creates commits for trackable changes:</p>"},{"location":"advanced/git-integration/#automatic-commits","title":"Automatic Commits","text":"<pre><code># Source: workflows/implement.yml\n- claude: \"/implement-feature\"\n  commit_required: true  # Expect git commit from Claude\n\n- shell: \"cargo fmt\"\n  commit_required: true  # Create commit for formatting changes\n</code></pre> <p>When <code>commit_required: true</code> is set, Prodigy validates that a commit was created:</p> <ul> <li>Before execution: Captures HEAD commit SHA</li> <li>After execution: Compares HEAD to detect new commits</li> <li>Validation failure: Workflow fails with error \"No changes were committed by [command]\"</li> <li>Skip validation: Set <code>PRODIGY_NO_COMMIT_VALIDATION=true</code> to bypass (test mode only)</li> </ul> <p>Commit Validation</p> <p>If a command with <code>commit_required: true</code> doesn't create a commit, the workflow will fail. This prevents silent failures where expected changes weren't made. Use this for commands that should always modify files (implementations, formatting, etc.).</p> <pre><code>// Source: src/cook/workflow/executor.rs:819-824\n// Get HEAD before command execution if we need to verify commits\nlet head_before = if !execution_flags.skip_validation\n    &amp;&amp; step.commit_required\n    &amp;&amp; !execution_flags.test_mode\n{\n    Some(self.get_current_head(&amp;env.working_dir).await?)\n} else {\n    None\n};\n</code></pre>"},{"location":"advanced/git-integration/#commit-messages","title":"Commit Messages","text":"<p>Generated commit messages include: - Command that created the change - Workflow context - Session ID for traceability</p>"},{"location":"advanced/git-integration/#branch-tracking","title":"Branch Tracking","text":"<p>Prodigy tracks the original branch for intelligent merging:</p>"},{"location":"advanced/git-integration/#original-branch-detection","title":"Original Branch Detection","text":"<pre><code>flowchart TD\n    Start[Workflow Start] --&gt; Detect{Current&lt;br/&gt;Branch?}\n    Detect --&gt;|Named Branch| Capture[Capture Branch Name]\n    Detect --&gt;|Detached HEAD| Default[Use Default Branch]\n\n    Capture --&gt; Store[Store in Worktree State]\n    Default --&gt; Store\n\n    Store --&gt; Workflow[Execute Workflow]\n    Workflow --&gt; Complete[Workflow Complete]\n\n    Complete --&gt; MergeDecision{Merge Target&lt;br/&gt;Exists?}\n    MergeDecision --&gt;|Yes| MergeOriginal[Merge to Original Branch]\n    MergeDecision --&gt;|No| MergeDefault[Merge to Default Branch]\n\n    style Capture fill:#e8f5e9\n    style Default fill:#fff3e0\n    style MergeOriginal fill:#e1f5ff\n    style MergeDefault fill:#f3e5f5</code></pre> <p>Figure: Branch tracking logic showing how Prodigy determines merge target based on workflow start context.</p> <p>When creating a worktree: - Captures current branch name (e.g., <code>feature/ui-improvements</code>) - Falls back to default branch for detached HEAD - Stores in worktree state for session lifetime</p>"},{"location":"advanced/git-integration/#merge-target-logic","title":"Merge Target Logic","text":"<p>Merge back to the tracked original branch: <pre><code># User on feature/auth-refactor when starting workflow\n# Worktree merges back to feature/auth-refactor (not main!)\n</code></pre></p> <p>Smart Merge Targeting</p> <p>Prodigy always merges back to the branch you were on when you started the workflow, not to a hardcoded main/master branch. This makes it safe to use on feature branches, release branches, or any development branch.</p>"},{"location":"advanced/git-integration/#special-cases","title":"Special Cases","text":"<ul> <li>Feature branches: Merge back to exact feature branch</li> <li>Detached HEAD: Falls back to default branch (main/master)</li> <li>Deleted branch: Falls back to default branch if original deleted</li> <li>Branch rename: Uses branch name from worktree creation time</li> </ul>"},{"location":"advanced/git-integration/#merge-workflows","title":"Merge Workflows","text":"<p>Customize the merge process with validation and testing:</p> <pre><code># Source: workflows/implement.yml\nmerge:\n  commands:\n    - shell: \"git fetch origin\"\n    - shell: \"git merge origin/main\"  # (1)!\n    - shell: \"cargo test\"             # (2)!\n    - shell: \"cargo clippy\"           # (3)!\n    - claude: \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"  # (4)!\n  timeout: 600  # (5)!\n\n1. Sync worktree with latest main branch changes before merging\n2. Run full test suite to ensure nothing breaks\n3. Verify code quality and catch linting issues\n4. Execute the actual merge back to original branch\n5. Allow 10 minutes for merge operations (adjust for large test suites)\n</code></pre> <p>Always Pass Both Branch Variables</p> <p>Always use <code>${merge.source_branch}</code> and <code>${merge.target_branch}</code> when calling <code>/prodigy-merge-worktree</code>. This ensures merges go to the branch you started from, not a hardcoded main/master.</p>"},{"location":"advanced/git-integration/#merge-variables","title":"Merge Variables","text":"<p>Available in merge workflows: - <code>${merge.worktree}</code> - Worktree name being merged - <code>${merge.source_branch}</code> - Source branch (worktree branch) - <code>${merge.target_branch}</code> - Target branch (original branch) - <code>${merge.session_id}</code> - Session ID for correlation</p>"},{"location":"advanced/git-integration/#debugging-merge-operations","title":"Debugging Merge Operations","text":"<p>Control Claude output visibility during merge operations:</p> <pre><code># Default: Clean output, no Claude streaming\nprodigy run workflow.yml\n\n# Verbose: Show Claude JSON streaming output\nprodigy run workflow.yml -v\n\n# Force streaming output via environment variable\nPRODIGY_CLAUDE_CONSOLE_OUTPUT=true prodigy run workflow.yml\n</code></pre> <p>Debug Merge Failures</p> <p>Use <code>-v</code> flag to see real-time Claude interactions during merge operations. This shows tool invocations, decision-making, and helps diagnose merge conflicts or validation failures.</p>"},{"location":"advanced/git-integration/#example-pre-merge-validation","title":"Example: Pre-Merge Validation","text":"<pre><code>merge:\n  commands:\n    - shell: \"cargo build --release\"\n    - shell: \"cargo test --all\"\n    - shell: \"cargo fmt --check\"\n    - shell: \"cargo clippy -- -D warnings\"\n    - claude: \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"\n</code></pre>"},{"location":"advanced/git-integration/#example-conflict-resolution","title":"Example: Conflict Resolution","text":"<pre><code>merge:\n  commands:\n    - shell: \"git fetch origin\"\n    - shell: \"git merge origin/main --no-commit\"\n    - claude: \"/resolve-conflicts\"\n      on_failure:\n        shell: \"git merge --abort\"\n    - shell: \"git add -A\"\n    - shell: \"git commit -m 'Merge main and resolve conflicts'\"\n    - claude: \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"\n</code></pre>"},{"location":"advanced/git-integration/#mapreduce-worktree-isolation","title":"MapReduce Worktree Isolation","text":"<p>In MapReduce workflows, all phases execute in isolated worktrees:</p> <pre><code>graph TD\n    Original[Original Branch&lt;br/&gt;feature/my-feature] --&gt; Parent[Parent Worktree&lt;br/&gt;session-xxx]\n\n    Parent --&gt; Setup[Setup Phase&lt;br/&gt;Executes Here]\n    Setup --&gt; Map[Map Phase&lt;br/&gt;Spawn Agents]\n\n    Map --&gt; A1[Agent Worktree 1&lt;br/&gt;agent-1-item-1]\n    Map --&gt; A2[Agent Worktree 2&lt;br/&gt;agent-2-item-2]\n    Map --&gt; AN[Agent Worktree N&lt;br/&gt;agent-n-item-n]\n\n    A1 --&gt;|Merge Changes| Parent\n    A2 --&gt;|Merge Changes| Parent\n    AN --&gt;|Merge Changes| Parent\n\n    Parent --&gt; Reduce[Reduce Phase&lt;br/&gt;Executes Here]\n    Reduce --&gt; Prompt{User&lt;br/&gt;Approval}\n\n    Prompt --&gt;|Approve| Merge[Merge to Original]\n    Prompt --&gt;|Decline| Keep[Keep Worktree]\n\n    Merge --&gt; Original\n\n    style Original fill:#e8f5e9\n    style Parent fill:#e1f5ff\n    style A1 fill:#fff3e0\n    style A2 fill:#fff3e0\n    style AN fill:#fff3e0\n    style Setup fill:#f3e5f5\n    style Reduce fill:#f3e5f5</code></pre> <p>Figure: MapReduce worktree hierarchy showing parent worktree for setup/reduce and child worktrees for parallel map agents.</p>"},{"location":"advanced/git-integration/#isolation-guarantees","title":"Isolation Guarantees","text":"<ol> <li>Setup phase: Executes in parent worktree</li> <li>Map phase: Each agent runs in child worktree</li> <li>Reduce phase: Executes in parent worktree</li> <li>Final merge: Parent worktree merges back to original branch</li> </ol> <p>Complete Isolation</p> <p>Your main repository remains completely untouched during MapReduce execution. All setup, map, and reduce operations happen in isolated worktrees. Changes only return to your original branch when you explicitly approve the merge.</p>"},{"location":"advanced/git-integration/#verification","title":"Verification","text":"<p>Verify main repository is clean after MapReduce:</p> <pre><code># Check main repo (should be clean)\ngit status\n\n# Check worktree has changes\ncd ~/.prodigy/worktrees/{repo}/session-xxx/\ngit status\ngit log\n</code></pre>"},{"location":"advanced/git-integration/#orphaned-worktree-recovery","title":"Orphaned Worktree Recovery","text":"<p>Handle cleanup failures gracefully:</p>"},{"location":"advanced/git-integration/#orphaned-worktree-registry","title":"Orphaned Worktree Registry","text":"<p>When cleanup fails, worktree path is registered: <pre><code>~/.prodigy/orphaned_worktrees/{repo_name}/{job_id}.json\n</code></pre></p>"},{"location":"advanced/git-integration/#cleanup-command","title":"Cleanup Command","text":"<pre><code># List orphaned worktrees\nprodigy worktree clean-orphaned &lt;job_id&gt;\n\n# Dry run\nprodigy worktree clean-orphaned &lt;job_id&gt; --dry-run\n\n# Force cleanup\nprodigy worktree clean-orphaned &lt;job_id&gt; --force\n</code></pre>"},{"location":"advanced/git-integration/#common-cleanup-issues","title":"Common Cleanup Issues","text":"<ul> <li>Permission denied: Check directory permissions</li> <li>Disk full: Free up space before retry</li> <li>Directory busy: Close editors/processes using worktree</li> <li>Git locks: Wait for concurrent git operations to complete</li> </ul> <p>Orphaned Worktrees</p> <p>If cleanup fails, worktrees become \"orphaned\" but agent work is still preserved. Use <code>prodigy worktree clean-orphaned &lt;job_id&gt;</code> to safely remove them after resolving the underlying issue (permissions, disk space, etc.).</p>"},{"location":"advanced/git-integration/#examples","title":"Examples","text":""},{"location":"advanced/git-integration/#feature-branch-workflow","title":"Feature Branch Workflow","text":"<pre><code># User on feature/authentication\nname: implement-auth\n\n- claude: \"/implement-auth-module\"\n  commit_required: true\n\n- shell: \"cargo test\"\n  on_failure:\n    claude: \"/fix-auth-tests\"\n\n# Merge back to feature/authentication (not main!)\nmerge:\n  commands:\n    - shell: \"cargo test --all\"\n    - claude: \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"\n</code></pre>"},{"location":"advanced/git-integration/#multi-stage-ci-with-merge-validation","title":"Multi-Stage CI with Merge Validation","text":"<pre><code>name: comprehensive-ci\n\n- shell: \"cargo build\"\n  commit_required: true\n\n- shell: \"cargo test\"\n  on_failure:\n    claude: \"/debug-test-failure\"\n    commit_required: true\n\n- shell: \"cargo clippy\"\n  commit_required: true\n\nmerge:\n  commands:\n    - shell: \"git fetch origin\"\n    - shell: \"git merge origin/${merge.target_branch}\"\n      on_failure:\n        claude: \"/resolve-merge-conflicts\"\n    - shell: \"cargo test --release\"\n    - shell: \"cargo doc --no-deps\"\n    - claude: \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"\n  timeout: 900\n</code></pre>"},{"location":"advanced/git-integration/#see-also","title":"See Also","text":"<ul> <li>Worktree Management - Isolation and lifecycle management</li> <li>Branch Tracking - Original branch detection</li> <li>Session Management - Session lifecycle and worktree coordination</li> <li>MapReduce Workflows - Parallel execution with worktrees</li> </ul>"},{"location":"advanced/goal-seeking-operations/","title":"Goal-Seeking Operations","text":""},{"location":"advanced/goal-seeking-operations/#goal-seeking-operations","title":"Goal-Seeking Operations","text":"<p>Iteratively refine implementations until they meet validation criteria. Goal-seeking addresses the fundamental challenge that AI often fails on first attempts but succeeds with validation feedback and retry mechanisms.</p>"},{"location":"advanced/goal-seeking-operations/#basic-goal-seek","title":"Basic Goal Seek","text":"<p>Define a goal and validation command using either <code>shell</code> or <code>claude</code>:</p> <p>Using Shell Command:</p> <pre><code>- goal_seek:\n    goal: \"All tests pass\"\n    shell: \"cargo fix\"\n    validate: \"cargo test 2&gt;&amp;1 | grep -q 'test result: ok' &amp;&amp; echo 'score: 100' || echo 'score: 0'\"\n    threshold: 100\n</code></pre> <p>Using Claude Command:</p> <pre><code>- goal_seek:\n    goal: \"Code quality improved\"\n    claude: \"/fix-issues\"\n    validate: \"quality-check.sh\"  # Must output score in parseable format\n    threshold: 95\n</code></pre> <p>Important: Validation commands must output a score in one of these formats: - <code>score: 85</code> (simple text) - <code>85%</code> (percentage) - <code>85/100</code> (ratio) - <code>85 out of 100</code> (natural language) - <code>{\"score\": 85, \"gaps\": [\"list of issues\"]}</code> (JSON with optional gaps)</p> <p>Source: Score extraction patterns defined in src/cook/goal_seek/validator.rs:37-62</p> <p>The goal-seeking operation will: 1. Run the command (shell or claude) 2. Run the validation and extract numeric score (0-100) 3. Pass validation context to next attempt via environment variables 4. Retry if validation threshold not met 5. Stop when goal achieved, max attempts reached, or convergence detected</p>"},{"location":"advanced/goal-seeking-operations/#advanced-goal-seek-configuration","title":"Advanced Goal Seek Configuration","text":"<p>Control iteration behavior and convergence detection:</p> <pre><code>- goal_seek:\n    goal: \"Code passes all quality checks\"\n    shell: \"auto-fix.sh\"\n    validate: \"quality-check.sh\"\n    threshold: 95\n    max_attempts: 5\n    timeout_seconds: 300\n    fail_on_incomplete: true\n</code></pre> <p>Configuration Fields (from src/cook/goal_seek/mod.rs:13-27): - <code>goal</code>: Description of what you're trying to achieve - <code>shell</code> or <code>claude</code>: The command to execute (use one or the other) - <code>validate</code>: Shell command to validate progress (must output score 0-100) - <code>threshold</code>: Minimum score to consider goal achieved (0-100) - <code>max_attempts</code>: Maximum number of iterations (default: 3) - <code>timeout_seconds</code>: Optional timeout for the entire goal-seeking operation - <code>fail_on_incomplete</code>: Fail workflow if goal not achieved (default: true)</p>"},{"location":"advanced/goal-seeking-operations/#automatic-convergence-detection","title":"Automatic Convergence Detection","text":"<p>Goal-seeking automatically detects when no progress is being made and stops early to prevent wasted iterations.</p> <p>Convergence Criteria (src/cook/goal_seek/engine.rs:179-197): - Triggers after 3+ attempts - Last 3 scores are within 2 points of each other - Returns <code>Converged</code> result with reason</p> <p>Example: If attempts produce scores [82, 83, 82], the system detects convergence and stops instead of continuing to max_attempts.</p> <p>Benefits: - Saves time and computational resources - Prevents infinite loops when stuck - Provides clear feedback about why iteration stopped</p> <pre><code># Example: Convergence will stop early if no improvement\n- goal_seek:\n    goal: \"Improve test coverage\"\n    claude: \"/add-tests\"\n    validate: \"coverage-check.sh\"  # Returns score: 0-100\n    threshold: 90\n    max_attempts: 10  # May stop earlier if converged\n</code></pre>"},{"location":"advanced/goal-seeking-operations/#validation-integration","title":"Validation Integration","text":"<p>Goal-seeking integrates with the validation system through flexible score extraction.</p> <p>Validation Output Formats (src/cook/goal_seek/validator.rs:65-96):</p> <p>1. JSON Format with Score: <pre><code>{\"score\": 85}\n</code></pre></p> <p>2. JSON Format with Score and Gaps: <pre><code>{\n  \"score\": 75,\n  \"gaps\": [\n    \"Missing test for user authentication\",\n    \"No error handling tests\",\n    \"Coverage below 80% in auth module\"\n  ]\n}\n</code></pre></p> <p>3. Simple Text Formats: - <code>score: 85</code> - <code>85%</code> - <code>85/100</code> - <code>85 out of 100</code></p> <p>Example Validation Command: <pre><code>- goal_seek:\n    goal: \"100% test coverage\"\n    claude: \"/add-tests\"\n    validate: |\n      # Extract coverage percentage and format as score\n      cargo tarpaulin --print-summary 2&gt;/dev/null | \\\n        grep 'Coverage' | \\\n        sed 's/.*Coverage=\\([0-9]*\\).*/score: \\1/'\n    threshold: 100\n    timeout_seconds: 600\n    max_attempts: 10\n</code></pre></p> <p>Real-World Example (from workflows/goal-seeking-examples.yml:6-14): <pre><code>- goal_seek:\n    goal: \"Achieve 90% test coverage\"\n    claude: \"/prodigy-coverage --improve\"\n    validate: \"cargo tarpaulin --print-summary 2&gt;/dev/null | grep 'Coverage' | sed 's/.*Coverage=\\\\([0-9]*\\\\).*/score: \\\\1/'\"\n    threshold: 90\n    max_attempts: 5\n    timeout_seconds: 300\n    fail_on_incomplete: true\n  commit_required: true\n</code></pre></p> <p>Validation Context Environment Variables (src/cook/goal_seek/engine.rs:128-153):</p> <p>After the first attempt, refinement commands receive validation feedback via environment variables: - <code>PRODIGY_VALIDATION_SCORE</code>: Previous attempt's numeric score (0-100) - <code>PRODIGY_VALIDATION_OUTPUT</code>: Full text output from validation command - <code>PRODIGY_VALIDATION_GAPS</code>: Parsed JSON gaps array (if validation returned JSON with gaps field)</p> <p>This allows Claude or shell scripts to understand what failed and make targeted improvements.</p>"},{"location":"advanced/goal-seeking-operations/#progressive-refinement","title":"Progressive Refinement","text":"<p>Use goal-seeking for incremental improvements through multi-stage workflows.</p> <p>Multi-Stage Example (from workflows/goal-seeking-examples.yml:99-129):</p> <pre><code># Stage 1: Implement feature\n- name: \"Complete feature implementation\"\n  goal_seek:\n    goal: \"Implement user profile feature\"\n    claude: \"/implement-feature user-profile\"\n    validate: \"test -f src/features/user_profile.rs &amp;&amp; echo 'score: 100' || echo 'score: 0'\"\n    threshold: 100\n    max_attempts: 2\n\n# Stage 2: Add comprehensive tests\n- name: \"Add tests for user profile\"\n  goal_seek:\n    goal: \"Add tests for user profile feature\"\n    claude: \"/add-tests src/features/user_profile.rs\"\n    validate: |\n      test_count=$(grep -c \"#\\[test\\]\" src/features/user_profile.rs || echo 0)\n      if [ \"$test_count\" -ge 5 ]; then\n        echo \"score: 100\"\n      else\n        score=$((test_count * 20))\n        echo \"score: $score\"\n      fi\n    threshold: 100\n    max_attempts: 3\n\n# Stage 3: Ensure tests pass\n- name: \"Make all tests pass\"\n  goal_seek:\n    goal: \"Make all user profile tests pass\"\n    claude: \"/fix-tests user_profile\"\n    validate: \"cargo test user_profile 2&gt;&amp;1 | grep -q 'test result: ok' &amp;&amp; echo 'score: 100' || echo 'score: 0'\"\n    threshold: 100\n    max_attempts: 4\n    fail_on_incomplete: true\n</code></pre> <p>Progressive Quality Improvement:</p> <pre><code># First pass: Get to 80% quality\n- goal_seek:\n    goal: \"Basic quality standards met\"\n    shell: \"quick-fix.sh\"\n    validate: \"quality-check.sh\"  # Outputs score: 0-100\n    threshold: 80\n    max_attempts: 3\n\n# Second pass: Polish to 100%\n- goal_seek:\n    goal: \"Perfect quality\"\n    claude: \"/polish-code\"\n    validate: \"quality-check.sh\"  # Same validator, higher threshold\n    threshold: 100\n    max_attempts: 5\n    timeout_seconds: 600\n</code></pre> <p>Benefits of Progressive Refinement: - Each stage has clear, achievable goals - Earlier stages provide foundation for later ones - Context from previous attempts helps Claude make targeted improvements - Convergence detection prevents wasted effort at each stage</p>"},{"location":"advanced/goal-seeking-operations/#error-handling","title":"Error Handling","text":"<p>Goal-seeking operations can terminate in several ways (src/cook/goal_seek/mod.rs:44-76):</p> <p>Result Types: - <code>Success</code>: Threshold reached within max_attempts - <code>MaxAttemptsReached</code>: Exhausted retries without reaching threshold - <code>Timeout</code>: Time limit exceeded - <code>Converged</code>: No improvement detected in last 3 attempts - <code>Failed</code>: Command execution error</p> <p>Handling Incomplete Goals:</p> <pre><code>- goal_seek:\n    goal: \"All tests pass\"\n    shell: \"cargo fix\"\n    validate: \"cargo test 2&gt;&amp;1 | grep -q 'test result: ok' &amp;&amp; echo 'score: 100' || echo 'score: 0'\"\n    threshold: 100\n    max_attempts: 3\n    fail_on_incomplete: false  # Continue workflow even if goal not achieved\n\n# Next step proceeds regardless of goal-seek result\n- shell: \"echo 'Continuing workflow despite incomplete goal'\"\n</code></pre> <p>Using Goal-Seeking in on_failure Handlers (from workflows/goal-seeking-examples.yml:76-95):</p> <pre><code>- shell: \"cargo test\"\n  on_failure:\n    goal_seek:\n      goal: \"Fix all failing tests\"\n      claude: \"/debug-test-failures\"\n      validate: |\n        cargo test 2&gt;&amp;1 | grep -q \"test result: ok\" &amp;&amp; echo \"score: 100\" || {\n          passed=$(cargo test 2&gt;&amp;1 | grep -oP '\\d+(?= passed)' | head -1)\n          failed=$(cargo test 2&gt;&amp;1 | grep -oP '\\d+(?= failed)' | head -1)\n          total=$((passed + failed))\n          if [ \"$total\" -gt 0 ]; then\n            score=$((passed * 100 / total))\n            echo \"score: $score\"\n          else\n            echo \"score: 0\"\n          fi\n        }\n      threshold: 100\n      max_attempts: 3\n      fail_on_incomplete: true\n</code></pre> <p>Note: Goal-seeking operations return <code>GoalSeekResult</code> variants, not shell exit codes. The workflow executor converts these to step results based on <code>fail_on_incomplete</code> configuration (src/cook/workflow/executor/commands.rs).</p>"},{"location":"advanced/goal-seeking-operations/#good-explicit-score-output","title":"Good: Explicit score output","text":"<p>validate: \"cargo test 2&gt;&amp;1 | grep -q 'ok' &amp;&amp; echo 'score: 100' || echo 'score: 0'\"</p>"},{"location":"advanced/goal-seeking-operations/#better-proportional-score-based-on-actual-results","title":"Better: Proportional score based on actual results","text":"<p>validate: |   passed=$(cargo test 2&gt;&amp;1 | grep -oP '\\d+(?= passed)')   total=$(cargo test 2&gt;&amp;1 | grep -oP '\\d+ tests')   score=$((passed * 100 / total))   echo \"score: $score\"</p>"},{"location":"advanced/goal-seeking-operations/#best-json-with-actionable-feedback","title":"Best: JSON with actionable feedback","text":"<p>validate: |   result=$(cargo test --format json)   passed=$(echo \"$result\" | jq '.passed')   total=$(echo \"$result\" | jq '.total')   score=$((passed * 100 / total))   gaps=$(echo \"$result\" | jq '.failures')   echo \"{\\\"score\\\": $score, \\\"gaps\\\": $gaps}\" ```</p> <p>Limit Iterations: - Set reasonable max_attempts (3-10) - Use timeout_seconds for long-running operations (per-operation timeout) - Consider fail_on_incomplete based on criticality - Trust convergence detection to prevent wasted attempts</p> <p>Leverage Validation Context: - Claude commands can access previous scores via <code>PRODIGY_VALIDATION_SCORE</code> - Use gaps information (<code>PRODIGY_VALIDATION_GAPS</code>) for targeted fixes - Design validation to provide actionable feedback, not just scores</p> <p>Troubleshooting Common Issues:</p> Issue Cause Solution Validation always returns 0 Score not in parseable format Test validation command, ensure it outputs <code>score: N</code> format Convergence happens too early Score variance within 2 points Ensure validation is precise enough to detect small improvements Timeout vs MaxAttemptsReached timeout_seconds too short Increase timeout or reduce max_attempts Context not available to Claude Environment variables not passed Verify command executor supports env vars (src/cook/goal_seek/engine.rs:128-153) <p>See Also: - Implementation Validation - Using goal-seeking for spec validation - Error Handling - Workflow-level error handling - Technical documentation: docs/goal-seeking.md in repository</p>"},{"location":"advanced/implementation-validation/","title":"Implementation Validation","text":""},{"location":"advanced/implementation-validation/#implementation-validation","title":"Implementation Validation","text":"<p>Validate that implementations meet requirements using the <code>validate</code> field.</p>"},{"location":"advanced/implementation-validation/#basic-validation","title":"Basic Validation","text":"<p>Run validation commands after a step completes:</p> <pre><code>- claude: \"/implement-feature\"\n  validate:\n    shell: \"cargo test\"\n    threshold: 100  # Require 100% completion (default)\n</code></pre> <p>Note: The <code>threshold</code> field defaults to 100 if not specified, requiring full implementation completion.</p> <p>Source: <code>src/cook/workflow/validation.rs:280-282</code></p>"},{"location":"advanced/implementation-validation/#validation-with-claude","title":"Validation with Claude","text":"<p>Use Claude to validate implementation quality:</p> <pre><code>- shell: \"generate-code.sh\"\n  validate:\n    claude: \"/verify-implementation\"\n    threshold: 95\n</code></pre>"},{"location":"advanced/implementation-validation/#multi-step-validation","title":"Multi-Step Validation","text":"<p>Run multiple validation commands in sequence using the <code>commands</code> array:</p> <pre><code>- claude: \"/refactor\"\n  validate:\n    commands:\n      - shell: \"cargo test\"\n      - shell: \"cargo clippy\"\n      - shell: \"cargo fmt --check\"\n    threshold: 100\n</code></pre> <p>Convenience Array Syntax: For simple cases, you can use an array format directly:</p> <pre><code>- claude: \"/refactor\"\n  validate:\n    - shell: \"cargo test\"\n    - shell: \"cargo clippy\"\n    - shell: \"cargo fmt --check\"\n</code></pre>"},{"location":"advanced/implementation-validation/#validation-with-result-files","title":"Validation with Result Files","text":"<p>Read validation results from a file instead of stdout:</p> <pre><code>- claude: \"/implement-feature\"\n  validate:\n    shell: \"run-validator.sh\"\n    result_file: \"validation-results.json\"\n    threshold: 95\n</code></pre> <p>When to Use result_file:</p> <p>The <code>result_file</code> option is useful when you need to separate validation output from command logs:</p> <ul> <li>Complex JSON Output: Validation produces structured JSON that shouldn't be mixed with logs</li> <li>Separate Concerns: Keep validation results separate from command stdout/stderr</li> <li>Additional Logging: Validation command produces diagnostic output alongside results</li> <li>Debugging: Preserve validation output in a file for later inspection</li> </ul> <p>The file must contain valid JSON matching the ValidationResult schema. When the validation command completes, Prodigy reads the specified file and parses it as JSON. If the file doesn't exist or contains invalid JSON, the validation fails.</p> <p>Source: <code>src/cook/workflow/executor/validation.rs:700-715</code></p>"},{"location":"advanced/implementation-validation/#advanced-result-files-with-commands-array","title":"Advanced: Result Files with Commands Array","text":"<p>You can use <code>result_file</code> with the <code>commands</code> array for multi-step validation where the final result is written to a file:</p> <pre><code>- claude: \"/implement-spec $ARG\"\n  validate:\n    commands:\n      - claude: \"/prodigy-validate-spec $ARG --output .prodigy/validation-result.json\"\n    result_file: \".prodigy/validation-result.json\"\n    threshold: 100\n    on_incomplete:\n      claude: \"/prodigy-complete-spec $ARG --gaps ${validation.gaps}\"\n      max_attempts: 5\n      commit_required: true\n</code></pre> <p>In this pattern, the validation command writes its results to a JSON file, and Prodigy reads that file after all commands complete.</p> <p>Source: Real-world example from <code>workflows/implement.yml:6-16</code></p>"},{"location":"advanced/implementation-validation/#handling-incomplete-implementations","title":"Handling Incomplete Implementations","text":"<p>Automatically remediate when validation fails to meet the threshold.</p> <p>Convenience Array Syntax - For simple remediation workflows:</p> <pre><code>- claude: \"/implement-spec\"\n  validate:\n    shell: \"check-completeness.sh\"\n    threshold: 100\n    on_incomplete:\n      - claude: \"/fill-gaps\"\n      - shell: \"cargo fmt\"\n</code></pre> <p>Verbose Configuration - For complex cases requiring additional control:</p> <pre><code>- claude: \"/implement-spec\"\n  validate:\n    shell: \"check-completeness.sh\"\n    threshold: 100\n    on_incomplete:\n      claude: \"/fill-gaps\"\n      max_attempts: 2          # Default: 2 (not 3)\n      fail_workflow: true      # Default: true\n      commit_required: false   # Default: false\n</code></pre> <p>Default Values: - <code>max_attempts</code>: 2 (maximum remediation attempts before giving up) - <code>fail_workflow</code>: true (workflow fails if remediation doesn't reach threshold) - <code>commit_required</code>: false (remediation command doesn't need to create a commit)</p> <p>Source: <code>src/cook/workflow/validation.rs:284-289</code></p> <p>The <code>on_incomplete</code> configuration supports: - <code>claude</code>: Claude command to execute for gap-filling - <code>shell</code>: Shell command to execute for gap-filling - <code>commands</code>: Array of commands to execute in sequence - <code>max_attempts</code>: Maximum remediation attempts (default: 2) - <code>fail_workflow</code>: Whether to fail workflow if remediation fails (default: true) - <code>commit_required</code>: Whether remediation command should create a commit (default: false)</p> <p>Source: <code>src/cook/workflow/validation.rs:123-152</code></p>"},{"location":"advanced/implementation-validation/#timeout-configuration","title":"Timeout Configuration","text":"<p>Set a timeout for validation commands to prevent hanging:</p> <pre><code>- claude: \"/implement-feature\"\n  validate:\n    shell: \"long-running-test.sh\"\n    threshold: 100\n    timeout: 300  # 5 minutes timeout\n</code></pre> <p>The <code>timeout</code> field specifies the maximum number of seconds the validation command can run. If the command exceeds this time, it's terminated and the validation fails.</p> <p>Source: <code>src/cook/workflow/validation.rs:37-39</code></p>"},{"location":"advanced/implementation-validation/#validationresult-schema","title":"ValidationResult Schema","text":"<p>When using <code>result_file</code>, the JSON file must match this structure:</p> <pre><code>{\n  \"completion_percentage\": 95.5,\n  \"status\": \"incomplete\",\n  \"implemented\": [\n    \"Feature A is fully implemented\",\n    \"Feature B includes unit tests\"\n  ],\n  \"missing\": [\n    \"Feature C lacks error handling\",\n    \"Feature D needs integration tests\"\n  ],\n  \"gaps\": {\n    \"error_handling\": {\n      \"description\": \"Missing error handling in parser\",\n      \"location\": \"src/parser.rs:45\",\n      \"severity\": \"high\",\n      \"suggested_fix\": \"Add Result&lt;T, E&gt; return type and handle parse errors\"\n    }\n  }\n}\n</code></pre> <p>Fields: - <code>completion_percentage</code>: Float (0-100) indicating implementation completeness - <code>status</code>: Enum - <code>\"complete\"</code>, <code>\"incomplete\"</code>, <code>\"failed\"</code>, or <code>\"skipped\"</code> - <code>implemented</code>: Array of strings describing completed requirements - <code>missing</code>: Array of strings describing incomplete requirements - <code>gaps</code>: Object mapping gap IDs to GapDetail objects with description, location, severity, and suggested_fix</p> <p>Source: <code>src/cook/workflow/validation.rs:216-239</code></p>"},{"location":"advanced/implementation-validation/#validation-patterns","title":"Validation Patterns","text":"<p>Progressive Validation - Validate in stages:</p> <pre><code>- claude: \"/implement-feature\"\n  validate:\n    commands:\n      - shell: \"cargo check\"     # Fast syntax check first\n      - shell: \"cargo test\"      # Then run tests\n      - shell: \"cargo bench\"     # Finally benchmarks\n    threshold: 100\n    timeout: 600  # 10 minute timeout for all commands\n    on_incomplete:\n      - claude: \"/analyze-failures\"\n      - claude: \"/fix-issues\"\n</code></pre> <p>Conditional Validation - Validate based on previous results:</p> <pre><code>- claude: \"/optimize-code\"\n  id: \"optimization\"\n  validate:\n    shell: \"benchmark.sh\"\n    threshold: 90\n\n- shell: \"verify-performance.sh\"\n  when: \"${optimization.success}\"\n  validate:\n    shell: \"stress-test.sh\"\n    threshold: 100\n</code></pre> <p>Complex Multi-Step Validation with Result Files - Real-world pattern from Prodigy's debtmap workflow:</p> <pre><code>- claude: \"/implement-changes\"\n  commit_required: true\n  validate:\n    commands:\n      - shell: \"just coverage-lcov\"\n      - shell: \"debtmap analyze . --lcov target/coverage/lcov.info --output .prodigy/debtmap-after.json\"\n      - shell: \"debtmap compare --before .prodigy/debtmap-before.json --after .prodigy/debtmap-after.json --output .prodigy/comparison.json\"\n      - claude: \"/validate-improvement --comparison .prodigy/comparison.json --output .prodigy/validation.json\"\n    result_file: \".prodigy/validation.json\"\n    threshold: 75\n    on_incomplete:\n      commands:\n        - claude: \"/fix-remaining-gaps --validation .prodigy/validation.json\"\n          commit_required: true\n        - shell: \"just coverage-lcov\"\n        - shell: \"debtmap analyze . --lcov target/coverage/lcov.info --output .prodigy/debtmap-after.json\"\n      max_attempts: 5\n      fail_workflow: true\n</code></pre> <p>Source: <code>workflows/debtmap.yml:26-42</code></p> <p>This pattern demonstrates: - Multiple validation commands executed in sequence - Reading results from a file after all commands complete - Multi-command remediation with commit requirements - Iterative validation and fixing</p>"},{"location":"advanced/implementation-validation/#configuration-reference","title":"Configuration Reference","text":"<p>Complete list of validation configuration fields:</p> Field Type Default Description <code>shell</code> String None Shell command to run for validation <code>claude</code> String None Claude command to run for validation <code>commands</code> Array None Array of commands for multi-step validation <code>threshold</code> Number 100 Completion percentage required (0-100) <code>timeout</code> Number None Timeout in seconds for validation commands <code>result_file</code> String None File path to read validation results from <code>on_incomplete</code> Object None Configuration for handling validation failures <p>Source: <code>src/cook/workflow/validation.rs:11-49</code></p>"},{"location":"advanced/observability/","title":"Observability and Logging","text":"<p>Prodigy provides comprehensive execution monitoring and debugging through event tracking, Claude execution logs, and configurable verbosity levels.</p>"},{"location":"advanced/observability/#overview","title":"Overview","text":"<p>Observability features: - Event tracking: JSONL event streams for all operations - Claude observability: Detailed Claude execution logs with tool invocations - Verbosity control: Granular output control from clean to trace-level - Log analysis: Tools for inspecting execution history - Performance metrics: Token usage and timing information</p> <pre><code>graph TD\n    Workflow[Workflow Execution] --&gt; Events[Event System]\n    Workflow --&gt; Claude[Claude Commands]\n    Workflow --&gt; Verbosity[Verbosity Control]\n\n    Events --&gt; JSONL[JSONL Event Files&lt;br/&gt;~/.prodigy/events/]\n    Events --&gt; Types[Event Types&lt;br/&gt;AgentStarted, Completed, Failed]\n\n    Claude --&gt; JSONLog[JSON Logs&lt;br/&gt;~/.local/state/claude/logs/]\n    Claude --&gt; Tools[Tool Invocations]\n    Claude --&gt; Tokens[Token Usage]\n\n    Verbosity --&gt; Clean[Default: Clean Output]\n    Verbosity --&gt; Verbose[\"-v: Show Streaming\"]\n    Verbosity --&gt; Debug[\"-vv: Debug Logs\"]\n    Verbosity --&gt; Trace[\"-vvv: Trace Details\"]\n\n    JSONL --&gt; Analysis[Log Analysis]\n    JSONLog --&gt; Analysis\n    Analysis --&gt; Debugging[Debugging &amp; Monitoring]\n\n    style Events fill:#e1f5ff\n    style Claude fill:#fff3e0\n    style Verbosity fill:#f3e5f5\n    style Analysis fill:#e8f5e9</code></pre> <p>Figure: Prodigy's observability architecture showing event tracking, Claude logs, and verbosity control.</p>"},{"location":"advanced/observability/#event-tracking","title":"Event Tracking","text":"<p>All workflow operations are logged to JSONL event files:</p> <pre><code>~/.prodigy/events/{repo_name}/{job_id}/\n\u2514\u2500\u2500 events-{timestamp}.jsonl\n</code></pre> <p>Event Storage Best Practice</p> <p>Events are stored globally in <code>~/.prodigy/events/</code> to enable cross-worktree aggregation. Multiple worktrees working on the same job share the same event log, making it easy to monitor parallel execution.</p>"},{"location":"advanced/observability/#event-types","title":"Event Types","text":"<p>AgentStarted - Agent execution begins: <pre><code>{\n  \"type\": \"AgentStarted\",\n  \"job_id\": \"mapreduce-123\",\n  \"agent_id\": \"agent-1\",\n  \"item_id\": \"item-1\",\n  \"timestamp\": \"2025-01-11T12:00:00Z\"\n}\n</code></pre></p> <p>AgentCompleted - Agent finishes successfully: <pre><code>{\n  \"type\": \"AgentCompleted\",  // (1)!\n  \"job_id\": \"mapreduce-123\",  // (2)!\n  \"agent_id\": \"agent-1\",  // (3)!\n  \"duration\": {\"secs\": 30, \"nanos\": 0},  // (4)!\n  \"commits\": [\"abc123\", \"def456\"],  // (5)!\n  \"json_log_location\": \"/path/to/logs/session-xyz.json\"  // (6)!\n}\n</code></pre></p> <ol> <li>Event type indicating successful completion</li> <li>MapReduce job identifier</li> <li>Unique agent identifier for this work item</li> <li>Total execution time for the agent</li> <li>Git commits created during execution</li> <li>Path to Claude's detailed JSON log for debugging</li> </ol> <p>AgentFailed - Agent encounters errors: <pre><code>{\n  \"type\": \"AgentFailed\",\n  \"job_id\": \"mapreduce-123\",\n  \"agent_id\": \"agent-1\",\n  \"error\": \"Timeout after 300 seconds\",\n  \"json_log_location\": \"/path/to/logs/session-xyz.json\"\n}\n</code></pre></p> <p>WorkItemProcessed - Item completion: <pre><code>{\n  \"type\": \"WorkItemProcessed\",\n  \"job_id\": \"mapreduce-123\",\n  \"item_id\": \"item-1\",\n  \"status\": \"completed\",\n  \"result\": {...}\n}\n</code></pre></p> <p>CheckpointSaved - State persistence: <pre><code>{\n  \"type\": \"CheckpointSaved\",\n  \"job_id\": \"mapreduce-123\",\n  \"phase\": \"map\",\n  \"checkpoint_path\": \"/path/to/checkpoint.json\",\n  \"timestamp\": \"2025-01-11T12:05:00Z\"\n}\n</code></pre></p> <p>ClaudeMessage - Claude interaction messages: <pre><code>// Source: src/cook/execution/events/event_types.rs:164-169\n{\n  \"type\": \"ClaudeMessage\",\n  \"agent_id\": \"agent-1\",\n  \"content\": \"Analyzing file structure...\",\n  \"message_type\": \"assistant\",\n  \"json_log_location\": \"/path/to/logs/session-xyz.json\"\n}\n</code></pre></p>"},{"location":"advanced/observability/#event-organization","title":"Event Organization","text":"<p>Events are organized by repository and job: <pre><code>~/.prodigy/events/\n\u2514\u2500\u2500 prodigy/                    # (1)!\n    \u251c\u2500\u2500 mapreduce-123/          # (2)!\n    \u2502   \u2514\u2500\u2500 events-20250111.jsonl  # (3)!\n    \u2514\u2500\u2500 mapreduce-456/\n        \u2514\u2500\u2500 events-20250111.jsonl\n</code></pre></p> <ol> <li>Repository name for multi-repo support</li> <li>Job ID groups all events for this MapReduce run</li> <li>JSONL file with one event per line (append-only)</li> </ol>"},{"location":"advanced/observability/#claude-observability","title":"Claude Observability","text":"<p>Detailed Claude execution logs capture complete interactions:</p>"},{"location":"advanced/observability/#json-log-location","title":"JSON Log Location","text":"<p>Every Claude command creates a JSON log file: <pre><code>~/.local/state/claude/logs/session-{session_id}.json\n</code></pre></p>"},{"location":"advanced/observability/#log-contents","title":"Log Contents","text":"<p>Complete conversation history: - User messages and prompts - Claude responses - Tool invocations with parameters - Tool results - Token usage statistics - Error details and stack traces</p>"},{"location":"advanced/observability/#accessing-json-logs","title":"Accessing JSON Logs","text":"<p>Via Verbose Output (-v flag): <pre><code>prodigy run workflow.yml -v\n</code></pre></p> <p>Output includes log location: <pre><code>Executing: claude /my-command\nClaude JSON log: /Users/user/.local/state/claude/logs/session-abc123.json\n\u2713 Command completed\n</code></pre></p> <p>In MapReduce Events: <pre><code>{\n  \"type\": \"AgentCompleted\",\n  \"agent_id\": \"agent-1\",\n  \"json_log_location\": \"/path/to/logs/session-xyz.json\"\n}\n</code></pre></p> <p>In DLQ Items: <pre><code>{\n  \"item_id\": \"item-1\",\n  \"failure_history\": [{\n    \"error\": \"Command failed\",\n    \"json_log_location\": \"/path/to/logs/session-xyz.json\"\n  }]\n}\n</code></pre></p>"},{"location":"advanced/observability/#analyzing-json-logs","title":"Analyzing JSON Logs","text":"<p>Common Log Analysis Tasks</p> <p>The examples below show how to extract specific information from Claude JSON logs using <code>jq</code>. These patterns are useful for debugging agent failures, tracking token usage, and understanding Claude's decision-making process.</p> <p>View complete conversation: <pre><code>cat ~/.local/state/claude/logs/session-abc123.json | jq '.messages'\n</code></pre></p> <p>Check tool invocations: <pre><code>cat ~/.local/state/claude/logs/session-abc123.json | \\\n  jq '.messages[].content[] | select(.type == \"tool_use\")'\n</code></pre></p> <p>Analyze token usage: <pre><code>cat ~/.local/state/claude/logs/session-abc123.json | jq '.usage'\n</code></pre></p> <p>Extract errors: <pre><code>cat ~/.local/state/claude/logs/session-abc123.json | \\\n  jq '.messages[] | select(.role == \"assistant\") | .content[] | select(.type == \"error\")'\n</code></pre></p>"},{"location":"advanced/observability/#verbosity-control","title":"Verbosity Control","text":"<p>Granular output control with verbosity flags:</p> <p>Choosing the Right Verbosity Level</p> <p>Start with default output for production workflows. Use <code>-v</code> when debugging Claude interactions or when you need to see streaming output. Reserve <code>-vv</code> and <code>-vvv</code> for deep troubleshooting of Prodigy internals.</p>"},{"location":"advanced/observability/#levels","title":"Levels","text":"<p>Default (verbosity = 0): - Clean, minimal output - Progress indicators - Results only</p> <p>Verbose (-v, verbosity = 1): - Claude streaming JSON output - Command execution details - Log file locations</p> <p>Debug (-vv, verbosity = 2): - Internal debug logs - Execution traces - State transitions</p> <p>Trace (-vvv, verbosity = 3): - Trace-level internal logging - Full execution details - Performance metrics</p>"},{"location":"advanced/observability/#usage","title":"Usage","text":"<pre><code># Default: clean output\nprodigy run workflow.yml\n\n# Verbose: show Claude streaming\nprodigy run workflow.yml -v\n\n# Debug: internal logs\nprodigy run workflow.yml -vv\n\n# Trace: maximum detail\nprodigy run workflow.yml -vvv\n</code></pre>"},{"location":"advanced/observability/#environment-override","title":"Environment Override","text":"<p>Force streaming output regardless of verbosity: <pre><code>export PRODIGY_CLAUDE_CONSOLE_OUTPUT=true\nprodigy run workflow.yml\n</code></pre></p>"},{"location":"advanced/observability/#debugging-mapreduce-failures","title":"Debugging MapReduce Failures","text":""},{"location":"advanced/observability/#using-json-logs","title":"Using JSON Logs","text":"<p>When a MapReduce agent fails, use this debugging workflow:</p> <pre><code>flowchart TD\n    Start[Agent Fails] --&gt; DLQ[Check DLQ Item]\n    DLQ --&gt; GetLog{json_log_location&lt;br/&gt;present?}\n\n    GetLog --&gt;|Yes| InspectLog[Inspect Claude JSON Log]\n    GetLog --&gt;|No| CheckEvents[Check Event Stream]\n\n    InspectLog --&gt; FindError[Find Failing Tool/Message]\n    FindError --&gt; Context[Analyze Context]\n\n    Context --&gt; Messages[Review Message History]\n    Context --&gt; Tools[Check Tool Invocations]\n    Context --&gt; Tokens[Examine Token Usage]\n    Context --&gt; Errors[Extract Error Details]\n\n    Messages --&gt; Root[Identify Root Cause]\n    Tools --&gt; Root\n    Tokens --&gt; Root\n    Errors --&gt; Root\n\n    Root --&gt; Fix[Apply Fix]\n    Fix --&gt; Retry[Retry via DLQ]\n\n    CheckEvents --&gt; EventLog[Parse Event JSONL]\n    EventLog --&gt; Root\n\n    style Start fill:#ffebee\n    style GetLog fill:#fff3e0\n    style Root fill:#e8f5e9\n    style Fix fill:#e1f5ff</code></pre> <p>Figure: MapReduce debugging workflow showing how to trace failures using JSON logs and events.</p> <p>When a MapReduce agent fails:</p> <ol> <li> <p>Check DLQ for json_log_location: <pre><code>prodigy dlq show &lt;job_id&gt; | jq '.items[].failure_history[].json_log_location'\n</code></pre></p> </li> <li> <p>Inspect the Claude JSON log: <pre><code>cat /path/from/step1/session-xyz.json | jq\n</code></pre></p> </li> <li> <p>Identify failing tool: <pre><code>cat /path/from/step1/session-xyz.json | jq '.messages[-3:]'\n</code></pre></p> </li> <li> <p>Understand context:</p> </li> <li>Review full conversation history</li> <li>Check tool invocations and results</li> <li>Examine token usage for context issues</li> <li>Look for error messages</li> </ol>"},{"location":"advanced/observability/#performance-metrics","title":"Performance Metrics","text":""},{"location":"advanced/observability/#token-usage","title":"Token Usage","text":"<p>Track token consumption: <pre><code>{\n  \"usage\": {\n    \"input_tokens\": 1234,\n    \"output_tokens\": 567,\n    \"cache_read_tokens\": 89,\n    \"cache_creation_tokens\": 0\n  }\n}\n</code></pre></p>"},{"location":"advanced/observability/#execution-timing","title":"Execution Timing","text":"<p>Monitor performance: <pre><code>{\n  \"timings\": {\n    \"step1\": {\"secs\": 10, \"nanos\": 500000000},\n    \"step2\": {\"secs\": 25, \"nanos\": 0},\n    \"total\": {\"secs\": 35, \"nanos\": 500000000}\n  }\n}\n</code></pre></p>"},{"location":"advanced/observability/#event-query-examples","title":"Event Query Examples","text":""},{"location":"advanced/observability/#correlation-ids","title":"Correlation IDs","text":"<p>Events include optional correlation IDs for tracing related operations across multiple agents:</p> <pre><code>// Source: src/storage/types.rs:75\n{\n  \"type\": \"AgentStarted\",\n  \"job_id\": \"mapreduce-123\",  // (1)!\n  \"agent_id\": \"agent-1\",  // (2)!\n  \"correlation_id\": \"trace-abc-123\",  // (3)!\n  \"timestamp\": \"2025-01-11T12:00:00Z\"\n}\n</code></pre> <ol> <li>Job identifier - groups all agents in this MapReduce run</li> <li>Agent identifier - unique to this work item</li> <li>Correlation ID - traces related operations across agents (optional)</li> </ol> <p>Filter events by correlation ID: <pre><code># Source: src/cook/execution/events/filter.rs:63\n# Find all events related to a specific workflow trace\ncat ~/.prodigy/events/prodigy/mapreduce-123/events-*.jsonl | \\\n  jq -c 'select(.correlation_id == \"trace-abc-123\")'\n</code></pre></p> <p>Track an agent workflow end-to-end: <pre><code># Get correlation ID from initial event\nCORRELATION_ID=$(cat events.jsonl | jq -r 'select(.type == \"AgentStarted\") | .correlation_id' | head -1)\n\n# Find all related events\ncat events.jsonl | jq -c \"select(.correlation_id == \\\"$CORRELATION_ID\\\")\"\n</code></pre></p>"},{"location":"advanced/observability/#find-failed-agents","title":"Find Failed Agents","text":"<pre><code>cat ~/.prodigy/events/prodigy/mapreduce-123/events-*.jsonl | \\\n  jq -c 'select(.type == \"AgentFailed\")'\n</code></pre>"},{"location":"advanced/observability/#calculate-success-rate","title":"Calculate Success Rate","text":"<pre><code># Count completed\ncompleted=$(cat events.jsonl | jq 'select(.type == \"AgentCompleted\")' | wc -l)\n\n# Count failed\nfailed=$(cat events.jsonl | jq 'select(.type == \"AgentFailed\")' | wc -l)\n\n# Calculate rate\necho \"Success rate: $(($completed * 100 / ($completed + $failed)))%\"\n</code></pre>"},{"location":"advanced/observability/#find-slowest-agents","title":"Find Slowest Agents","text":"<pre><code>cat events.jsonl | \\\n  jq -c 'select(.type == \"AgentCompleted\") | {agent_id, duration: .duration.secs}' | \\\n  sort -k2 -n -r | \\\n  head -10\n</code></pre>"},{"location":"advanced/observability/#log-management","title":"Log Management","text":""},{"location":"advanced/observability/#log-locations","title":"Log Locations","text":"LinuxmacOSWindows <pre><code># Prodigy events\n~/.prodigy/events/{repo_name}/{job_id}/\n\n# Claude logs\n~/.local/state/claude/logs/\n\n# Session state\n~/.prodigy/sessions/\n\n# Checkpoints\n~/.prodigy/state/{repo_name}/\n</code></pre> <pre><code># Prodigy events\n~/.prodigy/events/{repo_name}/{job_id}/\n\n# Claude logs\n~/.local/state/claude/logs/\n\n# Session state\n~/.prodigy/sessions/\n\n# Checkpoints\n~/.prodigy/state/{repo_name}/\n</code></pre> <pre><code># Prodigy events\n%USERPROFILE%\\.prodigy\\events\\{repo_name}\\{job_id}\\\n\n# Claude logs\n%USERPROFILE%\\.local\\state\\claude\\logs\\\n\n# Session state\n%USERPROFILE%\\.prodigy\\sessions\\\n\n# Checkpoints\n%USERPROFILE%\\.prodigy\\state\\{repo_name}\\\n</code></pre> <p>Log Storage Considerations</p> <p>Claude JSON logs can grow large with extensive tool usage. Monitor disk space when running many MapReduce agents. Consider setting up automated cleanup for logs older than 30 days in production environments.</p>"},{"location":"advanced/observability/#cleanup","title":"Cleanup","text":"<pre><code># Clean old event logs (older than 30 days)\nfind ~/.prodigy/events -name \"*.jsonl\" -mtime +30 -delete\n\n# Clean old Claude logs\nfind ~/.local/state/claude/logs -name \"*.json\" -mtime +30 -delete\n\n# Clean completed sessions\nprodigy sessions clean --completed\n</code></pre>"},{"location":"advanced/observability/#examples","title":"Examples","text":""},{"location":"advanced/observability/#debug-workflow-failure","title":"Debug Workflow Failure","text":"<pre><code># Run with verbose output\nprodigy run workflow.yml -v\n\n# Check event log for errors\ncat ~/.prodigy/events/prodigy/latest/events-*.jsonl | \\\n  jq -c 'select(.type == \"AgentFailed\")'\n\n# Inspect Claude log\ncat $(jq -r '.json_log_location' dlq-item.json) | jq '.messages[-5:]'\n</code></pre>"},{"location":"advanced/observability/#monitor-mapreduce-progress","title":"Monitor MapReduce Progress","text":"<pre><code># Run in verbose mode\nprodigy run mapreduce.yml -v &amp;\n\n# Watch event stream\ntail -f ~/.prodigy/events/prodigy/mapreduce-123/events-*.jsonl | \\\n  jq -c 'select(.type == \"AgentCompleted\")'\n</code></pre>"},{"location":"advanced/observability/#analyze-token-usage","title":"Analyze Token Usage","text":"<pre><code># Extract token usage from all agents\nfor log in ~/.local/state/claude/logs/session-*.json; do\n  echo \"$log:\"\n  jq '.usage' \"$log\"\ndone\n</code></pre>"},{"location":"advanced/observability/#see-also","title":"See Also","text":"<ul> <li>Event Tracking (MapReduce) - MapReduce event details</li> <li>Dead Letter Queue - Failure tracking and retry</li> <li>Session Management - Session state and checkpoints</li> <li>Claude Observability - JSON log tracking and analysis</li> </ul>"},{"location":"advanced/parallel-iteration-with-foreach/","title":"Parallel Iteration with Foreach","text":""},{"location":"advanced/parallel-iteration-with-foreach/#parallel-iteration-with-foreach","title":"Parallel Iteration with Foreach","text":"<p>Process multiple items in parallel using the <code>foreach</code> command.</p> <p>Source: Configuration defined in src/config/command.rs:191-211</p>"},{"location":"advanced/parallel-iteration-with-foreach/#basic-foreach","title":"Basic Foreach","text":"<p>The <code>foreach</code> command uses a nested object structure where the <code>foreach</code> field specifies the input source:</p> <pre><code>- foreach:\n    foreach: [\"a\", \"b\", \"c\"]\n    do:\n      - shell: \"process ${item}\"\n</code></pre> <p>Input Formats (src/config/command.rs:193-194): - List: Static array of items: <code>[\"item1\", \"item2\"]</code> - Command: Shell command that outputs items: <code>\"find . -name '*.rs'\"</code></p> <p>Source: Example from src/cook/execution/foreach_tests.rs:16-21</p>"},{"location":"advanced/parallel-iteration-with-foreach/#dynamic-item-lists","title":"Dynamic Item Lists","text":"<p>Generate items dynamically from command output. Commands are executed via shell, and output is parsed line-by-line:</p> <pre><code>- foreach:\n    foreach: \"find . -name '*.rs'\"\n    do:\n      - shell: \"rustfmt ${item}\"\n</code></pre> <p>Command Output Parsing (src/cook/execution/foreach.rs:201-206): - Output is split into lines - Empty lines are automatically filtered out - Each non-empty line becomes one item</p> <p>Source: Example from src/cook/execution/foreach_tests.rs:59-64</p>"},{"location":"advanced/parallel-iteration-with-foreach/#variables-available-in-foreach","title":"Variables Available in Foreach","text":"<p>The following variables are automatically available in <code>do</code> block commands (src/cook/execution/foreach.rs:223-225):</p> <ul> <li><code>${item}</code> - Current item value</li> <li><code>${index}</code> - Zero-based index of current item (0, 1, 2, ...)</li> <li><code>${total}</code> - Total number of items being processed</li> </ul> <p>Example with Variables: <pre><code>- foreach:\n    foreach: [\"test1.rs\", \"test2.rs\", \"test3.rs\"]\n    do:\n      - shell: \"echo Processing ${item} (${index}/${total})\"\n</code></pre></p> <p>This would output: <pre><code>Processing test1.rs (0/3)\nProcessing test2.rs (1/3)\nProcessing test3.rs (2/3)\n</code></pre></p>"},{"location":"advanced/parallel-iteration-with-foreach/#parallel-execution","title":"Parallel Execution","text":"<p>Control parallelism with the <code>parallel</code> field (src/config/command.rs:196-198). It accepts both boolean and numeric values:</p> <p>Boolean - Default Parallelism:</p> <pre><code>- foreach:\n    foreach: \"ls *.txt\"\n    parallel: true  # Default: 10 concurrent workers\n    do:\n      - shell: \"analyze ${item}\"\n</code></pre> <p>Important: <code>parallel: true</code> uses a fixed default of 10 concurrent workers, not \"all available cores\" (src/cook/execution/foreach.rs:81).</p> <p>Number - Explicit Concurrency Limit:</p> <pre><code>- foreach:\n    foreach: \"ls *.txt\"\n    parallel: 5  # Process 5 items concurrently\n    do:\n      - shell: \"analyze ${item}\"\n</code></pre> <p>Source: Example from src/cook/execution/foreach_tests.rs:102-114</p> <p>Choosing Parallelism: - Use <code>parallel: false</code> (default) for sequential processing - Use <code>parallel: true</code> for moderate parallelism (10 workers) - Use <code>parallel: N</code> to specify exact concurrency level - Consider I/O limits, CPU cores, and rate limits when choosing N</p>"},{"location":"advanced/parallel-iteration-with-foreach/#error-handling","title":"Error Handling","text":"<p>Continue processing remaining items on failure:</p> <pre><code>- foreach:\n    foreach: [\"test1\", \"test2\", \"test3\"]\n    continue_on_error: true\n    do:\n      - shell: \"run-test ${item}\"\n</code></pre> <p>Behavior (src/config/command.rs:205-206): - <code>continue_on_error: true</code> - Process all items even if some fail - <code>continue_on_error: false</code> (default) - Stop on first failure - Failed items are tracked and reported in results</p>"},{"location":"advanced/parallel-iteration-with-foreach/#limiting-items","title":"Limiting Items","text":"<p>Process only a subset of items:</p> <pre><code>- foreach:\n    foreach: \"find . -name '*.log'\"\n    max_items: 10  # Process first 10 items only\n    do:\n      - shell: \"compress ${item}\"\n</code></pre> <p>Source: Field definition in src/config/command.rs:209-210</p> <p>Useful for: - Testing workflows on a small subset - Rate-limiting batch operations - Processing most recent items only</p>"},{"location":"advanced/parallel-iteration-with-foreach/#nested-commands","title":"Nested Commands","text":"<p>Each item can execute multiple commands. Both <code>shell</code> and <code>claude</code> commands are supported in <code>do</code> blocks:</p> <pre><code>- foreach:\n    foreach: \"cargo metadata --format-version 1 | jq -r '.packages[].name'\"\n    do:\n      - shell: \"cargo build -p ${item}\"\n      - shell: \"cargo test -p ${item}\"\n      - shell: \"cargo doc -p ${item}\"\n</code></pre> <p>Command Types Supported (src/cook/execution/foreach.rs:286-375): - <code>shell</code> - Execute shell commands - <code>claude</code> - Execute Claude commands</p> <p>Each command in the <code>do</code> block has access to the same variables (<code>${item}</code>, <code>${index}</code>, <code>${total}</code>).</p>"},{"location":"advanced/parallel-iteration-with-foreach/#practical-use-cases","title":"Practical Use Cases","text":"<p>Process Multiple Directories: <pre><code>- foreach:\n    foreach: [\"frontend\", \"backend\", \"shared\"]\n    parallel: 3\n    do:\n      - shell: \"cd ${item} &amp;&amp; npm install\"\n      - shell: \"cd ${item} &amp;&amp; npm test\"\n</code></pre></p> <p>Batch File Processing: <pre><code>- foreach:\n    foreach: \"find src -name '*.rs'\"\n    parallel: 10\n    continue_on_error: true\n    do:\n      - shell: \"rustfmt ${item}\"\n      - shell: \"cargo clippy --manifest-path=${item}\"\n</code></pre></p>"},{"location":"advanced/parallel-iteration-with-foreach/#when-to-use-foreach-vs-mapreduce","title":"When to Use Foreach vs MapReduce","text":"<p>Use Foreach when: - Simple iteration over items (&lt; 100 items) - All items processed in the same worktree - No need for checkpoint/resume - Lightweight operations</p> <p>Use MapReduce when: - Processing many items (100+) - Need checkpoint and resume capability - Need isolated worktrees per item - Complex failure handling and retry logic - Dead letter queue for failed items</p> <p>See MapReduce for large-scale parallel processing.</p>"},{"location":"advanced/parallel-iteration-with-foreach/#progress-tracking","title":"Progress Tracking","text":"<p>During execution, a progress bar displays (src/cook/execution/foreach.rs:88): - Current item number - Total items - Processing rate - Estimated time remaining</p>"},{"location":"advanced/parallel-iteration-with-foreach/#see-also","title":"See Also","text":"<ul> <li>Command Types - All available command types</li> <li>Variables - Variable interpolation system</li> <li>MapReduce - Large-scale parallel processing</li> <li>Environment Variables - Using environment variables in workflows</li> </ul>"},{"location":"advanced/sessions/","title":"Session Management","text":"<p>Prodigy provides unified session tracking and lifecycle management for both standard workflows and MapReduce jobs.</p>"},{"location":"advanced/sessions/#overview","title":"Overview","text":"<p>Session management features: - Unified tracking: Single system for workflows and MapReduce - Lifecycle states: Running, Paused, Completed, Failed, Cancelled - State persistence: Sessions stored in <code>~/.prodigy/sessions/</code> - Resume capabilities: Resume from checkpoints with session or job IDs - Concurrent protection: Lock-based prevention of simultaneous resumes</p>"},{"location":"advanced/sessions/#session-types","title":"Session Types","text":""},{"location":"advanced/sessions/#workflow-sessions","title":"Workflow Sessions","text":"<p>Standard workflow execution tracking:</p> <pre><code>{\n  \"id\": \"session-abc123\",\n  \"session_type\": \"Workflow\",\n  \"status\": \"Running\",\n  \"started_at\": \"2025-01-11T12:00:00Z\",\n  \"workflow_data\": {\n    \"workflow_name\": \"my-workflow\",\n    \"current_step\": 2,\n    \"total_steps\": 5,\n    \"completed_steps\": [0, 1],\n    \"variables\": {},\n    \"files_changed\": 3,\n    \"worktree_name\": \"session-abc123\"\n  }\n}\n</code></pre>"},{"location":"advanced/sessions/#mapreduce-sessions","title":"MapReduce Sessions","text":"<p>MapReduce job state management:</p> <pre><code>// Source: src/unified_session/state.rs:117-125\n{\n  \"id\": \"session-mapreduce-xyz\",\n  \"session_type\": \"MapReduce\",\n  \"status\": \"Running\",\n  \"started_at\": \"2025-01-11T12:00:00Z\",\n  \"mapreduce_data\": {\n    \"job_id\": \"mapreduce-xyz\",\n    \"phase\": \"map\",\n    \"total_items\": 100,\n    \"processed_items\": 45,\n    \"failed_items\": 2,\n    \"agent_count\": 0,\n    \"reduce_results\": null\n  }\n}\n</code></pre>"},{"location":"advanced/sessions/#session-lifecycle","title":"Session Lifecycle","text":""},{"location":"advanced/sessions/#states","title":"States","text":"<ol> <li>Running: Active execution in progress</li> <li>Paused: Interrupted, ready to resume from checkpoint</li> <li>Completed: Successfully finished</li> <li>Failed: Terminated with errors</li> <li>Cancelled: User-initiated stop</li> </ol>"},{"location":"advanced/sessions/#state-transitions","title":"State Transitions","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Running: Workflow Start\n    Running --&gt; Paused: Interrupt (Ctrl+C)\n    Running --&gt; Completed: Success\n    Running --&gt; Failed: Error\n    Running --&gt; Cancelled: User Stop\n\n    Paused --&gt; Running: Resume\n    Paused --&gt; Cancelled: Cleanup\n\n    Completed --&gt; [*]\n    Failed --&gt; [*]\n    Cancelled --&gt; [*]\n\n    note right of Running\n        Checkpoint created\n        State persisted\n    end note\n\n    note right of Paused\n        Resumable state\n        Work preserved\n    end note</code></pre> <p>Figure: Session lifecycle showing state transitions and checkpoint behavior.</p>"},{"location":"advanced/sessions/#session-storage","title":"Session Storage","text":"<p>Sessions are stored globally for easy access:</p> <pre><code>~/.prodigy/sessions/\n\u251c\u2500\u2500 session-abc123.json\n\u251c\u2500\u2500 session-mapreduce-xyz.json\n\u2514\u2500\u2500 session-def456.json\n</code></pre>"},{"location":"advanced/sessions/#session-file-structure","title":"Session File Structure","text":"<pre><code>{\n  \"id\": \"session-abc123\",                    // (1)!\n  \"session_type\": \"Workflow\",                // (2)!\n  \"status\": \"Paused\",                        // (3)!\n  \"started_at\": \"2025-01-11T12:00:00Z\",\n  \"updated_at\": \"2025-01-11T12:05:00Z\",\n  \"completed_at\": null,\n  \"metadata\": {\n    \"execution_start_time\": \"2025-01-11T12:00:00Z\",\n    \"workflow_type\": \"standard\",\n    \"total_steps\": 5,\n    \"current_step\": 2\n  },\n  \"checkpoints\": [                            // (4)!\n    \"checkpoint-1-2025-01-11T12:02:00Z\"\n  ],\n  \"timings\": {                                // (5)!\n    \"step1\": {\"secs\": 10, \"nanos\": 0},\n    \"step2\": {\"secs\": 15, \"nanos\": 0}\n  },\n  \"error\": null\n}\n</code></pre> <ol> <li>Unique session identifier used for resume commands</li> <li>Session type: <code>Workflow</code> or <code>MapReduce</code></li> <li>Current lifecycle state: <code>Running</code>, <code>Paused</code>, <code>Completed</code>, <code>Failed</code>, <code>Cancelled</code></li> <li>List of checkpoint identifiers for resume operations</li> <li>Execution timing for performance analysis</li> </ol>"},{"location":"advanced/sessions/#resume-capabilities","title":"Resume Capabilities","text":""},{"location":"advanced/sessions/#resume-command","title":"Resume Command","text":"<p>Resume sessions using session ID or job ID:</p> <pre><code># Resume with session ID\nprodigy resume session-abc123\n\n# Resume with job ID (MapReduce)\nprodigy resume-job mapreduce-xyz\n\n# Unified resume (auto-detects ID type)\nprodigy resume mapreduce-xyz\n</code></pre> <p>Resume ID Types</p> <p>You can use either session IDs or job IDs to resume. Prodigy automatically detects the ID type and loads the correct session.</p>"},{"location":"advanced/sessions/#session-job-mapping","title":"Session-Job Mapping","text":"<p>Bidirectional mapping enables resume with either ID:</p> <pre><code>~/.prodigy/state/{repo_name}/mappings/\n\u251c\u2500\u2500 session-to-job.json\n\u2514\u2500\u2500 job-to-session.json\n</code></pre> <p>Each mapping contains:</p> <pre><code>// Source: src/storage/session_job_mapping.rs:14-26\n{\n  \"session_id\": \"session-mapreduce-xyz\",\n  \"job_id\": \"mapreduce-xyz\",\n  \"workflow_name\": \"my-workflow\",\n  \"created_at\": \"2025-01-11T12:00:00Z\"\n}\n</code></pre>"},{"location":"advanced/sessions/#checkpoint-based-resume","title":"Checkpoint-Based Resume","text":"<p>Resume reconstructs state from checkpoints:</p> <ol> <li>Load session from <code>~/.prodigy/sessions/</code></li> <li>Verify session is in Paused state</li> <li>Load checkpoint from <code>~/.prodigy/state/</code></li> <li>Reconstruct execution context</li> <li>Continue from last completed step</li> </ol> <pre><code>flowchart TD\n    Start[Resume Command] --&gt; Load[Load Session File]\n    Load --&gt; Verify{Status = Paused?}\n    Verify --&gt;|No| Error[Error: Not Resumable]\n    Verify --&gt;|Yes| Checkpoint[Load Checkpoint]\n    Checkpoint --&gt; Lock{Acquire Lock}\n    Lock --&gt;|Failed| Blocked[Error: Already Running]\n    Lock --&gt;|Success| Context[Reconstruct Context]\n    Context --&gt; Variables[Restore Variables]\n    Context --&gt; State[Restore Work State]\n    Variables --&gt; Continue[Continue Execution]\n    State --&gt; Continue\n    Continue --&gt; Complete[Update Session]\n    Complete --&gt; Release[Release Lock]\n\n    style Start fill:#e1f5ff\n    style Continue fill:#e8f5e9\n    style Error fill:#ffebee\n    style Blocked fill:#ffebee</code></pre> <p>Figure: Resume flow showing checkpoint loading, lock acquisition, and state reconstruction.</p>"},{"location":"advanced/sessions/#state-preservation","title":"State Preservation","text":""},{"location":"advanced/sessions/#variables-and-context","title":"Variables and Context","text":"<p>Preserved across resume: - Workflow variables - Captured command outputs - Environment variables - Map results (in MapReduce)</p>"},{"location":"advanced/sessions/#work-item-state","title":"Work Item State","text":"<p>In MapReduce workflows: - Completed items: Preserved with full results - In-progress items: Moved back to pending on resume - Failed items: Tracked in DLQ with retry counts - Pending items: Continue processing from where left off</p> <p>In-Progress Items on Resume</p> <p>When a MapReduce workflow is interrupted, any items that were being processed by agents are moved back to the pending queue. This ensures they are reprocessed on resume and prevents partial results.</p>"},{"location":"advanced/sessions/#concurrent-resume-protection","title":"Concurrent Resume Protection","text":"<p>Prodigy prevents multiple resumes of the same session:</p>"},{"location":"advanced/sessions/#raii-based-locking","title":"RAII-Based Locking","text":"<pre><code>~/.prodigy/resume_locks/\n\u251c\u2500\u2500 session-abc123.lock\n\u2514\u2500\u2500 mapreduce-xyz.lock\n</code></pre>"},{"location":"advanced/sessions/#lock-metadata","title":"Lock Metadata","text":"<p>Lock files contain: <pre><code>{\n  \"pid\": 12345,\n  \"hostname\": \"machine.local\",\n  \"acquired_at\": \"2025-01-11T12:00:00Z\",\n  \"session_id\": \"session-abc123\"\n}\n</code></pre></p>"},{"location":"advanced/sessions/#stale-lock-detection","title":"Stale Lock Detection","text":"<ul> <li>Platform-specific process checks (Unix: <code>kill -0</code>, Windows: <code>tasklist</code>)</li> <li>Automatic cleanup of locks from crashed processes</li> <li>New resume succeeds after stale lock removal</li> </ul> <p>Automatic Stale Lock Cleanup</p> <p>If a resume process crashes or is killed, the lock file may remain. Prodigy automatically detects when the holding process is no longer running and removes stale locks, allowing new resume attempts to succeed.</p>"},{"location":"advanced/sessions/#lock-errors","title":"Lock Errors","text":"<p>If resume blocked by active lock:</p> <pre><code>Error: Resume already in progress for job mapreduce-xyz\nLock held by: PID 12345 on hostname (acquired 2025-01-11 10:30:00 UTC)\nPlease wait for the other process to complete.\n</code></pre> <p>Concurrent Resume Prevention</p> <p>Attempting to resume a session that is already being resumed will fail with a lock error. Wait for the other process to complete, or verify the process is still running before manually removing the lock file.</p>"},{"location":"advanced/sessions/#session-management-commands","title":"Session Management Commands","text":""},{"location":"advanced/sessions/#list-sessions","title":"List Sessions","text":"<pre><code># List all sessions\nprodigy sessions list\n\n# Filter by status\nprodigy sessions list --status paused\n\n# Show details\nprodigy sessions show session-abc123\n</code></pre>"},{"location":"advanced/sessions/#clean-sessions","title":"Clean Sessions","text":"<pre><code># Clean completed sessions\nprodigy sessions clean\n\n# Clean specific session\nprodigy sessions clean session-abc123\n\n# Force cleanup\nprodigy sessions clean -f\n</code></pre>"},{"location":"advanced/sessions/#session-metadata","title":"Session Metadata","text":""},{"location":"advanced/sessions/#execution-timing","title":"Execution Timing","text":"<p>Track performance metrics: <pre><code>{\n  \"timings\": {\n    \"step1\": {\"secs\": 10, \"nanos\": 0},\n    \"step2\": {\"secs\": 15, \"nanos\": 0},\n    \"total\": {\"secs\": 25, \"nanos\": 0}\n  }\n}\n</code></pre></p>"},{"location":"advanced/sessions/#progress-tracking","title":"Progress Tracking","text":"<p>Monitor execution progress: <pre><code>{\n  \"metadata\": {\n    \"total_steps\": 10,\n    \"current_step\": 5,\n    \"completion_percentage\": 50.0\n  }\n}\n</code></pre></p>"},{"location":"advanced/sessions/#examples","title":"Examples","text":""},{"location":"advanced/sessions/#resume-interrupted-workflow","title":"Resume Interrupted Workflow","text":"<p>Workflow Resume</p> <pre><code># Workflow interrupted during step 3\n^C\n\n# List paused sessions\nprodigy sessions list --status paused\n\n# Resume from checkpoint\nprodigy resume session-abc123\n</code></pre>"},{"location":"advanced/sessions/#resume-mapreduce-job","title":"Resume MapReduce Job","text":"<p>MapReduce Resume</p> <pre><code># MapReduce job interrupted during map phase\n^C\n\n# Check job status\nprodigy sessions show session-mapreduce-xyz\n\n# Resume using session ID\nprodigy resume session-mapreduce-xyz\n\n# OR resume using job ID\nprodigy resume-job mapreduce-xyz\n</code></pre>"},{"location":"advanced/sessions/#prevent-concurrent-resume","title":"Prevent Concurrent Resume","text":"<p>Concurrent Resume Protection</p> <pre><code># Terminal 1: Resume in progress\n$ prodigy resume session-abc123\n# ... executing ...\n\n# Terminal 2: Attempt concurrent resume\n$ prodigy resume session-abc123\nError: Resume already in progress for session-abc123\nLock held by: PID 12345 on machine.local (acquired 2025-01-11 10:30:00 UTC)\n</code></pre>"},{"location":"advanced/sessions/#see-also","title":"See Also","text":"<ul> <li>Checkpoint and Resume (MapReduce) - MapReduce checkpointing details</li> <li>Worktree Management - Git worktree coordination with sessions</li> <li>Storage Architecture - Session storage structure</li> <li>Concurrent Resume Protection - Lock-based resume safety</li> </ul>"},{"location":"advanced/step-identification/","title":"Step Identification","text":""},{"location":"advanced/step-identification/#step-identification","title":"Step Identification","text":"<p>Assign unique IDs to steps for explicit output referencing. This is particularly useful in complex workflows where multiple steps produce outputs and you need to reference specific results.</p>"},{"location":"advanced/step-identification/#available-step-reference-fields","title":"Available Step Reference Fields","text":"<p>When you assign an ID to a step, you can reference multiple fields from that step's execution:</p> Field Type Description Example <code>${step-id.output}</code> string Standard output (stdout) from step <code>${test-step.output}</code> <code>${step-id.exit_code}</code> number Process exit code (0 = success) <code>${build.exit_code}</code> <code>${step-id.success}</code> boolean Whether step succeeded (exit_code == 0) <code>${lint.success}</code> <p>Source: Field resolution implemented in <code>src/cook/expression/mod.rs:187-200</code></p> <p>These fields are automatically available for any step with an <code>id</code> field. They're commonly used in conditionals and error handling:</p>"},{"location":"advanced/step-identification/#basic-step-ids-with-auto-captured-fields","title":"Basic Step IDs with Auto-Captured Fields","text":"<pre><code>- shell: \"cargo test\"\n  id: \"test-step\"\n\n# Reference step's automatic fields\n- shell: \"echo 'Exit code: ${test-step.exit_code}'\"\n- shell: \"echo 'Success: ${test-step.success}'\"\n- claude: \"/analyze-test-output '${test-step.output}'\"\n  when: \"${test-step.exit_code != 0}\"\n</code></pre> <p>Note: The <code>.output</code>, <code>.exit_code</code>, and <code>.success</code> fields are automatically captured for any step with an <code>id</code>. You don't need to explicitly configure output capture for these standard fields.</p>"},{"location":"advanced/step-identification/#custom-output-fields","title":"Custom Output Fields","text":"<p>For capturing specific files or structured data, use the <code>outputs</code> field:</p> <pre><code>- claude: \"/prodigy-code-review\"\n  id: \"review\"\n  outputs:\n    spec:\n      file_pattern: \"*-spec.md\"\n\n# Reference custom output field\n- claude: \"/prodigy-implement-spec ${review.spec}\"\n  id: \"implement\"\n</code></pre> <p>Source: Real example from <code>src/cook/mod_tests.rs:223-228</code> and <code>src/config/command.rs:1049</code></p> <p>Custom outputs are useful for: - Capturing generated files (specs, reports, configs) - Passing structured data between steps - Referencing specific artifacts by name</p>"},{"location":"advanced/step-identification/#when-to-use-step-ids","title":"When to Use Step IDs","text":"<p>1. Conditional Execution Based on Step Results</p> <p>Step IDs enable precise control flow using the auto-captured <code>.exit_code</code> and <code>.success</code> fields:</p> <pre><code>- shell: \"cargo test --lib\"\n  id: \"unit-tests\"\n\n- shell: \"cargo test --test integration\"\n  id: \"integration-tests\"\n\n# Execute only if specific test suite failed\n- claude: \"/analyze-failures '${unit-tests.output}'\"\n  when: \"${unit-tests.exit_code != 0}\"\n\n- claude: \"/analyze-failures '${integration-tests.output}'\"\n  when: \"${integration-tests.exit_code != 0}\"\n</code></pre> <p>Source: Conditional evaluation from <code>src/cook/workflow/conditional_tests.rs:95-97</code></p> <p>2. Error Handling with Step-Specific Outputs</p> <p>Use step IDs to pass the exact output from a failed step to error handlers:</p> <pre><code>- shell: \"npm run build\"\n  id: \"build\"\n\n- shell: \"npm run lint\"\n  id: \"lint\"\n\n- shell: \"npm test\"\n  id: \"test\"\n\n# Pass step output to Claude for analysis\n- claude: \"/debug-build-failure '${build.output}'\"\n  when: \"${build.exit_code != 0}\"\n</code></pre> <p>Source: Real pattern from <code>src/cook/mod.rs:721</code> and <code>src/cook/execution/mapreduce_integration_tests.rs:230</code></p> <p>3. Multi-Step Conditional Logic</p> <p>Combine multiple step results to control workflow execution:</p> <pre><code>- shell: \"cargo clippy\"\n  id: \"clippy-check\"\n\n- shell: \"cargo fmt --check\"\n  id: \"format-check\"\n\n# Only proceed if both checks pass\n- shell: \"cargo build --release\"\n  when: \"${clippy-check.exit_code == 0 &amp;&amp; format-check.exit_code == 0}\"\n\n# Fix clippy warnings if present\n- claude: \"/fix-clippy-warnings '${clippy-check.output}'\"\n  when: \"${clippy-check.exit_code != 0}\"\n</code></pre> <p>Source: Boolean logic support from <code>src/cook/workflow/conditional_tests.rs:56</code></p> <p>4. Passing Step Outputs to Subsequent Commands</p> <p>Reference earlier step outputs in later commands, including in <code>on_failure</code> handlers:</p> <pre><code>- shell: \"cargo test\"\n  id: \"test\"\n  on_failure:\n    claude: \"/fix-failing-tests '${test.output}'\"\n    max_attempts: 3\n\n# Use test results in summary\n- shell: \"generate-report.sh '${test.output}' ${test.exit_code}\"\n</code></pre> <p>Source: Real <code>on_failure</code> pattern from <code>src/cook/mod.rs:721-722</code></p>"},{"location":"advanced/step-identification/#when-not-to-use-step-ids","title":"When NOT to Use Step IDs","text":"<p>Step IDs add complexity, so skip them when:</p> <ul> <li>Single command workflows: If you only have one or two steps, <code>${shell.output}</code> is clear enough</li> <li>No output references: If you never reference a step's output, exit code, or success status</li> <li>Simple sequential execution: Steps that always run in order without conditionals</li> </ul> <p>Example of when step IDs are unnecessary:</p> <pre><code>- shell: \"cargo build\"\n- shell: \"cargo test\"\n- shell: \"cargo clippy\"\n</code></pre> <p>None of these steps reference each other, so IDs would add no value.</p>"},{"location":"advanced/step-identification/#implementation-details","title":"Implementation Details","text":"<p>How It Works:</p> <ol> <li>When a step with <code>id: \"my-step\"</code> executes, Prodigy automatically captures:</li> <li><code>${my-step.output}</code> - stdout from the command</li> <li><code>${my-step.exit_code}</code> - process exit code (0 = success, non-zero = failure)</li> <li> <p><code>${my-step.success}</code> - boolean indicating success (computed as <code>exit_code == 0</code>)</p> </li> <li> <p>Custom outputs (via <code>outputs:</code> field) are stored separately and accessed by their output name:</p> </li> <li> <p><code>${my-step.custom-output-name}</code> - file path matching the pattern</p> </li> <li> <p>Variable resolution happens at runtime when constructing subsequent commands</p> </li> </ol> <p>Source: Implementation in <code>src/cook/expression/mod.rs:187-200</code> and <code>src/cook/orchestrator/execution_pipeline.rs:650-654</code></p>"},{"location":"advanced/step-identification/#troubleshooting","title":"Troubleshooting","text":"<p>\"Variable not found\" errors: - Ensure the step has completed before referencing its outputs - Verify the step ID matches exactly (case-sensitive) - Check that the step actually has an <code>id</code> field</p> <p>Empty output values: - Step output is only captured if the command writes to stdout - Use <code>outputs:</code> for file-based artifacts instead of stdout - Verify the command actually produces output</p> <p>Conditional not working: - The <code>.success</code> and <code>.exit_code</code> fields are only available for steps with <code>id</code> set - Ensure your <code>when:</code> expression uses correct syntax: <code>\"${step-id.exit_code != 0}\"</code></p>"},{"location":"advanced/storage/","title":"Storage Architecture","text":"<p>Prodigy uses a global storage architecture for persistent state, events, and failure tracking across all workflows and sessions.</p>"},{"location":"advanced/storage/#overview","title":"Overview","text":"<p>Global storage features: - Centralized storage: All data in <code>~/.prodigy/</code> - Repository organization: Data grouped by repository name - Cross-worktree sharing: Multiple worktrees access shared state - Persistent state: Job checkpoints survive worktree cleanup - Efficient deduplication: Minimize storage overhead</p>"},{"location":"advanced/storage/#storage-structure","title":"Storage Structure","text":"<pre><code>graph TD\n    Root[\"~/.prodigy/\"] --&gt; Events[\"events/\"]\n    Root --&gt; DLQ[\"dlq/\"]\n    Root --&gt; State[\"state/\"]\n    Root --&gt; Sessions[\"sessions/\"]\n    Root --&gt; Worktrees[\"worktrees/\"]\n    Root --&gt; Orphaned[\"orphaned_worktrees/\"]\n\n    Events --&gt; ERepo[\"{repo_name}/\"]\n    ERepo --&gt; EJob[\"{job_id}/\"]\n    EJob --&gt; EFiles[\"events-{timestamp}.jsonl\"]\n\n    DLQ --&gt; DRepo[\"{repo_name}/\"]\n    DRepo --&gt; DJob[\"{job_id}/\"]\n    DJob --&gt; DFiles[\"dlq-items.json\"]\n\n    State --&gt; SRepo[\"{repo_name}/\"]\n    SRepo --&gt; MapReduce[\"mapreduce/jobs/{job_id}/\"]\n    SRepo --&gt; Mappings[\"mappings/\"]\n    MapReduce --&gt; Setup[\"setup-checkpoint.json\"]\n    MapReduce --&gt; Map[\"map-checkpoint-*.json\"]\n    MapReduce --&gt; Reduce[\"reduce-checkpoint-v1-*.json\"]\n\n    Sessions --&gt; SessionFiles[\"{session_id}.json\"]\n\n    Worktrees --&gt; WRepo[\"{repo_name}/\"]\n    WRepo --&gt; WSession[\"session-{session_id}/\"]\n\n    Orphaned --&gt; ORepo[\"{repo_name}/\"]\n    ORepo --&gt; OJob[\"{job_id}.json\"]\n\n    style Root fill:#e1f5ff\n    style Events fill:#fff3e0\n    style DLQ fill:#ffebee\n    style State fill:#e8f5e9\n    style Sessions fill:#f3e5f5\n    style Worktrees fill:#e0f2f1</code></pre> <p>Figure: Global storage hierarchy showing repository-organized structure.</p> <pre><code>~/.prodigy/\n\u251c\u2500\u2500 events/                     # Event logs\n\u2502   \u2514\u2500\u2500 {repo_name}/\n\u2502       \u2514\u2500\u2500 {job_id}/\n\u2502           \u2514\u2500\u2500 events-{timestamp}.jsonl\n\u251c\u2500\u2500 dlq/                        # Dead Letter Queue\n\u2502   \u2514\u2500\u2500 {repo_name}/\n\u2502       \u2514\u2500\u2500 {job_id}/\n\u2502           \u2514\u2500\u2500 dlq-items.json\n\u251c\u2500\u2500 state/                      # State and checkpoints\n\u2502   \u2514\u2500\u2500 {repo_name}/\n\u2502       \u251c\u2500\u2500 mapreduce/\n\u2502       \u2502   \u2514\u2500\u2500 jobs/{job_id}/\n\u2502       \u2502       \u251c\u2500\u2500 setup-checkpoint.json\n\u2502       \u2502       \u251c\u2500\u2500 map-checkpoint-{timestamp}.json\n\u2502       \u2502       \u2514\u2500\u2500 reduce-checkpoint-v1-{timestamp}.json\n\u2502       \u2514\u2500\u2500 mappings/\n\u2502           \u251c\u2500\u2500 session-to-job.json\n\u2502           \u2514\u2500\u2500 job-to-session.json\n\u251c\u2500\u2500 sessions/                   # Session tracking\n\u2502   \u2514\u2500\u2500 {session_id}.json\n\u251c\u2500\u2500 worktrees/                  # Git worktrees\n\u2502   \u2514\u2500\u2500 {repo_name}/\n\u2502       \u2514\u2500\u2500 session-{session_id}/\n\u2514\u2500\u2500 orphaned_worktrees/         # Cleanup failure tracking\n    \u2514\u2500\u2500 {repo_name}/\n        \u2514\u2500\u2500 {job_id}.json\n</code></pre>"},{"location":"advanced/storage/#event-storage","title":"Event Storage","text":"<p>Event logs are stored as JSONL files for efficient streaming:</p> <pre><code>~/.prodigy/events/{repo_name}/{job_id}/events-{timestamp}.jsonl\n</code></pre> <p>Auto-Generated Paths</p> <p>Event file paths include auto-generated timestamp suffixes (e.g., <code>events-20250111120000.jsonl</code>). This enables log rotation and prevents conflicts when multiple processes write events simultaneously.</p>"},{"location":"advanced/storage/#event-organization","title":"Event Organization","text":"<ul> <li>By repository: Events grouped by repo for easy filtering</li> <li>By job: Each job has dedicated event directory</li> <li>JSONL format: One JSON event per line for streaming</li> <li>Timestamped files: Rotate logs by timestamp</li> </ul>"},{"location":"advanced/storage/#event-persistence","title":"Event Persistence","text":"<p>Events are persisted immediately: - Agent lifecycle events (started, completed, failed) - Work item status changes - Checkpoint saves - Error details with correlation IDs</p>"},{"location":"advanced/storage/#cross-worktree-aggregation","title":"Cross-Worktree Aggregation","text":"<p>Multiple worktrees working on same job share event logs:</p> <pre><code>graph LR\n    WT1[Worktree 1&lt;br/&gt;Agent processing item-1] --&gt; Events[\"~/.prodigy/events/&lt;br/&gt;prodigy/job-123/\"]\n    WT2[Worktree 2&lt;br/&gt;Agent processing item-2] --&gt; Events\n    WT3[Worktree N&lt;br/&gt;Agent processing item-N] --&gt; Events\n\n    Events --&gt; Monitor[Centralized&lt;br/&gt;Monitoring]\n\n    style WT1 fill:#fff3e0\n    style WT2 fill:#fff3e0\n    style WT3 fill:#fff3e0\n    style Events fill:#e8f5e9\n    style Monitor fill:#e1f5ff</code></pre> <p>Figure: Cross-worktree event aggregation enabling centralized monitoring.</p>"},{"location":"advanced/storage/#state-storage","title":"State Storage","text":"<p>Job state and checkpoints are stored globally:</p> <pre><code>~/.prodigy/state/{repo_name}/mapreduce/jobs/{job_id}/\n\u251c\u2500\u2500 job-state.json              # Overall job state\n\u251c\u2500\u2500 setup-checkpoint.json       # Setup phase results\n\u251c\u2500\u2500 map-checkpoint-*.json       # Map phase progress\n\u2514\u2500\u2500 reduce-checkpoint-v1-*.json # Reduce phase progress\n</code></pre>"},{"location":"advanced/storage/#checkpoint-types","title":"Checkpoint Types","text":"<p>Setup Phase: <pre><code>// Source: src/cook/execution/mapreduce/checkpoint/types.rs\n{\n  \"phase\": \"setup\",\n  \"completed\": true,\n  \"outputs\": {...},\n  \"timestamp\": \"2025-01-11T12:00:00Z\"\n}\n</code></pre></p> <p>Map Phase: <pre><code>// Source: src/cook/execution/mapreduce/checkpoint/types.rs\n{\n  \"phase\": \"map\",\n  \"completed_items\": [\"item-1\", \"item-2\"],      // (1)!\n  \"in_progress_items\": [\"item-3\"],              // (2)!\n  \"pending_items\": [\"item-4\", \"item-5\"],        // (3)!\n  \"agent_results\": {...},                       // (4)!\n  \"timestamp\": \"2025-01-11T12:05:00Z\"\n}\n</code></pre></p> <ol> <li>Work items successfully processed by agents</li> <li>Items currently being processed (moved back to pending on resume)</li> <li>Items waiting to be processed</li> <li>Full results from completed agents (commits, outputs, timings)</li> </ol> <p>Reduce Phase: <pre><code>// Source: src/cook/execution/mapreduce/checkpoint/types.rs\n{\n  \"phase\": \"reduce\",\n  \"completed_steps\": [0, 1],     // (1)!\n  \"current_step\": 2,              // (2)!\n  \"step_results\": {...},          // (3)!\n  \"map_results\": {...},           // (4)!\n  \"timestamp\": \"2025-01-11T12:10:00Z\"\n}\n</code></pre></p> <ol> <li>Indices of reduce commands that have completed</li> <li>Index of the currently executing reduce command</li> <li>Output captured from completed reduce steps</li> <li>Aggregated results from all map agents (available to reduce commands)</li> </ol> <p>Checkpoint File Naming</p> <p>Checkpoint files include auto-generated timestamp suffixes: - Setup: <code>setup-checkpoint.json</code> (no timestamp, only one per job) - Map: <code>map-checkpoint-{timestamp}.json</code> (multiple checkpoints during map phase) - Reduce: <code>reduce-checkpoint-v1-{timestamp}.json</code> (versioned with timestamp)</p>"},{"location":"advanced/storage/#session-storage","title":"Session Storage","text":"<p>Sessions are stored in a flat directory:</p> <pre><code>~/.prodigy/sessions/\n\u251c\u2500\u2500 session-abc123.json\n\u251c\u2500\u2500 session-mapreduce-xyz.json\n\u2514\u2500\u2500 session-def456.json\n</code></pre>"},{"location":"advanced/storage/#session-file-format","title":"Session File Format","text":"<pre><code>// Source: src/storage/types.rs\n{\n  \"id\": \"session-abc123\",\n  \"session_type\": \"Workflow\",\n  \"status\": \"Running\",\n  \"started_at\": \"2025-01-11T12:00:00Z\",\n  \"metadata\": {...},\n  \"checkpoints\": [...],\n  \"timings\": {...}\n}\n</code></pre>"},{"location":"advanced/storage/#dead-letter-queue-storage","title":"Dead Letter Queue Storage","text":"<p>Failed work items are stored per job:</p> <pre><code>~/.prodigy/dlq/{repo_name}/{job_id}/dlq-items.json\n</code></pre>"},{"location":"advanced/storage/#dlq-item-format","title":"DLQ Item Format","text":"<pre><code>{\n  \"item_id\": \"item-1\",\n  \"item_data\": {...},\n  \"failure_history\": [\n    {\n      \"timestamp\": \"2025-01-11T12:00:00Z\",\n      \"error\": \"Timeout after 300 seconds\",\n      \"json_log_location\": \"/path/to/logs/session-xyz.json\",\n      \"retry_count\": 0\n    }\n  ],\n  \"last_failure\": \"2025-01-11T12:00:00Z\"\n}\n</code></pre>"},{"location":"advanced/storage/#dlq-features","title":"DLQ Features","text":"<ul> <li>Cross-worktree tracking: Shared across parallel worktrees</li> <li>Failure history: Track all retry attempts</li> <li>Log linkage: JSON log location for debugging</li> <li>Retry support: Command to reprocess failed items</li> </ul>"},{"location":"advanced/storage/#worktree-storage","title":"Worktree Storage","text":"<p>Git worktrees are created per session:</p> <pre><code>~/.prodigy/worktrees/{repo_name}/\n\u251c\u2500\u2500 session-abc123/             # Workflow session\n\u2514\u2500\u2500 session-mapreduce-xyz/      # MapReduce parent worktree\n    \u251c\u2500\u2500 agent-1/                # MapReduce agent worktree\n    \u2514\u2500\u2500 agent-2/                # MapReduce agent worktree\n</code></pre>"},{"location":"advanced/storage/#worktree-lifecycle","title":"Worktree Lifecycle","text":"<ol> <li>Creation: Worktree created when workflow starts</li> <li>Execution: All commands run in worktree context</li> <li>Persistence: Worktree remains until merge or cleanup</li> <li>Cleanup: Removed after successful merge</li> </ol>"},{"location":"advanced/storage/#orphaned-worktree-tracking","title":"Orphaned Worktree Tracking","text":"<p>When cleanup fails, worktree paths are registered:</p> <pre><code>~/.prodigy/orphaned_worktrees/{repo_name}/{job_id}.json\n</code></pre>"},{"location":"advanced/storage/#registry-format","title":"Registry Format","text":"<pre><code>{\n  \"job_id\": \"mapreduce-123\",\n  \"orphaned_worktrees\": [\n    {\n      \"agent_id\": \"agent-1\",\n      \"item_id\": \"item-1\",\n      \"worktree_path\": \"/Users/user/.prodigy/worktrees/prodigy/agent-1\",\n      \"timestamp\": \"2025-01-11T12:00:00Z\",\n      \"error\": \"Permission denied\"\n    }\n  ]\n}\n</code></pre>"},{"location":"advanced/storage/#session-job-mapping","title":"Session-Job Mapping","text":"<p>Bidirectional mapping enables resume with session or job IDs:</p> <pre><code>~/.prodigy/state/{repo_name}/mappings/\n\u251c\u2500\u2500 session-to-job.json\n\u2514\u2500\u2500 job-to-session.json\n</code></pre>"},{"location":"advanced/storage/#mapping-format","title":"Mapping Format","text":"<p>session-to-job.json: <pre><code>{\n  \"session-mapreduce-xyz\": \"mapreduce-123\"\n}\n</code></pre></p> <p>job-to-session.json: <pre><code>{\n  \"mapreduce-123\": \"session-mapreduce-xyz\"\n}\n</code></pre></p>"},{"location":"advanced/storage/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"advanced/storage/#jsonl-streaming-vs-batch-operations","title":"JSONL Streaming vs Batch Operations","text":"<p>Prodigy uses JSONL (JSON Lines) format for event storage to enable efficient streaming:</p> <p>JSONL Streaming Benefits: - Incremental writes: Events append without reading entire file - Memory efficient: Process one event at a time - Concurrent safe: Multiple processes can append simultaneously - Resumable: Stream from any position in the file</p> <p>Usage Patterns:</p> <pre><code># Streaming read (memory efficient for large logs)\ncat ~/.prodigy/events/prodigy/job-123/events-*.jsonl | \\\n  while IFS= read -r line; do\n    echo \"$line\" | jq -c .\n  done\n\n# Batch read (faster for small logs)\ncat ~/.prodigy/events/prodigy/job-123/events-*.jsonl | jq -s '.'\n</code></pre> <p>Performance Comparison: | Operation | JSONL Streaming | Batch JSON | |-----------|----------------|------------| | Memory usage | O(1) per event | O(n) all events | | Write speed | Fast (append) | Slow (rewrite) | | Read speed | Slower (parse per line) | Faster (parse once) | | Concurrent writes | Safe | Requires locking | | Resume support | Built-in | Complex |</p> <p>Choose the Right Pattern</p> <ul> <li>Use streaming for: Large event logs (&gt;10K events), real-time monitoring, concurrent writers</li> <li>Use batch for: Small logs (&lt;1K events), one-time analysis, reporting</li> </ul> <p>Streaming Example</p> <pre><code># Memory-efficient processing of large event logs\ncat ~/.prodigy/events/prodigy/job-123/events-*.jsonl | \\\n  jq -c 'select(.type == \"AgentCompleted\")' | \\\n  while read event; do\n    echo \"$event\" | jq -r '.agent_id'\n  done\n</code></pre>"},{"location":"advanced/storage/#storage-access-patterns","title":"Storage Access Patterns","text":"<p>Event Log Access: - Write: Append-only, lock-free - Read: Sequential streaming or batch analysis - Typical size: 1KB-10KB per event, 100-10000 events per job</p> <p>Checkpoint Access: - Write: Atomic file replacement - Read: Full file load into memory - Typical size: 10KB-1MB per checkpoint</p> <p>Session Access: - Write: Atomic update (read-modify-write with lock) - Read: Direct file access - Typical size: 1KB-10KB per session</p> <p>Storage Access Considerations</p> <p>Event logs grow linearly with job execution time. For long-running MapReduce jobs (&gt;1000 work items), event logs can reach 10MB+. Use streaming reads to avoid memory exhaustion.</p> <p>Checkpoint Strategy</p> <p>Checkpoints use atomic file replacement to prevent corruption from interrupted writes. The system writes to a temp file, then renames it\u2014ensuring checkpoint integrity even during crashes.</p>"},{"location":"advanced/storage/#storage-benefits","title":"Storage Benefits","text":""},{"location":"advanced/storage/#cross-worktree-data-sharing","title":"Cross-Worktree Data Sharing","text":"<p>Multiple worktrees working on same job share: - Event logs - DLQ items - Checkpoints - Job state</p> <p>This enables: - Parallel execution visibility - Centralized failure tracking - Consistent state management</p>"},{"location":"advanced/storage/#persistent-state-management","title":"Persistent State Management","text":"<p>State survives worktree cleanup: - Resume after worktree deleted - Access job data without worktree - Historical analysis of completed jobs</p>"},{"location":"advanced/storage/#centralized-monitoring","title":"Centralized Monitoring","text":"<p>All job data accessible from single location: - View events across all worktrees - Monitor job progress globally - Analyze performance metrics</p>"},{"location":"advanced/storage/#efficient-storage","title":"Efficient Storage","text":"<p>Deduplication across worktrees: - Single event log per job (not per worktree) - Shared checkpoint files - Reduced storage overhead</p>"},{"location":"advanced/storage/#storage-maintenance","title":"Storage Maintenance","text":""},{"location":"advanced/storage/#cleanup-commands","title":"Cleanup Commands","text":"<pre><code># Clean old events (30+ days)\nfind ~/.prodigy/events -name \"*.jsonl\" -mtime +30 -delete  # (1)!\n\n# Clean completed sessions\nprodigy sessions clean --completed  # (2)!\n\n# Clean orphaned worktrees\nprodigy worktree clean-orphaned &lt;job_id&gt;  # (3)!\n\n# Clean DLQ after successful retry\nprodigy dlq clear &lt;job_id&gt;  # (4)!\n</code></pre> <ol> <li>Removes event logs older than 30 days to prevent unbounded growth</li> <li>Removes session files for completed workflows (preserves failed/paused)</li> <li>Cleans up worktrees that failed to cleanup during agent execution</li> <li>Removes DLQ items after successful retry (only use after verifying retry succeeded)</li> </ol> <p>Data Loss Prevention</p> <p>Always verify jobs are complete before cleaning: <pre><code># Check if job is truly complete\nprodigy events show &lt;job_id&gt; | tail -1 | jq '.type'\n# Should show \"JobCompleted\" or \"JobFailed\"\n</code></pre></p>"},{"location":"advanced/storage/#storage-usage","title":"Storage Usage","text":"<p>Check storage consumption: <pre><code># Total storage\ndu -sh ~/.prodigy/\n\n# By category\ndu -sh ~/.prodigy/events\ndu -sh ~/.prodigy/state\ndu -sh ~/.prodigy/sessions\ndu -sh ~/.prodigy/worktrees\n</code></pre></p>"},{"location":"advanced/storage/#migration-from-local-storage","title":"Migration from Local Storage","text":"<p>Legacy local storage (deprecated): <pre><code>.prodigy/\n\u251c\u2500\u2500 session_state.json          # Deprecated\n\u251c\u2500\u2500 events/                     # Moved to ~/.prodigy/events\n\u2514\u2500\u2500 dlq/                        # Moved to ~/.prodigy/dlq\n</code></pre></p> <p>Global storage benefits: - Cross-repository visibility - Persistent state across worktrees - Centralized monitoring and debugging</p>"},{"location":"advanced/storage/#examples","title":"Examples","text":""},{"location":"advanced/storage/#access-job-data","title":"Access Job Data","text":"<pre><code># View events\ncat ~/.prodigy/events/prodigy/mapreduce-123/events-*.jsonl | jq\n\n# Check checkpoint\ncat ~/.prodigy/state/prodigy/mapreduce/jobs/mapreduce-123/map-checkpoint-*.json | jq\n\n# Inspect DLQ\ncat ~/.prodigy/dlq/prodigy/mapreduce-123/dlq-items.json | jq\n</code></pre>"},{"location":"advanced/storage/#find-session-by-job-id","title":"Find Session by Job ID","text":"<pre><code># Look up session ID\njob_id=\"mapreduce-123\"\nsession_id=$(jq -r \".\\\"$job_id\\\"\" ~/.prodigy/state/prodigy/mappings/job-to-session.json)\n\n# View session\ncat ~/.prodigy/sessions/$session_id.json | jq\n</code></pre>"},{"location":"advanced/storage/#analyze-storage-growth","title":"Analyze Storage Growth","text":"<pre><code># Event log size over time\nfind ~/.prodigy/events -name \"*.jsonl\" -printf '%TY-%Tm-%Td %s %p\\n' | \\\n  sort | \\\n  awk '{size+=$2} END {print \"Total events:\", size/1024/1024, \"MB\"}'\n\n# Checkpoint size\ndu -sh ~/.prodigy/state/*/mapreduce/jobs/*/\n</code></pre>"},{"location":"advanced/storage/#see-also","title":"See Also","text":"<ul> <li>Event Tracking - Event log format and usage</li> <li>Session Management - Session storage and lifecycle</li> <li>Dead Letter Queue - DLQ storage and retry</li> <li>Git Integration - Worktree storage and management</li> </ul>"},{"location":"advanced/timeout-configuration/","title":"Timeout Configuration","text":""},{"location":"advanced/timeout-configuration/#timeout-configuration","title":"Timeout Configuration","text":"<p>Set execution timeouts to prevent workflows from hanging indefinitely. Prodigy supports two distinct timeout mechanisms: command-level timeouts for standard workflows and MapReduce-specific timeouts with advanced configuration options.</p>"},{"location":"advanced/timeout-configuration/#command-level-timeouts","title":"Command-Level Timeouts","text":"<p>Command-level timeouts apply to individual commands in standard workflows. These accept numeric values only (in seconds).</p> <p>Source: <code>src/config/command.rs:384</code> - <code>pub timeout: Option&lt;u64&gt;</code></p> <pre><code>commands:\n  # Shell command with 10 minute timeout\n  - shell: \"cargo bench\"\n    timeout: 600\n\n  # Claude command with 30 minute timeout\n  - claude: \"/analyze-codebase\"\n    timeout: 1800\n\n  # No timeout specified = no limit\n  - shell: \"cargo build\"\n</code></pre> <p>Real-world examples from workflows:</p> <p>From <code>workflows/complex-build-pipeline.yml:12</code>: <pre><code>- shell: \"cargo bench\"\n  timeout: 600  # 10 minutes\n  capture_output: \"benchmark_results\"\n</code></pre></p> <p>From <code>workflows/documentation-drift.yml:15</code>: <pre><code>- shell: \"cargo test --doc\"\n  timeout: 300  # 5 minutes\n  on_failure:\n    claude: \"/prodigy-debug-test-failure --output ${shell.output}\"\n</code></pre></p> <p>Important: Command-level timeouts only accept numeric values. For environment variable support, use MapReduce timeouts (see below).</p>"},{"location":"advanced/timeout-configuration/#mapreduce-timeouts","title":"MapReduce Timeouts","text":"<p>MapReduce workflows support more sophisticated timeout configuration with environment variable support and advanced policies.</p>"},{"location":"advanced/timeout-configuration/#setup-phase-timeout","title":"Setup Phase Timeout","text":"<p>Control how long the setup phase can run before timing out.</p> <p>Source: <code>src/config/mapreduce.rs:148</code> - Uses <code>deserialize_optional_u64_or_string</code></p> <pre><code>mode: mapreduce\n\nsetup:\n  timeout: 300  # 5 minutes for setup\n  commands:\n    - shell: \"generate-work-items.sh\"\n</code></pre> <p>With environment variables: <pre><code>setup:\n  timeout: $SETUP_TIMEOUT  # References environment variable\n  commands:\n    - shell: \"cargo build\"\n</code></pre></p>"},{"location":"advanced/timeout-configuration/#map-phase-agent-timeout","title":"Map Phase Agent Timeout","text":"<p>Set a global timeout for all map agents or configure per-agent policies.</p> <p>Source: <code>src/config/mapreduce.rs:269</code> - <code>pub agent_timeout_secs: Option&lt;String&gt;</code></p> <p>Simple agent timeout: <pre><code>map:\n  agent_timeout_secs: 600  # 10 minutes per agent\n  agent_template:\n    - claude: \"/process '${item}'\"\n</code></pre></p> <p>With environment variable: <pre><code>map:\n  agent_timeout_secs: $AGENT_TIMEOUT  # Configurable via environment\n  agent_template:\n    - claude: \"/process '${item}'\"\n</code></pre></p>"},{"location":"advanced/timeout-configuration/#advanced-timeout-configuration","title":"Advanced Timeout Configuration","text":"<p>For fine-grained control, use <code>timeout_config</code> to specify policies, per-command overrides, and timeout actions.</p> <p>Source: <code>src/cook/execution/mapreduce/timeout.rs:38-63</code> - <code>TimeoutConfig</code> struct</p> <pre><code>map:\n  timeout_config:\n    agent_timeout_secs: 600          # Global 10 minute timeout\n    timeout_policy: hybrid           # Apply per-agent with overrides\n    cleanup_grace_period_secs: 30    # 30s to clean up after timeout\n    timeout_action: dlq              # Send timed-out items to DLQ\n    enable_monitoring: true          # Track timeout metrics\n\n    # Per-command timeout overrides\n    command_timeouts:\n      claude: 300                    # Claude commands: 5 minutes\n      shell: 60                      # Shell commands: 1 minute\n      claude_0: 600                  # First Claude command: 10 minutes\n\n  agent_template:\n    - claude: \"/analyze '${item}'\"   # Uses 300s from command_timeouts\n    - shell: \"test ${item.path}\"     # Uses 60s from command_timeouts\n</code></pre> <p>Real example from tests (<code>tests/timeout_integration_test.rs:215-233</code>): <pre><code>agent_timeout_secs: 600\ntimeout_config:\n  timeout_policy: hybrid\n  cleanup_grace_period_secs: 30\n  timeout_action: dlq\n  enable_monitoring: true\n  command_timeouts:\n    claude: 300\n    shell: 60\n    claude_0: 600\n</code></pre></p>"},{"location":"advanced/timeout-configuration/#timeout-policies","title":"Timeout Policies","text":"<p>Source: <code>src/cook/execution/mapreduce/timeout.rs:79-88</code> - <code>TimeoutPolicy</code> enum</p> <ul> <li><code>per_agent</code> (default): Timeout applies to entire agent execution</li> <li>Agent must complete all commands within timeout</li> <li> <p>Best for workflows where total time matters</p> </li> <li> <p><code>per_command</code>: Timeout applies to each command individually</p> </li> <li>Each command gets full timeout duration</li> <li> <p>Best for workflows with highly variable command durations</p> </li> <li> <p><code>hybrid</code>: Per-agent timeout with command-specific overrides</p> </li> <li>Commands use <code>command_timeouts</code> if specified, otherwise agent timeout</li> <li>Most flexible option</li> </ul> <p>Example: Per-command policy <pre><code>timeout_config:\n  agent_timeout_secs: 100\n  timeout_policy: per_command  # Each command gets 100 seconds\n</code></pre></p>"},{"location":"advanced/timeout-configuration/#timeout-actions","title":"Timeout Actions","text":"<p>Source: <code>src/cook/execution/mapreduce/timeout.rs:91-102</code> - <code>TimeoutAction</code> enum</p> <ul> <li><code>dlq</code> (default): Send item to Dead Letter Queue for retry</li> <li><code>skip</code>: Skip the item and continue with other items</li> <li><code>fail</code>: Fail the entire MapReduce job</li> <li><code>graceful_terminate</code>: Attempt graceful shutdown before force kill</li> </ul> <pre><code>timeout_config:\n  timeout_action: skip  # Skip timed-out items instead of retrying\n</code></pre>"},{"location":"advanced/timeout-configuration/#default-values","title":"Default Values","text":"<p>Source: <code>src/cook/execution/mapreduce/timeout.rs</code> - Default implementations</p> Configuration Default Value Description <code>agent_timeout_secs</code> 600 (10 minutes) Global agent timeout <code>cleanup_grace_period_secs</code> 30 seconds Time for cleanup after timeout <code>enable_monitoring</code> true Track timeout metrics <code>timeout_policy</code> <code>per_agent</code> Apply timeout to entire agent <code>timeout_action</code> <code>dlq</code> Send timed-out items to DLQ"},{"location":"automated-documentation/","title":"Automated Documentation with mdBook","text":"<p>This guide shows you how to set up automated, always-up-to-date documentation for any project using Prodigy's book workflow system. This same system maintains the documentation you're reading right now.</p>"},{"location":"automated-documentation/#overview","title":"Overview","text":"<p>The book workflow system: - Analyzes your codebase to build a feature inventory - Detects documentation drift by comparing docs to implementation - Updates documentation automatically using Claude - Maintains consistency across all chapters - Runs on any project - just configure and go</p> <p>The generalized commands work for any codebase: Rust, Python, JavaScript, etc.</p>"},{"location":"automated-documentation/#why-automated-documentation","title":"Why Automated Documentation?","text":"<p>The Problem: Documentation drifts out of sync with code. New features get added, APIs change, examples break, and manually updating docs is time-consuming and error-prone.</p> <p>The Solution: This workflow system solves documentation drift by:</p> <ul> <li>Always up-to-date - Automatically detects when docs don't match implementation</li> <li>Code-grounded examples - All examples extracted from actual source code with file references</li> <li>Parallel processing - MapReduce architecture processes chapters concurrently</li> <li>Quality guaranteed - Validation ensures 100% completeness before accepting updates</li> <li>Version controlled - All changes tracked via git commits with full audit trail</li> <li>CI/CD ready - Runs in GitHub Actions or any CI system</li> </ul> <p>Real Results: This documentation you're reading right now is maintained by this system. Every code example references actual source files. When Prodigy's features change, the docs update automatically.</p>"},{"location":"automated-documentation/#quick-start","title":"Quick Start","text":"<p>Ready to get started? Here's the fastest path:</p> <ol> <li>Install Prodigy - Prodigy, Claude Code CLI, Rust</li> <li>Follow the Quick Start guide - 15-20 minute setup for automated docs</li> <li>Or dive deeper with the Tutorial - 30 minute comprehensive walkthrough</li> </ol> <p>Choose your path based on your time and experience level</p>"},{"location":"automated-documentation/#how-it-works","title":"How It Works","text":"<p>The documentation workflow uses a MapReduce pattern to process your codebase in parallel:</p>"},{"location":"automated-documentation/#workflow-phases","title":"Workflow Phases","text":"<ol> <li>Setup Phase (Feature Analysis):</li> <li>Analyzes your codebase to build a complete feature inventory</li> <li>Detects documentation gaps by comparing existing docs to implementation</li> <li>Creates missing chapter/subsection files with placeholders</li> <li>Generates work items for the map phase</li> <li> <p>Source: workflows/book-docs-drift.yml:24-34</p> </li> <li> <p>Map Phase (Parallel Processing):</p> </li> <li>Processes each chapter/subsection in parallel using isolated git worktrees</li> <li>For each documentation item:<ul> <li>Analyzes drift between documentation and implementation</li> <li>Fixes identified issues with real code examples</li> <li>Validates fixes meet quality standards</li> </ul> </li> <li>Runs up to 3 items concurrently (configurable via MAX_PARALLEL)</li> <li>Failed items go to Dead Letter Queue (DLQ) for retry</li> <li> <p>Source: workflows/book-docs-drift.yml:37-59</p> </li> <li> <p>Reduce Phase (Validation):</p> </li> <li>Rebuilds the entire book to ensure chapters compile together</li> <li>Checks for broken links between chapters</li> <li>Fixes any build errors discovered during compilation</li> <li>Cleans up temporary analysis files</li> <li> <p>Source: workflows/book-docs-drift.yml:62-82</p> </li> <li> <p>Merge Phase (Integration):</p> </li> <li>Merges updated documentation back to your original branch</li> <li>Preserves your working tree state</li> <li>Uses Claude to handle any merge conflicts</li> <li>Source: workflows/book-docs-drift.yml:93-100</li> </ol>"},{"location":"automated-documentation/#worktree-isolation","title":"Worktree Isolation","text":"<p>All phases execute in an isolated git worktree: - Your main repository remains untouched during execution - Each map agent runs in its own child worktree - Changes merge back only after successful completion - Failed workflows don't pollute your working directory - Learn more: Understanding the Workflow</p>"},{"location":"automated-documentation/#quality-guarantees","title":"Quality Guarantees","text":"<p>The workflow ensures documentation quality through: - Code-grounded examples: All examples extracted from actual implementation - Validation checkpoints: Each fix validated before proceeding - Build verification: Full book rebuild ensures no broken references - Source attribution: Examples include file paths and line numbers - Automatic retry: Failed items can be retried via <code>prodigy dlq retry</code></p> <p>For detailed information about each phase, see the subsections below.</p>"},{"location":"automated-documentation/#additional-topics","title":"Additional Topics","text":""},{"location":"automated-documentation/#getting-started","title":"Getting Started","text":"<ul> <li>Quick Start - 15-20 minute guide</li> <li>Tutorial - 30 minute comprehensive walkthrough</li> </ul>"},{"location":"automated-documentation/#understanding-the-system","title":"Understanding the System","text":"<ul> <li>Understanding the Workflow - How it works under the hood</li> <li>Automatic Gap Detection - Finding missing documentation</li> </ul>"},{"location":"automated-documentation/#advanced-usage","title":"Advanced Usage","text":"<ul> <li>Advanced Configuration - Fine-tuning options</li> <li>GitHub Actions Integration - CI/CD automation</li> <li>Documentation Versioning - Managing versions</li> </ul>"},{"location":"automated-documentation/#reference","title":"Reference","text":"<ul> <li>Troubleshooting - Common issues and solutions</li> <li>Real-World Example: Prodigy's Own Documentation - Case study</li> </ul>"},{"location":"automated-documentation/advanced-configuration/","title":"Advanced Configuration","text":""},{"location":"automated-documentation/advanced-configuration/#advanced-configuration","title":"Advanced Configuration","text":"<p>This subsection covers advanced configuration topics for optimizing and customizing your automated documentation workflows. These configurations enable fine-tuning of performance, security, and behavior for documentation generation at scale.</p>"},{"location":"automated-documentation/advanced-configuration/#configuration-files-and-locations","title":"Configuration Files and Locations","text":"<p>Prodigy supports configuration at multiple levels with a clear precedence chain:</p> <p>Configuration File Locations (Source: src/config/mod.rs:39-86):</p> <ol> <li>Global Configuration: <code>~/.prodigy/config.toml</code></li> <li>Applies across all projects</li> <li> <p>Contains defaults for editor, log level, API keys, and global settings</p> </li> <li> <p>Project Configuration: <code>.prodigy/config.toml</code></p> </li> <li>Project-specific overrides</li> <li> <p>Contains project name, description, spec directory, and custom variables</p> </li> <li> <p>Workflow Environment: <code>env:</code> block in workflow YAML files</p> </li> <li>Workflow-specific configuration</li> <li>Defines variables, secrets, and profiles for the workflow</li> </ol> <p>Configuration Precedence Chain: <pre><code>Step env &gt; Workflow profile &gt; Workflow env &gt; Project config &gt; Global config &gt; System env\n</code></pre></p> <p>Higher-priority configurations override lower-priority ones. For example, a step-level environment variable will override the same variable defined in the workflow env block.</p>"},{"location":"automated-documentation/advanced-configuration/#environment-variables","title":"Environment Variables","text":"<p>Environment variables parameterize workflows and can be defined in the <code>env:</code> block at the workflow root (Source: src/cook/environment/config.rs:12-36).</p> <p>Environment Configuration Structure (Source: src/cook/environment/config.rs:12-36):</p> <pre><code>env:\n  # Plain variables\n  PROJECT_NAME: \"Prodigy\"\n  VERSION: \"1.0.0\"\n  BOOK_DIR: \"book\"\n\n  # Secret variables (masked in logs)\n  API_KEY:\n    secret: true\n    value: \"sk-abc123\"\n\n  # Profile-specific variables\n  DATABASE_URL:\n    default: \"postgres://localhost/dev\"\n    prod: \"postgres://prod-server/db\"\n</code></pre> <p>Variable Interpolation Syntax: - <code>$VAR</code> - Simple variable reference (shell-style) - <code>${VAR}</code> - Bracketed reference for clarity</p> <p>Secret Masking (Source: src/cook/environment/mod.rs:45-61):</p> <p>Variables marked with <code>secret: true</code> are automatically masked in command output logs, error messages, event logs, and checkpoint files. The masking utility replaces secret values with <code>***MASKED***</code>.</p> <p>Profile Support:</p> <p>Activate different environment profiles using the <code>--profile</code> flag: <pre><code>prodigy run workflow.yml --profile prod\n</code></pre></p> <p>Real-World Example (Source: workflows/book-docs-drift.yml:8-21):</p> <pre><code>env:\n  # Project configuration\n  PROJECT_NAME: \"Prodigy\"\n  PROJECT_CONFIG: \".prodigy/book-config.json\"\n  FEATURES_PATH: \".prodigy/book-analysis/features.json\"\n\n  # Book-specific settings\n  BOOK_DIR: \"book\"\n  ANALYSIS_DIR: \".prodigy/book-analysis\"\n  CHAPTERS_FILE: \"workflows/data/prodigy-chapters.json\"\n\n  # Workflow settings\n  MAX_PARALLEL: \"3\"\n</code></pre> <p>These variables are referenced throughout the workflow using <code>$VARIABLE_NAME</code> or <code>${VARIABLE_NAME}</code> syntax.</p>"},{"location":"automated-documentation/advanced-configuration/#mapreduce-performance-tuning","title":"MapReduce Performance Tuning","text":"<p>For documentation workflows using MapReduce, several configuration options control parallelism and resource usage (Source: src/config/mapreduce.rs:238-241, 276-278).</p> <p>max_parallel Configuration (Source: src/config/mapreduce.rs:238-241):</p> <p>Controls the number of concurrent documentation agents processing chapters/subsections in parallel:</p> <pre><code>map:\n  input: \"${ANALYSIS_DIR}/flattened-items.json\"\n  json_path: \"$[*]\"\n\n  agent_template:\n    - claude: \"/prodigy-fix-subsection-drift --project $PROJECT_NAME --json '${item}'\"\n\n  max_parallel: ${MAX_PARALLEL}  # Default: 10\n</code></pre> <p>Performance Trade-offs: - Higher parallelism (10+): Faster completion, higher resource usage (CPU, memory, disk I/O) - Lower parallelism (3-5): More conservative resource usage, longer total execution time - Balanced approach (5-7): Good for most documentation workflows</p> <p>The <code>book-docs-drift.yml</code> workflow uses <code>MAX_PARALLEL: 3</code> for balanced performance and resource management.</p> <p>Timeout Configuration:</p> <p>While not explicitly shown in the MapReduce configuration, agent timeouts can be configured for long-running documentation tasks: - <code>agent_timeout_secs</code>: Maximum time allowed for each map agent - <code>setup_timeout</code>: Maximum time for feature analysis phase - <code>reduce_timeout</code>: Maximum time for book build phase</p>"},{"location":"automated-documentation/advanced-configuration/#book-configuration","title":"Book Configuration","text":"<p>The <code>.prodigy/book-config.json</code> file defines book-specific analysis and generation settings (Source: .prodigy/book-config.json:1-220).</p> <p>Book Configuration Structure (Source: .prodigy/book-config.json):</p> <pre><code>{\n  \"project_name\": \"Prodigy\",\n  \"project_type\": \"cli_tool\",\n  \"book_dir\": \"book\",\n  \"book_src\": \"book/src\",\n  \"book_build_dir\": \"book/book\",\n  \"analysis_targets\": [\n    {\n      \"area\": \"configuration\",\n      \"source_files\": [\n        \"src/config/mod.rs\",\n        \"src/config/settings.rs\"\n      ],\n      \"feature_categories\": [\n        \"file_locations\",\n        \"precedence\",\n        \"claude_settings\",\n        \"storage_settings\"\n      ]\n    }\n  ],\n  \"chapter_file\": \"workflows/data/prodigy-chapters.json\",\n  \"custom_analysis\": {\n    \"include_examples\": true,\n    \"include_best_practices\": true,\n    \"include_troubleshooting\": true\n  }\n}\n</code></pre> <p>Key Fields: - <code>analysis_targets</code>: Defines codebase areas to analyze for feature extraction - <code>source_files</code>: Source code files to scan for each area - <code>feature_categories</code>: Categories of features to document for each area - <code>custom_analysis</code>: Options for including examples, best practices, and troubleshooting sections</p> <p>Adapting for Different Project Types: - Rust: Use <code>src/**/*.rs</code> patterns - Python: Use <code>src/**/*.py</code> or package structure - JavaScript: Use <code>src/**/*.js</code>, <code>src/**/*.ts</code></p>"},{"location":"automated-documentation/advanced-configuration/#claude-specific-configuration","title":"Claude-Specific Configuration","text":"<p>Control Claude's behavior during documentation generation with environment variables and verbosity flags.</p> <p>Claude Streaming Configuration:</p> <ul> <li><code>PRODIGY_CLAUDE_STREAMING=false</code>: Disable JSON streaming output (useful in CI/CD)</li> <li><code>PRODIGY_CLAUDE_CONSOLE_OUTPUT=true</code>: Force streaming output regardless of verbosity</li> <li><code>-v</code> flag: Enable verbose mode to see Claude streaming output for debugging</li> </ul> <p>Claude Log Locations:</p> <p>Claude creates detailed JSON log files for each command execution at: <pre><code>~/.local/state/claude/logs/session-{session_id}.json\n</code></pre></p> <p>Analyzing Claude Logs for Debugging:</p> <pre><code># View complete Claude interaction\ncat ~/.local/state/claude/logs/session-abc123.json | jq '.messages'\n\n# Check tool invocations\ncat ~/.local/state/claude/logs/session-abc123.json | jq '.messages[].content[] | select(.type == \"tool_use\")'\n\n# Analyze token usage\ncat ~/.local/state/claude/logs/session-abc123.json | jq '.usage'\n</code></pre> <p>Use <code>-v</code> flag during workflow execution to see real-time streaming output from Claude for troubleshooting failed documentation agents.</p>"},{"location":"automated-documentation/advanced-configuration/#error-handling-configuration","title":"Error Handling Configuration","text":"<p>Configure how documentation workflows handle failures and errors (Source: workflows/book-docs-drift.yml:85-90).</p> <p>Error Policy Configuration (Source: workflows/book-docs-drift.yml:85-90):</p> <pre><code>error_policy:\n  on_item_failure: dlq            # Send failed items to Dead Letter Queue\n  continue_on_failure: true       # Continue processing other items\n  max_failures: 2                 # Stop workflow after 2 failures\n  error_collection: aggregate     # Aggregate errors for reporting\n</code></pre> <p>Error Policy Options: - <code>on_item_failure</code>: <code>dlq</code> (Dead Letter Queue), <code>fail</code> (stop immediately), <code>skip</code> (continue) - <code>continue_on_failure</code>: Whether to continue processing remaining items after a failure - <code>max_failures</code>: Maximum number of failures before stopping the entire workflow - <code>error_collection</code>: How to collect and report errors (<code>aggregate</code>, <code>individual</code>)</p> <p>Dead Letter Queue (DLQ) Usage:</p> <p>Failed documentation items are stored in <code>~/.prodigy/dlq/{repo_name}/{job_id}/</code> for review and retry:</p> <pre><code># View failed items\nprodigy dlq show &lt;job_id&gt;\n\n# Retry all failed items\nprodigy dlq retry &lt;job_id&gt;\n\n# Retry with custom parallelism\nprodigy dlq retry &lt;job_id&gt; --max-parallel 5\n</code></pre> <p>Retry Strategies:</p> <p>While not shown in the example workflow, retry configuration can be added to commands: - Backoff strategies: exponential, linear, fibonacci - Max retry attempts - Retry budget limits</p>"},{"location":"automated-documentation/advanced-configuration/#storage-and-worktree-configuration","title":"Storage and Worktree Configuration","text":"<p>Prodigy uses global storage for centralized state management and git worktrees for isolation.</p> <p>Global Storage Locations: - Events: <code>~/.prodigy/events/{repo_name}/{job_id}/</code> - DLQ: <code>~/.prodigy/dlq/{repo_name}/{job_id}/</code> - State: <code>~/.prodigy/state/{repo_name}/mapreduce/jobs/{job_id}/</code> - Worktrees: <code>~/.prodigy/worktrees/{repo_name}/</code></p> <p>Repository Grouping:</p> <p>All storage is grouped by repository name, enabling: - Cross-worktree event aggregation - Persistent state across worktree cleanup - Centralized monitoring of all jobs for a repository</p> <p>Cleanup Policies: - Automatic cleanup on success: Worktrees are removed after successful agent completion - Orphan registry on failure: Failed worktrees are registered in <code>~/.prodigy/orphaned_worktrees/{repo_name}/{job_id}.json</code></p> <p>Cleaning Orphaned Worktrees:</p> <pre><code># List orphaned worktrees\nprodigy worktree clean-orphaned &lt;job_id&gt;\n\n# Clean with confirmation\nprodigy worktree clean-orphaned &lt;job_id&gt; --force\n</code></pre>"},{"location":"automated-documentation/advanced-configuration/#validation-configuration","title":"Validation Configuration","text":"<p>Configure quality gates and validation for documentation generation (Source: workflows/book-docs-drift.yml:49-57).</p> <p>Validation Configuration (Source: workflows/book-docs-drift.yml:49-57):</p> <pre><code>validate:\n  claude: \"/prodigy-validate-doc-fix --project $PROJECT_NAME --json '${item}' --output .prodigy/validation-result.json\"\n  result_file: \".prodigy/validation-result.json\"\n  threshold: 100  # Documentation must meet 100% quality standards\n  on_incomplete:\n    claude: \"/prodigy-complete-doc-fix --project $PROJECT_NAME --json '${item}' --gaps ${validation.gaps}\"\n    max_attempts: 3\n    fail_workflow: false  # Continue even if we can't reach 100%\n    commit_required: true\n</code></pre> <p>Validation Options: - <code>threshold</code>: Completion percentage required to pass (0-100) - <code>result_file</code>: File where validation results are written - <code>on_incomplete</code>: Handler to execute when validation threshold is not met - <code>max_attempts</code>: Maximum attempts to complete validation - <code>fail_workflow</code>: Whether to fail the entire workflow if validation never passes</p> <p>Quality Gates:</p> <p>The validation system ensures: - All critical drift issues are addressed - Documentation meets minimum content requirements - Examples are grounded in actual codebase - Links are valid and point to existing files</p>"},{"location":"automated-documentation/advanced-configuration/#configuration-checklist-for-optimizing-documentation-workflows","title":"Configuration Checklist for Optimizing Documentation Workflows","text":"<p>Performance Optimization: - [ ] Set <code>MAX_PARALLEL</code> based on available CPU cores (recommend: cores / 2) - [ ] Configure agent timeouts appropriate for documentation complexity - [ ] Use global storage for centralized state management</p> <p>Security: - [ ] Mark API keys and sensitive data as secrets (<code>secret: true</code>) - [ ] Use profiles to separate development and production credentials - [ ] Enable secret masking for logs and error output</p> <p>Quality Control: - [ ] Set validation threshold to 100% for production documentation - [ ] Configure <code>on_incomplete</code> handlers to automatically fix validation failures - [ ] Enable <code>error_policy.on_item_failure: dlq</code> for failed item recovery</p> <p>Resource Management: - [ ] Configure cleanup policies for worktrees - [ ] Set <code>max_failures</code> to prevent runaway workflows - [ ] Use <code>continue_on_failure: true</code> to maximize successful documentation coverage</p> <p>Debugging: - [ ] Enable Claude streaming in development (<code>-v</code> flag or <code>PRODIGY_CLAUDE_CONSOLE_OUTPUT=true</code>) - [ ] Configure verbose logging for troubleshooting - [ ] Preserve Claude JSON logs for post-mortem analysis</p>"},{"location":"automated-documentation/advanced-configuration/#troubleshooting-common-configuration-issues","title":"Troubleshooting Common Configuration Issues","text":"<p>Issue: Documentation workflow is too slow - Solution: Increase <code>MAX_PARALLEL</code> value, but monitor resource usage - Check: CPU and memory utilization during workflow execution</p> <p>Issue: Out of memory errors during MapReduce - Solution: Decrease <code>max_parallel</code> to reduce concurrent agent count - Check: Each agent may load large amounts of documentation into context</p> <p>Issue: Secrets appearing in logs - Solution: Ensure secrets are marked with <code>secret: true</code> in environment config - Check: Review event logs and Claude logs for masked values</p> <p>Issue: Validation always failing at 100% threshold - Solution: Review validation command output to identify quality gaps - Check: Use <code>on_incomplete</code> handler with <code>max_attempts</code> to iteratively improve</p> <p>Issue: Orphaned worktrees consuming disk space - Solution: Run <code>prodigy worktree clean-orphaned &lt;job_id&gt;</code> regularly - Check: Monitor <code>~/.prodigy/worktrees/</code> directory size</p>"},{"location":"automated-documentation/advanced-configuration/#see-also","title":"See Also","text":"<ul> <li>Understanding the Workflow - Overview of documentation workflow phases</li> <li>Troubleshooting - Common issues and solutions</li> <li>Quick Start - Getting started with automated documentation</li> <li>Environment Variables - Detailed environment variable reference</li> <li>Configuration Precedence Rules - How configuration values are resolved</li> </ul>"},{"location":"automated-documentation/automatic-gap-detection/","title":"Automatic Gap Detection","text":""},{"location":"automated-documentation/automatic-gap-detection/#automatic-gap-detection","title":"Automatic Gap Detection","text":"<p>Automatic gap detection is a critical component of Prodigy's documentation workflow that identifies undocumented features and automatically creates chapter/subsection definitions with stub markdown files. This ensures comprehensive documentation coverage and prevents features from being implemented without corresponding user guidance.</p> <p>Source: Implemented in <code>.claude/commands/prodigy-detect-documentation-gaps.md:1-1048</code> and tested in <code>tests/documentation_gap_detection_test.rs:1-678</code></p>"},{"location":"automated-documentation/automatic-gap-detection/#overview","title":"Overview","text":"<p>Gap detection runs in the setup phase of the book workflow (workflows/book-docs-drift.yml:31-34) and performs several key functions:</p> <ol> <li>Analyzes features.json (from feature analysis) against existing chapters/subsections</li> <li>Classifies gaps by severity (high, medium, low)</li> <li>Validates content sufficiency before creating subsections (Step 0)</li> <li>Syncs chapters.json with actual file structure (Phase 7.5)</li> <li>Creates missing chapter definitions and stub markdown files</li> <li>Updates SUMMARY.md with proper hierarchy</li> <li>Generates flattened-items.json for the map phase (mandatory)</li> </ol> <p>The gap detection process ensures that: - Features aren't documented without sufficient codebase material (prevents stub subsections) - Multi-subsection chapter structures are accurately reflected in chapters.json - The map phase receives a complete, flat list of all chapters and subsections to process - Documentation organization matches implementation reality</p>"},{"location":"automated-documentation/automatic-gap-detection/#command-usage","title":"Command Usage","text":"<p>Command: <code>/prodigy-detect-documentation-gaps</code></p> <p>Parameters (.claude/commands/prodigy-detect-documentation-gaps.md:5-11): <pre><code>/prodigy-detect-documentation-gaps \\\n  --project \"Prodigy\" \\\n  --config \".prodigy/book-config.json\" \\\n  --features \".prodigy/book-analysis/features.json\" \\\n  --chapters \"workflows/data/prodigy-chapters.json\" \\\n  --book-dir \"book\"\n</code></pre></p> <p>Workflow Integration (workflows/book-docs-drift.yml:31-34): <pre><code>setup:\n  # Step 1: Analyze features\n  - claude: \"/prodigy-analyze-features-for-book --project $PROJECT_NAME --config $PROJECT_CONFIG\"\n\n  # Step 2: Detect gaps and generate flattened-items.json\n  - claude: \"/prodigy-detect-documentation-gaps \\\n      --project $PROJECT_NAME \\\n      --config $PROJECT_CONFIG \\\n      --features $FEATURES_PATH \\\n      --chapters $CHAPTERS_FILE \\\n      --book-dir $BOOK_DIR\"\n</code></pre></p>"},{"location":"automated-documentation/automatic-gap-detection/#gap-severity-classification","title":"Gap Severity Classification","text":"<p>Gap detection classifies documentation gaps into three severity levels based on feature importance and documentation completeness (.claude/commands/prodigy-detect-documentation-gaps.md:66-112):</p>"},{"location":"automated-documentation/automatic-gap-detection/#high-severity-missing-chaptersubsection","title":"High Severity (Missing Chapter/Subsection)","text":"<p>Criteria: - Feature area exists in features.json - NO corresponding chapter OR subsection found - Major user-facing capability with no guidance</p> <p>Example: <pre><code>{\n  \"severity\": \"high\",\n  \"type\": \"missing_chapter\",\n  \"feature_category\": \"agent_merge\",\n  \"feature_description\": \"Custom merge workflows for map agents\",\n  \"recommended_chapter_id\": \"agent-merge-workflows\",\n  \"recommended_title\": \"Agent Merge Workflows\"\n}\n</code></pre></p> <p>Action: Create new chapter definition with stub markdown file</p>"},{"location":"automated-documentation/automatic-gap-detection/#medium-severity-incomplete-chaptersubsection","title":"Medium Severity (Incomplete Chapter/Subsection)","text":"<p>Criteria: - Chapter or multi-subsection structure exists for feature area - But specific sub-capabilities are missing - Could be addressed by adding subsection or expanding content</p> <p>Example: - \"mapreduce\" chapter exists but missing \"performance_tuning\" subsection</p> <p>Action: Create subsection definition and add to existing multi-subsection chapter</p>"},{"location":"automated-documentation/automatic-gap-detection/#low-severity-minor-gap","title":"Low Severity (Minor Gap)","text":"<p>Criteria: - Edge cases or advanced features not documented - Internal APIs exposed to users - Less common use cases</p> <p>Action: Log as warning but may not create new content</p>"},{"location":"automated-documentation/automatic-gap-detection/#content-sufficiency-validation-step-0","title":"Content Sufficiency Validation (Step 0)","text":"<p>CRITICAL SAFEGUARD: Before creating any subsection, gap detection validates that sufficient material exists in the codebase to support meaningful documentation.</p> <p>Source: <code>.claude/commands/prodigy-detect-documentation-gaps.md:166-335</code></p>"},{"location":"automated-documentation/automatic-gap-detection/#preservation-of-single-file-chapters","title":"Preservation of Single-File Chapters","text":"<p>Gap detection ALWAYS preserves well-written single-file chapters (.claude/commands/prodigy-detect-documentation-gaps.md:174-209):</p> <p>Preservation Rules: - &lt; 1000 lines AND &lt; 10 H2 sections: PRESERVE as single-file - \u2265 1000 lines OR \u2265 10 H2 sections: Consider subsections for readability</p> <p>Why: The original flat documentation structure works well for moderate-sized chapters. Subsections should only be created when they genuinely improve navigation.</p>"},{"location":"automated-documentation/automatic-gap-detection/#content-availability-validation","title":"Content Availability Validation","text":"<p>Step 0a: Discover Codebase Structure (.claude/commands/prodigy-detect-documentation-gaps.md:211-222)</p> <p>Before counting content, the command discovers where code and examples are located using language-agnostic patterns:</p> <pre><code># Discover test locations\nTEST_DIRS=$(find . -type d -name \"*test*\" -o -name \"*spec*\" | grep -v node_modules | grep -v .git | head -5)\n\n# Discover example/workflow/config locations\nEXAMPLE_DIRS=$(find . -type d -name \"*example*\" -o -name \"*workflow*\" -o -name \"*sample*\" -o -name \"*config*\" | grep -v node_modules | grep -v .git | head -5)\n\n# Discover primary source locations (works for Rust, Python, JS, TS, Go, Java)\nSOURCE_DIRS=$(find . -type f \\( -name \"*.rs\" -o -name \"*.py\" -o -name \"*.js\" -o -name \"*.ts\" -o -name \"*.go\" -o -name \"*.java\" \\) | sed 's|/[^/]*$||' | sort -u | grep -v node_modules | grep -v .git | head -10)\n</code></pre> <p>Step 0b: Count Potential Content Sources (.claude/commands/prodigy-detect-documentation-gaps.md:224-255)</p> <p>For each proposed subsection, the command counts language-agnostic content sources:</p> <pre><code>FEATURE_CATEGORY=\"&lt;feature-category-name&gt;\"\n\n# Type definitions (struct, class, interface, enum, type)\nTYPE_COUNT=$(rg \"(struct|class|interface|type|enum).*${FEATURE_CATEGORY}\" --hidden --iglob '!.git' --iglob '!node_modules' -c | awk '{s+=$1} END {print s}')\n\n# Function/method definitions\nFUNCTION_COUNT=$(rg \"(fn|function|def|func|public|private).*${FEATURE_CATEGORY}\" --hidden --iglob '!.git' --iglob '!node_modules' -c | awk '{s+=$1} END {print s}')\n\n# Test mentions in discovered test directories\nTEST_COUNT=0\nfor test_dir in $TEST_DIRS; do\n  count=$(rg \"${FEATURE_CATEGORY}\" \"$test_dir\" --hidden -c 2&gt;/dev/null | awk '{s+=$1} END {print s}')\n  TEST_COUNT=$((TEST_COUNT + count))\ndone\n\n# Example/config file mentions in discovered example directories\nEXAMPLE_COUNT=0\nfor example_dir in $EXAMPLE_DIRS; do\n  count=$(rg \"${FEATURE_CATEGORY}\" \"$example_dir\" --hidden -c 2&gt;/dev/null | awk '{s+=$1} END {print s}')\n  EXAMPLE_COUNT=$((EXAMPLE_COUNT + count))\ndone\n\n# Calculate totals\nTOTAL_MENTIONS=$((TYPE_COUNT + FUNCTION_COUNT + TEST_COUNT + EXAMPLE_COUNT))\n\n# Estimate documentation lines (rule of thumb)\n# Each type = ~30 lines docs, each function = ~10 lines, each example = ~40 lines, each test = ~15 lines\nESTIMATED_LINES=$((TYPE_COUNT * 30 + FUNCTION_COUNT * 10 + EXAMPLE_COUNT * 40 + TEST_COUNT * 15))\n</code></pre>"},{"location":"automated-documentation/automatic-gap-detection/#content-sufficiency-thresholds","title":"Content Sufficiency Thresholds","text":"<p>MUST HAVE (to create subsection) - (.claude/commands/prodigy-detect-documentation-gaps.md:259-265): - <code>TOTAL_MENTIONS &gt;= 5</code> - Feature mentioned in at least 5 places - <code>ESTIMATED_LINES &gt;= 50</code> - Can generate at least 50 lines of documentation - At least ONE of:   - <code>TYPE_COUNT &gt;= 1</code> (has configuration type/struct/class)   - <code>EXAMPLE_COUNT &gt;= 1</code> (has real example/config file)</p> <p>SHOULD HAVE (for quality subsection) - (.claude/commands/prodigy-detect-documentation-gaps.md:266-269): - <code>TOTAL_MENTIONS &gt;= 10</code> - <code>ESTIMATED_LINES &gt;= 100</code> - <code>TYPE_COUNT &gt;= 1 AND EXAMPLE_COUNT &gt;= 1</code> (both type definition and example)</p>"},{"location":"automated-documentation/automatic-gap-detection/#decision-tree","title":"Decision Tree","text":"<p>If TOTAL_MENTIONS &lt; 5 OR ESTIMATED_LINES &lt; 50: - \u2717 DO NOT create subsection - Alternative: Add as section within parent chapter's index.md - Log: \"\u26a0 Skipping subsection '${SUBSECTION_TITLE}': only ${TOTAL_MENTIONS} mentions, ${ESTIMATED_LINES} estimated lines\" - Gap Report: Record as <code>\"action\": \"skipped_subsection_creation\", \"reason\": \"insufficient_content\"</code></p> <p>If TOTAL_MENTIONS &gt;= 5 AND ESTIMATED_LINES &gt;= 50 BUT &lt; 100: - ~ Create subsection with \"MINIMAL\" flag - Add metadata: <code>{\"content_warning\": \"minimal\", \"estimated_lines\": ESTIMATED_LINES}</code> - Signals to fix phase that limited content is expected</p> <p>If TOTAL_MENTIONS &gt;= 10 AND ESTIMATED_LINES &gt;= 100: - \u2713 Proceed with full subsection creation</p>"},{"location":"automated-documentation/automatic-gap-detection/#special-case-meta-subsections","title":"Special Case: Meta-Subsections","text":"<p>Meta-subsections like \"Best Practices\", \"Troubleshooting\", and \"Examples\" use different validation criteria (.claude/commands/prodigy-detect-documentation-gaps.md:306-334):</p> <p>Best Practices Subsection: <pre><code>BEST_PRACTICE_COUNT=$(rg \"best.practice|pattern|guideline\" --hidden --iglob '!.git' --iglob '!node_modules' -i -c | awk '{s+=$1} END {print s}')\n# Requirement: BEST_PRACTICE_COUNT &gt;= 3 OR documented patterns in code\n</code></pre></p> <p>Troubleshooting Subsection: <pre><code>ERROR_COUNT=$(rg \"error|warn|fail\" --hidden --iglob '!.git' --iglob '!node_modules' -c | awk '{s+=$1} END {print s}')\nISSUE_COUNT=$(rg \"TODO|FIXME|XXX\" --hidden --iglob '!.git' --iglob '!node_modules' -c | awk '{s+=$1} END {print s}')\n# Requirement: ERROR_COUNT &gt;= 10 OR ISSUE_COUNT &gt;= 5\n</code></pre></p> <p>Examples Subsection: <pre><code>EXAMPLE_FILE_COUNT=0\nfor example_dir in $EXAMPLE_DIRS; do\n  count=$(find \"$example_dir\" -type f \\( -name \"*.yml\" -o -name \"*.yaml\" -o -name \"*.json\" -o -name \"*.toml\" \\) 2&gt;/dev/null | wc -l)\n  EXAMPLE_FILE_COUNT=$((EXAMPLE_FILE_COUNT + count))\ndone\n# Requirement: EXAMPLE_FILE_COUNT &gt;= 2 real config files\n</code></pre></p> <p>If threshold not met: Add brief section to parent chapter's index.md instead of creating separate subsection.</p>"},{"location":"automated-documentation/automatic-gap-detection/#structure-validation-phase-75","title":"Structure Validation (Phase 7.5)","text":"<p>MANDATORY: Ensures chapters.json accurately reflects the actual file structure before generating flattened-items.json.</p> <p>Source: <code>.claude/commands/prodigy-detect-documentation-gaps.md:678-743</code></p>"},{"location":"automated-documentation/automatic-gap-detection/#validation-process","title":"Validation Process","text":"<p>Step 1: Scan for Multi-Subsection Directories</p> <p>Find all directories under <code>book/src/</code> with an <code>index.md</code> file and count <code>.md</code> subsection files:</p> <pre><code>for dir in $(find \"${BOOK_DIR}/src/\" -maxdepth 1 -type d); do\n  if [ -f \"${dir}/index.md\" ]; then\n    SUBSECTION_COUNT=$(find \"${dir}\" -maxdepth 1 -name \"*.md\" ! -name \"index.md\" | wc -l)\n    if [ \"$SUBSECTION_COUNT\" -gt 0 ]; then\n      # This is a multi-subsection chapter\n      CHAPTER_ID=$(basename \"$dir\")\n      echo \"Found multi-subsection chapter: $CHAPTER_ID\"\n    fi\n  fi\ndone\n</code></pre> <p>Step 2: Compare Against chapters.json</p> <p>For each discovered multi-subsection chapter: 1. Look up definition in chapters.json 2. Check if <code>type</code> field is \"multi-subsection\" or \"single-file\" 3. If type is \"single-file\" or missing: MISMATCH - add to mismatches list 4. If type is \"multi-subsection\": Compare subsection counts    - If counts don't match: MISMATCH</p> <p>Step 3: Check for Orphaned Single-File Definitions</p> <p>For each chapter with <code>type: \"single-file\"</code>: 1. Check if expected file (<code>book/src/chapter-id.md</code>) exists 2. Check if directory (<code>book/src/chapter-id/</code>) exists instead 3. If file missing but directory exists: MISMATCH</p> <p>Step 4: Auto-Migrate Mismatched Chapters</p> <p>For each mismatched chapter: 1. Scan directory to discover all subsection files 2. For each <code>.md</code> file (excluding <code>index.md</code>):    - Extract subsection ID from filename (remove <code>.md</code>)    - Read file and extract title from first H1/H2 heading    - Extract topics from section headings    - Create subsection definition 3. Update chapter in chapters.json:    - Change <code>type</code> to \"multi-subsection\"    - Change <code>file</code> to <code>index_file</code> (pointing to <code>index.md</code>)    - Add <code>subsections</code> array with all discovered subsections    - Preserve existing <code>topics</code> and <code>validation</code> fields 4. Write updated chapters.json to disk 5. Record migration in gap report</p>"},{"location":"automated-documentation/automatic-gap-detection/#example-migration","title":"Example Migration","text":"<p>Before (chapters.json - incorrect): <pre><code>{\n  \"id\": \"mapreduce\",\n  \"title\": \"MapReduce Workflows\",\n  \"file\": \"mapreduce.md\",\n  \"type\": \"single-file\",\n  \"topics\": [\"Map phase\", \"Reduce phase\"]\n}\n</code></pre></p> <p>Actual File Structure (reality): <pre><code>book/src/mapreduce/\n\u251c\u2500\u2500 index.md\n\u251c\u2500\u2500 checkpoint-and-resume.md\n\u251c\u2500\u2500 performance-tuning.md\n\u2514\u2500\u2500 worktree-isolation.md\n</code></pre></p> <p>After Migration (chapters.json - corrected): <pre><code>{\n  \"id\": \"mapreduce\",\n  \"title\": \"MapReduce Workflows\",\n  \"index_file\": \"mapreduce/index.md\",\n  \"type\": \"multi-subsection\",\n  \"topics\": [\"Map phase\", \"Reduce phase\"],\n  \"subsections\": [\n    {\n      \"id\": \"checkpoint-and-resume\",\n      \"title\": \"Checkpoint and Resume\",\n      \"file\": \"mapreduce/checkpoint-and-resume.md\"\n    },\n    {\n      \"id\": \"performance-tuning\",\n      \"title\": \"Performance Tuning\",\n      \"file\": \"mapreduce/performance-tuning.md\"\n    },\n    {\n      \"id\": \"worktree-isolation\",\n      \"title\": \"Worktree Isolation\",\n      \"file\": \"mapreduce/worktree-isolation.md\"\n    }\n  ]\n}\n</code></pre></p> <p>Commit: Structure fixes are committed BEFORE generating flattened-items.json with message: \"docs: sync chapters.json with actual file structure\"</p>"},{"location":"automated-documentation/automatic-gap-detection/#flattened-items-generation-phase-8","title":"Flattened Items Generation (Phase 8)","text":"<p>CRITICAL: This file MUST be generated regardless of whether gaps are found. The map phase depends on it.</p> <p>Source: <code>.claude/commands/prodigy-detect-documentation-gaps.md:744-827</code></p>"},{"location":"automated-documentation/automatic-gap-detection/#purpose","title":"Purpose","text":"<p>Creates a flat array of all chapters and subsections for parallel processing in the map phase. This enables each map agent to work on a single chapter or subsection independently.</p>"},{"location":"automated-documentation/automatic-gap-detection/#processing-logic","title":"Processing Logic","text":"<pre><code>For each chapter in chapters.json:\n  If type == \"multi-subsection\":\n    For each subsection in chapter.subsections:\n      Create item with parent metadata\n      Add to flattened array\n\n  If type == \"single-file\":\n    Create item with type marker\n    Add to flattened array\n</code></pre>"},{"location":"automated-documentation/automatic-gap-detection/#output-structure","title":"Output Structure","text":"<p>File: <code>.prodigy/book-analysis/flattened-items.json</code></p> <p>Example: <pre><code>[\n  {\n    \"id\": \"workflow-basics\",\n    \"title\": \"Workflow Basics\",\n    \"file\": \"book/src/workflow-basics.md\",\n    \"topics\": [\n      \"Setup phase\",\n      \"Command types\",\n      \"Variable interpolation\"\n    ],\n    \"validation\": \"Check that workflow syntax and variable documentation are complete\",\n    \"type\": \"single-file\"\n  },\n  {\n    \"id\": \"checkpoint-and-resume\",\n    \"title\": \"Checkpoint and Resume\",\n    \"file\": \"book/src/mapreduce/checkpoint-and-resume.md\",\n    \"parent_chapter_id\": \"mapreduce\",\n    \"parent_chapter_title\": \"MapReduce Workflows\",\n    \"type\": \"subsection\",\n    \"topics\": [\n      \"Checkpoint creation\",\n      \"Resume behavior\",\n      \"State preservation\"\n    ],\n    \"validation\": \"Check that checkpoint mechanism and resume procedures are documented\",\n    \"feature_mapping\": [\n      \"mapreduce.checkpoint\",\n      \"mapreduce.resume\"\n    ]\n  },\n  {\n    \"id\": \"performance-tuning\",\n    \"title\": \"Performance Tuning\",\n    \"file\": \"book/src/mapreduce/performance-tuning.md\",\n    \"parent_chapter_id\": \"mapreduce\",\n    \"parent_chapter_title\": \"MapReduce Workflows\",\n    \"type\": \"subsection\",\n    \"topics\": [\n      \"Parallel execution\",\n      \"Resource limits\"\n    ],\n    \"feature_mapping\": [\n      \"mapreduce.performance\",\n      \"mapreduce.resource_limits\"\n    ]\n  }\n]\n</code></pre></p>"},{"location":"automated-documentation/automatic-gap-detection/#map-phase-integration","title":"Map Phase Integration","text":"<p>The map phase consumes flattened-items.json (workflows/book-docs-drift.yml:36-48):</p> <pre><code>map:\n  input: \"${ANALYSIS_DIR}/flattened-items.json\"\n  json_path: \"$[*]\"  # Each item is a chapter or subsection\n\n  agent_template:\n    # Analyze drift for this specific chapter/subsection\n    - claude: \"/prodigy-analyze-subsection-drift --project $PROJECT_NAME --json '${item}' --features $FEATURES_PATH\"\n\n    # Fix drift for this specific chapter/subsection\n    - claude: \"/prodigy-fix-subsection-drift --project $PROJECT_NAME --json '${item}'\"\n</code></pre> <p>Why Required: Without flattened-items.json, the map phase cannot parallelize drift analysis and fixing across chapters/subsections.</p>"},{"location":"automated-documentation/automatic-gap-detection/#topic-normalization","title":"Topic Normalization","text":"<p>Gap detection uses normalization logic to accurately match feature categories against documented topics (.claude/commands/prodigy-detect-documentation-gaps.md:42-50):</p>"},{"location":"automated-documentation/automatic-gap-detection/#normalization-steps","title":"Normalization Steps","text":"<ol> <li>Convert to lowercase</li> <li>Remove punctuation and special characters</li> <li>Trim whitespace</li> <li>Extract key terms from compound names</li> </ol>"},{"location":"automated-documentation/automatic-gap-detection/#examples","title":"Examples","text":"<pre><code>\"MapReduce Workflows\"     \u2192 [\"mapreduce\", \"workflows\"]\n\"agent_merge\"             \u2192 \"agent-merge\"\n\"command-types\"           \u2192 \"command-types\"\n\"Goal Seeking Operations\" \u2192 [\"goal\", \"seeking\", \"operations\"]\n</code></pre>"},{"location":"automated-documentation/automatic-gap-detection/#matching-logic","title":"Matching Logic","text":"<p>For each feature area in features.json, the command checks if any of these match: 1. Chapter ID contains normalized_category 2. normalized_category contains Chapter ID 3. Chapter title contains normalized_category 4. Chapter topics contain normalized_category 5. Section headings in markdown match normalized_category 6. Subsection feature_mapping arrays match</p> <p>Test Case (tests/documentation_gap_detection_test.rs:236-274): <pre><code>#[test]\nfn test_gap_detection_normalizes_topic_names() -&gt; Result&lt;()&gt; {\n    // Features with underscores\n    let features = vec![\n        MockFeature {\n            category: \"command_types\".to_string(),\n            // ...\n        },\n    ];\n\n    // Chapters with normalized names (hyphens)\n    let chapters = vec![\n        MockChapter {\n            id: \"command-types\".to_string(),  // Hyphen vs underscore\n            // ...\n        },\n    ];\n\n    let gaps = detect_gaps(&amp;features, &amp;chapters);\n\n    // Result: No gaps because normalization matches them\n    assert_eq!(gaps.len(), 0, \"Normalization should match underscore and hyphen variations\");\n\n    Ok(())\n}\n</code></pre></p>"},{"location":"automated-documentation/automatic-gap-detection/#idempotence","title":"Idempotence","text":"<p>Gap detection can be run multiple times safely without creating duplicate chapters or subsections (.claude/commands/prodigy-detect-documentation-gaps.md:867-887).</p>"},{"location":"automated-documentation/automatic-gap-detection/#idempotence-guarantees","title":"Idempotence Guarantees","text":"<ol> <li>Checks for existing chapters before creating</li> <li>Uses normalized comparison for matching</li> <li>Skips already-created chapters</li> <li>Can run repeatedly without side effects</li> </ol>"},{"location":"automated-documentation/automatic-gap-detection/#test-case","title":"Test Case","text":"<p>Source: tests/documentation_gap_detection_test.rs:236-274</p> <pre><code>#[test]\nfn test_gap_detection_idempotence() -&gt; Result&lt;()&gt; {\n    let features = vec![MockFeature {\n        category: \"new_feature\".to_string(),\n        description: \"A new feature\".to_string(),\n        capabilities: vec![\"capability1\".to_string()],\n    }];\n\n    // First run with no chapters\n    let gaps_first = detect_gaps(&amp;features, &amp;vec![]);\n    assert_eq!(gaps_first.len(), 1, \"First run detects 1 gap\");\n\n    // Simulate creating the chapter\n    let updated_chapters = vec![MockChapter {\n        id: \"new-feature\".to_string(),\n        title: \"New Feature\".to_string(),\n        file: \"new-feature.md\".to_string(),\n        topics: vec![\"New feature overview\".to_string()],\n    }];\n\n    // Second run with the new chapter\n    let gaps_second = detect_gaps(&amp;features, &amp;updated_chapters);\n    assert_eq!(gaps_second.len(), 0, \"Second run detects no gaps\");\n\n    Ok(())\n}\n</code></pre>"},{"location":"automated-documentation/automatic-gap-detection/#gap-report-structure","title":"Gap Report Structure","text":"<p>Output: <code>.prodigy/book-analysis/gap-report.json</code></p>"},{"location":"automated-documentation/automatic-gap-detection/#example-report","title":"Example Report","text":"<pre><code>{\n  \"analysis_date\": \"2025-11-09T12:34:56Z\",\n  \"features_analyzed\": 12,\n  \"documented_topics\": 10,\n  \"gaps_found\": 2,\n  \"gaps\": [\n    {\n      \"severity\": \"high\",\n      \"type\": \"missing_chapter\",\n      \"feature_category\": \"agent_merge\",\n      \"feature_description\": \"Custom merge workflows for map agents\",\n      \"recommended_chapter_id\": \"agent-merge-workflows\",\n      \"recommended_title\": \"Agent Merge Workflows\",\n      \"recommended_location\": \"book/src/agent-merge-workflows.md\",\n      \"is_subsection\": false\n    },\n    {\n      \"severity\": \"high\",\n      \"type\": \"missing_chapter\",\n      \"feature_category\": \"circuit_breaker\",\n      \"feature_description\": \"Circuit breaker for error handling\",\n      \"recommended_chapter_id\": \"circuit-breaker\",\n      \"recommended_title\": \"Circuit Breaker\",\n      \"recommended_location\": \"book/src/circuit-breaker.md\",\n      \"is_subsection\": false\n    }\n  ],\n  \"actions_taken\": [\n    {\n      \"action\": \"created_chapter_definition\",\n      \"chapter_id\": \"agent-merge-workflows\",\n      \"file_path\": \"workflows/data/prodigy-chapters.json\"\n    },\n    {\n      \"action\": \"created_stub_file\",\n      \"file_path\": \"book/src/agent-merge-workflows.md\",\n      \"type\": \"chapter\"\n    },\n    {\n      \"action\": \"updated_summary\",\n      \"file_path\": \"book/src/SUMMARY.md\",\n      \"items_added\": [\n        {\"type\": \"chapter\", \"id\": \"agent-merge-workflows\"}\n      ]\n    }\n  ],\n  \"structure_validation\": {\n    \"mismatches_found\": 1,\n    \"mismatched_chapters\": [\"mapreduce\"],\n    \"migrations_performed\": [\n      {\n        \"chapter_id\": \"mapreduce\",\n        \"action\": \"migrated_to_multi_subsection\",\n        \"subsections_discovered\": 3\n      }\n    ],\n    \"validation_timestamp\": \"2025-11-09T12:34:56Z\"\n  }\n}\n</code></pre>"},{"location":"automated-documentation/automatic-gap-detection/#execution-progress","title":"Execution Progress","text":"<p>When gap detection runs, it displays progress through multiple phases:</p> <pre><code>\ud83d\udd0d Analyzing documentation coverage...\n   \u2713 Loaded 12 feature areas from features.json\n   \u2713 Loaded 10 existing chapters\n   \u2713 Parsed SUMMARY.md structure\n\n\ud83d\udcca Comparing features against documentation...\n   \u2713 Analyzed workflow_basics: documented \u2713\n   \u2713 Analyzed mapreduce: documented \u2713\n   \u26a0 Analyzed agent_merge: not documented (gap detected)\n   \u2713 Analyzed command_types: documented \u2713\n   \u26a0 Analyzed circuit_breaker: not documented (gap detected)\n\n\ud83d\udd0d Validating chapter structure (Phase 7.5)...\n   \u2713 Scanning for multi-subsection directories\n   \u2713 Comparing against chapters.json definitions\n   \u26a0 Found mismatch in mapreduce chapter (was single-file, now multi-subsection)\n   \u2713 Auto-migrated mapreduce chapter structure\n\n\ud83d\udcdd Creating missing chapters...\n   \u2713 Generated definition: agent-merge-workflows\n   \u2713 Created stub: book/src/agent-merge-workflows.md\n   \u2713 Generated definition: circuit-breaker\n   \u2713 Created stub: book/src/circuit-breaker.md\n   \u2713 Updated SUMMARY.md\n\n\ud83d\udcbe Generating flattened items for map phase...\n   \u2713 Processed 1 single-file chapter (workflow-basics)\n   \u2713 Processed 3 subsections from mapreduce chapter\n   \u2713 Processed 10 additional chapters/subsections\n   \u2713 Generated .prodigy/book-analysis/flattened-items.json\n\n\ud83d\udcbe Committing changes...\n   \u2713 Staged 6 files\n   \u2713 Committed: docs: auto-discover missing chapters for agent-merge-workflows, circuit-breaker\n   \u2713 Committed: docs: sync chapters.json with actual file structure\n</code></pre>"},{"location":"automated-documentation/automatic-gap-detection/#final-summary","title":"Final Summary","text":"<pre><code>\ud83d\udcca Documentation Gap Analysis Complete\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\nFeatures Analyzed: 12\nDocumented Topics: 10\nGaps Found: 2\n\n\ud83d\udd34 High Severity Gaps (Missing Chapters): 2\n  \u2022 agent_merge - Custom merge workflows for map agents\n  \u2022 circuit_breaker - Workflow error circuit breaking\n\n\u2705 Actions Taken:\n  \u2713 Created 2 chapter definitions in workflows/data/prodigy-chapters.json\n  \u2713 Created 2 stub files in book/src/\n  \u2713 Updated book/src/SUMMARY.md\n  \u2713 Generated flattened-items.json with 14 items\n  \u2713 Auto-migrated 1 chapter structure\n  \u2713 Committed changes (2 commits)\n\n\ud83d\udcdd Next Steps:\n  The map phase will now process 14 chapters/subsections to populate content.\n  Review the generated stubs and customize as needed.\n</code></pre>"},{"location":"automated-documentation/automatic-gap-detection/#error-handling","title":"Error Handling","text":"<p>Source: <code>.claude/commands/prodigy-detect-documentation-gaps.md:889-919</code></p>"},{"location":"automated-documentation/automatic-gap-detection/#common-errors","title":"Common Errors","text":"<p>Missing features.json: - Cause: Feature analysis step hasn't run yet - Solution: Ensure <code>/prodigy-analyze-features-for-book</code> runs before gap detection in setup phase - Error Message: \"Error: features.json not found at {path}. Run feature analysis first.\"</p> <p>Missing/Invalid chapters.json: - Cause: Chapter definitions file doesn't exist or has invalid JSON - Solution: Create valid chapters.json or fix JSON syntax errors - Recovery: Gap detection can initialize empty chapters.json if needed</p> <p>File Write Failures: - Cause: Permission issues or disk full - Solution: Check directory permissions and disk space - Rollback: Gap detection records partial state in gap report for manual cleanup</p> <p>Invalid JSON Handling: - Cause: Malformed JSON in input files - Solution: Validate JSON with <code>jq</code> before running workflow - Error Recording: Details added to gap report for debugging</p>"},{"location":"automated-documentation/automatic-gap-detection/#testing","title":"Testing","text":"<p>Gap detection has comprehensive test coverage in <code>tests/documentation_gap_detection_test.rs:1-678</code>:</p>"},{"location":"automated-documentation/automatic-gap-detection/#test-coverage","title":"Test Coverage","text":"<p>Core Functionality: - Identifying missing chapters (tests/documentation_gap_detection_test.rs:1-50) - Idempotence behavior (tests/documentation_gap_detection_test.rs:236-274) - Topic normalization logic (tests/documentation_gap_detection_test.rs:275-320) - Chapter definition generation (tests/documentation_gap_detection_test.rs:321-370)</p> <p>Edge Cases: - False positive prevention via normalization - Handling chapters with multiple topics - Subsection discovery and validation - Structure migration for multi-subsection chapters</p> <p>Quality Assurance: - Stub file structure validation - SUMMARY.md update correctness - Gap report JSON schema validation</p>"},{"location":"automated-documentation/automatic-gap-detection/#see-also","title":"See Also","text":"<ul> <li>./understanding-the-workflow.md - How gap detection fits in the overall workflow</li> <li>MapReduce Workflows - MapReduce phase that consumes flattened-items.json</li> <li>./index.md - Automated documentation overview</li> </ul>"},{"location":"automated-documentation/documentation-versioning/","title":"Documentation Versioning","text":""},{"location":"automated-documentation/documentation-versioning/#documentation-versioning","title":"Documentation Versioning","text":"<p>Status: Planned Feature</p> <p>Documentation versioning is currently in the design phase (Specifications 154, 155, 156 in draft status). This page documents the planned implementation for projects that need to serve multiple documentation versions.</p> <p>For projects that need to serve multiple documentation versions (e.g., users on different software releases), Prodigy is designing a comprehensive versioned documentation system. This will allow users to select which version of the docs they want to view using a dropdown selector.</p>"},{"location":"automated-documentation/documentation-versioning/#overview","title":"Overview","text":"<p>The documentation versioning system consists of three integrated components:</p> <ol> <li>Version Selector UI (Spec 154) - A dropdown component in the mdBook navigation that lets users switch between versions</li> <li>Versioned Deployment (Spec 155) - GitHub Actions workflow that deploys each version to its own subdirectory</li> <li>Version-Aware Workflows (Spec 156) - Enhanced book workflow that accepts a VERSION parameter for building version-specific documentation</li> </ol> <p>Planned Architecture: <pre><code>GitHub Repository\n\u251c\u2500\u2500 main branch (source code + docs)\n\u251c\u2500\u2500 Tags: v0.2.6, v0.2.5, v0.2.4, ...\n\u2514\u2500\u2500 gh-pages branch (deployed docs)\n    \u251c\u2500\u2500 index.html \u2192 redirects to /latest/\n    \u251c\u2500\u2500 versions.json\n    \u251c\u2500\u2500 latest/ \u2192 copy of newest version\n    \u251c\u2500\u2500 v0.2.6/ \u2192 built from v0.2.6 tag\n    \u251c\u2500\u2500 v0.2.5/ \u2192 built from v0.2.5 tag\n    \u2514\u2500\u2500 v0.2.4/ \u2192 built from v0.2.4 tag\n</code></pre></p> <p>Source: specs/155-versioned-documentation-deployment.md:80-102</p>"},{"location":"automated-documentation/documentation-versioning/#version-selector-ui-component","title":"Version Selector UI Component","text":"<p>Design Overview (Spec 154):</p> <p>The version selector will be a lightweight JavaScript component that: - Displays a dropdown in mdBook's navigation bar - Fetches version metadata from <code>/versions.json</code> at the documentation root - Preserves the current page path when switching versions (e.g., switching from <code>/v0.2.6/mapreduce/index.html</code> to <code>/v0.2.5/mapreduce/index.html</code>) - Works across all mdBook themes with minimal configuration - Gracefully degrades if <code>versions.json</code> is missing</p> <p>Integration Pattern: <pre><code># book/book.toml\n[output.html]\nadditional-css = [\"theme/version-selector.css\"]\nadditional-js = [\"theme/version-selector.js\"]\n</code></pre></p> <p>Source: specs/154-mdbook-version-selector-ui.md:110-116</p> <p>Key Features: - Current Version Detection: Automatically detects which version the user is viewing based on URL path pattern - Visual Indicators: Highlights current version and marks latest version with \"(Latest)\" label - Fallback Handling: If the current page doesn't exist in the target version, redirects to that version's index - Accessibility: Keyboard navigable (Tab, Enter, Arrow keys) and screen reader compatible - Performance: &lt; 5KB combined JavaScript and CSS, no external dependencies</p> <p>Source: specs/154-mdbook-version-selector-ui.md:36-53</p>"},{"location":"automated-documentation/documentation-versioning/#versionsjson-schema","title":"versions.json Schema","text":"<p>The version selector fetches version metadata from a central <code>versions.json</code> file:</p> <pre><code>{\n  \"latest\": \"v0.2.6\",\n  \"versions\": [\n    {\n      \"version\": \"v0.2.6\",\n      \"path\": \"/v0.2.6/\",\n      \"label\": \"v0.2.6 (Latest)\",\n      \"released\": \"2025-01-15\"\n    },\n    {\n      \"version\": \"v0.2.5\",\n      \"path\": \"/v0.2.5/\",\n      \"label\": \"v0.2.5\",\n      \"released\": \"2025-01-10\"\n    }\n  ]\n}\n</code></pre> <p>Field Descriptions: - <code>latest</code>: Version string of the newest release - <code>version</code>: Semantic version tag (e.g., \"v0.2.6\") - <code>path</code>: URL path to that version's documentation root - <code>label</code>: Display text in dropdown (includes \"(Latest)\" for newest) - <code>released</code>: ISO 8601 date of release (optional)</p> <p>Source: specs/154-mdbook-version-selector-ui.md:118-137</p>"},{"location":"automated-documentation/documentation-versioning/#versioned-deployment-workflow","title":"Versioned Deployment Workflow","text":"<p>Design Overview (Spec 155):</p> <p>The deployment system will automatically build and deploy documentation when version tags are pushed:</p> <p>Workflow Triggers: <pre><code>on:\n  push:\n    tags:\n      - 'v*.*.*'  # Trigger on semver tags (e.g., v0.2.6)\n  workflow_dispatch:\n    inputs:\n      version:\n        description: 'Version to deploy (tag name, \"all\", or \"latest\")'\n        required: true\n        default: 'latest'\n      rebuild_all:\n        description: 'Rebuild all versions'\n        type: boolean\n        default: false\n</code></pre></p> <p>Source: specs/155-versioned-documentation-deployment.md:108-124</p> <p>Deployment Process:</p> <ol> <li>Tag-Triggered Build: When you push a tag like <code>v0.2.6</code>, GitHub Actions automatically:</li> <li>Checks out that specific tag</li> <li>Runs the book workflow for that version</li> <li>Builds documentation with mdBook</li> <li> <p>Deploys to <code>gh-pages:/v0.2.6/</code> directory</p> </li> <li> <p>versions.json Generation: After deploying a version, a script scans the <code>gh-pages</code> branch to generate <code>versions.json</code> with all deployed versions</p> </li> <li> <p>Latest Pointer Update: If the deployed version is the newest (highest semver), the <code>/latest/</code> directory is updated to point to it</p> </li> <li> <p>Root Redirect: An <code>index.html</code> at the root redirects visitors to <code>/latest/</code></p> </li> </ol> <p>Key Features: - Preserves Existing Versions: Using <code>keep_files: true</code> ensures deploying v0.2.6 doesn't delete v0.2.5 - Parallel Builds: Manual rebuild workflow can rebuild multiple versions concurrently - Idempotent: Rebuilding the same version produces identical output - Fail-Safe: Build failures don't corrupt existing deployed versions</p> <p>Source: specs/155-versioned-documentation-deployment.md:134-176</p>"},{"location":"automated-documentation/documentation-versioning/#version-aware-book-workflow","title":"Version-Aware Book Workflow","text":"<p>Design Overview (Spec 156):</p> <p>The current <code>book-docs-drift.yml</code> workflow will be enhanced to accept a <code>VERSION</code> parameter:</p> <p>Workflow Configuration: <pre><code>name: book-docs-drift-detection\nmode: mapreduce\n\nenv:\n  VERSION: \"${VERSION:-latest}\"  # Accept from caller or default to \"latest\"\n\n  # Version-aware paths\n  ANALYSIS_DIR: \".prodigy/book-analysis/${VERSION}\"\n  FEATURES_PATH: \"${ANALYSIS_DIR}/features.json\"\n</code></pre></p> <p>Source: specs/156-version-aware-book-workflow.md:77-100</p> <p>Key Enhancements:</p> <ol> <li> <p>VERSION Parameter: The workflow will accept a <code>VERSION</code> environment variable (e.g., \"v0.2.6\", \"latest\")</p> </li> <li> <p>Version-Scoped Analysis: Drift analysis results will be stored in version-specific directories:</p> </li> <li><code>.prodigy/book-analysis/v0.2.6/</code></li> <li><code>.prodigy/book-analysis/v0.2.5/</code></li> <li> <p><code>.prodigy/book-analysis/latest/</code></p> </li> <li> <p>Version Validation: Before processing, the workflow will validate the VERSION format (semver <code>vX.Y.Z</code> or \"latest\") and verify the tag exists</p> </li> <li> <p>Version in Documentation: Generated documentation will include version metadata in headers or footers</p> </li> </ol> <p>Claude Command Integration: <pre><code>setup:\n  - claude: \"/prodigy-analyze-features-for-book --project $PROJECT_NAME --version $VERSION\"\n\nmap:\n  agent_template:\n    - claude: \"/prodigy-analyze-book-chapter-drift --project $PROJECT_NAME --json '${item}' --version $VERSION\"\n</code></pre></p> <p>Source: specs/156-version-aware-book-workflow.md:143-153</p> <p>Backward Compatibility: If <code>VERSION</code> is not provided, the workflow defaults to \"latest\" and maintains current behavior.</p>"},{"location":"automated-documentation/documentation-versioning/#setup-instructions-planned","title":"Setup Instructions (Planned)","text":"<p>When implemented, setting up versioned documentation will involve:</p> <ol> <li> <p>Add Version Selector Theme Files: <pre><code># Copy version selector components to your mdBook theme\ncp version-selector.js book/theme/\ncp version-selector.css book/theme/\n</code></pre></p> </li> <li> <p>Update book.toml Configuration: <pre><code>[output.html]\nadditional-css = [\"theme/version-selector.css\"]\nadditional-js = [\"theme/version-selector.js\"]\n</code></pre></p> </li> <li> <p>Add Deployment Workflow: <pre><code># Copy the versioned deployment workflow\ncp templates/workflows/deploy-docs-versioned.yml .github/workflows/\n</code></pre></p> </li> <li> <p>Create Initial versions.json: <pre><code>{\n  \"latest\": \"v0.2.6\",\n  \"versions\": [\n    {\n      \"version\": \"v0.2.6\",\n      \"path\": \"/v0.2.6/\",\n      \"label\": \"v0.2.6 (Latest)\",\n      \"released\": \"2025-01-15\"\n    }\n  ]\n}\n</code></pre></p> </li> <li> <p>Deploy and Test:</p> </li> <li>Push a version tag to trigger deployment</li> <li>Verify version selector appears in navigation</li> <li>Test switching between versions</li> </ol> <p>Source: specs/154-mdbook-version-selector-ui.md:163-175</p>"},{"location":"automated-documentation/documentation-versioning/#version-retention-strategy-planned","title":"Version Retention Strategy (Planned)","text":"<p>The design includes a version retention policy to manage storage on GitHub Pages:</p> <p>Retention Rules: - Keep all major versions: v1.0.0, v2.0.0, v3.0.0 - Keep last 3 minor versions per major: v2.3.0, v2.2.0, v2.1.0 - Keep last 5 patch versions per minor: v2.3.5, v2.3.4, v2.3.3, v2.3.2, v2.3.1</p> <p>Cleanup Process: A planned cleanup script will identify and remove old versions: <pre><code># scripts/cleanup-old-versions.sh (planned)\n# Removes versions not matching retention policy\n# Regenerates versions.json after cleanup\n</code></pre></p> <p>Considerations: - GitHub Pages has a 1GB soft limit - Each documentation version is typically 5-10MB - Retention policy allows ~50-100 versions before cleanup needed</p> <p>Source: specs/155-versioned-documentation-deployment.md:392-402</p>"},{"location":"automated-documentation/documentation-versioning/#manual-deployment-planned","title":"Manual Deployment (Planned)","text":"<p>The deployment workflow will support manual triggering for specific use cases:</p> <p>Rebuild Specific Version: <pre><code># Trigger workflow manually from GitHub UI\n# Set version: v0.2.5\n# Or use GitHub CLI:\ngh workflow run deploy-docs-versioned.yml -f version=v0.2.5\n</code></pre></p> <p>Rebuild All Versions: <pre><code># Useful after theme updates or global doc changes\ngh workflow run deploy-docs-versioned.yml -f rebuild_all=true\n</code></pre></p> <p>Use Cases for Manual Deployment: - Updating documentation theme across all versions - Fixing critical documentation errors in historical versions - Regenerating <code>versions.json</code> after manual gh-pages branch cleanup - Testing deployment workflow changes</p> <p>Source: specs/155-versioned-documentation-deployment.md:436-444</p>"},{"location":"automated-documentation/documentation-versioning/#integration-with-automated-documentation","title":"Integration with Automated Documentation","text":"<p>The versioning system will integrate with Prodigy's existing automated documentation workflow:</p> <p>Workflow Integration: <pre><code># .github/workflows/deploy-docs-versioned.yml\nsteps:\n  - name: Run book workflow for version\n    run: prodigy run workflows/book-docs-drift.yml\n    env:\n      VERSION: ${{ steps.version.outputs.version }}\n</code></pre></p> <p>When a version tag is pushed, the deployment workflow will: 1. Check out the tagged code 2. Run the version-aware book workflow to analyze features at that version 3. Build documentation matching that version's implementation 4. Deploy to the version-specific subdirectory</p> <p>This ensures documentation always matches the code at each version.</p> <p>Source: specs/155-versioned-documentation-deployment.md:155-167, specs/156-version-aware-book-workflow.md:143-153</p>"},{"location":"automated-documentation/documentation-versioning/#testing-versioned-documentation-locally","title":"Testing Versioned Documentation Locally","text":"<p>Planned Testing Workflow:</p> <ol> <li> <p>Build Multiple Versions Locally: <pre><code># Checkout and build v0.2.6\ngit checkout v0.2.6\nprodigy run workflows/book-docs-drift.yml\nmdbook build book\nmv book/book build/v0.2.6\n\n# Checkout and build v0.2.5\ngit checkout v0.2.5\nprodigy run workflows/book-docs-drift.yml\nmdbook build book\nmv book/book build/v0.2.5\n</code></pre></p> </li> <li> <p>Create Test versions.json: <pre><code>cat &gt; build/versions.json &lt;&lt;EOF\n{\n  \"latest\": \"v0.2.6\",\n  \"versions\": [\n    {\"version\": \"v0.2.6\", \"path\": \"/v0.2.6/\", \"label\": \"v0.2.6 (Latest)\"},\n    {\"version\": \"v0.2.5\", \"path\": \"/v0.2.5/\", \"label\": \"v0.2.5\"}\n  ]\n}\nEOF\n</code></pre></p> </li> <li> <p>Serve Locally: <pre><code>cd build\npython -m http.server 8000\n# Visit http://localhost:8000/v0.2.6/\n</code></pre></p> </li> <li> <p>Test Version Selector:</p> </li> <li>Verify dropdown appears in navigation</li> <li>Switch between versions</li> <li>Confirm page paths are preserved</li> <li>Test fallback when page doesn't exist in older version</li> </ol>"},{"location":"automated-documentation/documentation-versioning/#browser-compatibility-planned","title":"Browser Compatibility (Planned)","text":"<p>The version selector will be designed to work across modern browsers:</p> <p>Supported Browsers: - Chrome, Firefox, Safari, Edge (latest versions) - Mobile browsers (iOS Safari, Chrome Mobile)</p> <p>Technology Choices: - Uses <code>fetch()</code> API (ES6, widely supported) - Semantic HTML (<code>&lt;select&gt;</code> element) - CSS Grid/Flexbox for layout - No external dependencies (no jQuery)</p> <p>Graceful Degradation: - If <code>fetch()</code> unavailable (very old browsers), selector won't render but docs remain accessible - If <code>versions.json</code> missing, component silently skips rendering (no errors shown)</p> <p>Source: specs/154-mdbook-version-selector-ui.md:233-247</p>"},{"location":"automated-documentation/github-actions-integration/","title":"GitHub Actions Integration","text":""},{"location":"automated-documentation/github-actions-integration/#github-actions-integration","title":"GitHub Actions Integration","text":"<p>Automate your mdBook documentation deployment to GitHub Pages using a standardized workflow that validates documentation on pull requests and deploys on merges to your main branch.</p> <p>Source: This guide is based on Prodigy's production workflow (<code>.github/workflows/deploy-docs.yml</code>) and Spec 128 GitHub Workflow Documentation Standards.</p>"},{"location":"automated-documentation/github-actions-integration/#quick-start","title":"Quick Start","text":"<p>Deploy mdBook documentation to GitHub Pages in under 5 minutes:</p> <p>Step 1: Copy the workflow file</p> <pre><code>curl -o .github/workflows/deploy-docs.yml \\\n  https://raw.githubusercontent.com/iepathos/prodigy/main/.github/workflows/deploy-docs.yml\n</code></pre> <p>Or create <code>.github/workflows/deploy-docs.yml</code> manually:</p> <pre><code>name: Deploy Documentation\n\non:\n  push:\n    branches: [main, master]\n    paths:\n      - 'book/**'\n      - '.github/workflows/deploy-docs.yml'\n  pull_request:\n    branches: [main, master]\n    paths:\n      - 'book/**'\n      - '.github/workflows/deploy-docs.yml'\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v5\n\n      - name: Setup mdBook\n        uses: peaceiris/actions-mdbook@v2\n        with:\n          mdbook-version: 'latest'\n\n      - name: Build book\n        run: mdbook build book\n\n      - name: Deploy to GitHub Pages\n        if: github.event_name == 'push' &amp;&amp; (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master')\n        uses: peaceiris/actions-gh-pages@v4\n        with:\n          github_token: ${{ secrets.GITHUB_TOKEN }}\n          publish_dir: ./book/book\n</code></pre> <p>Source: <code>.github/workflows/deploy-docs.yml:1-38</code></p> <p>Step 2: Commit and push</p> <pre><code>git add .github/workflows/deploy-docs.yml\ngit commit -m \"Add documentation deployment workflow\"\ngit push\n</code></pre> <p>Step 3: Enable GitHub Pages</p> <ol> <li>Go to your repository's Settings \u2192 Pages</li> <li>Under \"Source\", select Deploy from a branch</li> <li>Choose branch: gh-pages and directory: / (root)</li> <li>Click Save</li> </ol> <p>Done! Your documentation will deploy automatically on the next push to main/master.</p> <p>Source: Spec 128:136-142</p>"},{"location":"automated-documentation/github-actions-integration/#how-it-works","title":"How It Works","text":"<p>The workflow executes in two contexts:</p> <ol> <li>Pull Requests: Validates that documentation builds successfully (prevents merging broken docs)</li> <li>Push to main/master: Builds and deploys to GitHub Pages (publishes the documentation)</li> </ol> <p>Key Components:</p> <ul> <li>Triggers: Runs only when documentation files change (<code>book/**</code>) or the workflow itself is modified</li> <li>Path Filters: Prevents unnecessary workflow runs, saving CI/CD minutes</li> <li>Permissions: <code>contents: write</code> allows pushing to the <code>gh-pages</code> branch</li> <li>Conditional Deployment: The <code>if: github.event_name == 'push'</code> condition ensures PRs only validate, never deploy</li> </ul> <p>Source: <code>.github/workflows/deploy-docs.yml:3-13</code>, <code>.github/workflows/deploy-docs.yml:18-19</code>, <code>.github/workflows/deploy-docs.yml:33</code></p>"},{"location":"automated-documentation/github-actions-integration/#workflow-file-structure-explained","title":"Workflow File Structure Explained","text":"<pre><code># Triggers: When to run this workflow\non:\n  push:\n    branches: [main, master]    # Deploy on push to default branch\n    paths:                      # Only when these files change\n      - 'book/**'              # Documentation content\n      - '.github/workflows/deploy-docs.yml'  # This workflow file\n  pull_request:                 # Validate documentation in PRs\n    branches: [main, master]\n    paths:\n      - 'book/**'\n</code></pre> <p>Why path filters? - Prevents workflow from running on code changes unrelated to documentation - Saves CI/CD minutes (workflow only runs when docs actually change) - Faster feedback for non-documentation PRs</p> <p>Source: <code>.github/workflows/deploy-docs.yml:6-13</code>, Spec 128:235-255</p> <pre><code># Permissions needed for deployment\npermissions:\n  contents: write  # Required to push to gh-pages branch\n</code></pre> <p>Why <code>contents: write</code>?</p> <p>The <code>peaceiris/actions-gh-pages</code> action deploys by pushing to the <code>gh-pages</code> branch. This requires write access to repository contents.</p> <p>Note: This differs from the newer <code>actions/deploy-pages</code> approach which uses <code>pages: write</code> and <code>id-token: write</code>. Prodigy standardizes on the <code>gh-pages</code> branch method for consistency and broader compatibility.</p> <p>Source: <code>.github/workflows/deploy-docs.yml:18-19</code>, Spec 128:150-157</p> <pre><code>steps:\n  # Step 1: Get repository code\n  - uses: actions/checkout@v5\n\n  # Step 2: Install mdBook\n  - name: Setup mdBook\n    uses: peaceiris/actions-mdbook@v2\n    with:\n      mdbook-version: 'latest'\n\n  # Step 3: Build documentation\n  - name: Build book\n    run: mdbook build book\n\n  # Step 4: Deploy to GitHub Pages (only on push to main/master, not PRs)\n  - name: Deploy to GitHub Pages\n    if: github.event_name == 'push'\n    uses: peaceiris/actions-gh-pages@v4\n    with:\n      github_token: ${{ secrets.GITHUB_TOKEN }}\n      publish_dir: ./book/book\n</code></pre> <p>Deployment Condition: <code>if: github.event_name == 'push'</code></p> <p>This critical condition ensures: - Pull Requests: Build and validate documentation (catches errors before merge) - Push to main/master: Build, validate, AND deploy to GitHub Pages</p> <p>Without this condition, every PR would attempt to deploy, which is unnecessary and can cause permission issues.</p> <p>Source: <code>.github/workflows/deploy-docs.yml:22-37</code>, Spec 128:225-233</p>"},{"location":"automated-documentation/github-actions-integration/#recommended-action-versions","title":"Recommended Action Versions","text":"<p>Use these specific action versions for stability and security:</p> <ul> <li><code>actions/checkout@v5</code> - Fetches repository code</li> <li><code>peaceiris/actions-mdbook@v2</code> - Installs mdBook</li> <li><code>peaceiris/actions-gh-pages@v4</code> - Deploys to gh-pages branch</li> </ul> <p>Source: <code>.github/workflows/deploy-docs.yml:22,24,34</code>, Spec 128:669-674</p>"},{"location":"automated-documentation/github-actions-integration/#repository-settings","title":"Repository Settings","text":"<p>After adding the workflow file, configure GitHub Pages in your repository settings:</p> <ol> <li>Navigate to Settings \u2192 Pages</li> <li>Under Source, select Deploy from a branch</li> <li>Choose Branch: gh-pages and Directory: / (root)</li> <li>Click Save</li> </ol> <p>The workflow will create the <code>gh-pages</code> branch automatically on the first deployment. You don't need to create it manually.</p> <p>Source: Spec 128:136-141</p>"},{"location":"automated-documentation/github-actions-integration/#integration-with-prodigy-workflows","title":"Integration with Prodigy Workflows","text":"<p>This GitHub Actions workflow deploys the documentation that Prodigy's book workflow generates and maintains.</p> <p>How they work together:</p> <ol> <li>Prodigy MapReduce Workflow (<code>book-docs-drift.yml</code>):</li> <li>Analyzes code for features and changes</li> <li>Detects documentation drift</li> <li>Updates markdown files in <code>book/src/</code></li> <li> <p>Commits fixes to your repository</p> </li> <li> <p>GitHub Actions Workflow (<code>deploy-docs.yml</code>):</p> </li> <li>Detects changes to <code>book/**</code> files</li> <li>Builds the mdBook</li> <li>Deploys to GitHub Pages</li> </ol> <p>In Practice: - Prodigy keeps your docs accurate and up-to-date - GitHub Actions makes your docs publicly accessible - Together, they create a fully automated documentation system</p> <p>Source: <code>book/src/automated-documentation/index.md:82-135</code>, Spec 128:518-562</p>"},{"location":"automated-documentation/github-actions-integration/#common-mistakes-and-solutions","title":"Common Mistakes and Solutions","text":""},{"location":"automated-documentation/github-actions-integration/#mistake-1-wrong-filename","title":"Mistake 1: Wrong Filename","text":"<p>\u274c Wrong: <pre><code>.github/workflows/docs.yml\n.github/workflows/documentation.yml\n.github/workflows/mdbook.yml\n</code></pre></p> <p>\u2705 Correct: <pre><code>.github/workflows/deploy-docs.yml\n</code></pre></p> <p>Why it matters: Consistent naming across projects aids discovery and maintenance.</p> <p>Source: Spec 128:296-312</p>"},{"location":"automated-documentation/github-actions-integration/#mistake-2-wrong-deployment-action","title":"Mistake 2: Wrong Deployment Action","text":"<p>\u274c Wrong: <pre><code>- uses: actions/upload-pages-artifact@v3\n- uses: actions/deploy-pages@v4\n</code></pre></p> <p>\u2705 Correct: <pre><code>- uses: peaceiris/actions-gh-pages@v4\n  with:\n    github_token: ${{ secrets.GITHUB_TOKEN }}\n    publish_dir: ./book/book\n</code></pre></p> <p>Why it matters: Different actions require different permissions and repository settings. Using <code>actions/deploy-pages</code> requires changing your GitHub Pages source to \"GitHub Actions\" and using different permissions (<code>pages: write</code>, <code>id-token: write</code>).</p> <p>Source: Spec 128:314-330</p>"},{"location":"automated-documentation/github-actions-integration/#mistake-3-missing-path-filters","title":"Mistake 3: Missing Path Filters","text":"<p>\u274c Wrong: <pre><code>on:\n  push:\n    branches: [main]\n</code></pre></p> <p>\u2705 Correct: <pre><code>on:\n  push:\n    branches: [main, master]\n    paths:\n      - 'book/**'\n      - '.github/workflows/deploy-docs.yml'\n</code></pre></p> <p>Impact: Workflow runs on every commit (even code-only changes), wasting CI resources and creating unnecessary deployments.</p> <p>Source: Spec 128:332-351</p>"},{"location":"automated-documentation/github-actions-integration/#mistake-4-wrong-permissions","title":"Mistake 4: Wrong Permissions","text":"<p>\u274c Wrong: <pre><code>permissions:\n  pages: write\n  id-token: write\n</code></pre></p> <p>\u2705 Correct: <pre><code>permissions:\n  contents: write\n</code></pre></p> <p>Why it matters: The <code>gh-pages</code> deployment method needs <code>contents: write</code> to push to the gh-pages branch. The permissions shown in \"Wrong\" are for the <code>actions/deploy-pages</code> approach.</p> <p>Source: Spec 128:353-368</p>"},{"location":"automated-documentation/github-actions-integration/#mistake-5-missing-pr-validation","title":"Mistake 5: Missing PR Validation","text":"<p>\u274c Wrong: <pre><code>on:\n  push:\n    branches: [main]\n</code></pre></p> <p>\u2705 Correct: <pre><code>on:\n  push:\n    branches: [main, master]\n    paths: ['book/**']\n  pull_request:\n    branches: [main, master]\n    paths: ['book/**']\n</code></pre></p> <p>Why it matters: Without PR validation, documentation build errors aren't caught until after merge, potentially breaking your deployed documentation.</p> <p>Source: Spec 128:370-390</p>"},{"location":"automated-documentation/github-actions-integration/#mistake-6-deploying-on-pr","title":"Mistake 6: Deploying on PR","text":"<p>\u274c Wrong: <pre><code>- uses: peaceiris/actions-gh-pages@v4\n  with:\n    github_token: ${{ secrets.GITHUB_TOKEN }}\n    publish_dir: ./book/book\n</code></pre></p> <p>\u2705 Correct: <pre><code>- uses: peaceiris/actions-gh-pages@v4\n  if: github.event_name == 'push'\n  with:\n    github_token: ${{ secrets.GITHUB_TOKEN }}\n    publish_dir: ./book/book\n</code></pre></p> <p>Why it matters: PRs should validate documentation builds but not deploy them. Deploying on PR can cause conflicts and unnecessary deployments.</p> <p>Source: Spec 128:392-410</p>"},{"location":"automated-documentation/github-actions-integration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"automated-documentation/github-actions-integration/#issue-workflow-runs-on-every-commit","title":"Issue: Workflow Runs on Every Commit","text":"<p>Symptom: Workflow executes even when documentation hasn't changed</p> <p>Cause: Missing or incorrect path filters</p> <p>Solution: <pre><code>on:\n  push:\n    paths:\n      - 'book/**'\n      - '.github/workflows/deploy-docs.yml'\n</code></pre></p> <p>Source: Spec 128:415-428</p>"},{"location":"automated-documentation/github-actions-integration/#issue-permission-denied-when-deploying","title":"Issue: Permission Denied When Deploying","text":"<p>Symptom: Error like \"failed to push some refs\" or \"permission denied\"</p> <p>Cause: Missing <code>contents: write</code> permission</p> <p>Solution: <pre><code>permissions:\n  contents: write\n</code></pre></p> <p>Also verify repository settings: - Go to Settings \u2192 Actions \u2192 General - Under Workflow permissions, select Read and write permissions - Click Save</p> <p>Source: Spec 128:430-445</p>"},{"location":"automated-documentation/github-actions-integration/#issue-documentation-not-updating","title":"Issue: Documentation Not Updating","text":"<p>Symptom: Workflow succeeds but GitHub Pages shows old content</p> <p>Causes and Solutions:</p> <ol> <li> <p>Verify gh-pages branch updated:    <pre><code>git fetch origin gh-pages\ngit log origin/gh-pages\n</code></pre>    Check if the latest commit matches your expectations.</p> </li> <li> <p>Check GitHub Pages settings:</p> </li> <li>Go to Settings \u2192 Pages</li> <li>Verify Source: Deploy from branch</li> <li> <p>Verify Branch: gh-pages and Directory: / (root)</p> </li> <li> <p>Force clear browser cache:</p> </li> <li>Add query parameter to URL: <code>https://username.github.io/repo?v=2</code></li> <li> <p>Hard refresh: Ctrl+Shift+R (Windows/Linux) or Cmd+Shift+R (Mac)</p> </li> <li> <p>Check publish_dir path:    <pre><code>publish_dir: ./book/book  # Correct - points to mdBook output\n# NOT ./book              # Wrong - points to source directory\n</code></pre></p> </li> </ol> <p>Source: Spec 128:447-471</p>"},{"location":"automated-documentation/github-actions-integration/#issue-404-error-on-github-pages","title":"Issue: 404 Error on GitHub Pages","text":"<p>Symptom: Page shows \"404 There isn't a GitHub Pages site here\"</p> <p>Causes and Solutions:</p> <ol> <li>GitHub Pages not enabled:</li> <li>Go to Settings \u2192 Pages</li> <li> <p>Enable Pages if it shows as disabled</p> </li> <li> <p>Wrong source branch:</p> </li> <li> <p>Change source to gh-pages branch</p> </li> <li> <p>Wrong root directory:</p> </li> <li> <p>Ensure source is / (root) not /docs</p> </li> <li> <p>Private repository without GitHub Pro:</p> </li> <li>GitHub Pages on private repositories requires GitHub Pro, Team, or Enterprise</li> <li>Make repository public or upgrade your GitHub plan</li> </ol> <p>Source: Spec 128:473-489</p>"},{"location":"automated-documentation/github-actions-integration/#issue-workflow-syntax-error","title":"Issue: Workflow Syntax Error","text":"<p>Symptom: Workflow doesn't appear in Actions tab</p> <p>Cause: Invalid YAML syntax</p> <p>Solution: <pre><code># Validate YAML locally\nyamllint .github/workflows/deploy-docs.yml\n\n# Or use GitHub's workflow validator\n# (GitHub shows syntax errors when you navigate to the workflow file)\n</code></pre></p> <p>Source: Spec 128:491-503</p>"},{"location":"automated-documentation/github-actions-integration/#issue-deployment-job-skipped-on-push","title":"Issue: Deployment Job Skipped on Push","text":"<p>Symptom: Build job runs but deploy step is skipped on push to main</p> <p>Cause: Missing or incorrect condition</p> <p>Solution: <pre><code>- name: Deploy to GitHub Pages\n  if: github.event_name == 'push'  # This line is critical\n  uses: peaceiris/actions-gh-pages@v4\n</code></pre></p> <p>Verify the condition matches exactly. Common mistakes: - Typo in <code>github.event_name</code> - Using single <code>=</code> instead of <code>==</code> - Wrong event name (e.g., <code>if: github.event == 'push'</code>)</p> <p>Source: Spec 128:505-516</p>"},{"location":"automated-documentation/github-actions-integration/#see-also","title":"See Also","text":"<ul> <li>Quick Start - Get started with Prodigy's automated documentation</li> <li>Understanding the Workflow - Deep dive into how the book workflow works</li> <li>Troubleshooting - General troubleshooting for documentation workflows</li> <li>Real-World Example - See Prodigy's own documentation workflow in action</li> </ul>"},{"location":"automated-documentation/mkdocs-workflow/","title":"MkDocs Documentation Workflow","text":"<p>The <code>mkdocs-drift.yml</code> workflow automatically generates and maintains MkDocs Material documentation by detecting gaps, analyzing drift, and fixing documentation to stay synchronized with your codebase.</p>"},{"location":"automated-documentation/mkdocs-workflow/#overview","title":"Overview","text":"<p>This workflow is designed for projects using MkDocs Material as their documentation system. It provides the same capabilities as the mdbook workflow but targets MkDocs-specific features and structure.</p> <p>Key Features: - Automatic gap detection for undocumented features - Drift analysis comparing docs against source code - Intelligent fixes with source attribution - MkDocs build validation with <code>--strict</code> mode - Navigation completeness checking - Broken link detection</p>"},{"location":"automated-documentation/mkdocs-workflow/#quick-start","title":"Quick Start","text":""},{"location":"automated-documentation/mkdocs-workflow/#1-run-the-workflow","title":"1. Run the Workflow","text":"<pre><code>prodigy run workflows/mkdocs-drift.yml\n</code></pre> <p>The workflow will: 1. Analyze your codebase for features 2. Detect documentation gaps 3. Create missing documentation pages 4. Analyze existing pages for drift 5. Fix drift with source references 6. Validate MkDocs build 7. Generate validation report</p>"},{"location":"automated-documentation/mkdocs-workflow/#2-review-generated-documentation","title":"2. Review Generated Documentation","text":"<p>After completion, check: - Generated pages: New markdown files in your docs directory - Validation report: <code>.prodigy/mkdocs-analysis/validation.json</code> - Gap report: <code>.prodigy/mkdocs-analysis/gap-report.json</code></p>"},{"location":"automated-documentation/mkdocs-workflow/#3-merge-changes","title":"3. Merge Changes","text":"<p>The workflow runs in an isolated git worktree. When complete, you'll be prompted:</p> <pre><code>Merge session-abc123 to mkdocs? [y/N]\n</code></pre> <p>Review the changes and merge when satisfied.</p>"},{"location":"automated-documentation/mkdocs-workflow/#configuration-options","title":"Configuration Options","text":""},{"location":"automated-documentation/mkdocs-workflow/#environment-variables","title":"Environment Variables","text":"<p>All configuration is done through environment variables in the workflow YAML:</p> <pre><code>env:\n  # Project Configuration\n  PROJECT_NAME: \"Prodigy\"              # Your project name\n  PROJECT_CONFIG: \".prodigy/mkdocs-config.json\"  # MkDocs-specific config\n  FEATURES_PATH: \".prodigy/mkdocs-analysis/features.json\"  # Feature inventory\n\n  # MkDocs-Specific Settings\n  DOCS_DIR: \"book/src\"                 # Documentation source directory\n  MKDOCS_CONFIG: \"mkdocs.yml\"          # MkDocs configuration file\n  ANALYSIS_DIR: \".prodigy/mkdocs-analysis\"  # Analysis output directory\n  CHAPTERS_FILE: \"workflows/data/prodigy-chapters.json\"  # Chapter definitions\n\n  # Workflow Settings\n  MAX_PARALLEL: \"3\"                    # Number of parallel agents\n</code></pre>"},{"location":"automated-documentation/mkdocs-workflow/#configuring-documentation-directory","title":"Configuring Documentation Directory","text":"<p>The workflow supports flexible documentation directory configuration through the <code>DOCS_DIR</code> variable:</p>"},{"location":"automated-documentation/mkdocs-workflow/#option-1-separate-mkdocs-directory-default","title":"Option 1: Separate MkDocs Directory (Default)","text":"<pre><code>env:\n  DOCS_DIR: \"docs\"\n  CHAPTERS_FILE: \"workflows/data/mkdocs-chapters.json\"\n</code></pre> <p>Use this when: - You want MkDocs-specific documentation separate from mdbook - You need a curated subset of documentation for MkDocs - You're testing both documentation systems</p> <p>Structure: <pre><code>docs/\n\u251c\u2500\u2500 index.md\n\u251c\u2500\u2500 workflow-basics/\n\u2502   \u251c\u2500\u2500 variables.md\n\u2502   \u2514\u2500\u2500 environment.md\n\u2514\u2500\u2500 mapreduce/\n    \u2514\u2500\u2500 overview.md\nmkdocs.yml (docs_dir: docs)\n</code></pre></p>"},{"location":"automated-documentation/mkdocs-workflow/#option-2-shared-source-with-mdbook","title":"Option 2: Shared Source with mdbook","text":"<pre><code>env:\n  DOCS_DIR: \"book/src\"\n  CHAPTERS_FILE: \"workflows/data/prodigy-chapters.json\"\n</code></pre> <p>Use this when: - You want a single source of truth for both mdbook and MkDocs - You're migrating from mdbook to MkDocs - You want complete documentation in both formats</p> <p>Structure: <pre><code>book/src/\n\u251c\u2500\u2500 index.md\n\u251c\u2500\u2500 SUMMARY.md (for mdbook)\n\u251c\u2500\u2500 workflow-basics/\n\u2502   \u251c\u2500\u2500 index.md\n\u2502   \u2514\u2500\u2500 *.md\n\u2514\u2500\u2500 mapreduce/\n    \u251c\u2500\u2500 index.md\n    \u2514\u2500\u2500 *.md\nmkdocs.yml (docs_dir: book/src, exclude: SUMMARY.md)\n</code></pre></p> <p>Important: When using <code>book/src</code>, update <code>mkdocs.yml</code>:</p> <pre><code>docs_dir: book/src\nexclude_docs: |\n  SUMMARY.md\n</code></pre>"},{"location":"automated-documentation/mkdocs-workflow/#chapter-definitions","title":"Chapter Definitions","text":"<p>Chapter definitions are stored in JSON files that define the documentation structure:</p> <p>For separate MkDocs docs: <pre><code>// workflows/data/mkdocs-chapters.json\n[\n  {\n    \"id\": \"workflow-basics\",\n    \"title\": \"Workflow Basics\",\n    \"pages\": [\n      {\"id\": \"variables\", \"title\": \"Variables\"},\n      {\"id\": \"environment\", \"title\": \"Environment\"}\n    ]\n  }\n]\n</code></pre></p> <p>For shared book/src: <pre><code>// workflows/data/prodigy-chapters.json\n// (More comprehensive structure matching mdbook SUMMARY.md)\n</code></pre></p>"},{"location":"automated-documentation/mkdocs-workflow/#parallelism-configuration","title":"Parallelism Configuration","text":"<p>Control how many documentation pages are processed simultaneously:</p> <pre><code>env:\n  MAX_PARALLEL: \"3\"  # Process 3 pages at once\n\nmap:\n  max_parallel: ${MAX_PARALLEL}\n</code></pre> <p>Guidelines: - <code>1-3</code>: Conservative, good for development - <code>4-6</code>: Balanced, recommended for most projects - <code>7-10</code>: Aggressive, faster but higher resource usage</p>"},{"location":"automated-documentation/mkdocs-workflow/#workflow-phases","title":"Workflow Phases","text":""},{"location":"automated-documentation/mkdocs-workflow/#setup-phase","title":"Setup Phase","text":"<p>Step 1: Analyze Features <pre><code>- claude: \"/prodigy-analyze-features-for-mkdocs --project $PROJECT_NAME --config $PROJECT_CONFIG\"\n</code></pre></p> <p>Scans your codebase to build a feature inventory including: - Command types and syntax - Configuration options - Workflow features - MapReduce capabilities</p> <p>Output: <code>.prodigy/mkdocs-analysis/features.json</code></p> <p>Step 2: Detect Gaps <pre><code>- claude: \"/prodigy-detect-mkdocs-gaps --project $PROJECT_NAME --config $PROJECT_CONFIG --features $FEATURES_PATH --chapters $CHAPTERS_FILE --docs-dir $DOCS_DIR\"\n</code></pre></p> <p>Compares feature inventory against existing documentation to find: - Missing pages for undocumented features - Incomplete pages missing key information - Structural gaps in navigation</p> <p>Outputs: - <code>.prodigy/mkdocs-analysis/gap-report.json</code> - <code>.prodigy/mkdocs-analysis/flattened-items.json</code> (for map phase) - New stub markdown files (if gaps found)</p>"},{"location":"automated-documentation/mkdocs-workflow/#map-phase","title":"Map Phase","text":"<p>Processes each documentation page in parallel:</p> <p>Step 1: Analyze Drift <pre><code>- claude: \"/prodigy-analyze-mkdocs-drift --project $PROJECT_NAME --json '${item}' --features $FEATURES_PATH\"\n  commit_required: true\n</code></pre></p> <p>Analyzes each page for: - Outdated information - Missing features - Incorrect examples - Broken references</p> <p>Output: Drift analysis JSON with severity ratings</p> <p>Step 2: Fix Drift <pre><code>- claude: \"/prodigy-fix-mkdocs-drift --project $PROJECT_NAME --json '${item}'\"\n  commit_required: true\n</code></pre></p> <p>Fixes detected drift by: - Updating documentation to match current implementation - Adding source code references - Fixing broken links - Adding missing examples</p> <p>Step 3: Validate Fix <pre><code>validate:\n  claude: \"/prodigy-validate-mkdocs-page --project $PROJECT_NAME --json '${item}' --output .prodigy/validation-result.json\"\n  result_file: \".prodigy/validation-result.json\"\n  threshold: 100\n</code></pre></p> <p>Ensures the fix meets quality standards: - All required topics covered - Examples are accurate - Links are valid - Source attribution present</p>"},{"location":"automated-documentation/mkdocs-workflow/#reduce-phase","title":"Reduce Phase","text":"<p>Aggregates results and performs holistic validation:</p> <p>Step 1: Build Documentation <pre><code>- shell: \"mkdocs build --strict\"\n  on_failure:\n    claude: \"/prodigy-fix-mkdocs-build-errors --project $PROJECT_NAME\"\n    commit_required: true\n</code></pre></p> <p>Runs <code>mkdocs build --strict</code> to catch: - Broken internal links - Missing files referenced in navigation - Invalid markdown syntax - Configuration errors</p> <p>Step 2: Holistic Validation <pre><code>- claude: \"/prodigy-validate-mkdocs-holistically --project $PROJECT_NAME --docs-dir $DOCS_DIR --output $ANALYSIS_DIR/validation.json --auto-fix true\"\n  commit_required: true\n</code></pre></p> <p>Performs cross-cutting validation: - Missing index.md - Creates landing page if missing - Orphaned files - Detects files not in navigation - Navigation completeness - Ensures all files are accessible - Build validation - Confirms mkdocs builds successfully - Content anti-patterns - Detects redundant sections, circular references</p> <p>Step 3: Cleanup <pre><code>- shell: \"rm -rf ${ANALYSIS_DIR}/features.json ${ANALYSIS_DIR}/flattened-items.json ${ANALYSIS_DIR}/drift-*.json\"\n- shell: \"git add -A &amp;&amp; git commit -m 'chore: remove temporary mkdocs analysis files for ${PROJECT_NAME}' || true\"\n</code></pre></p> <p>Removes temporary analysis files while preserving validation report.</p>"},{"location":"automated-documentation/mkdocs-workflow/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"automated-documentation/mkdocs-workflow/#custom-project-configuration","title":"Custom Project Configuration","text":"<p>Create a project-specific configuration file:</p> <pre><code>// .prodigy/mkdocs-config.json\n{\n  \"project_name\": \"Prodigy\",\n  \"docs_dir\": \"book/src\",\n  \"mkdocs_config\": \"mkdocs.yml\",\n  \"theme\": \"material\",\n  \"validation\": {\n    \"require_source_attribution\": true,\n    \"require_examples\": true,\n    \"max_drift_severity\": \"medium\"\n  },\n  \"gap_detection\": {\n    \"auto_create_stubs\": true,\n    \"min_coverage_threshold\": 0.8\n  }\n}\n</code></pre>"},{"location":"automated-documentation/mkdocs-workflow/#validation-thresholds","title":"Validation Thresholds","text":"<p>Configure validation strictness in the workflow:</p> <pre><code>validate:\n  threshold: 100  # 100% = strict, 80% = lenient\n  on_incomplete:\n    claude: \"/prodigy-complete-mkdocs-fix --project $PROJECT_NAME --json '${item}' --gaps ${validation.gaps}\"\n    max_attempts: 3\n    fail_workflow: false  # Continue even if can't reach 100%\n</code></pre>"},{"location":"automated-documentation/mkdocs-workflow/#error-handling","title":"Error Handling","text":"<p>Configure how the workflow responds to failures:</p> <pre><code>error_policy:\n  on_item_failure: dlq       # Send failures to dead letter queue\n  continue_on_failure: true  # Process remaining items\n  max_failures: 2            # Stop after 2 failures\n  error_collection: aggregate # Report all errors at end\n</code></pre>"},{"location":"automated-documentation/mkdocs-workflow/#workflow-commands-reference","title":"Workflow Commands Reference","text":""},{"location":"automated-documentation/mkdocs-workflow/#setup-commands","title":"Setup Commands","text":"<p><code>/prodigy-analyze-features-for-mkdocs</code> - Scans codebase for features - Outputs: <code>.prodigy/mkdocs-analysis/features.json</code> - Reuses existing analysis if recent</p> <p><code>/prodigy-detect-mkdocs-gaps</code> - Compares features against documentation - Creates missing page stubs - Outputs: gap report and flattened items for map phase</p>"},{"location":"automated-documentation/mkdocs-workflow/#map-phase-commands","title":"Map Phase Commands","text":"<p><code>/prodigy-analyze-mkdocs-drift</code> - Analyzes single page for drift - Compares against feature inventory - Outputs: drift analysis JSON</p> <p><code>/prodigy-fix-mkdocs-drift</code> - Fixes drift in single page - Adds source attribution - Updates examples and explanations</p> <p><code>/prodigy-validate-mkdocs-page</code> - Validates page completeness - Checks quality standards - Returns quality score</p> <p><code>/prodigy-complete-mkdocs-fix</code> - Iteratively improves page to meet threshold - Addresses validation gaps - Runs up to <code>max_attempts</code> times</p>"},{"location":"automated-documentation/mkdocs-workflow/#reduce-phase-commands","title":"Reduce Phase Commands","text":"<p><code>/prodigy-fix-mkdocs-build-errors</code> - Fixes mkdocs build failures - Repairs broken links - Fixes navigation issues</p> <p><code>/prodigy-validate-mkdocs-holistically</code> - Cross-cutting validation - Checks navigation completeness - Validates mkdocs build - Detects content anti-patterns - Auto-fixes with <code>--auto-fix true</code></p>"},{"location":"automated-documentation/mkdocs-workflow/#using-with-existing-mkdocs-projects","title":"Using with Existing MkDocs Projects","text":""},{"location":"automated-documentation/mkdocs-workflow/#migrating-from-manual-documentation","title":"Migrating from Manual Documentation","text":"<ol> <li> <p>Initial Setup: <pre><code># Create chapter definitions\nprodigy run workflows/mkdocs-drift.yml\n</code></pre></p> </li> <li> <p>Review Generated Content:    The workflow will detect your existing pages and only create stubs for missing ones.</p> </li> <li> <p>Iterative Improvement:    Run the workflow periodically to catch drift as your code evolves.</p> </li> </ol>"},{"location":"automated-documentation/mkdocs-workflow/#integrating-with-cicd","title":"Integrating with CI/CD","text":"<p>GitHub Actions Example:</p> <pre><code>name: Update Documentation\n\non:\n  schedule:\n    - cron: '0 0 * * 0'  # Weekly on Sunday\n  workflow_dispatch:\n\njobs:\n  update-docs:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install Prodigy\n        run: cargo install prodigy\n\n      - name: Run MkDocs Drift Detection\n        run: prodigy run workflows/mkdocs-drift.yml --auto-merge\n\n      - name: Create Pull Request\n        uses: peter-evans/create-pull-request@v5\n        with:\n          title: \"docs: automated MkDocs drift fixes\"\n          branch: docs/mkdocs-drift\n</code></pre>"},{"location":"automated-documentation/mkdocs-workflow/#troubleshooting","title":"Troubleshooting","text":""},{"location":"automated-documentation/mkdocs-workflow/#issue-missing-indexmd","title":"Issue: Missing index.md","text":"<p>Symptom: 404 error on homepage when running <code>mkdocs serve</code></p> <p>Solution: The holistic validation should catch this automatically: <pre><code># The validation command will create it if auto-fix is enabled\n/prodigy-validate-mkdocs-holistically --auto-fix true\n</code></pre></p> <p>Or create manually: <pre><code>cat &gt; ${DOCS_DIR}/index.md &lt;&lt;EOF\n# Project Documentation\nWelcome to the documentation.\nEOF\n</code></pre></p>"},{"location":"automated-documentation/mkdocs-workflow/#issue-orphaned-files","title":"Issue: Orphaned Files","text":"<p>Symptom: Files exist in docs directory but not accessible through navigation</p> <p>Solution: Check validation report: <pre><code>cat .prodigy/mkdocs-analysis/validation.json | jq '.mkdocs_specific.orphaned_files'\n</code></pre></p> <p>Add files to <code>mkdocs.yml</code>: <pre><code>nav:\n  - New Section:\n    - Title: path/to/orphaned-file.md\n</code></pre></p>"},{"location":"automated-documentation/mkdocs-workflow/#issue-mkdocs-build-strict-failures","title":"Issue: mkdocs build --strict Failures","text":"<p>Symptom: Workflow fails in reduce phase with build errors</p> <p>Solution: The workflow automatically calls <code>/prodigy-fix-mkdocs-build-errors</code>: <pre><code>- shell: \"mkdocs build --strict\"\n  on_failure:\n    claude: \"/prodigy-fix-mkdocs-build-errors --project $PROJECT_NAME\"\n</code></pre></p> <p>If issues persist, manually check: <pre><code>mkdocs build --strict 2&gt;&amp;1 | less\n</code></pre></p>"},{"location":"automated-documentation/mkdocs-workflow/#issue-parallel-agents-overwhelming-system","title":"Issue: Parallel Agents Overwhelming System","text":"<p>Symptom: System slowdown during map phase</p> <p>Solution: Reduce parallelism: <pre><code>env:\n  MAX_PARALLEL: \"2\"  # Reduce from 3 to 2\n</code></pre></p>"},{"location":"automated-documentation/mkdocs-workflow/#issue-validation-threshold-too-strict","title":"Issue: Validation Threshold Too Strict","text":"<p>Symptom: Pages keep failing validation at 100% threshold</p> <p>Solution: Lower threshold or allow incomplete: <pre><code>validate:\n  threshold: 80  # Lower to 80%\n  on_incomplete:\n    fail_workflow: false  # Don't fail, just warn\n</code></pre></p>"},{"location":"automated-documentation/mkdocs-workflow/#best-practices","title":"Best Practices","text":""},{"location":"automated-documentation/mkdocs-workflow/#1-run-regularly","title":"1. Run Regularly","text":"<p>Schedule periodic runs to catch drift early: - Weekly: For active projects - Monthly: For stable projects - After major changes: When adding new features</p>"},{"location":"automated-documentation/mkdocs-workflow/#2-review-before-merging","title":"2. Review Before Merging","text":"<p>Always review generated documentation: <pre><code># Check what changed\ncd ~/.prodigy/worktrees/prodigy/session-abc123/\ngit log --oneline\ngit diff master\n</code></pre></p>"},{"location":"automated-documentation/mkdocs-workflow/#3-maintain-chapter-definitions","title":"3. Maintain Chapter Definitions","text":"<p>Keep your chapter definitions file updated: <pre><code>// Add new sections as your project grows\n{\n  \"id\": \"new-feature\",\n  \"title\": \"New Feature\",\n  \"pages\": [...]\n}\n</code></pre></p>"},{"location":"automated-documentation/mkdocs-workflow/#4-use-auto-fix-judiciously","title":"4. Use Auto-Fix Judiciously","text":"<p>Enable auto-fix for clear-cut issues: <pre><code>- claude: \"/prodigy-validate-mkdocs-holistically --auto-fix true\"\n</code></pre></p> <p>But review auto-fixes before merging!</p>"},{"location":"automated-documentation/mkdocs-workflow/#5-version-your-validation-reports","title":"5. Version Your Validation Reports","text":"<p>Keep validation reports in git for tracking: <pre><code>git add .prodigy/mkdocs-analysis/validation.json\ngit commit -m \"docs: validation report for mkdocs drift run\"\n</code></pre></p>"},{"location":"automated-documentation/mkdocs-workflow/#examples","title":"Examples","text":""},{"location":"automated-documentation/mkdocs-workflow/#example-1-full-documentation-from-scratch","title":"Example 1: Full Documentation from Scratch","text":"<pre><code>name: prodigy-mkdocs-full-build\nmode: mapreduce\n\nenv:\n  PROJECT_NAME: \"MyProject\"\n  DOCS_DIR: \"docs\"\n  CHAPTERS_FILE: \"workflows/data/mkdocs-chapters.json\"\n  MAX_PARALLEL: \"5\"\n\nsetup:\n  - shell: \"mkdir -p .prodigy/mkdocs-analysis\"\n  - claude: \"/prodigy-analyze-features-for-mkdocs --project $PROJECT_NAME\"\n  - claude: \"/prodigy-detect-mkdocs-gaps --project $PROJECT_NAME --docs-dir $DOCS_DIR --chapters $CHAPTERS_FILE\"\n\nmap:\n  input: \".prodigy/mkdocs-analysis/flattened-items.json\"\n  json_path: \"$[*]\"\n  agent_template:\n    - claude: \"/prodigy-analyze-mkdocs-drift --project $PROJECT_NAME --json '${item}'\"\n    - claude: \"/prodigy-fix-mkdocs-drift --project $PROJECT_NAME --json '${item}'\"\n  max_parallel: ${MAX_PARALLEL}\n\nreduce:\n  - shell: \"mkdocs build --strict\"\n  - claude: \"/prodigy-validate-mkdocs-holistically --project $PROJECT_NAME --docs-dir $DOCS_DIR --auto-fix true\"\n</code></pre>"},{"location":"automated-documentation/mkdocs-workflow/#example-2-drift-detection-only-no-gaps","title":"Example 2: Drift Detection Only (No Gaps)","text":"<pre><code>name: prodigy-mkdocs-drift-only\nmode: mapreduce\n\nenv:\n  PROJECT_NAME: \"MyProject\"\n  DOCS_DIR: \"book/src\"\n  MAX_PARALLEL: \"3\"\n\nsetup:\n  - claude: \"/prodigy-analyze-features-for-mkdocs --project $PROJECT_NAME\"\n  # Skip gap detection - just process existing pages\n\nmap:\n  # Manually specify pages instead of using flattened-items.json\n  input:\n    list:\n      - {file: \"book/src/index.md\", title: \"Home\"}\n      - {file: \"book/src/guide.md\", title: \"Guide\"}\n\n  agent_template:\n    - claude: \"/prodigy-analyze-mkdocs-drift --project $PROJECT_NAME --json '${item}'\"\n    - claude: \"/prodigy-fix-mkdocs-drift --project $PROJECT_NAME --json '${item}'\"\n  max_parallel: ${MAX_PARALLEL}\n\nreduce:\n  - shell: \"mkdocs build --strict\"\n  - claude: \"/prodigy-validate-mkdocs-holistically --project $PROJECT_NAME --docs-dir $DOCS_DIR\"\n</code></pre>"},{"location":"automated-documentation/mkdocs-workflow/#example-3-shared-source-with-mdbook","title":"Example 3: Shared Source with mdbook","text":"<pre><code>name: prodigy-mkdocs-shared-source\nmode: mapreduce\n\nenv:\n  PROJECT_NAME: \"MyProject\"\n  DOCS_DIR: \"book/src\"  # Shared with mdbook\n  CHAPTERS_FILE: \"workflows/data/prodigy-chapters.json\"  # Use mdbook chapters\n  MAX_PARALLEL: \"4\"\n\nsetup:\n  - claude: \"/prodigy-analyze-features-for-mkdocs --project $PROJECT_NAME\"\n  - claude: \"/prodigy-detect-mkdocs-gaps --project $PROJECT_NAME --docs-dir $DOCS_DIR --chapters $CHAPTERS_FILE\"\n\nmap:\n  input: \".prodigy/mkdocs-analysis/flattened-items.json\"\n  json_path: \"$[*]\"\n  agent_template:\n    - claude: \"/prodigy-analyze-mkdocs-drift --project $PROJECT_NAME --json '${item}'\"\n    - claude: \"/prodigy-fix-mkdocs-drift --project $PROJECT_NAME --json '${item}'\"\n  max_parallel: ${MAX_PARALLEL}\n\nreduce:\n  # Validate both mdbook and mkdocs builds\n  - shell: \"cd book &amp;&amp; mdbook build\"\n  - shell: \"mkdocs build --strict\"\n  - claude: \"/prodigy-validate-mkdocs-holistically --project $PROJECT_NAME --docs-dir $DOCS_DIR --auto-fix true\"\n</code></pre>"},{"location":"automated-documentation/mkdocs-workflow/#see-also","title":"See Also","text":"<ul> <li>Book Documentation Workflow - mdbook equivalent</li> <li>Automatic Gap Detection - How gap detection works</li> <li>GitHub Actions Integration - CI/CD setup</li> <li>Real-World Example - Prodigy's own setup</li> </ul>"},{"location":"automated-documentation/quick-start/","title":"Quick Start","text":""},{"location":"automated-documentation/quick-start/#quick-start","title":"Quick Start","text":"<p>This guide walks you through setting up your first automated documentation workflow. You'll create a basic mdBook structure, configure Prodigy to analyze your codebase, and run the workflow to generate up-to-date documentation.</p> <p>Time Required: 15-20 minutes</p> <p>Prerequisites: Ensure you have installed Prodigy before proceeding.</p>"},{"location":"automated-documentation/quick-start/#step-1-initialize-your-book-structure","title":"Step 1: Initialize Your Book Structure","text":"<p>First, create the basic mdBook directory structure for your documentation:</p> <pre><code># Create and initialize the book\nmdbook init book\ncd book\n\n# View the generated structure\nls -la\n# Expected output:\n# book.toml      - mdBook configuration\n# src/           - Markdown source files\n#   SUMMARY.md   - Book navigation/table of contents\n#   chapter_1.md - Example chapter\n</code></pre> <p>What this creates: - <code>book.toml</code>: Configuration file for mdBook (title, authors, build settings) - <code>src/SUMMARY.md</code>: Defines your book's structure and navigation - <code>src/chapter_1.md</code>: Example chapter (you can delete or modify this)</p> <p>Source: book/book.toml:1-43, book/src/SUMMARY.md:1-122</p>"},{"location":"automated-documentation/quick-start/#step-2-configure-your-book","title":"Step 2: Configure Your Book","text":"<p>Edit <code>book/book.toml</code> to customize your book settings:</p> <pre><code>[book]\ntitle = \"My Project Documentation\"\nauthors = [\"Your Name &lt;you@example.com&gt;\"]\ndescription = \"Automated documentation for My Project\"\nsrc = \"src\"\nlanguage = \"en\"\n\n[build]\nbuild-dir = \"book\"\ncreate-missing = false\n\n[output.html]\ndefault-theme = \"rust\"\npreferred-dark-theme = \"navy\"\ngit-repository-url = \"https://github.com/youruser/yourproject\"\ngit-repository-icon = \"fa-github\"\n\n[output.html.search]\nenable = true\n</code></pre> <p>Key settings: - <code>title</code>: Your documentation title - <code>git-repository-url</code>: Link to your source repository - <code>create-missing = false</code>: Prevents mdBook from auto-creating missing chapters (Prodigy will manage this)</p> <p>Source: book/book.toml:1-43</p>"},{"location":"automated-documentation/quick-start/#step-3-define-your-documentation-structure","title":"Step 3: Define Your Documentation Structure","text":"<p>Edit <code>book/src/SUMMARY.md</code> to define your book's chapters:</p> <pre><code># Summary\n\n[Introduction](intro.md)\n\n# User Guide\n\n- [Getting Started](getting-started.md)\n- [Configuration](configuration.md)\n- [Commands](commands.md)\n\n# Reference\n\n- [API Reference](api-reference.md)\n- [Troubleshooting](troubleshooting.md)\n</code></pre> <p>Tips: - Start with 3-5 core chapters - Use simple, descriptive chapter names - Group related topics under section headers - You can add more chapters later</p> <p>Source: book/src/SUMMARY.md:1-122</p>"},{"location":"automated-documentation/quick-start/#step-4-create-prodigy-configuration-files","title":"Step 4: Create Prodigy Configuration Files","text":"<p>Now create the configuration files that tell Prodigy how to analyze your codebase.</p>"},{"location":"automated-documentation/quick-start/#4a-create-book-configuration","title":"4a. Create Book Configuration","text":"<p>Create <code>.prodigy/book-config.json</code>:</p> <pre><code>mkdir -p .prodigy\ncat &gt; .prodigy/book-config.json &lt;&lt; 'EOF'\n{\n  \"project_name\": \"MyProject\",\n  \"project_type\": \"cli_tool\",\n  \"book_dir\": \"book\",\n  \"book_src\": \"book/src\",\n  \"book_build_dir\": \"book/book\",\n  \"analysis_targets\": [\n    {\n      \"area\": \"getting_started\",\n      \"source_files\": [\n        \"README.md\",\n        \"examples/\"\n      ],\n      \"feature_categories\": [\n        \"installation\",\n        \"basic_usage\",\n        \"first_steps\"\n      ]\n    },\n    {\n      \"area\": \"configuration\",\n      \"source_files\": [\n        \"src/config/\"\n      ],\n      \"feature_categories\": [\n        \"config_files\",\n        \"settings\",\n        \"environment\"\n      ]\n    }\n  ],\n  \"chapter_file\": \"workflows/data/book-chapters.json\",\n  \"custom_analysis\": {\n    \"include_examples\": true,\n    \"include_best_practices\": true,\n    \"include_troubleshooting\": true\n  }\n}\nEOF\n</code></pre> <p>Configuration Explained: - <code>analysis_targets</code>: Defines which source files to analyze and what features to extract - <code>area</code>: Name for this analysis area (maps to documentation chapters) - <code>source_files</code>: Paths to analyze (can be files or directories) - <code>feature_categories</code>: What types of features to document</p> <p>Source: .prodigy/book-config.json:1-220</p>"},{"location":"automated-documentation/quick-start/#4b-create-chapter-definitions","title":"4b. Create Chapter Definitions","text":"<p>Create <code>workflows/data/book-chapters.json</code>:</p> <pre><code>mkdir -p workflows/data\ncat &gt; workflows/data/book-chapters.json &lt;&lt; 'EOF'\n{\n  \"chapters\": [\n    {\n      \"id\": \"intro\",\n      \"title\": \"Introduction\",\n      \"type\": \"single-file\",\n      \"file\": \"book/src/intro.md\",\n      \"topics\": [\"Project Overview\", \"Goals\"],\n      \"validation\": \"Check introduction explains project purpose and value\"\n    },\n    {\n      \"id\": \"getting-started\",\n      \"title\": \"Getting Started\",\n      \"type\": \"single-file\",\n      \"file\": \"book/src/getting-started.md\",\n      \"topics\": [\"Installation\", \"First Steps\", \"Quick Example\"],\n      \"validation\": \"Verify installation steps and first example work\"\n    },\n    {\n      \"id\": \"configuration\",\n      \"title\": \"Configuration\",\n      \"type\": \"single-file\",\n      \"file\": \"book/src/configuration.md\",\n      \"topics\": [\"Config Files\", \"Settings\", \"Environment\"],\n      \"validation\": \"Check all config options are documented\"\n    }\n  ]\n}\nEOF\n</code></pre> <p>Chapter Definition Explained: - <code>id</code>: Unique identifier for the chapter - <code>type</code>: \"single-file\" (one markdown file) or \"multi-subsection\" (chapter with subsections) - <code>file</code>: Path to the markdown file - <code>topics</code>: What this chapter should cover - <code>validation</code>: Instructions for verifying documentation quality</p> <p>Source: workflows/data/prodigy-chapters.json:1-1262</p>"},{"location":"automated-documentation/quick-start/#step-5-create-the-documentation-workflow","title":"Step 5: Create the Documentation Workflow","text":"<p>Create <code>workflows/book-docs-drift.yml</code>:</p> <pre><code>mkdir -p workflows\ncat &gt; workflows/book-docs-drift.yml &lt;&lt; 'EOF'\nname: book-docs-drift-detection\nmode: mapreduce\n\n# Environment variables\nenv:\n  PROJECT_NAME: \"MyProject\"\n  PROJECT_CONFIG: \".prodigy/book-config.json\"\n  FEATURES_PATH: \".prodigy/book-analysis/features.json\"\n  BOOK_DIR: \"book\"\n  ANALYSIS_DIR: \".prodigy/book-analysis\"\n  CHAPTERS_FILE: \"workflows/data/book-chapters.json\"\n  MAX_PARALLEL: \"3\"\n\n# Setup phase: Analyze codebase\nsetup:\n  - shell: \"mkdir -p $ANALYSIS_DIR\"\n  - claude: \"/prodigy-analyze-features-for-book --project $PROJECT_NAME --config $PROJECT_CONFIG\"\n  - claude: \"/prodigy-detect-documentation-gaps --project $PROJECT_NAME --config $PROJECT_CONFIG --features $FEATURES_PATH --chapters $CHAPTERS_FILE --book-dir $BOOK_DIR\"\n\n# Map phase: Fix each chapter in parallel\nmap:\n  input: \"${ANALYSIS_DIR}/flattened-items.json\"\n  json_path: \"$[*]\"\n\n  agent_template:\n    - claude: \"/prodigy-analyze-subsection-drift --project $PROJECT_NAME --json '${item}' --features $FEATURES_PATH\"\n      commit_required: true\n    - claude: \"/prodigy-fix-subsection-drift --project $PROJECT_NAME --json '${item}'\"\n      commit_required: true\n\n  max_parallel: ${MAX_PARALLEL}\n\n# Reduce phase: Build and validate\nreduce:\n  - shell: \"cd book &amp;&amp; mdbook build\"\n    on_failure:\n      claude: \"/prodigy-fix-book-build-errors --project $PROJECT_NAME\"\n      commit_required: true\n  - shell: \"rm -rf ${ANALYSIS_DIR}\"\n  - shell: \"git add -A &amp;&amp; git commit -m 'chore: clean up analysis files' || true\"\n\n# Error handling\nerror_policy:\n  on_item_failure: dlq\n  continue_on_failure: true\n\n# Merge workflow\nmerge:\n  commands:\n    - shell: \"git fetch origin\"\n    - claude: \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"\nEOF\n</code></pre> <p>Workflow Structure: - Setup Phase: Analyzes your codebase and detects documentation gaps - Map Phase: Processes each chapter in parallel to fix drift - Reduce Phase: Builds the complete book and validates - Merge Phase: Integrates changes back to your branch</p> <p>Source: workflows/book-docs-drift.yml:1-101</p>"},{"location":"automated-documentation/quick-start/#step-6-initialize-prodigy-commands","title":"Step 6: Initialize Prodigy Commands","text":"<p>Initialize the Claude commands that Prodigy uses for documentation generation:</p> <pre><code>prodigy init\n</code></pre> <p>This creates the <code>.claude/commands/</code> directory with commands like: - <code>/prodigy-analyze-features-for-book</code> - Analyzes codebase for features - <code>/prodigy-detect-documentation-gaps</code> - Finds missing documentation - <code>/prodigy-fix-subsection-drift</code> - Fixes outdated documentation - And others needed by the workflow</p> <p>Verify commands were created: <pre><code>ls -la .claude/commands/ | grep \"prodigy-.*book\\|doc\\|gap\"\n</code></pre></p> <p>Source: .claude/commands/prodigy-analyze-features-for-book.md:1-80, .claude/commands/prodigy-detect-documentation-gaps.md:1-80</p>"},{"location":"automated-documentation/quick-start/#step-7-run-your-first-documentation-workflow","title":"Step 7: Run Your First Documentation Workflow","text":"<p>Now run the workflow to generate your documentation:</p> <pre><code>prodigy run workflows/book-docs-drift.yml\n</code></pre> <p>What happens: 1. Setup Phase (~2-5 minutes):    - Analyzes your source code    - Builds feature inventory    - Detects documentation gaps    - Creates stub files for missing chapters</p> <ol> <li>Map Phase (~5-10 minutes):</li> <li>Processes each chapter in parallel</li> <li>Fixes documentation drift</li> <li>Adds code examples from your source</li> <li> <p>Validates quality</p> </li> <li> <p>Reduce Phase (~1-2 minutes):</p> </li> <li>Builds complete book with <code>mdbook build</code></li> <li>Validates all links work</li> <li> <p>Cleans up temporary files</p> </li> <li> <p>Merge Prompt:</p> </li> <li>Asks if you want to merge changes to your branch</li> <li>Type <code>y</code> to accept, <code>n</code> to review first</li> </ol> <p>Example output: <pre><code>\ud83d\udd27 Setup Phase\n\u2713 Created .prodigy/book-analysis/\n\u2713 Analyzed codebase features\n\u2713 Detected 3 documentation gaps\n\n\ud83d\uddfa\ufe0f  Map Phase (3 parallel agents)\n\u2713 Fixed getting-started.md\n\u2713 Fixed configuration.md\n\u2713 Fixed api-reference.md\n\n\ud83d\udd3b Reduce Phase\n\u2713 Built book successfully\n\u2713 Cleaned up analysis files\n\n\ud83d\udcca Summary:\n   3/3 chapters updated\n   12 commits created\n\nMerge to main? [y/N]\n</code></pre></p>"},{"location":"automated-documentation/quick-start/#step-8-review-and-build-your-documentation","title":"Step 8: Review and Build Your Documentation","text":"<p>After the workflow completes, review the generated documentation:</p> <pre><code># View the built book locally\ncd book\nmdbook serve\n\n# Open in browser: http://localhost:3000\n</code></pre> <p>What to review: - Check that all chapters have content - Verify code examples are accurate - Ensure links between chapters work - Validate examples match your codebase</p> <p>If you need to make changes: <pre><code># Edit any chapter\nvim book/src/getting-started.md\n\n# Rebuild the book\nmdbook build\n\n# Or use watch mode for live reload\nmdbook serve\n</code></pre></p>"},{"location":"automated-documentation/quick-start/#next-steps","title":"Next Steps","text":"<p>Congratulations! You've created your first automated documentation workflow. Here's what to do next:</p>"},{"location":"automated-documentation/quick-start/#keep-documentation-updated","title":"Keep Documentation Updated","text":"<p>Run the workflow regularly to keep docs in sync with code:</p> <pre><code># After adding new features\nprodigy run workflows/book-docs-drift.yml\n\n# Schedule in CI/CD (see GitHub Actions Integration)\n</code></pre>"},{"location":"automated-documentation/quick-start/#expand-your-documentation","title":"Expand Your Documentation","text":"<p>Add more chapters to <code>workflows/data/book-chapters.json</code>: - Add new <code>analysis_targets</code> in <code>.prodigy/book-config.json</code> - Define new chapters in <code>book-chapters.json</code> - Update <code>book/src/SUMMARY.md</code> with new chapters - Re-run the workflow</p>"},{"location":"automated-documentation/quick-start/#customize-the-workflow","title":"Customize the Workflow","text":"<ul> <li>Adjust <code>MAX_PARALLEL</code> for faster/slower processing</li> <li>Add validation steps in the reduce phase</li> <li>Customize error handling with <code>error_policy</code></li> </ul>"},{"location":"automated-documentation/quick-start/#integrate-with-cicd","title":"Integrate with CI/CD","text":"<p>Automate documentation updates in your CI/CD pipeline: - See GitHub Actions Integration - Run on every PR or nightly - Deploy to GitHub Pages or docs hosting</p>"},{"location":"automated-documentation/quick-start/#troubleshooting","title":"Troubleshooting","text":""},{"location":"automated-documentation/quick-start/#workflow-fails-in-setup-phase","title":"Workflow fails in setup phase","text":"<p>Symptoms: Error during feature analysis</p> <p>Fixes: - Verify <code>analysis_targets</code> in <code>.prodigy/book-config.json</code> point to existing files - Check that source files exist and are readable - Ensure Claude Code CLI is authenticated</p>"},{"location":"automated-documentation/quick-start/#mdbook-build-fails","title":"mdBook build fails","text":"<p>Symptoms: Error in reduce phase when building book</p> <p>Fixes: - Verify <code>book/book.toml</code> is valid TOML - Check <code>book/src/SUMMARY.md</code> references only existing files - Ensure all linked chapters exist - Run <code>cd book &amp;&amp; mdbook build</code> manually to see detailed error</p>"},{"location":"automated-documentation/quick-start/#no-chapters-were-updated","title":"No chapters were updated","text":"<p>Symptoms: Workflow completes but no changes made</p> <p>Fixes: - Check that chapters in <code>book-chapters.json</code> exist in your repo - Verify <code>analysis_targets</code> match your project structure - Ensure chapters actually need updates (no drift = no changes)</p>"},{"location":"automated-documentation/quick-start/#agent-failures-in-map-phase","title":"Agent failures in map phase","text":"<p>Symptoms: Some chapters fail to update, sent to DLQ</p> <p>Fixes: - Review DLQ: <code>prodigy dlq show &lt;job_id&gt;</code> - Check Claude JSON logs for detailed errors - Retry failed items: <code>prodigy dlq retry &lt;job_id&gt;</code> - See Troubleshooting for common issues</p>"},{"location":"automated-documentation/quick-start/#summary","title":"Summary","text":"<p>You've learned how to: - \u2705 Initialize an mdBook structure - \u2705 Configure Prodigy to analyze your codebase - \u2705 Define documentation chapters and structure - \u2705 Create a MapReduce workflow for documentation - \u2705 Run the workflow to generate docs automatically - \u2705 Review and build your documentation</p> <p>Your documentation is now linked to your code and can be kept up-to-date automatically!</p>"},{"location":"automated-documentation/quick-start/#related-topics","title":"Related Topics","text":"<ul> <li>Understanding the Workflow - Deep dive into how it works</li> <li>GitHub Actions Integration - Automate in CI/CD</li> <li>Troubleshooting - Solutions to common issues</li> </ul>"},{"location":"automated-documentation/real-world-example-prodigys-own-documentation/","title":"Real-World Example: Prodigy's Own Documentation","text":""},{"location":"automated-documentation/real-world-example-prodigys-own-documentation/#real-world-example-prodigys-own-documentation","title":"Real-World Example: Prodigy's Own Documentation","text":"<p>This documentation you're reading is maintained by the same workflow described in this chapter. This is a complete, production-ready workflow that demonstrates:</p> <ul> <li>MapReduce parallelism for processing multiple chapters/subsections concurrently</li> <li>Validation with thresholds to ensure documentation meets quality standards</li> <li>Automatic gap-filling to complete incomplete documentation</li> <li>Multi-subsection chapter support for organizing complex topics</li> <li>Subsection-aware commands that handle both single-file chapters and individual subsections</li> <li>Error handling with DLQ for robust failure recovery</li> </ul> <p>You can examine the actual configuration files used to maintain this documentation:</p>"},{"location":"automated-documentation/real-world-example-prodigys-own-documentation/#book-configuration","title":"Book Configuration","text":"<p>File: <code>.prodigy/book-config.json</code></p> <pre><code>{\n  \"project_name\": \"Prodigy\",\n  \"project_type\": \"cli_tool\",\n  \"book_dir\": \"book\",\n  \"book_src\": \"book/src\",\n  \"book_build_dir\": \"book/book\",\n  \"analysis_targets\": [\n    {\n      \"area\": \"workflow_basics\",\n      \"source_files\": [\"src/config/workflow.rs\", \"src/cook/workflow/executor.rs\"],\n      \"feature_categories\": [\"structure\", \"execution_model\", \"commit_tracking\"]\n    },\n    {\n      \"area\": \"mapreduce\",\n      \"source_files\": [\"src/config/mapreduce.rs\", \"src/cook/execution/mapreduce/\"],\n      \"feature_categories\": [\"phases\", \"capabilities\", \"configuration\"]\n    },\n    {\n      \"area\": \"command_types\",\n      \"source_files\": [\"src/config/command.rs\"],\n      \"feature_categories\": [\"shell\", \"claude\", \"goal_seek\", \"foreach\", \"validation\"]\n    }\n  ]\n}\n</code></pre> <p>Source: <code>.prodigy/book-config.json:1-46</code></p>"},{"location":"automated-documentation/real-world-example-prodigys-own-documentation/#chapter-structure","title":"Chapter Structure","text":"<p>File: <code>workflows/data/prodigy-chapters.json</code></p> <p>This file defines both single-file chapters (one markdown file per chapter) and multi-subsection chapters (chapters split across multiple files):</p> <pre><code>{\n  \"chapters\": [\n    {\n      \"id\": \"workflow-basics\",\n      \"title\": \"Workflow Basics\",\n      \"type\": \"multi-subsection\",\n      \"topics\": [\"Standard workflows\", \"Basic structure\", \"Command execution\"],\n      \"validation\": \"Check basic workflow syntax and structure documentation\",\n      \"index_file\": \"book/src/workflow-basics/index.md\",\n      \"subsections\": [\n        {\n          \"id\": \"command-types\",\n          \"title\": \"Command Types\",\n          \"file\": \"book/src/workflow-basics/command-types.md\",\n          \"topics\": [\"Command Types\"],\n          \"validation\": \"Check command types documentation matches implementation\"\n        },\n        {\n          \"id\": \"environment-configuration\",\n          \"title\": \"Environment Configuration\",\n          \"file\": \"book/src/workflow-basics/environment-configuration.md\",\n          \"topics\": [\"Environment Configuration\"],\n          \"validation\": \"Check environment configuration documentation matches implementation\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre> <p>Source: <code>workflows/data/prodigy-chapters.json:1-80</code></p> <p>The setup phase command <code>/prodigy-detect-documentation-gaps</code> creates a <code>flattened-items.json</code> file containing both single-file chapters and individual subsections with parent metadata. This enables the map phase to process each subsection independently with full awareness of its parent chapter context.</p>"},{"location":"automated-documentation/real-world-example-prodigys-own-documentation/#workflow-configuration","title":"Workflow Configuration","text":"<p>File: <code>workflows/book-docs-drift.yml</code></p> <p>This MapReduce workflow orchestrates the entire documentation maintenance process:</p> <p>Source: <code>workflows/book-docs-drift.yml:1-101</code></p> <p>Key Features Demonstrated:</p> <p>1. Setup Phase (lines 24-34): - Analyzes codebase for feature coverage - Detects documentation gaps and creates missing chapters/subsections - Generates <code>flattened-items.json</code> for subsection-aware processing</p> <p>2. Map Phase (lines 36-58): - Processes each chapter/subsection in parallel using subsection-aware commands:   - <code>/prodigy-analyze-subsection-drift</code> - Analyzes drift for single-file chapters or individual subsections   - <code>/prodigy-fix-subsection-drift</code> - Fixes drift while preserving subsection scope and cross-references   - <code>/prodigy-validate-doc-fix</code> - Validates documentation meets quality standards   - <code>/prodigy-complete-doc-fix</code> - Fills gaps if validation score is below threshold</p> <p>3. Validation with Threshold (lines 49-57): <pre><code>validate:\n  claude: \"/prodigy-validate-doc-fix --project $PROJECT_NAME --json '${item}' --output .prodigy/validation-result.json\"\n  result_file: \".prodigy/validation-result.json\"\n  threshold: 100  # Documentation must meet 100% quality standards\n  on_incomplete:\n    claude: \"/prodigy-complete-doc-fix --project $PROJECT_NAME --json '${item}' --gaps ${validation.gaps}\"\n    max_attempts: 3\n    fail_workflow: false  # Continue even if we can't reach 100%\n    commit_required: true  # Require commit to verify improvements were made\n</code></pre></p> <p>The validation step ensures documentation quality by checking against a score threshold. If the score is below 100, the <code>on_incomplete</code> handler attempts to fill gaps with up to 3 attempts.</p> <p>4. Error Handling (lines 86-90): <pre><code>error_policy:\n  on_item_failure: dlq\n  continue_on_failure: true\n  max_failures: 2\n  error_collection: aggregate\n</code></pre></p> <p>Failed items are sent to the Dead Letter Queue (DLQ) for later retry, allowing the workflow to continue processing other items.</p> <p>5. Custom Merge Workflow (lines 93-101): <pre><code>merge:\n  commands:\n    - shell: \"git fetch origin\"\n    - claude: \"/prodigy-merge-master --project ${PROJECT_NAME}\"\n    - claude: \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"\n</code></pre></p> <p>Custom merge commands handle integration with the main branch and final merge back to the original branch.</p>"},{"location":"automated-documentation/real-world-example-prodigys-own-documentation/#study-these-files","title":"Study These Files","text":"<p>To understand the complete implementation: - Configuration: <code>.prodigy/book-config.json</code> - Book and analysis configuration - Chapter Structure: <code>workflows/data/prodigy-chapters.json</code> - Chapter and subsection definitions - Workflow: <code>workflows/book-docs-drift.yml</code> - Complete MapReduce workflow - Commands: <code>.claude/commands/prodigy-analyze-subsection-drift.md</code> - Subsection-aware drift analysis - Commands: <code>.claude/commands/prodigy-fix-subsection-drift.md</code> - Subsection-aware drift fixing</p>"},{"location":"automated-documentation/troubleshooting/","title":"Troubleshooting","text":""},{"location":"automated-documentation/troubleshooting/#troubleshooting","title":"Troubleshooting","text":"<p>This guide helps you diagnose and fix issues when running automated documentation workflows. The workflow executes in three phases (setup, map, reduce), and each phase has specific failure modes and debugging techniques.</p> <p>Source: Based on <code>workflows/book-docs-drift.yml</code> MapReduce workflow implementation and command definitions in <code>.claude/commands/prodigy-*-book*.md</code></p>"},{"location":"automated-documentation/troubleshooting/#common-issues-by-phase","title":"Common Issues by Phase","text":""},{"location":"automated-documentation/troubleshooting/#setup-phase-issues","title":"Setup Phase Issues","text":"<p>The setup phase analyzes your codebase and detects documentation gaps. Common failures:</p>"},{"location":"automated-documentation/troubleshooting/#issue-featuresjson-not-generated","title":"Issue: \"features.json not generated\"","text":"<p>Symptoms: Setup completes but <code>.prodigy/book-analysis/features.json</code> doesn't exist</p> <p>Causes: - Invalid <code>book-config.json</code> configuration - Missing or incorrect <code>analysis_targets</code> paths - Source files specified in config don't exist</p> <p>Solution: <pre><code># 1. Verify book-config.json is valid JSON\ncat .prodigy/book-config.json | jq .\n\n# 2. Check that source files exist\ncat .prodigy/book-config.json | jq -r '.analysis_targets[].source_files[]' | while read f; do\n  [ -f \"$f\" ] || echo \"Missing: $f\"\ndone\n\n# 3. Re-run feature analysis manually\nprodigy run workflows/book-docs-drift.yml --stop-after setup\n</code></pre></p> <p>Source: Configuration structure from <code>.prodigy/book-config.json:7-213</code></p>"},{"location":"automated-documentation/troubleshooting/#issue-no-gaps-detected-when-gaps-should-exist","title":"Issue: \"No gaps detected when gaps should exist\"","text":"<p>Symptoms: <code>/prodigy-detect-documentation-gaps</code> reports no gaps but documentation is clearly incomplete</p> <p>Causes: - <code>chapters_file</code> path is incorrect in workflow - Chapter definitions JSON is malformed - Features not properly mapped to chapters</p> <p>Solution: <pre><code># 1. Verify chapters file exists and is valid\ncat workflows/data/prodigy-chapters.json | jq .\n\n# 2. Check gap detection output\ncat .prodigy/book-analysis/gaps.json | jq .\n\n# 3. Validate flattened items were generated\ncat .prodigy/book-analysis/flattened-items.json | jq 'length'\n</code></pre></p> <p>Source: Gap detection logic from <code>.claude/commands/prodigy-detect-documentation-gaps.md</code></p>"},{"location":"automated-documentation/troubleshooting/#issue-setup-phase-commits-nothing","title":"Issue: \"Setup phase commits nothing\"","text":"<p>Symptoms: Setup completes successfully but no commit is created</p> <p>Causes: This is often not an error - setup commands use <code>commit_required: false</code> by default and only commit when: - Features have changed since last analysis - New documentation gaps are detected - New stub files are created</p> <p>No action needed if this occurs - the workflow will continue to map phase with existing analysis files.</p> <p>Source: Setup phase design from <code>workflows/book-docs-drift.yml:24-34</code></p>"},{"location":"automated-documentation/troubleshooting/#map-phase-issues","title":"Map Phase Issues","text":"<p>The map phase processes each chapter/subsection in parallel. Each runs in its own agent worktree.</p>"},{"location":"automated-documentation/troubleshooting/#issue-agent-failed-with-validation-error","title":"Issue: \"Agent failed with validation error\"","text":"<p>Symptoms: Agent completes but fails validation threshold (100% required)</p> <p>Causes: - Documentation fix was incomplete - Examples don't match codebase implementation - Required sections are missing</p> <p>Solution: <pre><code># 1. Check which agent failed\nprodigy dlq show &lt;job_id&gt;\n\n# 2. Find the drift report for the failed item\nls -la .prodigy/book-analysis/drift-*.json\n\n# 3. Review what issues were identified\ncat .prodigy/book-analysis/drift-&lt;chapter-id&gt;-&lt;subsection-id&gt;.json | jq '.issues'\n\n# 4. Retry with DLQ\nprodigy dlq retry &lt;job_id&gt;\n</code></pre></p> <p>Source: Validation configuration from <code>workflows/book-docs-drift.yml:49-57</code></p>"},{"location":"automated-documentation/troubleshooting/#issue-drift-analysis-creates-empty-report","title":"Issue: \"Drift analysis creates empty report\"","text":"<p>Symptoms: <code>/prodigy-analyze-subsection-drift</code> commits but drift report shows no issues when drift clearly exists</p> <p>Causes: - Feature mappings are incorrect for the subsection - Source files referenced in feature analysis are empty - Chapter metadata doesn't match actual file</p> <p>Solution: <pre><code># 1. Inspect the drift report\ncat .prodigy/book-analysis/drift-&lt;chapter-id&gt;-&lt;subsection-id&gt;.json | jq .\n\n# 2. Verify feature mappings for this item\ncat .prodigy/book-analysis/flattened-items.json | jq '.[] | select(.id==\"&lt;subsection-id&gt;\")'\n\n# 3. Check that features.json has content for this area\ncat .prodigy/book-analysis/features.json | jq 'keys'\n</code></pre></p> <p>Source: Drift analysis command from <code>.claude/commands/prodigy-analyze-subsection-drift.md</code></p>"},{"location":"automated-documentation/troubleshooting/#issue-multiple-agents-fail-with-same-error","title":"Issue: \"Multiple agents fail with same error\"","text":"<p>Symptoms: Several parallel agents all fail with identical error messages</p> <p>Causes: - Shared resource conflict (rare with worktree isolation) - Configuration error affecting all agents - System resource exhaustion (disk space, memory)</p> <p>Solution: <pre><code># 1. Check system resources\ndf -h  # Disk space\nfree -h  # Memory (Linux) or vm_stat (macOS)\n\n# 2. Review error collection\nprodigy events &lt;job_id&gt; | jq 'select(.event_type==\"AgentFailed\")'\n\n# 3. Reduce parallelism and retry\n# Edit workflow: max_parallel: 1\nprodigy run workflows/book-docs-drift.yml\n</code></pre></p> <p>Source: Error handling policy from <code>workflows/book-docs-drift.yml:86-91</code></p>"},{"location":"automated-documentation/troubleshooting/#reduce-phase-issues","title":"Reduce Phase Issues","text":"<p>The reduce phase rebuilds the book and cleans up analysis files.</p>"},{"location":"automated-documentation/troubleshooting/#issue-mdbook-build-fails-with-broken-links","title":"Issue: \"mdbook build fails with broken links\"","text":"<p>Symptoms: <code>mdbook build</code> exits with error listing broken internal links</p> <p>Causes: - Cross-references to non-existent chapters - Relative paths calculated incorrectly - SUMMARY.md out of sync with actual files</p> <p>Solution: The workflow automatically handles this: <pre><code>- shell: \"cd book &amp;&amp; mdbook build\"\n  on_failure:\n    claude: \"/prodigy-fix-book-build-errors --project $PROJECT_NAME\"\n</code></pre></p> <p>If manual fix needed: <pre><code># 1. See the exact broken links\ncd book &amp;&amp; mdbook build 2&gt;&amp;1 | grep \"Broken link\"\n\n# 2. Fix broken links manually\n# Edit the markdown files to use correct relative paths\n\n# 3. Verify SUMMARY.md includes all files\ncat book/src/SUMMARY.md\n</code></pre></p> <p>Source: Reduce phase from <code>workflows/book-docs-drift.yml:62-68</code></p>"},{"location":"automated-documentation/troubleshooting/#issue-analysis-files-not-cleaned-up","title":"Issue: \"Analysis files not cleaned up\"","text":"<p>Symptoms: <code>.prodigy/book-analysis/</code> directory still exists after workflow completion</p> <p>Causes: - Reduce phase didn't complete - Cleanup command failed silently (uses <code>|| true</code>)</p> <p>Solution: <pre><code># Manual cleanup is safe\nrm -rf .prodigy/book-analysis\n\n# Check if this was part of incomplete workflow\nprodigy sessions list\n</code></pre></p> <p>This is cosmetic - analysis files are regenerated on each run.</p> <p>Source: Cleanup step from <code>workflows/book-docs-drift.yml:81-82</code></p>"},{"location":"automated-documentation/troubleshooting/#debugging-techniques","title":"Debugging Techniques","text":""},{"location":"automated-documentation/troubleshooting/#inspecting-analysis-artifacts","title":"Inspecting Analysis Artifacts","text":"<p>All intermediate analysis files are stored in <code>.prodigy/book-analysis/</code>:</p> <pre><code># Feature inventory from codebase analysis\ncat .prodigy/book-analysis/features.json | jq .\n\n# Documentation gaps detected\ncat .prodigy/book-analysis/gaps.json | jq .\n\n# Flattened items for map phase (chapters + subsections)\ncat .prodigy/book-analysis/flattened-items.json | jq .\n\n# Drift reports (one per chapter/subsection)\nls -la .prodigy/book-analysis/drift-*.json\ncat .prodigy/book-analysis/drift-&lt;chapter-id&gt;-&lt;subsection-id&gt;.json | jq .\n</code></pre> <p>Source: File locations from <code>workflows/book-docs-drift.yml:9-18</code> and setup phase commands</p>"},{"location":"automated-documentation/troubleshooting/#reviewing-event-logs","title":"Reviewing Event Logs","text":"<p>MapReduce workflows generate detailed event logs:</p> <pre><code># List all events for a job\nprodigy events &lt;job_id&gt;\n\n# Filter to agent failures\nprodigy events &lt;job_id&gt; | jq 'select(.event_type==\"AgentFailed\")'\n\n# See what items completed successfully\nprodigy events &lt;job_id&gt; | jq 'select(.event_type==\"AgentCompleted\") | .agent_id'\n\n# Find Claude JSON log locations for failed agents\nprodigy events &lt;job_id&gt; | jq 'select(.event_type==\"AgentCompleted\") | .json_log_location'\n</code></pre> <p>Source: Event tracking implementation from <code>src/cook/execution/events/event_types.rs</code> and CLI handler <code>src/cli/commands/events.rs</code></p>"},{"location":"automated-documentation/troubleshooting/#checking-dead-letter-queue-dlq","title":"Checking Dead Letter Queue (DLQ)","text":"<p>Failed work items are sent to the DLQ for review and retry:</p> <pre><code># Show all failed items for a job\nprodigy dlq show &lt;job_id&gt;\n\n# See failure details with JSON log locations\nprodigy dlq show &lt;job_id&gt; | jq '.items[].failure_history'\n\n# Get Claude log path for debugging\nprodigy dlq show &lt;job_id&gt; | jq '.items[].failure_history[].json_log_location'\n\n# Retry all failed items\nprodigy dlq retry &lt;job_id&gt;\n\n# Retry with custom parallelism\nprodigy dlq retry &lt;job_id&gt; --max-parallel 10\n\n# See DLQ statistics\nprodigy dlq stats &lt;job_id&gt;\n</code></pre> <p>Source: DLQ implementation from <code>src/cook/execution/dlq.rs</code> and CLI handler <code>src/cli/commands/dlq.rs</code></p>"},{"location":"automated-documentation/troubleshooting/#examining-claude-command-logs","title":"Examining Claude Command Logs","text":"<p>Each Claude command execution creates a JSON log file with complete conversation history:</p> <pre><code># Find the log location from workflow output (with -v flag)\nprodigy run workflows/book-docs-drift.yml -v\n\n# Or from DLQ item failure details\nLOG_PATH=$(prodigy dlq show &lt;job_id&gt; | jq -r '.items[0].failure_history[0].json_log_location')\n\n# View the full conversation\ncat \"$LOG_PATH\" | jq .\n\n# Extract tool invocations\ncat \"$LOG_PATH\" | jq '.messages[] | select(.role==\"assistant\") | .content[] | select(.type==\"tool_use\")'\n\n# Check for errors\ncat \"$LOG_PATH\" | jq '.messages[] | select(.type==\"error\")'\n</code></pre> <p>Source: Claude log tracking from Spec 121 (JSON Log Location Tracking) and <code>src/cook/execution/mapreduce/agent_result.rs</code></p>"},{"location":"automated-documentation/troubleshooting/#testing-individual-commands","title":"Testing Individual Commands","text":"<p>You can run workflow commands individually for debugging:</p> <pre><code># Run feature analysis only\nclaude /prodigy-analyze-features-for-book --project Prodigy --config .prodigy/book-config.json\n\n# Run gap detection only (requires features.json first)\nclaude /prodigy-detect-documentation-gaps --project Prodigy --config .prodigy/book-config.json --features .prodigy/book-analysis/features.json --chapters workflows/data/prodigy-chapters.json --book-dir book\n\n# Analyze specific chapter for drift (requires features.json)\nclaude /prodigy-analyze-subsection-drift --project Prodigy --json '{\"type\":\"subsection\",\"id\":\"troubleshooting\",\"parent_chapter_id\":\"automated-documentation\",\"file\":\"book/src/automated-documentation/troubleshooting.md\"}' --features .prodigy/book-analysis/features.json\n\n# Fix specific chapter drift (requires drift report)\nclaude /prodigy-fix-subsection-drift --project Prodigy --json '{\"type\":\"subsection\",\"id\":\"troubleshooting\",\"parent_chapter_id\":\"automated-documentation\",\"file\":\"book/src/automated-documentation/troubleshooting.md\"}'\n</code></pre> <p>Source: Command definitions from <code>.claude/commands/prodigy-*-book*.md</code></p>"},{"location":"automated-documentation/troubleshooting/#resume-and-recovery","title":"Resume and Recovery","text":""},{"location":"automated-documentation/troubleshooting/#resuming-interrupted-workflows","title":"Resuming Interrupted Workflows","text":"<p>MapReduce workflows support checkpoint-based resume. See the Checkpoint and Resume documentation for details.</p> <pre><code># Resume using session ID\nprodigy resume session-mapreduce-1234567890\n\n# Resume using job ID\nprodigy resume-job mapreduce-1234567890\n\n# Unified resume (auto-detects ID type)\nprodigy resume mapreduce-1234567890\n</code></pre> <p>Source: Resume functionality from Spec 134 (MapReduce Checkpoint and Resume)</p>"},{"location":"automated-documentation/troubleshooting/#retrying-failed-items-from-dlq","title":"Retrying Failed Items from DLQ","text":"<p>After a workflow completes with failures:</p> <pre><code># 1. Review what failed\nprodigy dlq show &lt;job_id&gt;\n\n# 2. Retry all failed items\nprodigy dlq retry &lt;job_id&gt;\n\n# 3. Monitor progress\nprodigy events &lt;job_id&gt;\n</code></pre> <p>The DLQ retry creates a new execution context but preserves correlation IDs for tracking.</p> <p>Source: DLQ retry implementation from <code>src/cook/execution/dlq_reprocessor.rs</code></p>"},{"location":"automated-documentation/troubleshooting/#file-locations-reference","title":"File Locations Reference","text":"<p>Key files and directories for troubleshooting:</p> Location Description Phase <code>.prodigy/book-config.json</code> Project configuration for documentation Setup input <code>workflows/data/prodigy-chapters.json</code> Chapter structure definitions Setup input <code>.prodigy/book-analysis/features.json</code> Extracted codebase features Setup output <code>.prodigy/book-analysis/gaps.json</code> Detected documentation gaps Setup output <code>.prodigy/book-analysis/flattened-items.json</code> Work items for map phase Setup output <code>.prodigy/book-analysis/drift-*.json</code> Per-chapter drift reports Map output <code>~/.prodigy/events/&lt;repo&gt;/&lt;job_id&gt;/</code> Event logs (global storage) All phases <code>~/.prodigy/dlq/&lt;repo&gt;/&lt;job_id&gt;/</code> Dead letter queue items Map phase <code>~/.prodigy/state/&lt;repo&gt;/mapreduce/jobs/&lt;job_id&gt;/</code> Checkpoint files All phases <code>~/.local/state/claude/logs/session-*.json</code> Claude command logs All phases <p>Source: Storage locations from global storage architecture (Spec 127) and workflow environment variables in <code>workflows/book-docs-drift.yml:9-18</code></p>"},{"location":"automated-documentation/troubleshooting/#performance-tips","title":"Performance Tips","text":""},{"location":"automated-documentation/troubleshooting/#adjusting-parallelism","title":"Adjusting Parallelism","text":"<p>The workflow uses <code>max_parallel: 3</code> by default. Adjust based on your system:</p> <pre><code>env:\n  MAX_PARALLEL: \"5\"  # Process 5 chapters concurrently\n\nmap:\n  max_parallel: ${MAX_PARALLEL}\n</code></pre> <p>Trade-offs: - Higher parallelism = faster completion, more system resources - Lower parallelism = slower completion, fewer failures from resource contention</p> <p>Source: Parallelism configuration from <code>workflows/book-docs-drift.yml:21,59</code></p>"},{"location":"automated-documentation/troubleshooting/#processing-subset-of-chapters","title":"Processing Subset of Chapters","text":"<p>Use JSONPath filters to target specific documentation:</p> <pre><code>map:\n  input: \"${ANALYSIS_DIR}/flattened-items.json\"\n  json_path: \"$[*]\"\n  filter: \"item.parent_chapter_id == 'mapreduce'\"  # Only MapReduce subsections\n</code></pre> <p>Or manually edit <code>flattened-items.json</code> to include only desired items.</p> <p>Source: Filter syntax from MapReduce workflow specification</p>"},{"location":"automated-documentation/troubleshooting/#skipping-validation-for-drafts","title":"Skipping Validation for Drafts","text":"<p>For faster iteration during development, reduce validation threshold:</p> <pre><code>map:\n  agent_template:\n    - claude: \"/prodigy-fix-subsection-drift --project $PROJECT_NAME --json '${item}'\"\n      commit_required: true\n      validate:\n        threshold: 70  # Accept 70% quality instead of 100%\n</code></pre> <p>Warning: This may result in lower quality documentation.</p> <p>Source: Validation configuration from <code>workflows/book-docs-drift.yml:49-57</code></p>"},{"location":"automated-documentation/troubleshooting/#configuration-issues","title":"Configuration Issues","text":""},{"location":"automated-documentation/troubleshooting/#invalid-book-configjson","title":"Invalid book-config.json","text":"<p>Symptoms: Setup phase fails immediately or generates no features</p> <p>Solution: <pre><code># Validate JSON syntax\ncat .prodigy/book-config.json | jq empty\n\n# Check required fields exist\ncat .prodigy/book-config.json | jq '{project_name, analysis_targets, chapter_file}'\n</code></pre></p> <p>Required fields: - <code>project_name</code> - Project display name - <code>analysis_targets</code> - Array of areas with source files - <code>chapter_file</code> - Path to chapter definitions</p> <p>Source: Configuration structure from <code>.prodigy/book-config.json:1-220</code></p>"},{"location":"automated-documentation/troubleshooting/#missing-source-files-in-analysis_targets","title":"Missing Source Files in analysis_targets","text":"<p>Symptoms: Features not extracted for certain areas</p> <p>Solution: <pre><code># Check all referenced source files exist\ncat .prodigy/book-config.json | jq -r '.analysis_targets[].source_files[]' | while read file; do\n  if [ ! -e \"$file\" ]; then\n    echo \"Missing: $file\"\n  fi\ndone\n</code></pre></p> <p>Update paths in <code>book-config.json</code> to match actual source file locations.</p> <p>Source: Analysis targets from <code>.prodigy/book-config.json:7-213</code></p>"},{"location":"automated-documentation/troubleshooting/#incorrect-chapter-definitions","title":"Incorrect Chapter Definitions","text":"<p>Symptoms: Gaps detected for chapters that already exist, or no gaps when chapters are missing</p> <p>Solution: <pre><code># Verify chapter definitions match actual book structure\ndiff &lt;(cat workflows/data/prodigy-chapters.json | jq -r '.chapters[].id' | sort) \\\n     &lt;(find book/src -name \"index.md\" -o -name \"[!index]*.md\" | sed 's|book/src/||; s|/index.md||; s|\\.md||' | sort)\n</code></pre></p> <p>Update <code>workflows/data/prodigy-chapters.json</code> to match your book structure.</p> <p>Source: Chapter definitions referenced in <code>workflows/book-docs-drift.yml:18</code></p>"},{"location":"automated-documentation/troubleshooting/#faq","title":"FAQ","text":"<p>Q: Why does setup phase show \"No changes\" even though I modified source code?</p> <p>A: Feature analysis only commits when features change. Code changes don't always mean feature changes (e.g., bug fixes, refactoring). This is expected behavior.</p> <p>Q: Can I run the workflow on a subset of chapters?</p> <p>A: Yes. Either: 1. Use <code>filter</code> in map phase to select specific items 2. Manually edit <code>.prodigy/book-analysis/flattened-items.json</code> after setup 3. Modify chapter definitions to exclude certain chapters</p> <p>Q: What happens if I interrupt the workflow?</p> <p>A: Use <code>prodigy resume &lt;job_id&gt;</code> to continue from the last checkpoint. See Checkpoint and Resume for details.</p> <p>Q: How do I debug why a specific chapter failed validation?</p> <p>A: <pre><code># 1. Find the validation result\ncat .prodigy/validation-result.json | jq .\n\n# 2. Check the drift report for this chapter\ncat .prodigy/book-analysis/drift-&lt;chapter-id&gt;-&lt;subsection-id&gt;.json | jq .\n\n# 3. Review Claude's attempt to fix it\nprodigy dlq show &lt;job_id&gt; | jq '.items[] | select(.id==\"&lt;subsection-id&gt;\")'\n</code></pre></p> <p>Q: Can I customize what gets analyzed?</p> <p>A: Yes. Edit <code>.prodigy/book-config.json</code> to: - Add/remove <code>analysis_targets</code> areas - Change which source files are analyzed per area - Adjust <code>feature_categories</code> to extract different information - Enable/disable examples, best practices, troubleshooting in <code>custom_analysis</code></p> <p>Q: The workflow is too slow. How can I speed it up?</p> <p>A: 1. Increase <code>max_parallel</code> (default: 3) 2. Process fewer chapters using filters 3. Use <code>--stop-after setup</code> to only regenerate analysis files 4. Reduce validation threshold for draft iterations</p>"},{"location":"automated-documentation/troubleshooting/#see-also","title":"See Also","text":"<ul> <li>Understanding the Workflow - Workflow phase details</li> <li>Checkpoint and Resume - Resume interrupted workflows</li> <li>Dead Letter Queue - Handling persistent failures</li> <li>Event Tracking - Monitoring workflow execution</li> <li>Advanced Configuration - Customizing the workflow</li> </ul>"},{"location":"automated-documentation/tutorial/","title":"Tutorial (30 Minutes)","text":""},{"location":"automated-documentation/tutorial/#tutorial-complete-documentation-workflow-30-minutes","title":"Tutorial: Complete Documentation Workflow (30 Minutes)","text":"<p>This comprehensive tutorial walks you through setting up and running your first automated documentation workflow. By the end, you'll have a complete understanding of how Prodigy maintains documentation automatically.</p>"},{"location":"automated-documentation/tutorial/#what-youll-accomplish","title":"What You'll Accomplish","text":"<p>By the end of this tutorial, you'll have: - Configured the automated documentation system for your project - Run the workflow to detect and fix documentation drift - Generated or updated an mdBook with accurate, code-grounded documentation - Understood all three phases: setup, map, and reduce</p>"},{"location":"automated-documentation/tutorial/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have installed Prodigy and the required tools: - Prodigy (via <code>cargo install prodigy</code>) - Claude Code CLI - mdBook (for building documentation) - Git (initialized repository)</p> <p>If you haven't installed these yet, visit the Installation guide first</p>"},{"location":"automated-documentation/tutorial/#step-1-initialize-your-book-structure","title":"Step 1: Initialize Your Book Structure","text":"<p>If you don't already have an mdBook, create one:</p> <pre><code># Create book directory and initialize\nmkdir book\ncd book\nmdbook init\n\n# This creates:\n# book/\n#   \u251c\u2500\u2500 book.toml\n#   \u2514\u2500\u2500 src/\n#       \u251c\u2500\u2500 SUMMARY.md\n#       \u2514\u2500\u2500 chapter_1.md\n</code></pre> <p>Source: Standard mdBook initialization pattern</p>"},{"location":"automated-documentation/tutorial/#step-2-create-book-configuration","title":"Step 2: Create Book Configuration","text":"<p>Create <code>.prodigy/book-config.json</code> at your project root with your project details:</p> <pre><code>{\n  \"project_name\": \"YourProject\",\n  \"project_type\": \"cli_tool\",\n  \"book_dir\": \"book\",\n  \"book_src\": \"book/src\",\n  \"book_build_dir\": \"book/book\",\n  \"analysis_targets\": [\n    {\n      \"area\": \"workflow_basics\",\n      \"source_files\": [\n        \"src/config/workflow.rs\",\n        \"src/workflow/executor.rs\"\n      ],\n      \"feature_categories\": [\n        \"structure\",\n        \"execution_model\",\n        \"commit_tracking\"\n      ]\n    }\n  ],\n  \"chapter_file\": \"workflows/data/your-project-chapters.json\",\n  \"custom_analysis\": {\n    \"include_examples\": true,\n    \"include_best_practices\": true,\n    \"include_troubleshooting\": true\n  }\n}\n</code></pre> <p>Source: Extracted from <code>.prodigy/book-config.json</code> structure (lines 1-219)</p> <p>Key Fields Explained: - <code>project_name</code> - Display name for your project - <code>book_dir</code> - Root directory for your mdBook - <code>analysis_targets</code> - Areas of your codebase to analyze for documentation   - <code>area</code> - Logical grouping name   - <code>source_files</code> - Files to analyze for this area   - <code>feature_categories</code> - Types of features to extract - <code>chapter_file</code> - Path to chapter definitions JSON</p>"},{"location":"automated-documentation/tutorial/#step-3-define-chapter-structure","title":"Step 3: Define Chapter Structure","text":"<p>Create <code>workflows/data/your-project-chapters.json</code> to define your documentation structure:</p> <pre><code>{\n  \"chapters\": [\n    {\n      \"id\": \"getting-started\",\n      \"title\": \"Getting Started\",\n      \"file\": \"getting-started.md\",\n      \"topics\": [\"Installation\", \"Quick Start\"],\n      \"feature_mapping\": [\"installation\", \"basic_usage\"],\n      \"validation\": \"Check getting started guide matches current setup\"\n    },\n    {\n      \"id\": \"advanced\",\n      \"title\": \"Advanced Features\",\n      \"file\": \"advanced/index.md\",\n      \"topics\": [\"Configuration\", \"API Reference\"],\n      \"feature_mapping\": [\"configuration\", \"api\"],\n      \"validation\": \"Verify advanced features are documented\"\n    }\n  ]\n}\n</code></pre> <p>Source: Based on <code>workflows/data/prodigy-chapters.json</code> structure pattern</p>"},{"location":"automated-documentation/tutorial/#step-4-create-the-workflow-file","title":"Step 4: Create the Workflow File","text":"<p>Create <code>workflows/book-docs-drift.yml</code>:</p> <pre><code>name: your-project-book-docs-drift-detection\nmode: mapreduce\n\nenv:\n  PROJECT_NAME: \"YourProject\"\n  PROJECT_CONFIG: \".prodigy/book-config.json\"\n  FEATURES_PATH: \".prodigy/book-analysis/features.json\"\n  BOOK_DIR: \"book\"\n  ANALYSIS_DIR: \".prodigy/book-analysis\"\n  CHAPTERS_FILE: \"workflows/data/your-project-chapters.json\"\n  MAX_PARALLEL: \"3\"\n\nsetup:\n  - shell: \"mkdir -p $ANALYSIS_DIR\"\n  - claude: \"/prodigy-analyze-features-for-book --project $PROJECT_NAME --config $PROJECT_CONFIG\"\n  - claude: \"/prodigy-detect-documentation-gaps --project $PROJECT_NAME --config $PROJECT_CONFIG --features $FEATURES_PATH --chapters $CHAPTERS_FILE --book-dir $BOOK_DIR\"\n\nmap:\n  input: \"${ANALYSIS_DIR}/flattened-items.json\"\n  json_path: \"$[*]\"\n\n  agent_template:\n    - claude: \"/prodigy-analyze-subsection-drift --project $PROJECT_NAME --json '${item}' --features $FEATURES_PATH\"\n      commit_required: true\n\n    - claude: \"/prodigy-fix-subsection-drift --project $PROJECT_NAME --json '${item}'\"\n      commit_required: true\n      validate:\n        claude: \"/prodigy-validate-doc-fix --project $PROJECT_NAME --json '${item}' --output .prodigy/validation-result.json\"\n        result_file: \".prodigy/validation-result.json\"\n        threshold: 100\n        on_incomplete:\n          claude: \"/prodigy-complete-doc-fix --project $PROJECT_NAME --json '${item}' --gaps ${validation.gaps}\"\n          max_attempts: 3\n          fail_workflow: false\n          commit_required: true\n\n  max_parallel: ${MAX_PARALLEL}\n\nreduce:\n  - shell: \"cd book &amp;&amp; mdbook build\"\n    on_failure:\n      claude: \"/prodigy-fix-book-build-errors --project $PROJECT_NAME\"\n      commit_required: true\n\n  - shell: \"rm -rf ${ANALYSIS_DIR}\"\n  - shell: \"git add -A &amp;&amp; git commit -m 'chore: remove temporary book analysis files' || true\"\n\nerror_policy:\n  on_item_failure: dlq\n  continue_on_failure: true\n  max_failures: 2\n  error_collection: aggregate\n\nmerge:\n  commands:\n    - shell: \"git fetch origin\"\n    - claude: \"/prodigy-merge-master --project ${PROJECT_NAME}\"\n    - claude: \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"\n</code></pre> <p>Source: Adapted from <code>workflows/book-docs-drift.yml</code> (lines 1-101)</p>"},{"location":"automated-documentation/tutorial/#step-5-run-the-workflow","title":"Step 5: Run the Workflow","text":"<p>Execute the documentation workflow:</p> <pre><code># Run with verbose output to see progress\nprodigy run workflows/book-docs-drift.yml -v\n</code></pre> <p>What Happens During Execution:</p> <p>Setup Phase (runs in parent worktree): <pre><code>\u2713 Creating analysis directory\n\u2713 Analyzing codebase features \u2192 .prodigy/book-analysis/features.json\n\u2713 Detecting documentation gaps \u2192 .prodigy/book-analysis/flattened-items.json\n</code></pre></p> <p>Map Phase (parallel agents, each in isolated worktree): <pre><code>Agent 1: Analyzing chapter 'Getting Started'\n  \u2713 Drift analysis complete \u2192 drift report\n  \u2713 Fixing outdated examples\n  \u2713 Validation: 100% complete\n  \u2713 Merged to parent worktree\n\nAgent 2: Analyzing chapter 'Advanced Features'\n  \u2713 Drift analysis complete \u2192 drift report\n  \u2713 Adding missing configuration docs\n  \u2713 Validation: 100% complete\n  \u2713 Merged to parent worktree\n</code></pre></p> <p>Reduce Phase (runs in parent worktree): <pre><code>\u2713 Building book with mdbook\n\u2713 Cleaning up temporary files\n\u2713 Ready to merge to master\n</code></pre></p> <p>Source: Execution flow based on MapReduce workflow phases in <code>workflows/book-docs-drift.yml</code> and features.json analysis</p>"},{"location":"automated-documentation/tutorial/#step-6-review-and-merge-changes","title":"Step 6: Review and Merge Changes","text":"<p>After the workflow completes, you'll see:</p> <pre><code>Workflow completed successfully!\n\nMerge session-abc123 to master? [Y/n]\n</code></pre> <p>Before merging, review the changes:</p> <pre><code># Check what was modified\ncd ~/.prodigy/worktrees/your-project/session-abc123/\ngit log --oneline\ngit diff master\n\n# Review specific chapter changes\ngit show HEAD:book/src/getting-started.md\n</code></pre> <p>Type <code>Y</code> to merge changes back to your original branch.</p>"},{"location":"automated-documentation/tutorial/#step-7-verify-the-updated-documentation","title":"Step 7: Verify the Updated Documentation","text":"<p>After merging, view your updated documentation:</p> <pre><code># Build and serve locally\ncd book\nmdbook serve --open\n\n# Your browser opens to http://localhost:3000\n# Navigate through chapters to see updated content\n</code></pre> <p>Expected Results: - \u2713 All code examples reference actual source files - \u2713 Configuration examples match your codebase types - \u2713 API documentation reflects current function signatures - \u2713 Outdated sections updated or removed - \u2713 Cross-references between chapters are valid</p>"},{"location":"automated-documentation/tutorial/#understanding-the-workflow-phases","title":"Understanding the Workflow Phases","text":"<p>The automated documentation workflow uses Prodigy's MapReduce pattern with three phases:</p>"},{"location":"automated-documentation/tutorial/#setup-phase","title":"Setup Phase","text":"<p>Purpose: Analyze your codebase and prepare work items</p> <p>Commands: 1. <code>mkdir -p $ANALYSIS_DIR</code> - Create temporary analysis directory 2. <code>/prodigy-analyze-features-for-book</code> - Extract features from source files into features.json 3. <code>/prodigy-detect-documentation-gaps</code> - Compare features to chapters, create flattened-items.json</p> <p>Output: JSON file with list of chapters/subsections to process</p> <p>Source: Setup phase from <code>workflows/book-docs-drift.yml:24-34</code></p>"},{"location":"automated-documentation/tutorial/#map-phase","title":"Map Phase","text":"<p>Purpose: Process each chapter/subsection in parallel to detect and fix drift</p> <p>For each chapter: 1. <code>/prodigy-analyze-subsection-drift</code> - Compare chapter to codebase, identify outdated/missing content 2. <code>/prodigy-fix-subsection-drift</code> - Update markdown file with accurate, grounded examples 3. Validation - Ensure documentation meets quality standards (100% threshold) 4. Gap filling - If validation fails, run completion attempts (max 3)</p> <p>Parallelism: Configured via <code>max_parallel: 3</code> - three chapters processed simultaneously</p> <p>Isolation: Each chapter processed in its own git worktree, merged back to parent automatically</p> <p>Source: Map phase from <code>workflows/book-docs-drift.yml:37-59</code></p>"},{"location":"automated-documentation/tutorial/#reduce-phase","title":"Reduce Phase","text":"<p>Purpose: Validate the complete book and clean up</p> <p>Commands: 1. <code>mdbook build</code> - Compile the book to catch broken links or formatting errors 2. <code>/prodigy-fix-book-build-errors</code> - Fix any build errors (only runs if build fails) 3. Cleanup temporary analysis files</p> <p>Source: Reduce phase from <code>workflows/book-docs-drift.yml:62-69</code></p>"},{"location":"automated-documentation/tutorial/#customization-tips","title":"Customization Tips","text":"<p>Adjust Parallelism: <pre><code>env:\n  MAX_PARALLEL: \"5\"  # Process 5 chapters at once (default: 3)\n</code></pre></p> <p>Focus on Specific Areas: Edit <code>.prodigy/book-config.json</code> to analyze only certain parts of your codebase: <pre><code>{\n  \"analysis_targets\": [\n    {\n      \"area\": \"api\",\n      \"source_files\": [\"src/api/**/*.rs\"],\n      \"feature_categories\": [\"endpoints\", \"authentication\"]\n    }\n  ]\n}\n</code></pre></p> <p>Change Validation Threshold: <pre><code>validate:\n  threshold: 95  # Allow 95% instead of 100%\n</code></pre></p>"},{"location":"automated-documentation/tutorial/#troubleshooting","title":"Troubleshooting","text":"<p>Issue: \"Claude command not found\" - Solution: Ensure Claude Code CLI is installed and in your PATH - Verify: <code>claude --version</code></p> <p>Issue: \"features.json not generated\" - Cause: Setup phase failed to analyze codebase - Solution: Check that <code>source_files</code> in book-config.json exist and are valid - Debug: Run with <code>-vv</code> for detailed logs</p> <p>Issue: \"mdbook build fails with broken links\" - Cause: Cross-references to non-existent chapters - Solution: The workflow automatically fixes this in reduce phase - Manual fix: Check <code>book/src/SUMMARY.md</code> for invalid links</p> <p>Issue: \"Validation threshold not met\" - Cause: Documentation doesn't meet 100% quality standard - Solution: The <code>on_incomplete</code> handler attempts to complete gaps (max 3 attempts) - If still incomplete: Review <code>.prodigy/validation-result.json</code> for details</p> <p>Source: Common issues from features.json troubleshooting section (lines 802-885)</p>"},{"location":"automated-documentation/tutorial/#next-steps","title":"Next Steps","text":"<p>Now that you have automated documentation working, explore:</p> <ul> <li>Understanding the Workflow - Deep dive into how the workflow operates</li> <li>GitHub Actions Integration - Automate documentation updates on every commit</li> <li>Troubleshooting - Solutions to common problems</li> </ul>"},{"location":"automated-documentation/tutorial/#what-youve-learned","title":"What You've Learned","text":"<p>\u2713 How to configure automated documentation for any project \u2713 The structure of a MapReduce documentation workflow \u2713 How setup/map/reduce phases process your codebase \u2713 How to review and merge documentation updates \u2713 Basic troubleshooting techniques</p>"},{"location":"automated-documentation/tutorial/#time-investment-breakdown","title":"Time Investment Breakdown","text":"<ul> <li>Prerequisites setup: 5-10 minutes (one-time)</li> <li>Configuration files: 10 minutes</li> <li>First workflow run: 5-10 minutes (depending on project size)</li> <li>Review and merge: 5 minutes</li> </ul> <p>Total: 25-35 minutes for your first complete run</p>"},{"location":"automated-documentation/understanding-the-workflow/","title":"Understanding the Workflow","text":""},{"location":"automated-documentation/understanding-the-workflow/#understanding-the-workflow","title":"Understanding the Workflow","text":"<p>The automated documentation workflow uses a MapReduce architecture to process documentation in parallel, ensuring efficiency and isolation. This section explains how the workflow phases execute, how worktrees provide isolation, and how the system ensures quality.</p>"},{"location":"automated-documentation/understanding-the-workflow/#four-phase-execution-model","title":"Four-Phase Execution Model","text":"<p>The documentation workflow consists of four sequential phases, each with a specific responsibility:</p>"},{"location":"automated-documentation/understanding-the-workflow/#1-setup-phase-feature-analysis","title":"1. Setup Phase (Feature Analysis)","text":"<p>Purpose: Analyze the codebase and prepare work items for parallel processing.</p> <p>What it does: - Scans source code to build a complete feature inventory - Compares existing documentation against implementation to detect gaps - Creates missing chapter and subsection placeholder files - Generates a JSON file containing work items for the map phase</p> <p>Source: <code>workflows/book-docs-drift.yml:24-34</code></p> <p>Example output: <code>.prodigy/book-analysis/doc-items.json</code> containing: <pre><code>[\n  {\n    \"type\": \"subsection\",\n    \"id\": \"checkpoint-and-resume\",\n    \"parent_chapter_id\": \"mapreduce\",\n    \"file\": \"book/src/mapreduce/checkpoint-and-resume.md\",\n    \"feature_mapping\": [\"mapreduce.checkpoint\", \"mapreduce.resume\"],\n    \"topics\": [\"Checkpoint behavior\", \"Resume strategies\"]\n  }\n]\n</code></pre></p>"},{"location":"automated-documentation/understanding-the-workflow/#2-map-phase-parallel-processing","title":"2. Map Phase (Parallel Processing)","text":"<p>Purpose: Process each documentation item concurrently in isolated environments.</p> <p>What it does: - Distributes work items across multiple parallel agents (default: 3 concurrent) - Each agent runs in an isolated git worktree (child of the parent worktree) - For each documentation item:   - Analyzes drift between documentation and implementation   - Searches codebase for real examples and type definitions   - Fixes identified issues with validated, code-grounded content   - Commits changes to the agent's worktree - Failed items automatically added to Dead Letter Queue (DLQ) for retry - Successful agents merge their changes back to the parent worktree</p> <p>Source: <code>workflows/book-docs-drift.yml:37-59</code></p> <p>Parallel execution control (from <code>src/cook/execution/mapreduce/coordination/executor.rs:617-824</code>): <pre><code>// Create semaphore to control concurrency\nlet semaphore = Arc::new(Semaphore::new(max_parallel));\n\n// Spawn async task per work item\nlet agent_futures: Vec&lt;_&gt; = work_items\n    .into_iter()\n    .map(|(item)| {\n        tokio::spawn(async move {\n            let _permit = sem.acquire().await?;  // Wait for slot\n            execute_agent_for_item(...).await\n        })\n    })\n    .collect();\n</code></pre></p>"},{"location":"automated-documentation/understanding-the-workflow/#3-reduce-phase-validation","title":"3. Reduce Phase (Validation)","text":"<p>Purpose: Validate that all documentation changes work together correctly.</p> <p>What it does: - Rebuilds the entire book using <code>mdbook build</code> - Checks for broken links between chapters and subsections - Detects any compilation errors or missing references - Fixes build errors if found (using Claude) - Cleans up temporary analysis files</p> <p>Source: <code>workflows/book-docs-drift.yml:62-82</code></p> <p>Variable context (from <code>src/cook/execution/mapreduce/aggregation/mod.rs:1637-1659</code>): <pre><code>// Reduce phase has access to map results\ncontext.set(\"map.successful\", summary.successful);\ncontext.set(\"map.failed\", summary.failed);\ncontext.set(\"map.total\", summary.total);\ncontext.set(\"map.results\", results_value);  // Full agent results\n</code></pre></p>"},{"location":"automated-documentation/understanding-the-workflow/#4-merge-phase-integration","title":"4. Merge Phase (Integration)","text":"<p>Purpose: Integrate validated changes back to your original branch.</p> <p>What it does: - Prompts user for confirmation to merge - Merges parent worktree changes to the branch you started from - Uses Claude to resolve any merge conflicts - Preserves your working tree state throughout</p> <p>Source: <code>workflows/book-docs-drift.yml:93-100</code></p> <p>Note: All workflow phases execute in isolated git worktrees using a parent/child architecture. A single parent worktree hosts setup, reduce, and merge phases, while each map agent runs in a child worktree branched from the parent. Agents automatically merge back to the parent upon completion. The parent is merged to the original branch only with user confirmation at the end. This isolation ensures the main repository remains untouched during execution (Spec 127).</p>"},{"location":"automated-documentation/understanding-the-workflow/#worktree-architecture","title":"Worktree Architecture","text":"<p>The workflow uses a sophisticated parent/child worktree hierarchy to achieve complete isolation:</p> <pre><code>Your Original Branch (e.g., main, develop, feature/docs)\n    \u2193\nParent Worktree (session-abc123)\n    \u251c\u2500 Setup Phase runs here\n    \u251c\u2500 Setup generates work items\n    \u2502\n    \u251c\u2500 Each map agent gets child worktree\n    \u2502  \u251c\u2500 Agent-1 Worktree \u2192 processes item-1 \u2192 commits\n    \u2502  \u251c\u2500 Agent-2 Worktree \u2192 processes item-2 \u2192 commits\n    \u2502  \u2514\u2500 Agent-N Worktree \u2192 processes item-N \u2192 commits\n    \u2502\n    \u251c\u2500 Agent changes merge back to parent (serially via MergeQueue)\n    \u251c\u2500 Reduce Phase runs here with aggregated results\n    \u2514\u2500 User confirms \u2192 merge parent to original branch\n</code></pre> <p>Source: <code>src/worktree/manager.rs</code>, <code>src/cook/execution/mapreduce/resources/worktree.rs</code></p>"},{"location":"automated-documentation/understanding-the-workflow/#isolation-guarantees","title":"Isolation Guarantees","text":"<ol> <li>Setup Phase Isolation:</li> <li>Executes in parent worktree</li> <li>All file modifications occur in worktree, not main repo</li> <li> <p>Git commits created in worktree context</p> </li> <li> <p>Map Phase Isolation:</p> </li> <li>Each agent runs in its own child worktree branched from parent</li> <li>No cross-contamination between agents</li> <li>Independent failure isolation (agent failures don't affect siblings)</li> <li> <p>Automatic merge back to parent worktree after success</p> </li> <li> <p>Reduce Phase Isolation:</p> </li> <li>Executes in parent worktree with aggregated agent results</li> <li> <p>Continues isolation guarantee from setup</p> </li> <li> <p>Final Merge:</p> </li> <li>User confirmation required before merging to original branch</li> <li>Main repository never modified until user approves</li> <li>Custom merge workflows supported (Spec 117)</li> </ol> <p>Branch tracking (Spec 110): The parent worktree tracks whatever branch you were on when you started the workflow. This is stored as <code>original_branch</code> in the worktree state, ensuring the final merge targets the correct branch.</p>"},{"location":"automated-documentation/understanding-the-workflow/#execution-model","title":"Execution Model","text":"<p>The MapReduce coordinator orchestrates parallel execution with precise control over resources and failures.</p> <p>Source: <code>src/cook/execution/mapreduce/coordination/executor.rs:200-269</code></p>"},{"location":"automated-documentation/understanding-the-workflow/#work-item-distribution","title":"Work Item Distribution","text":"<p>Work items are loaded from the setup phase output and distributed to agents:</p> <pre><code>pub async fn execute_map_phase_internal(\n    &amp;self,\n    map_phase: MapPhase,\n    work_items: Vec&lt;Value&gt;,\n    env: &amp;ExecutionEnvironment,\n) -&gt; MapReduceResult&lt;Vec&lt;AgentResult&gt;&gt; {\n    let total_items = work_items.len();\n    let max_parallel = map_phase.config.max_parallel.min(total_items);\n\n    // Create semaphore to control concurrency\n    let semaphore = Arc::new(Semaphore::new(max_parallel));\n\n    // Process items in parallel...\n}\n</code></pre> <p>Key features: - Semaphore-based concurrency control prevents resource exhaustion - Tokio async tasks enable efficient parallel execution - Automatic retry count tracking for failed items - DLQ integration for failure recovery</p>"},{"location":"automated-documentation/understanding-the-workflow/#agent-lifecycle","title":"Agent Lifecycle","text":"<p>Each agent follows a strict lifecycle managed by the <code>AgentLifecycleManager</code>:</p> <p>Source: <code>src/cook/execution/mapreduce/agent/lifecycle.rs</code></p> <pre><code>#[async_trait]\npub trait AgentLifecycleManager: Send + Sync {\n    async fn create_agent(&amp;self, config: AgentConfig, commands: Vec&lt;WorkflowStep&gt;)\n        -&gt; LifecycleResult&lt;AgentHandle&gt;;\n\n    async fn cleanup_agent(&amp;self, handle: AgentHandle)\n        -&gt; LifecycleResult&lt;()&gt;;\n\n    async fn merge_agent_to_parent(&amp;self, agent_branch: &amp;str, env: &amp;ExecutionEnvironment)\n        -&gt; LifecycleResult&lt;()&gt;;\n}\n</code></pre> <p>Lifecycle stages: 1. Create: Agent worktree created from parent worktree 2. Execute: Commands run in agent worktree with variable interpolation 3. Merge: Successful agents merge to parent via <code>MergeQueue</code> (serial) 4. Cleanup: Worktree removed (failures tracked in orphaned registry, Spec 136)</p>"},{"location":"automated-documentation/understanding-the-workflow/#result-collection-and-aggregation","title":"Result Collection and Aggregation","text":"<p>Agent results are collected and aggregated for the reduce phase:</p> <p>Source: <code>src/cook/execution/mapreduce/agent/results.rs:44-79</code></p> <pre><code>pub struct AgentResult {\n    pub item_id: String,\n    pub status: AgentStatus,\n    pub output: Option&lt;String&gt;,\n    pub commits: Vec&lt;String&gt;,\n    pub files_modified: Vec&lt;String&gt;,\n    pub duration: Duration,\n    pub error: Option&lt;String&gt;,\n    pub worktree_path: Option&lt;PathBuf&gt;,\n    pub branch_name: Option&lt;String&gt;,\n    pub json_log_location: Option&lt;String&gt;,\n    pub cleanup_status: Option&lt;CleanupStatus&gt;,\n}\n</code></pre> <p>Aggregation summary (<code>src/cook/execution/mapreduce/aggregation/mod.rs</code>): <pre><code>pub struct AggregationSummary {\n    pub successful: usize,\n    pub failed: usize,\n    pub total: usize,\n    pub avg_duration_secs: f64,\n    pub total_duration_secs: f64,\n}\n</code></pre></p>"},{"location":"automated-documentation/understanding-the-workflow/#error-handling-and-recovery","title":"Error Handling and Recovery","text":"<p>Dead Letter Queue (DLQ): Failed work items are automatically added to the DLQ for later retry.</p> <p>Orphaned Worktree Tracking (Spec 136): If agent cleanup fails, the worktree path is registered as orphaned. Use <code>prodigy worktree clean-orphaned &lt;job_id&gt;</code> to clean up after resolving issues.</p> <p>On-Failure Handlers: Workflows can define custom error recovery commands that execute when agents fail.</p> <p>Source: <code>src/cook/execution/mapreduce/coordination/executor.rs:1486-1591</code></p>"},{"location":"automated-documentation/understanding-the-workflow/#quality-guarantees","title":"Quality Guarantees","text":"<p>The workflow ensures documentation quality through multiple validation mechanisms:</p>"},{"location":"automated-documentation/understanding-the-workflow/#code-grounded-examples","title":"Code-Grounded Examples","text":"<p>All examples are extracted from actual implementation: - Type definitions verified from source files - Field names match struct/class definitions exactly - Enum variants validated against source code - CLI syntax verified from argument parser definitions - Examples include source file references (e.g., <code>src/config/retry.rs:45</code>)</p> <p>Source: Documented in parent chapter at <code>index.md:126-134</code></p>"},{"location":"automated-documentation/understanding-the-workflow/#validation-checkpoints","title":"Validation Checkpoints","text":"<p>Each phase includes validation: - Setup: Verifies feature inventory is complete - Map: Each agent validates fixes meet minimum content requirements - Reduce: Full book build ensures no broken references - Merge: User confirmation before final integration</p>"},{"location":"automated-documentation/understanding-the-workflow/#build-verification","title":"Build Verification","text":"<p>The reduce phase rebuilds the entire book: <pre><code>reduce:\n  - shell: \"mdbook build book\"\n  - claude: \"/fix-build-errors\"\n</code></pre></p> <p>This catches: - Broken links between chapters - Missing cross-references - Invalid markdown syntax - Compilation errors</p>"},{"location":"automated-documentation/understanding-the-workflow/#automatic-retry","title":"Automatic Retry","text":"<p>Failed items can be retried with preserved context: <pre><code># Retry all failed items for a job\nprodigy dlq retry &lt;job_id&gt;\n\n# Retry with custom parallelism\nprodigy dlq retry &lt;job_id&gt; --max-parallel 5\n</code></pre></p> <p>Source: DLQ retry implementation in <code>src/cook/execution/mapreduce/dlq/</code></p>"},{"location":"automated-documentation/understanding-the-workflow/#variable-interpolation","title":"Variable Interpolation","text":"<p>The workflow uses variable interpolation to pass data between phases:</p>"},{"location":"automated-documentation/understanding-the-workflow/#setup-phase-variables","title":"Setup Phase Variables","text":"<ul> <li>Output captured to files (e.g., <code>doc-items.json</code>)</li> <li>Variables available to map phase</li> </ul>"},{"location":"automated-documentation/understanding-the-workflow/#map-phase-variables","title":"Map Phase Variables","text":"<ul> <li><code>${item.field}</code> - Access work item fields</li> <li><code>${item.id}</code> - Work item identifier</li> <li><code>${item.file}</code> - Documentation file path</li> <li>Environment variables from workflow <code>env:</code> block</li> </ul> <p>Source: <code>src/cook/execution/mapreduce/coordination/executor.rs:1132-1158</code></p>"},{"location":"automated-documentation/understanding-the-workflow/#reduce-phase-variables","title":"Reduce Phase Variables","text":"<ul> <li><code>${map.successful}</code> - Count of successful agents</li> <li><code>${map.failed}</code> - Count of failed agents</li> <li><code>${map.total}</code> - Total work items</li> <li><code>${map.results}</code> - Full agent results as JSON</li> </ul> <p>Source: <code>src/cook/execution/mapreduce/aggregation/mod.rs:1637-1659</code></p>"},{"location":"automated-documentation/understanding-the-workflow/#performance-characteristics","title":"Performance Characteristics","text":"<p>Parallel execution: The map phase processes multiple items concurrently: - Default: 3 concurrent agents - Configurable via <code>max_parallel</code> in workflow YAML - Optimal parallelism calculated based on work item count</p> <p>Resource management: - Semaphore prevents resource exhaustion - Each agent isolated in separate worktree - Serial merge queue prevents conflicts</p> <p>Scalability: Successfully processes large documentation sets (tested with 50+ chapters).</p>"},{"location":"automated-documentation/understanding-the-workflow/#see-also","title":"See Also","text":"<ul> <li>Tutorial - Complete 30-minute walkthrough</li> <li>Installation - Set up Prodigy and prerequisites</li> <li>Automatic Gap Detection - How the workflow finds missing documentation</li> <li>Advanced Configuration - Customize workflow behavior</li> <li>Troubleshooting - Common issues and solutions</li> <li>MapReduce Workflows - General MapReduce workflows (not documentation-specific)</li> </ul>"},{"location":"composition/","title":"Workflow Composition","text":"<p>Prodigy provides powerful composition features that enable building complex workflows from reusable components. This chapter covers importing workflows, using templates, defining parameters, and composing workflows through inheritance.</p> <p>\u26a0\ufe0f Implementation Status</p> <p>Workflow composition is currently in phased implementation. The core composition engine and template system are fully implemented and tested, but integration with workflow execution varies by feature:</p> <p>\u2705 What Works Today: - Template management via <code>prodigy template</code> CLI commands (register, list, show, delete, etc.) - Programmatic workflow composition using <code>WorkflowComposer</code> API - Parameter validation with type checking - Template registry storage and retrieval (<code>~/.prodigy/templates/</code>)</p> <p>\u23f3 Limited Integration: - Using imports, extends, and templates in <code>prodigy run</code> workflows (detection works, execution integration is limited) - Composable workflow detection and parsing (functional but not extensively tested end-to-end)</p> <p>\u274c Not Yet Implemented: - Sub-workflow execution (types defined, executor is placeholder) - MapReduce workflow composition - <code>prodigy compose</code> command</p> <p>See the Implementation Roadmap section below for details.</p>"},{"location":"composition/#overview","title":"Overview","text":"<p>Workflow composition allows you to: - Import shared workflow configurations from other files - Extend base workflows to inherit common configurations - Use templates from a registry for standardized patterns - Define parameters with type validation for flexible workflows - Execute sub-workflows in parallel or sequentially (planned) - Set defaults for common parameter values</p> <p>These features promote code reuse, maintainability, and consistency across your automation workflows.</p>"},{"location":"composition/#when-to-use-composition","title":"When to Use Composition","text":"<p>Composition features are most valuable when:</p> <ol> <li>Multiple projects share common workflow patterns - Standardize CI/CD, deployment, or testing workflows across teams</li> <li>Workflows need environment-specific parameterization - Same workflow logic with different configurations for dev/staging/prod</li> <li>Building a library of reusable components - Create organizational workflow templates for consistent practices</li> </ol> <p>For simple, project-specific workflows, direct YAML without composition is often clearer and easier to maintain.</p>"},{"location":"composition/#workflow-imports","title":"Workflow Imports","text":"<p>Import external workflow files to reuse configurations and share common patterns across multiple workflows. Imports allow you to reference workflows from other files and incorporate them into your current workflow with optional aliasing and selective field imports.</p> <p>\ud83d\udcdd Note on Usage</p> <p>The examples below show composition syntax in workflow YAML files. While the core composition logic is fully implemented, integration with <code>prodigy run</code> is limited. For production use today, the recommended approach is using the Template System via <code>prodigy template</code> commands.</p> <p>The syntax shown is validated and supported by the composition engine but may have limited end-to-end testing in workflow execution. See Implementation Roadmap for current status.</p>"},{"location":"composition/#basic-import-syntax","title":"Basic Import Syntax","text":"<pre><code>name: my-workflow\nmode: standard\n\nimports:\n  # Simple import - loads entire workflow\n  - path: \"workflows/common-setup.yml\"\n\n  # Import with alias for namespacing\n  - path: \"workflows/deployment.yml\"\n    alias: \"prod-deploy\"\n\n  # Selective import - only import specific workflows\n  - path: \"workflows/utilities.yml\"\n    selective:\n      - \"test-runner\"\n      - \"linter\"\n</code></pre>"},{"location":"composition/#import-fields","title":"Import Fields","text":"<p>Each import can specify (defined in <code>WorkflowImport</code> struct, src/cook/workflow/composition/mod.rs:52-65): - path (required): Relative or absolute path to workflow file - alias (optional): Namespace alias for imported workflows - selective (optional): List of specific workflow names to import</p> <p>Source: <code>WorkflowImport</code> struct in src/cook/workflow/composition/mod.rs:52-65 Test example: tests/workflow_composition_test.rs:95-106 shows import usage with both alias and selective fields</p>"},{"location":"composition/#how-imports-work","title":"How Imports Work","text":"<p>When a workflow is imported: 1. The external file is loaded and parsed 2. If an alias is specified, imported content is namespaced under that alias 3. If selective is specified, only named workflows are included 4. Imported workflows are merged into the current workflow's configuration 5. Circular dependencies are detected and prevented</p> <p>Implementation: Import processing in src/cook/workflow/composition/composer.rs:98-133 (<code>process_imports</code> function) Circular dependency detection: src/cook/workflow/composition/composer.rs:56 and validation in <code>validate_composition</code> (lines 259-273)</p>"},{"location":"composition/#use-cases","title":"Use Cases","text":"<p>Shared Setup Steps: <pre><code># common-setup.yml\nsetup:\n  - shell: \"npm install\"\n  - shell: \"cargo build\"\n\n# main-workflow.yml\nimports:\n  - path: \"common-setup.yml\"\n\nname: integration-tests\nmode: standard\n# Inherits setup steps from common-setup.yml\n</code></pre></p> <p>Namespace Isolation: <pre><code>imports:\n  - path: \"prod-workflows.yml\"\n    alias: \"production\"\n  - path: \"staging-workflows.yml\"\n    alias: \"staging\"\n\n# Reference as ${production.deploy} vs ${staging.deploy}\n</code></pre></p> <p>Selective Imports: <pre><code># Only import specific utilities, not entire file\nimports:\n  - path: \"workflows/all-utilities.yml\"\n    selective:\n      - \"lint\"\n      - \"format\"\n      - \"test\"\n</code></pre></p> <p>For more advanced composition patterns, see the Template System and Workflow Extension sections.</p>"},{"location":"composition/#template-system-cli","title":"Template System CLI","text":"<p>While full workflow composition integration is in progress, the template management system is fully functional and ready for production use. Templates provide a practical way to reuse workflow patterns today.</p>"},{"location":"composition/#managing-templates","title":"Managing Templates","text":"<p>The <code>prodigy template</code> commands provide complete template lifecycle management:</p> <p>Register a template: <pre><code>prodigy template register workflow.yml --name my-template \\\n  --description \"CI pipeline for Rust projects\" \\\n  --version 1.0.0 \\\n  --tags rust,ci,testing \\\n  --author \"team@example.com\"\n</code></pre></p> <p>List available templates: <pre><code># List all templates\nprodigy template list\n\n# Long format with details\nprodigy template list --long\n\n# Filter by tag\nprodigy template list --tag rust\n</code></pre></p> <p>Show template details: <pre><code>prodigy template show my-template\n</code></pre></p> <p>Search templates: <pre><code>prodigy template search \"rust ci\"\n</code></pre></p> <p>Delete a template: <pre><code>prodigy template delete my-template\n</code></pre></p> <p>Validate template syntax: <pre><code>prodigy template validate workflow.yml\n</code></pre></p> <p>Initialize from template: <pre><code>prodigy template init my-template --output new-workflow.yml\n</code></pre></p>"},{"location":"composition/#template-storage","title":"Template Storage","text":"<p>Templates are stored in <code>~/.prodigy/templates/</code> with the following structure:</p> <pre><code>~/.prodigy/templates/\n\u251c\u2500\u2500 my-template.yml\n\u251c\u2500\u2500 ci-pipeline.yml\n\u251c\u2500\u2500 deployment.yml\n\u2514\u2500\u2500 metadata/\n    \u251c\u2500\u2500 my-template.json\n    \u251c\u2500\u2500 ci-pipeline.json\n    \u2514\u2500\u2500 deployment.json\n</code></pre> <p>Implementation: See Template System section for detailed template syntax and usage patterns.</p> <p>Source: Template CLI implementation in <code>src/cli/template.rs</code> (388 lines), wired in <code>src/cli/router.rs:199-234</code></p>"},{"location":"composition/#implementation-roadmap","title":"Implementation Roadmap","text":"<p>This section clarifies what's implemented, what's in progress, and what's planned for workflow composition features.</p>"},{"location":"composition/#phase-1-core-composition-engine-complete","title":"Phase 1: Core Composition Engine (\u2705 Complete)","text":"<p>The foundational composition system is fully implemented and tested:</p> <p>Core Types and Logic: - <code>WorkflowComposer</code> - Main composition orchestration (<code>src/cook/workflow/composition/composer.rs</code>, 986 lines) - <code>TemplateRegistry</code> - Template storage and retrieval (<code>src/cook/workflow/composition/registry.rs</code>, 779 lines) - <code>ComposableWorkflow</code> - Type system with validation (<code>src/cook/workflow/composition/mod.rs</code>, 334 lines) - Parameter validation with type checking - Circular dependency detection - Template parameter interpolation</p> <p>Quality Metrics: - 2,300+ lines of core composition code - 100+ unit tests - Zero <code>unwrap()</code> calls in production code - Full async/await support with tokio - Comprehensive error handling with <code>Result&lt;T&gt;</code></p> <p>Test Coverage: - <code>tests/workflow_composition_test.rs</code> - Integration tests with real workflows - Unit tests in each composition module - Parameter validation edge cases - Import circular dependency scenarios</p>"},{"location":"composition/#phase-2-cli-and-template-management-complete","title":"Phase 2: CLI and Template Management (\u2705 Complete)","text":"<p>Template management commands are fully functional and production-ready:</p> <p>Template CLI Commands (<code>src/cli/template.rs</code>, 388 lines): - \u2705 <code>prodigy template register</code> - Register templates with metadata - \u2705 <code>prodigy template list</code> - List templates with filtering - \u2705 <code>prodigy template show</code> - Display template details - \u2705 <code>prodigy template delete</code> - Remove templates - \u2705 <code>prodigy template search</code> - Search by name/tags - \u2705 <code>prodigy template validate</code> - Validate template syntax - \u2705 <code>prodigy template init</code> - Initialize from template</p> <p>Template Storage: - File-based storage in <code>~/.prodigy/templates/</code> - Metadata tracking (version, author, tags, timestamps) - Template caching for performance</p> <p>CLI Integration: - Commands wired in <code>src/cli/router.rs:199-234</code> - Argument parsing in <code>src/cli/args.rs:333-907</code> - Proper error handling and user feedback</p>"},{"location":"composition/#phase-3-workflow-execution-integration-partial","title":"Phase 3: Workflow Execution Integration (\u23f3 Partial)","text":"<p>Integration with workflow execution has limited implementation:</p> <p>What's Implemented: - Composable workflow detection (<code>src/cook/workflow/composer_integration.rs:43-90</code>) - Workflow parsing and conversion to <code>WorkflowConfig</code> - Integration point in workflow loading (<code>src/cook/mod.rs:438-442</code>) - Parameter passing via <code>--param</code> and <code>--param-file</code> flags</p> <p>What's Limited: - End-to-end testing of composition in <code>prodigy run</code> workflows - MapReduce workflow composition (not implemented) - Sub-workflow execution (executor is placeholder, <code>src/cook/workflow/composition/sub_workflow.rs:233-239</code>)</p> <p>Detection Logic: <pre><code>// From src/cook/workflow/composer_integration.rs\npub fn is_composable_workflow(yaml: &amp;str) -&gt; bool {\n    // Detects: imports, template, extends, parameters\n    yaml.contains(\"imports:\")\n        || yaml.contains(\"template:\")\n        || yaml.contains(\"extends:\")\n        || yaml.contains(\"parameters:\")\n}\n</code></pre></p> <p>Integration Point: <pre><code>// From src/cook/mod.rs:438-442\nif composer_integration::is_composable_workflow(&amp;content) {\n    let composable = composer_integration::parse_composable_workflow(&amp;content)?;\n    return Ok(composable.into());  // Converts to WorkflowConfig\n}\n</code></pre></p>"},{"location":"composition/#phase-4-advanced-features-not-implemented","title":"Phase 4: Advanced Features (\u274c Not Implemented)","text":"<p>Features planned but not yet started:</p> <ul> <li>Sub-Workflow Execution: Types defined, executor needs implementation</li> <li>MapReduce Composition: Composition in MapReduce agent templates</li> <li><code>prodigy compose</code> Command: Dedicated composition command for testing</li> <li>URL-based Templates: Load templates from remote URLs</li> <li>Template Inheritance: Templates extending other templates</li> <li>Template Override Application: Override fields during composition (structure exists, application logic TODO)</li> </ul>"},{"location":"composition/#current-recommendations","title":"Current Recommendations","text":"<p>For Production Use Today:</p> <ol> <li>Use <code>prodigy template</code> commands for managing reusable workflows</li> <li>Register templates in <code>~/.prodigy/templates/</code> for your team</li> <li>Use template parameters for environment-specific configuration</li> <li>Keep workflows simple unless you need heavy parameterization</li> </ol> <p>For Experimentation:</p> <ol> <li>Try composable workflow syntax in YAML files - detection and parsing work</li> <li>Report issues if composition doesn't work as expected</li> <li>Contribute tests for end-to-end composition scenarios</li> <li>Review Specs 131-133 for implementation progress tracking</li> </ol> <p>What to Avoid:</p> <ol> <li>\u274c Don't rely on sub-workflow execution (not implemented)</li> <li>\u274c Don't use composition in MapReduce workflows (not supported)</li> <li>\u274c Don't expect URL-based template loading (returns error)</li> <li>\u274c Don't assume template override fields are applied (TODO)</li> </ol>"},{"location":"composition/#contributing","title":"Contributing","text":"<p>The composition system has excellent code quality and test coverage, making it approachable for contributions:</p> <p>Good First Issues: - Implement sub-workflow executor (placeholder exists at <code>src/cook/workflow/composition/sub_workflow.rs:233-239</code>) - Add end-to-end integration tests for composition in workflows - Implement template override application (<code>apply_overrides</code> function) - Add support for URL-based template loading</p> <p>Code Quality Standards: - No <code>unwrap()</code> in production code (use <code>?</code> operator with <code>Result</code>) - Comprehensive error messages with context - Unit tests for all new functionality - Integration tests for user-facing features</p> <p>Source References: - Specs: Look for Spec 131-133 in project documentation - Core Implementation: <code>src/cook/workflow/composition/</code> - CLI Integration: <code>src/cli/template.rs</code>, <code>src/cli/router.rs</code> - Tests: <code>tests/workflow_composition_test.rs</code></p>"},{"location":"composition/#additional-topics","title":"Additional Topics","text":"<p>See also: - Workflow Extension (Inheritance) - Template System - Parameter Definitions - Default Values - Sub-Workflows - Composition Metadata - Complete Examples - Troubleshooting</p>"},{"location":"composition/complete-examples/","title":"Complete Examples","text":""},{"location":"composition/complete-examples/#complete-examples","title":"Complete Examples","text":"<p>This section provides end-to-end examples demonstrating multiple composition features working together.</p>"},{"location":"composition/complete-examples/#example-1-multi-environment-cicd-pipeline","title":"Example 1: Multi-Environment CI/CD Pipeline","text":"<p>This example uses templates, parameters, and inheritance for environment-specific deployments.</p> <p>base-ci-template.yml (template in registry): <pre><code>name: ci-pipeline-template\nmode: standard\n\nparameters:\n  required:\n    - name: environment\n      type: string\n      description: \"Deployment environment\"\n      validation: \"matches('^(dev|staging|prod)$')\"\n\n  optional:\n    - name: replicas\n      type: number\n      description: \"Number of service replicas\"\n      default: 1\n\n    - name: run_tests\n      type: boolean\n      description: \"Whether to run test suite\"\n      default: true\n\ndefaults:\n  timeout: 600\n  log_level: \"info\"\n\ncommands:\n  - shell: \"echo Deploying to ${environment} with ${replicas} replicas\"\n  - shell: \"cargo build --release\"\n  - shell: |\n      if [ \"${run_tests}\" = \"true\" ]; then\n        cargo test --release\n      fi\n  - shell: \"kubectl apply -f k8s/${environment}/deployment.yml\"\n  - shell: \"kubectl scale deployment app --replicas=${replicas}\"\n</code></pre></p> <p>Source: Template and parameter structure from src/cook/workflow/composition/mod.rs:75-129</p> <p>dev-deployment.yml: <pre><code>name: dev-deployment\nmode: standard\n\ntemplate:\n  source: \"ci-pipeline-template\"  # Registry name\n  with:\n    environment: \"dev\"\n    replicas: 1\n    run_tests: false  # Skip tests in dev for speed\n</code></pre></p> <p>Source: TemplateSource is an untagged enum accepting string values (src/cook/workflow/composition/mod.rs:86-95)</p> <p>prod-deployment.yml: <pre><code>name: prod-deployment\nmode: standard\n\ntemplate:\n  source: \"ci-pipeline-template\"  # Registry name\n  with:\n    environment: \"prod\"\n    replicas: 5\n    run_tests: true  # Always test before prod\n\n# Add production-specific safeguards\ncommands:\n  - shell: \"verify-release-notes.sh\"\n  - shell: \"notify-team 'Production deployment starting'\"\n</code></pre></p>"},{"location":"composition/complete-examples/#example-2-modular-monorepo-testing","title":"Example 2: Modular Monorepo Testing","text":"<p>Uses sub-workflows and imports for testing multiple services in parallel.</p> <p>shared/common-setup.yml: <pre><code>name: common-setup\n\ncommands:\n  - shell: \"git fetch origin\"\n  - shell: \"npm install\"\n  - shell: \"cargo build\"\n</code></pre></p> <p>monorepo-test.yml: <pre><code>name: monorepo-test\nmode: standard\n\nimports:\n  - path: \"shared/common-setup.yml\"\n\nworkflows:\n  # Test all services in parallel using workflow map (not array)\n  api-tests:\n    source: \"services/api/test.yml\"\n    working_dir: \"./services/api\"\n    parallel: true\n    outputs:\n      - \"coverage\"\n      - \"test_count\"\n\n  worker-tests:\n    source: \"services/worker/test.yml\"\n    working_dir: \"./services/worker\"\n    parallel: true\n    outputs:\n      - \"coverage\"\n      - \"test_count\"\n\n  frontend-tests:\n    source: \"apps/frontend/test.yml\"\n    working_dir: \"./apps/frontend\"\n    parallel: true\n    outputs:\n      - \"coverage\"\n      - \"test_count\"\n\ncommands:\n  - shell: |\n      echo \"Test Results:\"\n      echo \"  API: ${api-tests.test_count} tests, ${api-tests.coverage}% coverage\"\n      echo \"  Worker: ${worker-tests.test_count} tests, ${worker-tests.coverage}% coverage\"\n      echo \"  Frontend: ${frontend-tests.test_count} tests, ${frontend-tests.coverage}% coverage\"\n  - shell: \"generate-combined-report.sh\"\n</code></pre></p> <p>Source: Sub-workflows use HashMap, not array (src/cook/workflow/composition/mod.rs:49 and src/cook/workflow/composition/sub_workflow.rs:14-45)</p>"},{"location":"composition/complete-examples/#example-3-layered-configuration-with-extends","title":"Example 3: Layered Configuration with Extends","text":"<p>Uses inheritance to create environment-specific variations of a base workflow.</p> <p>base-config.yml: <pre><code>name: base-config\nmode: standard\n\ndefaults:\n  log_level: \"info\"\n  timeout: 300\n\nenv:\n  APP_NAME: \"my-service\"\n  DATABASE_POOL_SIZE: \"10\"\n\ncommands:\n  - shell: \"cargo fmt --check\"\n  - shell: \"cargo clippy\"\n  - shell: \"cargo test\"\n  - shell: \"cargo build\"\n</code></pre></p> <p>staging-config.yml: <pre><code>name: staging-config\nextends: \"base-config.yml\"\n\ndefaults:\n  log_level: \"debug\"\n  timeout: 600\n\nenv:\n  DATABASE_POOL_SIZE: \"20\"\n  ENABLE_DEBUG_ENDPOINTS: \"true\"\n\n# Inherits all commands from base, adds staging-specific\ncommands:\n  - shell: \"run-integration-tests.sh\"\n  - shell: \"deploy-to-staging.sh\"\n</code></pre></p> <p>production-config.yml: <pre><code>name: production-config\nextends: \"base-config.yml\"\n\ndefaults:\n  log_level: \"warn\"\n  timeout: 900\n\nenv:\n  DATABASE_POOL_SIZE: \"50\"\n  ENABLE_MONITORING: \"true\"\n  RATE_LIMIT_ENABLED: \"true\"\n\ncommands:\n  - shell: \"verify-security-scan.sh\"\n  - shell: \"cargo build --release\"\n  - shell: \"run-smoke-tests.sh\"\n  - shell: \"deploy-to-production.sh\"\n  - shell: \"notify-deployment-complete.sh\"\n</code></pre></p>"},{"location":"composition/complete-examples/#example-4-complex-composition-with-multiple-features","title":"Example 4: Complex Composition with Multiple Features","text":"<p>Combines imports, extends, template, parameters, and sub-workflows.</p> <p>templates/microservice-ci.yml: <pre><code>name: microservice-ci-template\nmode: standard\n\nparameters:\n  required:\n    - name: service_name\n      type: string\n      description: \"Name of the microservice\"\n\n    - name: language\n      type: string\n      description: \"Programming language\"\n      validation: \"matches('^(rust|typescript|python)$')\"\n\n  optional:\n    - name: test_timeout\n      type: number\n      description: \"Test timeout in seconds\"\n      default: 300\n\ndefaults:\n  coverage_threshold: \"80\"\n\nworkflows:\n  lint:\n    source: \"workflows/${language}/lint.yml\"\n    working_dir: \"./services/${service_name}\"\n\n  test:\n    source: \"workflows/${language}/test.yml\"\n    working_dir: \"./services/${service_name}\"\n    timeout: \"${test_timeout}\"\n    outputs:\n      - \"coverage\"\n\ncommands:\n  - shell: \"echo Testing ${service_name} (${language})\"\n  - shell: |\n      if [ \"${test.coverage}\" -lt \"${coverage_threshold}\" ]; then\n        echo \"Coverage ${test.coverage}% below threshold ${coverage_threshold}%\"\n        exit 1\n      fi\n</code></pre></p> <p>service-api-ci.yml (uses the template): <pre><code>name: api-service-ci\nmode: standard\n\nimports:\n  - path: \"shared/docker-utils.yml\"\n\ntemplate:\n  source: \"templates/microservice-ci.yml\"  # File path\n  with:\n    service_name: \"api\"\n    language: \"rust\"\n    test_timeout: 600\n\ncommands:\n  - shell: \"docker build -t api:latest ./services/api\"\n  - shell: \"docker push api:latest\"\n</code></pre></p>"},{"location":"composition/complete-examples/#example-5-workflow-registry-pattern","title":"Example 5: Workflow Registry Pattern","text":"<p>Demonstrates using a template registry for standardized workflows across teams.</p> <p>Setup Registry:</p> <p>Templates can be stored in two locations: - Global: <code>~/.prodigy/templates/</code> (available to all projects) - Local: <code>.prodigy/templates/</code> (project-specific)</p> <pre><code># Global templates (one-time setup)\nmkdir -p ~/.prodigy/templates\ncp standard-ci.yml ~/.prodigy/templates/\ncp security-scan.yml ~/.prodigy/templates/\n\n# Or use project-local templates\nmkdir -p .prodigy/templates\ncp deployment.yml .prodigy/templates/\n</code></pre> <p>Source: TemplateRegistry and FileTemplateStorage (src/cook/workflow/composition/registry.rs)</p> <p>team-workflow.yml: <pre><code>name: team-workflow\nmode: standard\n\n# Use registry template (string value, not nested object)\ntemplate:\n  source: \"standard-ci\"  # Registry name\n  with:\n    project_type: \"rust\"\n\n# Import additional registry workflows\nimports:\n  - path: \"~/.prodigy/templates/security-scan.yml\"\n\n# Extend with team-specific configuration\ncommands:\n  - shell: \"run-team-specific-tests.sh\"\n</code></pre></p>"},{"location":"composition/complete-examples/#example-6-progressive-composition","title":"Example 6: Progressive Composition","text":"<p>Builds complexity through layers of composition.</p> <p>Layer 1 - Base (minimal.yml): <pre><code>name: minimal\nmode: standard\ndefaults:\n  timeout: 300\ncommands:\n  - shell: \"cargo build\"\n</code></pre></p> <p>Layer 2 - Add Testing (with-tests.yml): <pre><code>name: with-tests\nmode: standard\nextends: \"minimal.yml\"\ncommands:\n  - shell: \"cargo test\"\n</code></pre></p> <p>Layer 3 - Add Linting (with-quality.yml): <pre><code>name: with-quality\nmode: standard\nextends: \"with-tests.yml\"\ncommands:\n  - shell: \"cargo fmt --check\"\n  - shell: \"cargo clippy\"\n</code></pre></p> <p>Layer 4 - Full CI (full-ci.yml): <pre><code>name: full-ci\nmode: standard\nextends: \"with-quality.yml\"\n\nparameters:\n  required:\n    - name: deploy_env\n      type: string\n      description: \"Deployment environment\"\n\ncommands:\n  - shell: \"cargo build --release\"\n  - shell: \"deploy-to-${deploy_env}.sh\"\n</code></pre></p>"},{"location":"composition/complete-examples/#running-the-examples","title":"Running the Examples","text":"<pre><code># Example 1: Template-based deployment\nprodigy run dev-deployment.yml\nprodigy run prod-deployment.yml --param replicas=10\n\n# Example 2: Monorepo testing\nprodigy run monorepo-test.yml\n\n# Example 3: Environment-specific configs\nprodigy run staging-config.yml\nprodigy run production-config.yml\n\n# Example 4: Complex composition\nprodigy run service-api-ci.yml\n\n# Example 5: Registry templates\nprodigy run team-workflow.yml\n\n# Example 6: Progressive composition\nprodigy run full-ci.yml --param deploy_env=staging\n</code></pre>"},{"location":"composition/composition-metadata/","title":"Composition Metadata","text":""},{"location":"composition/composition-metadata/#composition-metadata","title":"Composition Metadata","text":"<p>Prodigy tracks metadata about workflow composition for debugging and dependency analysis. This metadata provides visibility into how workflows are composed, what dependencies exist, and when composition occurred.</p> <p>Implementation Status: The composition metadata types and dependency tracking are fully implemented in the core composition system. These features are accessible programmatically via the WorkflowComposer API. CLI integration for viewing composition metadata in workflows is under development (Spec 131-133).</p>"},{"location":"composition/composition-metadata/#compositionmetadata-structure","title":"CompositionMetadata Structure","text":"<p>Every composed workflow includes metadata tracking all composition operations:</p> <p>Source: <code>src/cook/workflow/composition/mod.rs:153-170</code></p> <pre><code>/// Metadata about workflow composition\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CompositionMetadata {\n    /// Source files involved in composition\n    pub sources: Vec&lt;PathBuf&gt;,\n\n    /// Templates used\n    pub templates: Vec&lt;String&gt;,\n\n    /// Parameters applied\n    pub parameters: HashMap&lt;String, Value&gt;,\n\n    /// Composition timestamp\n    pub composed_at: chrono::DateTime&lt;chrono::Utc&gt;,\n\n    /// Dependency graph\n    pub dependencies: Vec&lt;DependencyInfo&gt;,\n}\n</code></pre> <p>Field Details: - <code>sources</code>: File paths of all workflow files involved in composition (as <code>PathBuf</code> objects) - <code>templates</code>: Template names/sources used during composition - <code>parameters</code>: Final parameter values applied to the workflow (as <code>serde_json::Value</code>) - <code>composed_at</code>: ISO 8601 timestamp when composition occurred (UTC timezone) - <code>dependencies</code>: Complete dependency graph with all imports, extends, templates, and sub-workflows</p>"},{"location":"composition/composition-metadata/#dependency-tracking","title":"Dependency Tracking","text":"<p>Each dependency includes detailed information:</p> <p>Source: <code>src/cook/workflow/composition/mod.rs:172-183</code></p> <pre><code>/// Information about workflow dependencies\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct DependencyInfo {\n    /// Source of the dependency\n    pub source: PathBuf,\n\n    /// Type of dependency\n    pub dep_type: DependencyType,\n\n    /// Resolved path or name\n    pub resolved: String,\n}\n</code></pre> <p>Field Details: - <code>source</code>: Source file path of the dependency (as <code>PathBuf</code>) - <code>dep_type</code>: Type of dependency (Import, Extends, Template, or SubWorkflow) - <code>resolved</code>: Resolved file path or template name (as <code>String</code>)</p>"},{"location":"composition/composition-metadata/#dependency-types","title":"Dependency Types","text":"<p>Prodigy tracks four types of dependencies:</p> <p>Source: <code>src/cook/workflow/composition/mod.rs:185-193</code></p> <pre><code>/// Type of workflow dependency\n#[derive(Debug, Clone, Serialize, Deserialize)]\n#[serde(rename_all = \"snake_case\")]\npub enum DependencyType {\n    Import,\n    Extends,\n    Template,\n    SubWorkflow,\n}\n</code></pre> <p>Import Dependencies: <pre><code>imports:\n  - path: \"shared/utilities.yml\"\n\n# Creates DependencyInfo:\n# dep_type: DependencyType::Import\n# source: PathBuf::from(\"shared/utilities.yml\")\n# resolved: \"/full/path/to/shared/utilities.yml\"\n</code></pre></p> <p>Extends Dependencies: <pre><code>extends: \"base-config.yml\"\n\n# Creates DependencyInfo:\n# dep_type: DependencyType::Extends\n# source: PathBuf::from(\"base-config.yml\")\n# resolved: \"/full/path/to/base-config.yml\"\n</code></pre></p> <p>Template Dependencies: <pre><code>template:\n  source:\n    registry: \"ci-pipeline\"\n\n# Creates DependencyInfo:\n# dep_type: DependencyType::Template\n# source: PathBuf::from(\"registry:ci-pipeline\")\n# resolved: \"~/.prodigy/templates/ci-pipeline.yml\"\n</code></pre></p> <p>SubWorkflow Dependencies: <pre><code>sub_workflows:\n  - name: \"tests\"\n    source: \"workflows/test.yml\"\n\n# Creates DependencyInfo:\n# dep_type: DependencyType::SubWorkflow\n# source: PathBuf::from(\"workflows/test.yml\")\n# resolved: \"/full/path/to/workflows/test.yml\"\n</code></pre></p>"},{"location":"composition/composition-metadata/#viewing-composition-metadata","title":"Viewing Composition Metadata","text":"<p>Note: CLI commands for viewing composition metadata in workflow execution are under development. Currently, metadata can be accessed programmatically via the WorkflowComposer API (see Programmatic Access below).</p> <p>Future CLI Usage (planned): <pre><code># Show composition metadata (planned feature)\nprodigy run workflow.yml --dry-run --show-composition\n</code></pre></p> <p>Expected Output: <pre><code>Composition Metadata:\n  Composed at: 2025-01-11T20:00:00Z\n\n  Sources (3):\n    - /path/to/workflow.yml\n    - /path/to/base-config.yml\n    - /path/to/shared/utilities.yml\n\n  Templates (1):\n    - registry:ci-pipeline\n\n  Dependencies (3):\n    [Import] shared/utilities.yml -&gt; /path/to/shared/utilities.yml\n    [Extends] base-config.yml -&gt; /path/to/base-config.yml\n    [Template] registry:ci-pipeline -&gt; ~/.prodigy/templates/ci-pipeline.yml\n\n  Parameters (2):\n    environment: \"production\"\n    timeout: 600\n</code></pre></p>"},{"location":"composition/composition-metadata/#programmatic-access","title":"Programmatic Access","text":"<p>Access metadata in code using the WorkflowComposer API:</p> <p>Source: <code>src/cook/workflow/composition/composer.rs:21-37</code></p> <pre><code>use prodigy::cook::workflow::composition::{WorkflowComposer, TemplateRegistry};\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse std::path::Path;\n\n// Create composer with template registry\nlet registry = Arc::new(TemplateRegistry::new());\nlet composer = WorkflowComposer::new(registry);\n\n// Compose workflow with parameters\nlet params = HashMap::new();\nlet composed = composer.compose(Path::new(\"workflow.yml\"), params).await?;\n\n// Access metadata\nlet metadata = &amp;composed.metadata;\n\n// Inspect dependencies\nfor dep in &amp;metadata.dependencies {\n    println!(\"{:?}: {} -&gt; {}\",\n        dep.dep_type,\n        dep.source.display(),\n        dep.resolved\n    );\n}\n\n// Check composition timestamp\nprintln!(\"Composed at: {}\", metadata.composed_at);\n\n// View final parameters\nfor (name, value) in &amp;metadata.parameters {\n    println!(\"Parameter {}: {:?}\", name, value);\n}\n\n// List source files\nprintln!(\"Sources:\");\nfor source in &amp;metadata.sources {\n    println!(\"  - {}\", source.display());\n}\n\n// List templates used\nprintln!(\"Templates:\");\nfor template in &amp;metadata.templates {\n    println!(\"  - {}\", template);\n}\n</code></pre> <p>Real-World Example (from <code>src/cook/workflow/composition/composer.rs:45-51</code>):</p> <pre><code>// Metadata is created during composition\nlet mut metadata = CompositionMetadata {\n    sources: vec![source.to_path_buf()],\n    templates: Vec::new(),\n    parameters: params.clone(),\n    composed_at: chrono::Utc::now(),\n    dependencies: Vec::new(),\n};\n</code></pre>"},{"location":"composition/composition-metadata/#dependency-graph-visualization","title":"Dependency Graph Visualization","text":"<p>Metadata enables dependency visualization:</p> <pre><code>workflow.yml\n\u251c\u2500 [Extends] base-config.yml\n\u2502  \u2514\u2500 [Import] shared/setup.yml\n\u251c\u2500 [Import] shared/utilities.yml\n\u2514\u2500 [Template] registry:ci-pipeline\n   \u2514\u2500 [SubWorkflow] workflows/test.yml\n</code></pre>"},{"location":"composition/composition-metadata/#use-cases","title":"Use Cases","text":"<p>Debugging Composition Issues: - Verify which files were loaded - Check parameter resolution order - Identify circular dependencies - Trace inheritance chains</p> <p>Dependency Analysis: - Find all workflows using a template - Identify shared imports - Map workflow relationships - Audit composition complexity</p> <p>Change Impact Assessment: <pre><code># Before changing base-config.yml, find all dependents\ngrep -r \"extends.*base-config\" workflows/\n\n# View composition metadata programmatically\n# (CLI integration for --show-composition is under development)\n</code></pre></p> <p>Compliance and Auditing: - Track template versions used - Record composition timestamps - Document parameter sources - Verify configuration origins</p>"},{"location":"composition/composition-metadata/#metadata-in-composed-workflows","title":"Metadata in Composed Workflows","text":"<p>Composed workflows carry metadata through the composition process:</p> <p>Source: <code>src/cook/workflow/composition/mod.rs:143-151</code></p> <pre><code>// ComposedWorkflow structure\npub struct ComposedWorkflow {\n    /// The composed workflow\n    pub workflow: ComposableWorkflow,\n\n    /// Metadata about the composition\n    pub metadata: CompositionMetadata,\n}\n\n// Access metadata from composed workflow\nlet composed = composer.compose(source, params).await?;\nprintln!(\"This workflow was composed from {} sources\",\n    composed.metadata.sources.len());\n</code></pre>"},{"location":"composition/composition-metadata/#circular-dependency-detection","title":"Circular Dependency Detection","text":"<p>Metadata enables circular dependency detection:</p> <pre><code># workflow-a.yml\nextends: \"workflow-b.yml\"\n\n# workflow-b.yml\nextends: \"workflow-a.yml\"\n</code></pre> <p>Detection: <pre><code>Error: Circular dependency detected\n  workflow-a.yml -&gt; workflow-b.yml -&gt; workflow-a.yml\n\nDependency chain:\n  1. workflow-a.yml (extends workflow-b.yml)\n  2. workflow-b.yml (extends workflow-a.yml) &lt;- Circular!\n</code></pre></p>"},{"location":"composition/composition-metadata/#parameter-tracking","title":"Parameter Tracking","text":"<p>Metadata tracks final parameter values applied during composition:</p> <pre><code># workflow.yml\nparameters:\n  definitions:\n    environment:\n      type: string\n      required: true\n    timeout:\n      type: integer\n      default: 600\n</code></pre> <p>Metadata captures: <pre><code>metadata.parameters = {\n    \"environment\": \"production\",\n    \"timeout\": 600,\n}\n</code></pre></p> <p>The <code>parameters</code> field in <code>CompositionMetadata</code> stores the final resolved parameter values as a <code>HashMap&lt;String, Value&gt;</code>. This enables reproducibility and debugging of composed workflows.</p>"},{"location":"composition/composition-metadata/#caching-and-performance","title":"Caching and Performance","text":"<p>Composition metadata enables future caching optimizations:</p> <ul> <li><code>composed_at</code> timestamp can be used for cache invalidation</li> <li><code>sources</code> list enables dependency-based cache busting</li> <li><code>dependencies</code> graph supports incremental composition</li> <li><code>parameters</code> hash can detect identical compositions</li> </ul> <p>Note: Workflow caching is a planned feature. Currently, metadata is generated fresh on each composition.</p>"},{"location":"composition/composition-metadata/#data-structure-properties","title":"Data Structure Properties","text":"<p>CompositionMetadata uses standard Rust types for broad compatibility:</p> <pre><code>#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CompositionMetadata {\n    // All fields use standard types\n    pub sources: Vec&lt;PathBuf&gt;,           // Cloneable\n    pub templates: Vec&lt;String&gt;,          // Cloneable\n    pub parameters: HashMap&lt;String, Value&gt;,  // Cloneable\n    pub composed_at: chrono::DateTime&lt;chrono::Utc&gt;,  // Copy\n    pub dependencies: Vec&lt;DependencyInfo&gt;,   // Cloneable\n}\n</code></pre> <p>The struct derives <code>Clone</code>, making it easy to share metadata across components without requiring explicit synchronization primitives.</p>"},{"location":"composition/composition-metadata/#related-topics","title":"Related Topics","text":"<ul> <li>Template System - Template caching and loading</li> <li>Workflow Extension - Inheritance tracking</li> <li>Best Practices (see composition/best-practices.md) - Using metadata for debugging</li> </ul>"},{"location":"composition/default-values/","title":"Default Values","text":""},{"location":"composition/default-values/#default-values","title":"Default Values","text":"<p>Set default parameter values and environment variables at the workflow level. Defaults reduce required parameters and simplify workflow usage by providing sensible fallback values.</p>"},{"location":"composition/default-values/#basic-syntax","title":"Basic Syntax","text":"<pre><code>name: my-workflow\nmode: standard\n\ndefaults:\n  timeout: 300\n  retry_count: 3\n  verbose: false\n  environment: \"development\"\n  log_level: \"info\"\n</code></pre>"},{"location":"composition/default-values/#defaults-field-structure","title":"Defaults Field Structure","text":"<p>The <code>defaults</code> field is a HashMap that accepts any JSON-compatible values: <pre><code>defaults:\n  # String values\n  environment: \"staging\"\n  log_level: \"debug\"\n\n  # Number values\n  timeout: 600\n  max_retries: 5\n\n  # Boolean values\n  dry_run: false\n  enable_cache: true\n\n  # Array values\n  allowed_regions: [\"us-west-2\", \"us-east-1\"]\n\n  # Object values\n  database_config:\n    host: \"localhost\"\n    port: 5432\n    pool_size: 10\n</code></pre>"},{"location":"composition/default-values/#parameter-precedence","title":"Parameter Precedence","text":"<p>When multiple sources provide values for the same parameter, they are resolved in this order:</p> <ol> <li>CLI <code>--param</code> flags (highest priority) - Always override all other sources</li> <li>Parameter <code>default</code> values - Defined in parameter definitions</li> <li>Workflow <code>defaults</code> values (lowest priority) - Only used if parameter has no default</li> </ol> <p>Source: Parameter precedence implemented in src/cook/workflow/composition/composer.rs:245-254 and src/cook/workflow/composer_integration.rs:68-72</p>"},{"location":"composition/default-values/#example-with-precedence","title":"Example with Precedence","text":"<pre><code># workflow.yml\ndefaults:\n  environment: \"development\"\n  timeout: 300\n  log_level: \"info\"\n\nparameters:\n  definitions:\n    environment:\n      type: String\n      # No parameter default - uses workflow default \"development\"\n\n    timeout:\n      type: Number\n      default: 600  # Parameter default overrides workflow default (600, not 300)\n\n    log_level:\n      type: String\n      # No parameter default - uses workflow default \"info\"\n</code></pre> <p>Behavior without CLI flags: <pre><code># Uses: environment=\"development\", timeout=600, log_level=\"info\"\n# Note: timeout uses parameter default (600), others use workflow defaults\nprodigy run workflow.yml\n</code></pre></p> <p>CLI override: <pre><code># Final values: environment=\"production\", timeout=900, log_level=\"debug\"\n# CLI flags override all defaults\nprodigy run workflow.yml \\\n  --param environment=production \\\n  --param timeout=900 \\\n  --param log_level=debug\n</code></pre></p> <p>Partial CLI override: <pre><code># environment=\"production\" (CLI), timeout=600 (param default), log_level=\"info\" (workflow default)\nprodigy run workflow.yml --param environment=production\n</code></pre></p>"},{"location":"composition/default-values/#cli-parameter-overrides","title":"CLI Parameter Overrides","text":"<p>Defaults can be overridden using CLI <code>--param</code> flags, providing flexibility for different execution contexts:</p> <pre><code># workflow.yml\ndefaults:\n  timeout: 300\n  environment: \"development\"\n  log_level: \"info\"\n</code></pre> <p>Override specific defaults: <pre><code># Override timeout to 900 seconds for long-running operation\nprodigy run workflow.yml --param timeout=900\n\n# Change environment to production\nprodigy run workflow.yml --param environment=production\n\n# Override multiple defaults\nprodigy run workflow.yml \\\n  --param environment=staging \\\n  --param log_level=debug\n</code></pre></p> <p>Source: CLI parameter handling in src/cook/workflow/composer_integration.rs:68-72 shows that CLI params are merged with workflow defaults, with CLI params taking precedence.</p>"},{"location":"composition/default-values/#defaults-with-parameters","title":"Defaults with Parameters","text":"<p>Defaults simplify parameter requirements:</p> <p>Without defaults: <pre><code>parameters:\n  required:\n    - environment\n    - timeout\n    - log_level\n    - retry_count\n\n# Users must provide all 4 parameters\nprodigy run workflow.yml \\\n  --param environment=staging \\\n  --param timeout=600 \\\n  --param log_level=info \\\n  --param retry_count=3\n</code></pre></p> <p>With defaults: <pre><code>defaults:\n  environment: \"development\"\n  timeout: 300\n  log_level: \"info\"\n  retry_count: 3\n\nparameters:\n  required:\n    - environment  # Still required but has default\n\n# Users can run without parameters (uses defaults)\nprodigy run workflow.yml\n\n# Or override specific values\nprodigy run workflow.yml --param environment=production\n</code></pre></p>"},{"location":"composition/default-values/#defaults-for-environment-variables","title":"Defaults for Environment Variables","text":"<p>Use defaults to set common environment variables:</p> <pre><code>defaults:\n  RUST_BACKTRACE: \"1\"\n  CARGO_INCREMENTAL: \"0\"\n  DATABASE_URL: \"postgres://localhost/dev\"\n  REDIS_URL: \"redis://localhost:6379\"\n\ncommands:\n  # Commands use default environment variables\n  - shell: \"cargo test\"\n  - shell: \"redis-cli -u $REDIS_URL ping\"\n</code></pre>"},{"location":"composition/default-values/#template-integration","title":"Template Integration","text":"<p>Templates can use defaults for parameterization:</p> <pre><code># template.yml\nname: deployment-template\n\ndefaults:\n  replicas: \"3\"\n  environment: \"staging\"\n\nparameters:\n  required:\n    - app_name\n\ncommands:\n  - shell: \"kubectl scale deployment ${app_name} --replicas=${replicas}\"\n  - shell: \"kubectl set env deployment/${app_name} ENV=${environment}\"\n</code></pre> <p>Using template: <pre><code>template:\n  source:\n    file: \"template.yml\"\n  with:\n    app_name: \"my-service\"\n    # Uses defaults: replicas=3, environment=staging\n</code></pre></p>"},{"location":"composition/default-values/#implementation-status","title":"Implementation Status","text":"<p>All default value features are fully implemented and functional:</p> <ul> <li>\u2705 Defaults field parsing and storage</li> <li>\u2705 Defaults validation during composition</li> <li>\u2705 Integration into composition flow</li> <li>\u2705 Merge logic with environment variables (composer.rs:230-242)</li> <li>\u2705 Merge logic with parameter definitions (composer.rs:245-254)</li> <li>\u2705 CLI parameter override support (composer_integration.rs:68-72)</li> </ul> <p>The <code>apply_defaults</code> function at src/cook/workflow/composition/composer.rs:217-257 handles: 1. Applying defaults to environment variables (only if not already set) 2. Applying defaults to parameter definitions (only if parameter has no default value) 3. Type conversion for environment variable values (strings, numbers, booleans)</p>"},{"location":"composition/parameter-definitions/","title":"Parameter Definitions","text":""},{"location":"composition/parameter-definitions/#parameter-definitions","title":"Parameter Definitions","text":"<p>Define parameters with type validation to create flexible, reusable workflows. Parameters enable workflows and templates to accept inputs with enforced types, default values, and validation rules.</p>"},{"location":"composition/parameter-definitions/#basic-parameter-definition","title":"Basic Parameter Definition","text":"<pre><code>name: deployment-workflow\n\nparameters:\n  # Required parameters (must be provided)\n  required:\n    - environment\n    - version\n\n  # Optional parameters (have defaults or can be omitted)\n  optional:\n    - debug_mode\n    - timeout\n</code></pre>"},{"location":"composition/parameter-definitions/#parameter-structure","title":"Parameter Structure","text":"<p>Parameters are organized into <code>required</code> and <code>optional</code> arrays. Each parameter specifies its type, description, and validation rules:</p> <pre><code>parameters:\n  required:\n    - name: environment\n      type: String\n      description: \"Target environment for deployment\"\n      validation: \"matches('^(dev|staging|prod)$')\"\n\n    - name: version\n      type: String\n      description: \"Application version to deploy\"\n\n  optional:\n    - name: port\n      type: Number\n      description: \"Server port number\"\n      default: 8080\n\n    - name: enable_ssl\n      type: Boolean\n      description: \"Enable SSL/TLS\"\n      default: true\n\n    - name: allowed_hosts\n      type: Array\n      description: \"List of allowed hostnames\"\n      default: [\"localhost\"]\n\n    - name: config\n      type: Object\n      description: \"Configuration object\"\n      default: {\"timeout\": 30}\n\n    - name: data\n      type: Any\n      description: \"Free-form data of any type\"\n</code></pre> <p>Source: <code>ParameterDefinitions</code> structure in src/cook/workflow/composition/mod.rs:97-107</p>"},{"location":"composition/parameter-definitions/#parameter-types","title":"Parameter Types","text":"<p>Prodigy supports six parameter types with validation (defined in src/cook/workflow/composition/mod.rs:131-141):</p> Type Description Example Values <code>String</code> Text values <code>\"production\"</code>, <code>\"v1.2.3\"</code> <code>Number</code> Integer or float <code>42</code>, <code>3.14</code>, <code>-100</code> <code>Boolean</code> True or false <code>true</code>, <code>false</code> <code>Array</code> List of values <code>[1, 2, 3]</code>, <code>[\"a\", \"b\"]</code> <code>Object</code> Key-value map <code>{\"key\": \"value\"}</code> <code>Any</code> Any JSON value Any valid JSON <p>Source: <code>ParameterType</code> enum in src/cook/workflow/composition/mod.rs:131-141</p> <p>Type Validation: - Type checking is enforced when parameters are provided (src/cook/workflow/composition/mod.rs:226-280) - Mismatched types cause workflow validation errors - <code>Any</code> type accepts any value without validation - Validation logic in <code>validate_parameters</code> function</p> <p>Test example: tests/workflow_composition_test.rs:49-79 demonstrates parameter validation with String type</p>"},{"location":"composition/parameter-definitions/#default-values","title":"Default Values","text":"<p>Parameters can specify default values used when no value is provided. Defaults can be set at two levels:</p> <p>Parameter-Level Defaults (in parameter definition):</p> <pre><code>parameters:\n  optional:\n    - name: timeout\n      type: Number\n      description: \"Operation timeout in seconds\"\n      default: 300\n\n    - name: log_level\n      type: String\n      description: \"Logging verbosity\"\n      default: \"info\"\n\n    - name: retry_enabled\n      type: Boolean\n      description: \"Enable retry logic\"\n      default: true\n</code></pre> <p>Workflow-Level Defaults (applies to all sub-workflows):</p> <pre><code>name: parent-workflow\n\ndefaults:\n  environment: \"development\"\n  debug_mode: true\n  timeout: 600\n\nparameters:\n  optional:\n    - name: environment\n      type: String\n      default: \"production\"  # Overridden by workflow defaults\n\n    - name: timeout\n      type: Number\n      default: 300  # Overridden by workflow defaults\n</code></pre> <p>Source: <code>defaults</code> field in src/cook/workflow/composition/mod.rs:204, <code>default</code> field in src/cook/workflow/composition/mod.rs:123-124</p>"},{"location":"composition/parameter-definitions/#validation-expressions","title":"Validation Expressions","text":"<p>The <code>validation</code> field allows custom validation logic:</p> <pre><code>parameters:\n  definitions:\n    email:\n      type: String\n      validation: \"matches('^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$')\"\n\n    priority:\n      type: Number\n      validation: \"value &gt;= 1 &amp;&amp; value &lt;= 10\"\n\n    status:\n      type: String\n      validation: \"in(['pending', 'active', 'completed'])\"\n</code></pre> <p>Note: Validation expressions are currently stored and validated for syntax, but custom expression evaluation is not yet implemented. Type validation is fully functional.</p>"},{"location":"composition/parameter-definitions/#providing-parameter-values","title":"Providing Parameter Values","text":"<p>Via Command Line: <pre><code># Individual parameters (with automatic type inference)\nprodigy run workflow.yml --param environment=production --param timeout=600\n\n# From JSON file\nprodigy run workflow.yml --param-file params.json\n</code></pre></p> <p>Automatic Type Inference:</p> <p>When using <code>--param</code> flags, Prodigy automatically infers parameter types: - Numbers: <code>--param port=8080</code> \u2192 parsed as Number (i64 or f64) - Booleans: <code>--param debug=true</code> \u2192 parsed as Boolean - Strings: <code>--param name=app</code> \u2192 parsed as String (default if no other type matches)</p> <pre><code># These are automatically typed correctly:\nprodigy run workflow.yml \\\n  --param port=8080 \\           # Number\n  --param timeout=30.5 \\        # Number (float)\n  --param debug=true \\          # Boolean\n  --param environment=prod      # String\n</code></pre> <p>Source: <code>parse_param_value</code> function in src/cli/params.rs:51-72</p> <p>params.json: <pre><code>{\n  \"environment\": \"staging\",\n  \"version\": \"2.1.0\",\n  \"debug_mode\": false,\n  \"timeout\": 300\n}\n</code></pre></p> <p>Parameter Precedence: 1. CLI <code>--param</code> flags (highest priority) 2. <code>--param-file</code> values 3. Workflow <code>defaults</code> values 4. Parameter <code>default</code> values (lowest priority)</p>"},{"location":"composition/parameter-definitions/#using-parameters-in-workflows","title":"Using Parameters in Workflows","text":"<p>Parameters are interpolated into commands using the standard variable interpolation system with <code>${param_name}</code> syntax. This is the same syntax used for all workflow variables (captured outputs, environment variables, etc.).</p> <pre><code>parameters:\n  required:\n    - name: app_name\n      type: String\n    - name: deploy_env\n      type: String\n\ncommands:\n  - shell: \"echo Deploying ${app_name} to ${deploy_env}\"\n  - shell: \"kubectl apply -f k8s/${deploy_env}/deployment.yml\"\n  - claude: \"/deploy ${app_name} --environment ${deploy_env}\"\n</code></pre> <p>Parameters are resolved during variable interpolation before command execution, making them available everywhere workflow variables are supported.</p> <p>Source: Variable interpolation system in src/cook/workflow/variables.rs</p>"},{"location":"composition/parameter-definitions/#complete-example","title":"Complete Example","text":"<pre><code>name: database-migration\nmode: standard\n\nparameters:\n  required:\n    - name: database_url\n      type: String\n      description: \"Database connection string\"\n      validation: \"matches('^postgres://')\"\n\n    - name: migration_version\n      type: String\n      description: \"Target migration version\"\n\n  optional:\n    - name: dry_run\n      type: Boolean\n      description: \"Run in dry-run mode\"\n      default: false\n\n    - name: timeout\n      type: Number\n      description: \"Migration timeout in seconds\"\n      default: 300\n\ncommands:\n  - shell: \"echo Running migration to ${migration_version}\"\n  - shell: |\n      migrate --database-url ${database_url} \\\n              --target ${migration_version} \\\n              --timeout ${timeout} \\\n              $( [ \"${dry_run}\" = \"true\" ] &amp;&amp; echo \"--dry-run\" )\n</code></pre> <p>Run with parameters: <pre><code>prodigy run migration.yml \\\n  --param database_url=\"postgres://localhost/mydb\" \\\n  --param migration_version=\"20250109_001\" \\\n  --param dry_run=true\n</code></pre></p>"},{"location":"composition/parameter-definitions/#parameter-validation-errors","title":"Parameter Validation Errors","text":"<p>When validation fails, Prodigy provides clear error messages:</p> <pre><code>Error: Parameter validation failed\n  - 'environment': Expected String, got Number\n  - 'port': Value 99999 exceeds valid range\n  - 'config': Required parameter not provided\n</code></pre>"},{"location":"composition/parameter-definitions/#implementation-status","title":"Implementation Status","text":"<ul> <li>\u2705 Parameter type definitions (all 6 types)</li> <li>\u2705 Type validation enforcement</li> <li>\u2705 Default value support</li> <li>\u2705 Required/optional parameter tracking</li> <li>\u2705 CLI parameter passing (--param, --param-file)</li> <li>\u2705 Parameter precedence handling</li> <li>\u23f3 Custom validation expression evaluation (field stored, not evaluated)</li> </ul>"},{"location":"composition/parameter-definitions/#related-topics","title":"Related Topics","text":"<ul> <li>Template System - Use parameters in templates</li> <li>Default Values - Set workflow-level defaults</li> <li>Providing Parameter Values - Command-line parameter usage</li> </ul>"},{"location":"composition/sub-workflows/","title":"Sub-Workflows","text":""},{"location":"composition/sub-workflows/#sub-workflows","title":"Sub-Workflows","text":"<p>Execute child workflows as part of a parent workflow. Sub-workflows can run in parallel and have their own parameters and outputs, enabling modular workflow design and reusable validation/test pipelines.</p>"},{"location":"composition/sub-workflows/#basic-sub-workflow-syntax","title":"Basic Sub-Workflow Syntax","text":"<pre><code>name: deployment-pipeline\nmode: standard\n\nsub_workflows:\n  lint-and-test:\n    source: \"workflows/quality-checks.yml\"\n\n  build:\n    source: \"workflows/build.yml\"\n    parameters:\n      environment: \"production\"\n\n  deploy:\n    source: \"workflows/deploy.yml\"\n    inputs:\n      build_artifact: \"${build.artifact_path}\"\n</code></pre>"},{"location":"composition/sub-workflows/#sub-workflow-configuration","title":"Sub-Workflow Configuration","text":"<p>Each sub-workflow is defined as a key-value pair in a HashMap, where the key is the sub-workflow name (defined in <code>SubWorkflow</code> struct, src/cook/workflow/composition/sub_workflow.rs:13-66):</p> <pre><code>sub_workflows:\n  validation:  # Key is the sub-workflow name\n    source: \"path/to/workflow.yml\"  # Required: workflow file path\n\n    parameters:                       # Optional: parameter values\n      env: \"staging\"\n      timeout: 600\n\n    inputs:                           # Optional: input from parent context\n      commit_sha: \"${git.commit}\"\n      branch: \"${git.branch}\"\n\n    outputs:                          # Optional: extract values from sub-workflow\n      - \"test_coverage\"\n      - \"artifact_url\"\n\n    parallel: false                   # Optional: run in parallel (default: false)\n\n    continue_on_error: false          # Optional: continue if sub-workflow fails\n\n    timeout: 1800                     # Optional: sub-workflow timeout (seconds)\n\n    working_dir: \"./sub-project\"      # Optional: working directory for sub-workflow\n                                       # Note: Parsed but not yet applied (implementation in progress)\n</code></pre> <p>Source: <code>SubWorkflow</code> struct in src/cook/workflow/composition/sub_workflow.rs:13-66 Validation: Sub-workflow validation in src/cook/workflow/composition/composer.rs:381-395 Test example: tests/workflow_composition_test.rs:152-173 demonstrates sub-workflow configuration</p>"},{"location":"composition/sub-workflows/#parent-child-context-isolation","title":"Parent-Child Context Isolation","text":"<p>Sub-workflows execute in isolated contexts with clean variable scopes (src/cook/workflow/composition/sub_workflow.rs:242-262):</p> <ul> <li>Empty variable scope: Sub-workflow starts with cleared variables (<code>sub_context.variables.clear()</code>)</li> <li>Explicit input passing: Use <code>inputs</code> to copy specific parent variables to child context</li> <li>No variable leakage: Sub-workflow variables don't leak back to parent</li> <li>Output extraction: Use <code>outputs</code> to explicitly capture child results</li> <li>Independent git state: Sub-workflows can operate in different directories</li> </ul> <p>The isolation process: 1. Clone parent context 2. Clear all variables in cloned context 3. Copy only specified <code>inputs</code> from parent to child 4. Execute sub-workflow in isolated context 5. Extract only specified <code>outputs</code> back to parent</p>"},{"location":"composition/sub-workflows/#output-variable-extraction","title":"Output Variable Extraction","text":"<p>Capture values from sub-workflow execution:</p> <pre><code># parent-workflow.yml\nsub_workflows:\n  build:  # Sub-workflow name\n    source: \"workflows/build.yml\"\n    outputs:\n      - \"docker_image_tag\"\n      - \"artifact_sha256\"\n\ncommands:\n  # Access outputs using ${sub-workflow-name.output-variable}\n  - shell: \"echo Deploying ${build.docker_image_tag}\"\n  - shell: \"verify-checksum ${build.artifact_sha256}\"\n</code></pre>"},{"location":"composition/sub-workflows/#parallel-execution","title":"Parallel Execution","text":"<p>Run multiple sub-workflows concurrently using <code>tokio::spawn</code> for concurrent task execution (src/cook/workflow/composition/sub_workflow.rs:179-226):</p> <pre><code>sub_workflows:\n  # These run in parallel\n  unit-tests:\n    source: \"workflows/unit-tests.yml\"\n    parallel: true\n\n  integration-tests:\n    source: \"workflows/integration-tests.yml\"\n    parallel: true\n\n  e2e-tests:\n    source: \"workflows/e2e-tests.yml\"\n    parallel: true\n\n# Parent waits for all parallel sub-workflows before continuing\ncommands:\n  - shell: \"echo All tests completed\"\n</code></pre> <p>Execution behavior: - Each parallel sub-workflow spawns as a separate async task (<code>tokio::spawn</code>) - Parent workflow waits for all parallel tasks to complete via <code>join</code> - All outputs are merged back to parent context after completion - If any parallel sub-workflow fails, execution stops (unless <code>continue_on_error: true</code>)</p>"},{"location":"composition/sub-workflows/#error-handling","title":"Error Handling","text":"<p>Control behavior when sub-workflows fail:</p> <pre><code>sub_workflows:\n  # Critical step - fail parent if this fails\n  security-scan:\n    source: \"workflows/security-scan.yml\"\n    continue_on_error: false  # Default behavior\n\n  # Optional step - parent continues even if this fails\n  performance-test:\n    source: \"workflows/perf-test.yml\"\n    continue_on_error: true\n</code></pre>"},{"location":"composition/sub-workflows/#modular-pipeline-example","title":"Modular Pipeline Example","text":"<p>parent-pipeline.yml: <pre><code>name: ci-cd-pipeline\nmode: standard\n\nsub_workflows:\n  # Step 1: Validation (sequential)\n  validate:\n    source: \"workflows/validation.yml\"\n    outputs:\n      - \"validation_passed\"\n\n  # Step 2: Tests (parallel)\n  unit-tests:\n    source: \"workflows/unit-tests.yml\"\n    parallel: true\n\n  integration-tests:\n    source: \"workflows/integration-tests.yml\"\n    parallel: true\n\n  # Step 3: Build (sequential, after tests)\n  build:\n    source: \"workflows/build.yml\"\n    parameters:\n      optimization_level: \"3\"\n    outputs:\n      - \"artifact_path\"\n\n  # Step 4: Deploy (sequential, uses build output)\n  deploy:\n    source: \"workflows/deploy.yml\"\n    inputs:\n      artifact: \"${build.artifact_path}\"\n      environment: \"production\"\n</code></pre></p> <p>validation.yml (reusable sub-workflow): <pre><code>name: validation\nmode: standard\n\ncommands:\n  - shell: \"cargo fmt --check\"\n  - shell: \"cargo clippy -- -D warnings\"\n  - shell: \"echo validation_passed=true &gt;&gt; $PRODIGY_OUTPUT\"\n</code></pre></p>"},{"location":"composition/sub-workflows/#working-directory-isolation","title":"Working Directory Isolation","text":"<p>Sub-workflows can specify different working directories (parsed but not yet applied - see implementation note in src/cook/workflow/composition/sub_workflow.rs:104-107):</p> <pre><code>sub_workflows:\n  # Backend tests in backend/\n  backend-tests:\n    source: \"workflows/rust-tests.yml\"\n    working_dir: \"./backend\"  # Parsed but not yet applied\n\n  # Frontend tests in frontend/\n  frontend-tests:\n    source: \"workflows/js-tests.yml\"\n    working_dir: \"./frontend\"  # Parsed but not yet applied\n</code></pre> <p>Note: The <code>working_dir</code> field is parsed and validated but not yet applied during execution. The <code>WorkflowContext</code> struct needs a <code>working_directory</code> field to enable this feature. Currently, all sub-workflows execute in the parent's working directory.</p>"},{"location":"composition/sub-workflows/#timeout-configuration","title":"Timeout Configuration","text":"<p>Set execution time limits:</p> <pre><code>sub_workflows:\n  quick-tests:\n    source: \"workflows/smoke-tests.yml\"\n    timeout: 120  # 2 minutes\n\n  comprehensive-tests:\n    source: \"workflows/full-suite.yml\"\n    timeout: 3600  # 1 hour\n</code></pre>"},{"location":"composition/sub-workflows/#use-cases","title":"Use Cases","text":"<p>Modular Testing: - Separate unit, integration, and e2e tests into sub-workflows - Run test suites in parallel for faster feedback - Reuse test workflows across multiple projects</p> <p>Multi-Language Projects: - Separate workflows for each language/component - Independent validation for microservices - Coordinated deployment of multiple services</p> <p>Reusable Validation: - Shared linting/formatting workflows - Common security scanning workflows - Standardized compliance checks</p> <p>Environment-Specific Pipelines: <pre><code>sub_workflows:\n  # Different deployment sub-workflows per environment\n  deploy-staging:\n    source: \"workflows/deploy.yml\"\n    parameters:\n      environment: \"staging\"\n      replicas: \"2\"\n\n  deploy-production:\n    source: \"workflows/deploy.yml\"\n    parameters:\n      environment: \"production\"\n      replicas: \"5\"\n</code></pre></p>"},{"location":"composition/sub-workflows/#complete-example","title":"Complete Example","text":"<pre><code>name: monorepo-ci\nmode: standard\n\nsub_workflows:\n  # Validate everything first\n  validate:\n    source: \"shared/validate.yml\"\n\n  # Test all services in parallel\n  api-tests:\n    source: \"services/api/test.yml\"\n    working_dir: \"./services/api\"\n    parallel: true\n    outputs:\n      - \"coverage\"\n\n  worker-tests:\n    source: \"services/worker/test.yml\"\n    working_dir: \"./services/worker\"\n    parallel: true\n    outputs:\n      - \"coverage\"\n\n  frontend-tests:\n    source: \"apps/frontend/test.yml\"\n    working_dir: \"./apps/frontend\"\n    parallel: true\n    outputs:\n      - \"coverage\"\n\n# After all sub-workflows complete\ncommands:\n  - shell: \"echo API coverage: ${api-tests.coverage}%\"\n  - shell: \"echo Worker coverage: ${worker-tests.coverage}%\"\n  - shell: \"echo Frontend coverage: ${frontend-tests.coverage}%\"\n  - shell: \"generate-combined-coverage-report.sh\"\n</code></pre>"},{"location":"composition/sub-workflows/#sub-workflow-result","title":"Sub-Workflow Result","text":"<p>Each sub-workflow execution produces a <code>SubWorkflowResult</code> (src/cook/workflow/composition/sub_workflow.rs:48-65):</p> <pre><code>SubWorkflowResult {\n    success: bool,                // Execution success\n    outputs: HashMap&lt;String, Value&gt;, // Extracted output variables\n    duration: Duration,           // Execution time\n    error: Option&lt;String&gt;,        // Error message if failed\n    logs: Vec&lt;String&gt;,            // Sub-workflow execution logs\n}\n</code></pre> <p>Note: The sub-workflow name is tracked separately as the HashMap key in the parent workflow's <code>sub_workflows</code> field, not as a field within <code>SubWorkflowResult</code>.</p>"},{"location":"composition/sub-workflows/#implementation-status","title":"Implementation Status","text":"<ul> <li>\u2705 Sub-workflow configuration parsing</li> <li>\u2705 Sub-workflow validation (<code>validate_sub_workflows</code>)</li> <li>\u2705 Parameter and input definitions</li> <li>\u2705 Output extraction structure</li> <li>\u2705 Parallel execution configuration</li> <li>\u2705 Error handling options (continue_on_error)</li> <li>\u2705 Timeout and working directory settings</li> <li>\u2705 SubWorkflowExecutor structure</li> <li>\u23f3 Executor integration with main workflow runtime (in progress)</li> </ul> <p>Note: Sub-workflow definitions are fully validated and composed, but execution integration with the main workflow orchestrator is currently in development.</p>"},{"location":"composition/sub-workflows/#related-topics","title":"Related Topics","text":"<ul> <li>Workflow Imports - Import shared configurations</li> <li>Template System - Parameterized workflows</li> <li>Parameter Definitions - Define sub-workflow parameters</li> </ul>"},{"location":"composition/template-system/","title":"Template System","text":""},{"location":"composition/template-system/#template-system","title":"Template System","text":"<p>Templates provide reusable workflow patterns that can be instantiated with different parameters. Templates can be stored in a registry or loaded from files, enabling standardized workflows across teams and projects.</p> <p>Source: Implemented in src/cook/workflow/composition/mod.rs and src/cook/workflow/composition/registry.rs</p>"},{"location":"composition/template-system/#template-basics","title":"Template Basics","text":"<p>A template is a reusable workflow definition that can be parameterized and instantiated multiple times with different values. Templates support: - Registry-based or file-based storage - Parameter substitution - Field overrides - Metadata and versioning</p>"},{"location":"composition/template-system/#template-configuration","title":"Template Configuration","text":"<p>Templates are defined using the <code>WorkflowTemplate</code> struct (src/cook/workflow/composition/mod.rs:67-83):</p> <pre><code>template:\n  # Template name (for identification)\n  name: \"standard-ci\"\n\n  # Template source (see Template Sources below)\n  source: \"template-name\"  # or { file: \"path.yml\" }\n\n  # Parameter values to pass to template\n  with:\n    param1: \"value1\"\n    param2: \"value2\"\n\n  # Override specific template fields\n  override:\n    timeout: 600\n    max_parallel: 5\n</code></pre> <p>Source: Field definitions from <code>WorkflowTemplate</code> struct in src/cook/workflow/composition/mod.rs:67-83</p>"},{"location":"composition/template-system/#template-sources","title":"Template Sources","text":"<p>Prodigy uses an untagged enum for <code>TemplateSource</code> (src/cook/workflow/composition/mod.rs:85-95), which means the YAML format varies based on the source type:</p>"},{"location":"composition/template-system/#registry-lookup-string","title":"Registry Lookup (String)","text":"<p>Load a template by name from the template registry:</p> <pre><code>template:\n  name: \"ci-pipeline\"\n  source: \"standard-ci\"  # Simple string = registry lookup\n  with:\n    project_name: \"my-project\"\n</code></pre> <p>Source: <code>TemplateSource::Registry(String)</code> variant in src/cook/workflow/composition/mod.rs:92</p> <p>How it works: When the source is a plain string, Prodigy looks up the template in the registry at <code>~/.prodigy/templates/</code> or <code>.prodigy/templates/</code>. Example from test: tests/workflow_composition_test.rs:125-133</p>"},{"location":"composition/template-system/#file-path","title":"File Path","text":"<p>Load a template from a file:</p> <pre><code>template:\n  name: \"deployment\"\n  source:\n    file: \"templates/k8s-deploy.yml\"  # File path in struct format\n  with:\n    environment: \"production\"\n</code></pre> <p>Source: <code>TemplateSource::File(PathBuf)</code> variant in src/cook/workflow/composition/mod.rs:90</p> <p>How it works: When the source uses a <code>file</code> field, Prodigy loads the template from the specified file path. Paths can be relative (to workflow file) or absolute.</p>"},{"location":"composition/template-system/#url-planned","title":"URL (Planned)","text":"<p>Load a template from a remote URL:</p> <pre><code>template:\n  name: \"remote-template\"\n  source: \"https://templates.example.com/ci.yml\"  # String starting with https://\n  with:\n    config: \"production\"\n</code></pre> <p>Source: <code>TemplateSource::Url(String)</code> variant in src/cook/workflow/composition/mod.rs:94</p> <p>Status: Currently returns an error. Planned for future implementation. See src/cook/workflow/composition/composer.rs for URL handling code.</p>"},{"location":"composition/template-system/#template-registry","title":"Template Registry","text":"<p>The template registry stores reusable workflow templates. Templates can be stored in two locations (similar to git worktrees):</p> <p>Registry Locations: <pre><code># Local (project-specific)\n.prodigy/templates/\n\u251c\u2500\u2500 project-ci.yml\n\u251c\u2500\u2500 custom-deployment.yml\n\u2514\u2500\u2500 ...\n\n# Global (user-wide)\n~/.prodigy/templates/\n\u251c\u2500\u2500 standard-ci.yml\n\u251c\u2500\u2500 deployment-pipeline.yml\n\u251c\u2500\u2500 test-suite.yml\n\u2514\u2500\u2500 ...\n</code></pre></p> <p>Implementation: <code>FileTemplateStorage</code> in src/cook/workflow/composition/registry.rs:26-29 uses a configurable base directory (defaults to \"templates\").</p>"},{"location":"composition/template-system/#programmatic-registration","title":"Programmatic Registration","text":"<p>Register templates using the API (src/cook/workflow/composition/registry.rs:41-73):</p> <pre><code>use prodigy::cook::workflow::composition::registry::TemplateRegistry;\n\nlet registry = TemplateRegistry::new();\n\n// Basic registration\nregistry\n    .register_template(\"ci-pipeline\".to_string(), template)\n    .await?;\n\n// Registration with metadata\nregistry\n    .register_template_with_metadata(\n        \"deployment\".to_string(),\n        template,\n        metadata\n    )\n    .await?;\n</code></pre> <p>Source: Example from tests/workflow_composition_test.rs:177-199</p>"},{"location":"composition/template-system/#manual-registry-management","title":"Manual Registry Management","text":"<pre><code># Copy template to local registry\ncp my-template.yml .prodigy/templates/my-template.yml\n\n# Copy to global registry (user-wide)\ncp my-template.yml ~/.prodigy/templates/my-template.yml\n\n# Templates are automatically discovered by filename\n</code></pre>"},{"location":"composition/template-system/#template-parameters","title":"Template Parameters","text":"<p>Templates can define parameters that are substituted when the template is instantiated. See Parameter Definitions for detailed parameter syntax and validation.</p> <p>Template with Parameters: <pre><code># templates/deployment.yml\nname: deployment-template\n\nparameters:\n  required:\n    - environment\n    - version\n  optional:\n    - timeout\n\ncommands:\n  - shell: \"deploy --env ${environment} --version ${version}\"\n  - shell: \"verify-deployment ${environment}\"\n</code></pre></p> <p>Using Parameterized Template: <pre><code>template:\n  source:\n    file: \"templates/deployment.yml\"\n  with:\n    environment: \"staging\"\n    version: \"2.0.0\"\n    timeout: \"300\"\n</code></pre></p>"},{"location":"composition/template-system/#template-metadata","title":"Template Metadata","text":"<p>Templates can include metadata for better organization and discovery (src/cook/workflow/composition/registry.rs:475-508):</p> <p>Metadata Fields: - <code>description</code>: Human-readable description - <code>author</code>: Template author - <code>version</code>: Semantic version string - <code>tags</code>: List of categorization tags - <code>created_at</code>: Creation timestamp - <code>updated_at</code>: Last modification timestamp</p> <p>Metadata Storage: Templates registered with metadata store an additional <code>.meta.json</code> file alongside the template YAML. For example, <code>standard-ci.yml</code> has metadata in <code>standard-ci.yml.meta.json</code>.</p> <p>Source: <code>TemplateMetadata</code> struct in src/cook/workflow/composition/registry.rs:475-495</p>"},{"location":"composition/template-system/#template-discovery","title":"Template Discovery","text":"<p>The registry provides search and listing capabilities (src/cook/workflow/composition/registry.rs:156-168):</p> <pre><code>// List all templates\nlet templates = registry.list().await?;\n\n// Search by tags\nlet ci_templates = registry\n    .search_by_tags(&amp;[\"ci\".to_string(), \"testing\".to_string()])\n    .await?;\n\n// Get specific template\nlet template = registry.get(\"standard-ci\").await?;\n\n// Delete template\nregistry.delete(\"old-template\").await?;\n</code></pre> <p>Source: Methods in <code>TemplateRegistry</code> implementation</p>"},{"location":"composition/template-system/#template-caching","title":"Template Caching","text":"<p>Prodigy caches loaded templates to improve performance: - Templates are loaded once and reused across workflow executions - Registry templates are cached until the registry is updated - In-memory cache stored in <code>Arc&lt;RwLock&lt;HashMap&gt;&gt;</code> (src/cook/workflow/composition/registry.rs:14)</p> <p>File Change Detection: The documentation previously claimed file-based templates are re-read on file changes, but this is not currently implemented in the caching layer. Templates are cached for the lifetime of the registry instance.</p>"},{"location":"composition/template-system/#template-override","title":"Template Override","text":"<p>The <code>override</code> field allows you to override specific template fields without modifying the template file:</p> <pre><code>template:\n  source: \"standard-workflow\"\n  override:\n    timeout: 1200  # Override default timeout\n    max_parallel: 10  # Override concurrency limit\n</code></pre> <p>Implementation Status: The <code>override_field</code> is defined in the <code>WorkflowTemplate</code> struct (src/cook/workflow/composition/mod.rs:80-83) and is properly deserialized, but the application logic in <code>apply_overrides()</code> is not yet implemented. The field is validated and stored but not applied during workflow composition.</p> <p>Source: See <code>override_field</code> in src/cook/workflow/composition/mod.rs:81-82</p>"},{"location":"composition/template-system/#use-cases","title":"Use Cases","text":"<p>Standardized CI/CD Pipelines: <pre><code># Use company-wide CI template\ntemplate:\n  source: \"company-ci-pipeline\"\n  with:\n    project_type: \"rust\"\n    test_coverage: \"80\"\n    deploy_targets: [\"staging\", \"production\"]\n</code></pre></p> <p>Environment-Specific Deployments: <pre><code># Reuse deployment template with different params\ntemplate:\n  source:\n    file: \"templates/k8s-deploy.yml\"\n  with:\n    cluster: \"us-west-2\"\n    namespace: \"production\"\n    replicas: \"5\"\n</code></pre></p> <p>Testing Workflow Variations: <pre><code># Test different configurations using same template\ntemplate:\n  source:\n    file: \"templates/integration-tests.yml\"\n  with:\n    database: \"postgres\"\n    cache: \"redis\"\n    message_queue: \"rabbitmq\"\n</code></pre></p>"},{"location":"composition/template-system/#implementation-status","title":"Implementation Status","text":"<ul> <li>\u2705 File-based template loading</li> <li>\u2705 Registry template storage and retrieval</li> <li>\u2705 Template parameter validation</li> <li>\u2705 Template caching (in-memory)</li> <li>\u2705 Template metadata and versioning</li> <li>\u2705 Template search and discovery</li> <li>\u2705 Programmatic registration API</li> <li>\u23f3 URL-based template loading (returns error, planned for future)</li> <li>\u23f3 Template override application (field exists but not applied in compose())</li> <li>\u23f3 File modification detection for cache invalidation</li> </ul>"},{"location":"composition/template-system/#related-topics","title":"Related Topics","text":"<ul> <li>Parameter Definitions - Define and validate template parameters</li> <li>Workflow Extension - Inherit from base workflows</li> <li>Default Values - Set default parameter values</li> </ul>"},{"location":"composition/troubleshooting/","title":"Troubleshooting","text":""},{"location":"composition/troubleshooting/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues with workflow composition and their solutions.</p>"},{"location":"composition/troubleshooting/#circular-dependency-errors","title":"Circular Dependency Errors","text":"<p>Error: <pre><code>Error: Circular dependency detected in workflow composition\n</code></pre></p> <p>Cause: Workflow inheritance or imports form a cycle.</p> <p>Solution: <pre><code># Bad: Circular dependency\n# workflow-a.yml extends workflow-b.yml\n# workflow-b.yml extends workflow-a.yml\n\n# Good: Use common base\n# base.yml (no extends)\n# workflow-a.yml extends base.yml\n# workflow-b.yml extends base.yml\n</code></pre></p> <p>Debugging:</p> <p>The error message doesn't include the dependency chain. To diagnose which workflows are involved:</p> <pre><code># Use verbose mode to see composition steps\nprodigy run workflow.yml --dry-run -vv\n\n# Manually trace the chain\n# 1. Check what the workflow extends\ngrep \"^extends:\" workflow-a.yml\n\n# 2. Check what that workflow extends\ngrep \"^extends:\" workflow-b.yml\n\n# 3. Continue until you find the cycle\n</code></pre> <p>Source: Error generated in src/cook/workflow/composition/composer.rs:727</p>"},{"location":"composition/troubleshooting/#template-not-found-in-registry","title":"Template Not Found in Registry","text":"<p>Error: <pre><code>Error: Template 'ci-pipeline' not found in registry\n</code></pre></p> <p>Cause: Template doesn't exist in the template search path.</p> <p>Solutions:</p> <ol> <li>Verify template locations (searched in order):</li> </ol> <p>Prodigy searches for templates in this priority order:</p> <pre><code># 1. Global templates (highest priority, shared across repos)\nls ~/.prodigy/templates/ci-pipeline.yml\n\n# 2. Project-local templates\nls .prodigy/templates/ci-pipeline.yml\n\n# 3. Legacy project-local templates\nls templates/ci-pipeline.yml\n</code></pre> <p>Source: Template search path defined in src/cook/workflow/composer_integration.rs:94-110</p> <ol> <li> <p>Check template name: <pre><code># Ensure template file name matches reference\ntemplate:\n  source:\n    registry: \"ci-pipeline\"  # Looks for ci-pipeline.yml\n</code></pre></p> </li> <li> <p>Add template to registry: <pre><code># Add to global registry (available to all projects)\ncp my-template.yml ~/.prodigy/templates/\n\n# Or add to project-local registry\ncp my-template.yml .prodigy/templates/\n</code></pre></p> </li> <li> <p>Use file-based template instead: <pre><code>template:\n  source:\n    file: \"path/to/template.yml\"\n</code></pre></p> </li> </ol> <p>See also: Template System for more details on template sources.</p>"},{"location":"composition/troubleshooting/#parameter-validation-failures","title":"Parameter Validation Failures","text":"<p>Error: <pre><code>Error: Parameter validation failed\n  - 'environment': Expected String, got Number\n</code></pre></p> <p>Cause: Parameter type mismatch.</p> <p>Solutions:</p> <ol> <li> <p>Check parameter type: <pre><code>parameters:\n  definitions:\n    environment:\n      type: String  # Must pass string value\n\n# Pass correct type\nprodigy run workflow.yml --param environment=\"production\"  # String\n# Not: --param environment=123  # Number\n</code></pre></p> </li> <li> <p>Verify validation expression: <pre><code>parameters:\n  definitions:\n    environment:\n      type: String\n      validation: \"matches('^(dev|staging|prod)$')\"\n\n# Must match regex pattern\nprodigy run workflow.yml --param environment=\"staging\"  # OK\n# Not: --param environment=\"test\"  # Fails validation\n</code></pre></p> </li> <li> <p>Check required vs optional: <pre><code>parameters:\n  required:\n    - environment  # Must provide\n\n# Error if missing:\nprodigy run workflow.yml  # Fails\n\n# Solution:\nprodigy run workflow.yml --param environment=\"dev\"\n</code></pre></p> </li> </ol> <p>See also: Parameter Definitions for complete parameter validation reference.</p>"},{"location":"composition/troubleshooting/#import-path-resolution-errors","title":"Import Path Resolution Errors","text":"<p>Error: <pre><code>Error: Failed to load import: shared/utilities.yml\n  No such file or directory\n</code></pre></p> <p>Cause: Import path doesn't exist or is incorrect.</p> <p>Solutions:</p> <ol> <li> <p>Use absolute path: <pre><code>imports:\n  - path: \"/full/path/to/shared/utilities.yml\"\n</code></pre></p> </li> <li> <p>Verify relative path: <pre><code># From workflow file directory\nls shared/utilities.yml\n\n# If in different location:\nimports:\n  - path: \"../shared/utilities.yml\"  # Go up one level\n</code></pre></p> </li> <li> <p>Check current directory: <pre><code># Run from correct directory\ncd /path/to/workflows\nprodigy run my-workflow.yml\n</code></pre></p> </li> </ol>"},{"location":"composition/troubleshooting/#type-mismatch-errors","title":"Type Mismatch Errors","text":"<p>Error: <pre><code>Error: Type mismatch for parameter 'timeout'\n  Expected Number, got String \"300\"\n</code></pre></p> <p>Cause: Parameter value type doesn't match definition.</p> <p>Solutions:</p> <ol> <li> <p>Pass correct type: <pre><code># Number type - no quotes\nprodigy run workflow.yml --param timeout=300\n\n# String type - use quotes\nprodigy run workflow.yml --param environment=\"production\"\n\n# Boolean type - no quotes\nprodigy run workflow.yml --param enable_debug=true\n</code></pre></p> </li> <li> <p>Check parameter file format: <pre><code>{\n  \"timeout\": 300,        // Number (no quotes)\n  \"environment\": \"prod\", // String (quotes)\n  \"debug\": true          // Boolean (no quotes)\n}\n</code></pre></p> </li> </ol>"},{"location":"composition/troubleshooting/#base-workflow-resolution-failures","title":"Base Workflow Resolution Failures","text":"<p>Error: <pre><code>Error: Failed to resolve base workflow: base-config.yml\n</code></pre></p> <p>Cause: Extended workflow file not found.</p> <p>Solutions:</p> <ol> <li> <p>Verify extends path: <pre><code># Relative to current workflow file\nextends: \"base-config.yml\"         # Same directory\nextends: \"../base/config.yml\"      # Parent directory\nextends: \"shared/base-config.yml\"  # Subdirectory\n</code></pre></p> </li> <li> <p>Use absolute path: <pre><code>extends: \"/full/path/to/base-config.yml\"\n</code></pre></p> </li> <li> <p>Check file exists: <pre><code>ls -la base-config.yml\n</code></pre></p> </li> </ol>"},{"location":"composition/troubleshooting/#parameter-substitution-issues","title":"Parameter Substitution Issues","text":"<p>Error: <pre><code>Workflow runs but ${param} appears literally in output\n</code></pre></p> <p>Cause: Parameter not found in the parameters map, or incorrect syntax.</p> <p>Status: Parameter substitution is fully implemented and works in all command types.</p> <p>Source: Implemented in src/cook/workflow/composition/composer.rs:760-877 (supports Simple, Structured, WorkflowStep, and SimpleObject command types)</p> <p>Solutions:</p> <ol> <li> <p>Verify parameter is defined: <pre><code>parameters:\n  definitions:\n    environment:\n      type: String\n\n# Then use in commands\ncommands:\n  - shell: \"echo Deploying to ${environment}\"\n  - claude: \"/deploy ${environment}\"\n</code></pre></p> </li> <li> <p>Check parameter is provided: <pre><code># Parameter must be passed at runtime\nprodigy run workflow.yml --param environment=\"production\"\n</code></pre></p> </li> <li> <p>Use correct syntax: <pre><code># Correct: ${param_name}\n- shell: \"process ${target_file}\"\n\n# Incorrect: $param_name (shell variable, not Prodigy parameter)\n- shell: \"process $target_file\"\n</code></pre></p> </li> <li> <p>Parameter substitution works in all command types: <pre><code>commands:\n  # Simple string commands\n  - \"shell: process ${file}\"\n\n  # Structured commands\n  - name: \"process\"\n    args: [\"${file}\", \"${output}\"]\n\n  # WorkflowStep format\n  - shell: \"test ${file}\"\n    id: \"test-${file}\"\n\n  # SimpleObject format\n  - name: \"build\"\n    args: [\"${target}\"]\n</code></pre></p> </li> </ol> <p>Supported value types: - Strings: Used as-is - Numbers: Converted to string representation - Booleans: Converted to \"true\" or \"false\" - Arrays/Objects: Serialized as JSON - Null: Becomes empty string</p> <p>See also: Parameter Definitions for parameter syntax reference.</p>"},{"location":"composition/troubleshooting/#sub-workflow-execution-issues","title":"Sub-Workflow Execution Issues","text":"<p>Error: <pre><code>Sub-workflows defined but not executing as expected\n</code></pre></p> <p>Status: Sub-workflow execution is fully implemented via SubWorkflowExecutor.</p> <p>Source: Implemented in src/cook/workflow/composition/sub_workflow.rs:67-176</p> <p>Common Issues:</p> <ol> <li>Sub-workflow file path incorrect: <pre><code># Verify the source path exists\nworkflows:\n  build:\n    source: \"workflows/build.yml\"  # Must exist relative to current file\n</code></pre></li> </ol> <pre><code># Check file exists\nls workflows/build.yml\n</code></pre> <ol> <li> <p>Parameter type mismatch: <pre><code># Sub-workflow parameters must match defined types\nworkflows:\n  deploy:\n    source: \"deploy.yml\"\n    parameters:\n      timeout: 300        # Number, not \"300\"\n      environment: \"prod\" # String with quotes\n</code></pre></p> </li> <li> <p>Input/output mapping errors: <pre><code># Input variables must exist in parent context\nworkflows:\n  test:\n    source: \"test.yml\"\n    inputs:\n      target_file: \"build_output\"  # Parent var 'build_output' must exist\n    outputs:\n      - \"test_result\"  # Will be available in parent after execution\n</code></pre></p> </li> <li> <p>Timeout too short: <pre><code>workflows:\n  long_running:\n    source: \"build.yml\"\n    timeout: 60  # Seconds - may be too short\n\n# Increase if sub-workflow times out\nworkflows:\n  long_running:\n    source: \"build.yml\"\n    timeout: 600  # 10 minutes\n</code></pre></p> </li> </ol> <p>Supported features: - Parameter passing (JSON values) - Input/output variable mapping - Context isolation (sub-workflow has clean context) - Error handling with <code>continue_on_error</code> flag - Timeout support - Parallel execution with <code>parallel: true</code></p> <p>See also: Sub-Workflows for complete usage guide.</p>"},{"location":"composition/troubleshooting/#default-values-not-applied","title":"Default Values Not Applied","text":"<p>Error: <pre><code>Parameters require values even though defaults are set\n</code></pre></p> <p>Status: Default values are fully applied through the apply_defaults method.</p> <p>Source: Implemented in src/cook/workflow/composition/composer.rs:217-257</p> <p>How defaults work:</p> <ol> <li> <p>Workflow-level defaults are applied to environment variables: <pre><code>defaults:\n  TIMEOUT: \"300\"\n  ENVIRONMENT: \"dev\"\n\n# These become available as env vars in all commands\ncommands:\n  - shell: \"echo Timeout: $TIMEOUT\"  # Uses default\n</code></pre></p> </li> <li> <p>Parameter-level defaults work differently: <pre><code>parameters:\n  definitions:\n    timeout:\n      type: Number\n      default: 300  # Used if not provided at runtime\n\n# Run without providing timeout\nprodigy run workflow.yml  # Uses default 300\n</code></pre></p> </li> <li> <p>Precedence order (highest to lowest):</p> </li> <li>Explicitly provided parameter values (--param)</li> <li>Parameter definition defaults</li> <li>Workflow-level defaults</li> <li>No value (error if required parameter)</li> </ol> <p>Common mistakes:</p> <ol> <li> <p>Workflow defaults don't set parameter values: <pre><code># This does NOT work as expected\ndefaults:\n  timeout: 300  # Sets env var TIMEOUT, not parameter 'timeout'\n\nparameters:\n  required:\n    - timeout  # Still required!\n\n# Solution: Use parameter default instead\nparameters:\n  definitions:\n    timeout:\n      type: Number\n      default: 300  # Now parameter has a default\n</code></pre></p> </li> <li> <p>Existing values are not overwritten: <pre><code># If a value is already set, defaults don't override\nparameters:\n  definitions:\n    timeout:\n      type: Number\n      default: 300  # Only used if not already set\n\n# This overrides the default\nprodigy run workflow.yml --param timeout=600\n</code></pre></p> </li> </ol> <p>See also: Default Values for complete default value semantics.</p>"},{"location":"composition/troubleshooting/#url-template-source-errors","title":"URL Template Source Errors","text":"<p>Error: <pre><code>Error: URL template sources are not yet implemented\n</code></pre></p> <p>Cause: URL-based template loading is planned but not implemented.</p> <p>Solutions:</p> <ol> <li> <p>Download template to file: <pre><code>curl https://example.com/template.yml &gt; /tmp/template.yml\n</code></pre></p> </li> <li> <p>Use file-based template: <pre><code>template:\n  source:\n    file: \"/tmp/template.yml\"\n</code></pre></p> </li> <li> <p>Add to registry: <pre><code>curl https://example.com/template.yml &gt; ~/.prodigy/templates/template.yml\n</code></pre></p> </li> </ol> <pre><code>template:\n  source:\n    registry: \"template\"\n</code></pre>"},{"location":"composition/troubleshooting/#debugging-strategies","title":"Debugging Strategies","text":"<p>Enable Verbose Logging: <pre><code># Show composition steps\nprodigy run workflow.yml -v\n\n# Show detailed debug output\nprodigy run workflow.yml -vv\n\n# Show trace-level output (includes full composition details)\nprodigy run workflow.yml -vvv\n</code></pre></p> <p>Dry-Run Validation: <pre><code># Validate composition without execution\nprodigy run workflow.yml --dry-run\n\n# Combine with verbose mode to see composition steps\nprodigy run workflow.yml --dry-run -vv\n</code></pre></p> <p>Isolate Composition Layers: <pre><code># Test base workflow alone\nprodigy run base-config.yml --dry-run\n\n# Add one composition feature at a time\n# 1. Test with imports only\n# 2. Add extends\n# 3. Add template\n# 4. Add parameters\n</code></pre></p> <p>Check File Permissions: <pre><code># Verify read access\nls -la workflow.yml base-config.yml\n\n# Check registry permissions\nls -la ~/.prodigy/templates/\nls -la .prodigy/templates/\n</code></pre></p> <p>Verify JSON Syntax: <pre><code># Validate param file\njq . params.json\n\n# Check for syntax errors\ncat params.json | jq empty\n</code></pre></p> <p>Trace Parameter Substitution: <pre><code># Use verbose mode to see parameter values\nprodigy run workflow.yml --param environment=\"prod\" -vv\n\n# Check which parameters are being substituted\n</code></pre></p> <p>Debug Sub-Workflow Execution: <pre><code># Test sub-workflow independently first\nprodigy run workflows/build.yml --dry-run\n\n# Then test from parent workflow\nprodigy run main.yml -vv  # See sub-workflow execution logs\n</code></pre></p>"},{"location":"composition/troubleshooting/#getting-help","title":"Getting Help","text":"<p>If issues persist:</p> <ol> <li>Check implementation status in relevant subsection docs</li> <li>Review error context in error messages</li> <li>Use verbose mode (-vv or -vvv) to understand what's happening</li> <li>Test components independently (base workflows, sub-workflows, templates)</li> <li>File issue with minimal reproduction case</li> <li>Include:</li> <li>Workflow files</li> <li>Command used</li> <li>Full error output</li> <li>Prodigy version</li> <li>Output from verbose mode (-vv)</li> </ol>"},{"location":"composition/workflow-extension-inheritance/","title":"Workflow Extension (Inheritance)","text":""},{"location":"composition/workflow-extension-inheritance/#workflow-extension-inheritance","title":"Workflow Extension (Inheritance)","text":"<p>Extend a base workflow to inherit its configuration. Child workflows override parent values, allowing you to customize specific aspects while maintaining common configuration. This enables environment-specific variations and layered configuration management.</p>"},{"location":"composition/workflow-extension-inheritance/#basic-extension-syntax","title":"Basic Extension Syntax","text":"<pre><code># production.yml\nname: production-deployment\nmode: standard\n\n# Inherit from base workflow\nextends: \"base-deployment.yml\"\n\n# Override specific values\nenv:\n  ENVIRONMENT: \"production\"\n  REPLICAS: \"5\"\n</code></pre>"},{"location":"composition/workflow-extension-inheritance/#how-extension-works","title":"How Extension Works","text":"<p>When a workflow extends a base workflow:</p> <ol> <li>Base workflow is loaded from the specified path</li> <li>Child values override parent for matching keys</li> <li>Parent values are preserved where child doesn't override</li> <li>Merging is deep - nested objects merge recursively</li> <li>Arrays are replaced - child arrays replace parent arrays entirely</li> </ol>"},{"location":"composition/workflow-extension-inheritance/#extension-vs-imports","title":"Extension vs Imports","text":"Feature Extension (<code>extends</code>) Imports Purpose Inherit and customize base workflow Reuse workflow components Relationship Parent-child hierarchy Modular composition Override behavior Child overrides parent Imports merge with main Use case Environment variations Shared utilities"},{"location":"composition/workflow-extension-inheritance/#multi-environment-example","title":"Multi-Environment Example","text":"<p>base-deployment.yml (shared configuration): <pre><code>name: base-deployment\nmode: standard\n\nenv:\n  APP_NAME: \"my-service\"\n  REPLICAS: \"1\"\n  LOG_LEVEL: \"info\"\n\ncommands:\n  - shell: \"docker build -t ${APP_NAME}:${VERSION} .\"\n  - shell: \"kubectl apply -f k8s/${ENVIRONMENT}/deployment.yml\"\n  - shell: \"kubectl scale deployment ${APP_NAME} --replicas=${REPLICAS}\"\n</code></pre></p> <p>dev.yml (development environment): <pre><code>name: dev-deployment\nextends: \"base-deployment.yml\"\n\nenv:\n  ENVIRONMENT: \"dev\"\n  REPLICAS: \"1\"\n  LOG_LEVEL: \"debug\"\n\n# Inherits all commands from base\n</code></pre></p> <p>staging.yml (staging environment): <pre><code>name: staging-deployment\nextends: \"base-deployment.yml\"\n\nenv:\n  ENVIRONMENT: \"staging\"\n  REPLICAS: \"3\"\n  LOG_LEVEL: \"info\"\n\n# Additional staging-specific commands\ncommands:\n  - shell: \"run-smoke-tests.sh\"\n</code></pre></p> <p>production.yml (production environment): <pre><code>name: production-deployment\nextends: \"base-deployment.yml\"\n\nenv:\n  ENVIRONMENT: \"production\"\n  REPLICAS: \"5\"\n  LOG_LEVEL: \"warn\"\n  ENABLE_MONITORING: \"true\"\n\n# Additional production safeguards\ncommands:\n  - shell: \"verify-release-notes.sh\"\n  - shell: \"notify-team 'Deploying to production'\"\n</code></pre></p>"},{"location":"composition/workflow-extension-inheritance/#merge-behavior","title":"Merge Behavior","text":"<p>Scalar Values - Child replaces parent: <pre><code># base.yml\ntimeout: 300\n\n# child.yml\nextends: \"base.yml\"\ntimeout: 600  # Replaces 300 with 600\n</code></pre></p> <p>Objects - Deep merge: <pre><code># base.yml\nenv:\n  APP_NAME: \"service\"\n  LOG_LEVEL: \"info\"\n\n# child.yml\nextends: \"base.yml\"\nenv:\n  LOG_LEVEL: \"debug\"  # Overrides\n  NEW_VAR: \"value\"     # Adds\n\n# Result:\nenv:\n  APP_NAME: \"service\"     # From base\n  LOG_LEVEL: \"debug\"      # Overridden\n  NEW_VAR: \"value\"        # Added\n</code></pre></p> <p>Environment Variables - Deep merge with selective override: <pre><code># base.yml\nenv:\n  APP_NAME: \"service\"\n  LOG_LEVEL: \"info\"\n  REPLICAS: \"3\"\n\n# child.yml\nextends: \"base.yml\"\nenv:\n  LOG_LEVEL: \"debug\"  # Overrides parent value\n  NEW_VAR: \"value\"     # Adds new variable\n\n# Result:\nenv:\n  APP_NAME: \"service\"     # Preserved from parent\n  LOG_LEVEL: \"debug\"      # Overridden by child\n  REPLICAS: \"3\"           # Preserved from parent\n  NEW_VAR: \"value\"        # Added by child\n</code></pre></p> <p>Source: Environment variables inherit the object deep merge behavior (src/cook/workflow/composition/composer.rs:325-355)</p> <p>Arrays (Commands) - Child replaces parent: <pre><code># base.yml\ncommands:\n  - shell: \"step1\"\n  - shell: \"step2\"\n\n# child.yml\nextends: \"base.yml\"\ncommands:\n  - shell: \"custom-step\"  # Completely replaces base commands\n\n# Result: Only custom-step runs\n</code></pre></p> <p>Source: In inheritance mode (<code>extends</code>), non-empty child command arrays replace parent arrays entirely (src/cook/workflow/composition/composer.rs:335-337)</p> <p>Note: This behavior differs from imports, where <code>merge_workflows</code> extends arrays instead of replacing them (src/cook/workflow/composition/composer.rs:275-323). See Workflow Imports for comparison.</p>"},{"location":"composition/workflow-extension-inheritance/#layered-extension","title":"Layered Extension","text":"<p>Workflows can extend workflows that themselves extend other workflows:</p> <pre><code># base.yml\nname: base-config\ntimeout: 300\n\n# intermediate.yml\nextends: \"base.yml\"\ntimeout: 600\nmax_parallel: 5\n\n# final.yml\nextends: \"intermediate.yml\"\nmax_parallel: 10\n\n# Result: timeout=600 (from intermediate), max_parallel=10 (from final)\n</code></pre>"},{"location":"composition/workflow-extension-inheritance/#path-resolution","title":"Path Resolution","text":"<p>When resolving base workflow paths, Prodigy searches the following locations in order:</p> <ol> <li><code>./bases/{name}.yml</code></li> <li><code>./templates/{name}.yml</code></li> <li><code>./workflows/{name}.yml</code></li> <li><code>./{name}.yml</code> (current directory)</li> </ol> <p>Source: Path resolution implementation in src/cook/workflow/composition/composer.rs:625-642</p> <p>Extension paths can be: - Relative: Resolved from workflow file's directory - Absolute: Full filesystem path - Registry: Future support for template registry paths</p> <pre><code># Name-based lookup (searches standard directories)\nextends: \"base-deployment\"  # Searches bases/, templates/, workflows/, ./\n\n# Relative path\nextends: \"../shared/base.yml\"\n\n# Absolute path\nextends: \"/etc/prodigy/workflows/base.yml\"\n</code></pre> <p>Search Order Example: <pre><code># For extends: \"ci-base\"\n# Prodigy searches:\n./bases/ci-base.yml       # First\n./templates/ci-base.yml   # Second\n./workflows/ci-base.yml   # Third\n./ci-base.yml             # Fourth (current directory)\n# Error if not found in any location\n</code></pre></p>"},{"location":"composition/workflow-extension-inheritance/#use-cases","title":"Use Cases","text":"<p>Environment-Specific Deployments: - Share common deployment steps - Override environment variables per environment - Customize resource limits (replicas, memory, CPU)</p> <p>Testing Variations: <pre><code># base-test.yml\nname: base-test\ncommands:\n  - shell: \"cargo build\"\n  - shell: \"cargo test\"\n\n# integration-test.yml\nextends: \"base-test.yml\"\nenv:\n  DATABASE_URL: \"postgres://localhost/test\"\ncommands:\n  - shell: \"setup-test-db.sh\"\n  # Runs instead of base commands\n\n# unit-test.yml\nextends: \"base-test.yml\"\nenv:\n  RUST_TEST_THREADS: \"1\"\n</code></pre></p> <p>Progressive Configuration: - Start with minimal base config - Add complexity in child workflows - Keep each layer focused on specific concerns</p>"},{"location":"composition/workflow-extension-inheritance/#circular-dependency-protection","title":"Circular Dependency Protection","text":"<p>Prodigy detects and prevents circular dependencies:</p> <pre><code># workflow-a.yml\nextends: \"workflow-b.yml\"\n\n# workflow-b.yml\nextends: \"workflow-a.yml\"\n\n# Error: Circular dependency detected\n</code></pre>"},{"location":"composition/workflow-extension-inheritance/#complete-example","title":"Complete Example","text":"<p>base-ci.yml: <pre><code>name: base-ci\nmode: standard\n\nenv:\n  RUST_BACKTRACE: \"1\"\n\ncommands:\n  - shell: \"cargo fmt --check\"\n  - shell: \"cargo clippy\"\n  - shell: \"cargo test\"\n</code></pre></p> <p>pr-ci.yml (runs on pull requests): <pre><code>name: pr-ci\nextends: \"base-ci.yml\"\n\nenv:\n  CARGO_INCREMENTAL: \"0\"  # Faster CI builds\n\n# Inherits format, clippy, test from base\n</code></pre></p> <p>release-ci.yml (runs on release): <pre><code>name: release-ci\nextends: \"base-ci.yml\"\n\nenv:\n  CARGO_INCREMENTAL: \"0\"\n\ncommands:\n  - shell: \"cargo build --release\"\n  - shell: \"cargo test --release\"\n  - shell: \"cargo publish --dry-run\"\n</code></pre></p>"},{"location":"composition/workflow-extension-inheritance/#debugging-extensions","title":"Debugging Extensions","text":"<p>Current Method - Enable verbose logging to see composition details:</p> <pre><code># Use -vvv for trace-level logging showing composition process\nprodigy run workflow.yml -vvv\n\n# Combine with --dry-run to preview without execution\nprodigy run workflow.yml --dry-run -vvv\n</code></pre> <p>Verbose logging output includes: - Base workflow loading events - Inheritance chain resolution - Merge operations for each configuration section - Circular dependency checks - Path resolution steps</p> <p>Source: Composition events logged via <code>tracing::debug!()</code> macros throughout src/cook/workflow/composition/composer.rs</p> <p>Planned Feature - Dedicated composition inspection flag (not yet implemented):</p> <pre><code># Future: --show-composition flag (Spec 131-133)\nprodigy run workflow.yml --dry-run --show-composition\n</code></pre> <p>This will display structured composition metadata including: - Sources and dependency types - Complete inheritance chain - Applied parameters and templates - Resolved paths for all dependencies</p> <p>See Composition Metadata for details on the metadata structure.</p>"},{"location":"composition/workflow-extension-inheritance/#implementation-status","title":"Implementation Status","text":"<ul> <li>\u2705 Base workflow loading</li> <li>\u2705 Deep merge of child and parent configurations</li> <li>\u2705 Circular dependency detection</li> <li>\u2705 Path resolution (relative and absolute)</li> <li>\u2705 Composition metadata tracking</li> <li>\u2705 Workflow caching - Files are cached during composition to improve performance and avoid redundant file system reads (src/cook/workflow/composition/composer.rs:662-698)</li> </ul>"},{"location":"composition/workflow-extension-inheritance/#related-topics","title":"Related Topics","text":"<ul> <li>Workflow Imports - Modular composition</li> <li>Template System - Parameterized workflows</li> <li>Composition Metadata - Inspect composition details</li> </ul>"},{"location":"configuration/","title":"Configuration","text":"<p>Prodigy supports comprehensive configuration through multiple files with a clear precedence hierarchy. This chapter explains all configuration options and how to use them effectively.</p>"},{"location":"configuration/#quick-start","title":"Quick Start","text":"<p>Prodigy uses two distinct types of configuration files:</p> <ol> <li>Project Configuration (<code>.prodigy/config.yml</code>) - Project settings and metadata</li> <li>Workflow Configuration (<code>.prodigy/workflow.yml</code> or explicit path) - Workflow definitions</li> </ol>"},{"location":"configuration/#minimal-project-configuration","title":"Minimal Project Configuration","text":"<p>Create <code>.prodigy/config.yml</code>:</p> <pre><code>name: my-project  # Required: Project identifier\n</code></pre> <p>The <code>name</code> field is the only required field. All other settings have sensible defaults.</p> <p>Source: ProjectConfig struct in src/config/mod.rs:66-74</p>"},{"location":"configuration/#minimal-workflow-configuration","title":"Minimal Workflow Configuration","text":"<p>Create <code>.prodigy/workflow.yml</code> or any <code>.yml</code> file:</p> <pre><code>commands:  # List of commands to execute in sequence\n  - prodigy-code-review  # Slash prefix is optional in workflow files\n  - /prodigy-lint        # Both styles work the same\n</code></pre> <p>Workflows define a sequence of commands to execute. Each command is a Prodigy slash command - the <code>/</code> prefix is optional in workflow files.</p> <p>Source: WorkflowConfig parsing in src/core/config/mod.rs:11-22, Examples in examples/mapreduce-json-input.yml</p> <p>That's all you need to get started! Prodigy provides sensible defaults for everything else. See the subsections below for detailed configuration options.</p>"},{"location":"configuration/#configuration-file-locations","title":"Configuration File Locations","text":"<p>Prodigy uses a search hierarchy to find configuration files. Configuration can come from multiple sources with the following precedence (highest to lowest):</p> <ol> <li>CLI Flags - Command-line arguments override all other settings</li> <li>Environment Variables - Environment variables (e.g., <code>PRODIGY_CLAUDE_API_KEY</code>)</li> <li>Project Config - <code>.prodigy/config.yml</code> in your project directory</li> <li>Global Config - <code>~/.prodigy/config.yml</code> in your home directory</li> <li>Defaults - Built-in default values</li> </ol> <p>Source: ConfigLoader.load_with_explicit_path() in src/config/loader.rs:31-55</p>"},{"location":"configuration/#workflow-file-search-hierarchy","title":"Workflow File Search Hierarchy","text":"<p>For workflow files specifically (different from project config):</p> <ol> <li>Explicit path - Path provided via <code>prodigy run path/to/workflow.yml</code> (error if not found)</li> <li>Default location - <code>.prodigy/workflow.yml</code> in the project directory (if exists)</li> <li>Built-in defaults - Default workflow configuration</li> </ol> <p>Source: ConfigLoader.load_with_explicit_path() in src/config/loader.rs:40-54</p>"},{"location":"configuration/#key-distinction-configyml-vs-workflowyml","title":"Key Distinction: config.yml vs workflow.yml","text":"<ul> <li><code>.prodigy/config.yml</code>: Contains project settings (name, version, API keys, editor preferences)</li> <li>Maps to <code>ProjectConfig</code> struct (src/config/mod.rs:66-74)</li> <li> <p>Loaded via ConfigLoader.load_project() (src/config/loader.rs:85-104)</p> </li> <li> <p><code>.prodigy/workflow.yml</code>: Contains workflow definitions (commands to execute)</p> </li> <li>Maps to <code>WorkflowConfig</code> struct (src/config/workflow.rs)</li> <li>Loaded via ConfigLoader.load_with_explicit_path() (src/config/loader.rs:35-55)</li> </ul> <p>Both can exist in the <code>.prodigy/</code> directory and serve different purposes.</p>"},{"location":"configuration/#configuration-architecture","title":"Configuration Architecture","text":"<p>Prodigy uses a three-tier configuration structure internally:</p> <pre><code>Config (Root)\n\u251c\u2500\u2500 GlobalConfig      - User-wide settings (~/.prodigy/config.yml)\n\u251c\u2500\u2500 ProjectConfig     - Project-specific settings (.prodigy/config.yml)\n\u2514\u2500\u2500 WorkflowConfig    - Workflow definitions (.prodigy/workflow.yml or explicit path)\n</code></pre> <p>Source: Config struct hierarchy in src/config/mod.rs:38-43</p> <p>How it works: 1. <code>Config::new()</code> creates the root with default <code>GlobalConfig</code> 2. <code>merge_project_config()</code> adds project-specific settings (src/core/config/mod.rs:36-40) 3. <code>merge_workflow_config()</code> adds workflow definitions (src/core/config/mod.rs:31-34) 4. <code>Config.merge_env_vars()</code> applies environment variable overrides (src/config/mod.rs:111-131)</p> <p>This design allows: - Separation of concerns: Global settings vs project settings vs workflows - Clear precedence: Project settings override global defaults - Environment overrides: Runtime configuration via env vars - Type safety: Each config tier has its own validated struct</p> <p>See Global Configuration Structure and Project Configuration Structure for detailed field definitions.</p>"},{"location":"configuration/#common-configuration-patterns","title":"Common Configuration Patterns","text":""},{"location":"configuration/#quick-validation","title":"Quick Validation","text":"<p>To verify which configuration Prodigy is using:</p> <pre><code># Check if config files exist\nls -la .prodigy/config.yml .prodigy/workflow.yml\n\n# Validate YAML syntax\nprodigy validate workflow.yml\n\n# Run with verbose output to see loaded configuration\nprodigy run workflow.yml -v\n</code></pre> <p>Validation rules (src/core/config/mod.rs:43-50): - Only <code>.yml</code> and <code>.yaml</code> extensions are supported (TOML is deprecated) - Config files must be valid YAML syntax - ProjectConfig requires <code>name</code> field - WorkflowConfig requires <code>commands</code> array</p>"},{"location":"configuration/#configuration-not-found","title":"Configuration Not Found?","text":"<p>If Prodigy doesn't find your configuration:</p> <ol> <li>Workflow file: Check explicit path vs <code>.prodigy/workflow.yml</code></li> <li>Explicit path: <code>prodigy run path/to/workflow.yml</code> (must exist or error)</li> <li> <p>Default location: <code>.prodigy/workflow.yml</code> (optional, uses defaults if missing)</p> </li> <li> <p>Project config: Must be at <code>.prodigy/config.yml</code> in project root</p> </li> <li>Prodigy searches upward from current directory for <code>.prodigy/</code> folder</li> <li> <p>Check you're running from within the project directory</p> </li> <li> <p>Global config: Optional, located at <code>~/.prodigy/config.yml</code></p> </li> <li>Use for API keys and editor preferences across all projects</li> </ol> <p>Source: ConfigLoader search logic in src/config/loader.rs:35-104</p>"},{"location":"configuration/#debugging-configuration-issues","title":"Debugging Configuration Issues","text":"<p>Common issues and solutions:</p> Issue Cause Solution \"Config not found\" File in wrong location Check <code>.prodigy/config.yml</code> exists in project root \"Invalid YAML\" Syntax error Validate YAML with online parser or <code>yamllint</code> \"Unknown field\" Typo in field name Check struct definitions in src/config/mod.rs Settings not applied Wrong precedence CLI flags &gt; env vars &gt; project &gt; global &gt; defaults Workflow not loaded Wrong file used Verify workflow.yml vs config.yml distinction"},{"location":"configuration/#additional-topics","title":"Additional Topics","text":"<p>See also: - Configuration Precedence Rules - Global Configuration Structure - Project Configuration Structure - Workflow Configuration - Storage Configuration - Environment Variables - Complete Configuration Examples - Default Values Reference - Best Practices - Troubleshooting - Migration Guide: TOML to YAML - Related Documentation</p>"},{"location":"configuration/best-practices/","title":"Best Practices","text":""},{"location":"configuration/best-practices/#best-practices","title":"Best Practices","text":"<p>This section provides practical guidance for configuring Prodigy workflows effectively, based on the implementation patterns and defaults in the codebase.</p>"},{"location":"configuration/best-practices/#start-with-sensible-defaults","title":"Start with Sensible Defaults","text":"<p>Prodigy provides carefully chosen defaults that work well for most use cases. You typically only need to configure exceptions.</p> <p>Default Storage Configuration (src/storage/config.rs:184-226): <pre><code># These defaults are already set - no need to configure unless you need different values\nstorage:\n  connection_pool_size: 10\n  timeout_secs: 30\n  max_retries: 3\n  initial_delay_ms: 1000\n  max_delay_ms: 30000\n  backoff_multiplier: 2.0\n  max_file_size_mb: 100\n  cache_size: 1000\n  cache_ttl_secs: 3600\n</code></pre></p> <p>Default Cleanup Configuration (src/cook/execution/mapreduce/cleanup/config.rs:39-87): <pre><code># Default preset: balanced cleanup\ncleanup:\n  auto_cleanup: true\n  cleanup_delay_secs: 30\n  max_worktrees_per_job: 50\n  max_total_worktrees: 200\n  disk_threshold_mb: 1024\n</code></pre></p> <p>When to Override Defaults: - Storage: Only if you have specific latency/throughput requirements - Cleanup: Use \"aggressive\" preset for disk-constrained environments - Timeouts: Increase for long-running operations</p>"},{"location":"configuration/best-practices/#use-configuration-layering-effectively","title":"Use Configuration Layering Effectively","text":"<p>Prodigy uses a three-tier configuration hierarchy (src/config/mod.rs:102-154):</p> <p>Configuration Precedence (highest to lowest): 1. Workflow configuration - Specific to a single workflow YAML file 2. Project configuration - <code>.prodigy/config.toml</code> in your project 3. Global configuration - <code>~/.prodigy/config.toml</code> in your home directory 4. Built-in defaults - Sensible defaults from the codebase</p> <p>Recommended Usage: <pre><code># Global config (~/.prodigy/config.toml): User-wide preferences\n[global]\nlog_level = \"info\"\nauto_commit = true\n\n# Project config (.prodigy/config.toml): Project-specific overrides\n[project]\nclaude_api_key = \"${CLAUDE_API_KEY}\"  # Environment variable reference\nauto_commit = false  # Override global setting\n\n# Workflow YAML: Workflow-specific settings\nenv:\n  MAX_PARALLEL: 5\n  PROJECT_NAME: \"my-project\"\n</code></pre></p> <p>See Configuration Precedence Rules for detailed examples.</p>"},{"location":"configuration/best-practices/#parameterize-with-environment-variables","title":"Parameterize with Environment Variables","text":"<p>Use environment variables for flexible, reusable workflows (src/config/mapreduce.rs:395-449).</p> <p>Variable Syntax: <pre><code>env:\n  PROJECT_NAME: \"prodigy\"\n  MAX_PARALLEL: 3\n  API_URL: \"https://api.example.com\"\n\nmap:\n  max_parallel: $MAX_PARALLEL  # Simple reference\n  agent_template:\n    - shell: \"echo Processing ${PROJECT_NAME}\"  # Bracketed reference\n    - shell: \"curl ${API_URL}/endpoint\"\n</code></pre></p> <p>Resolution Order (src/config/mapreduce.rs:395-449): 1. Workflow <code>env:</code> block 2. System environment variables 3. Error if not found</p> <p>Type Flexibility (src/config/mapreduce.rs:354-393): <pre><code># Both numeric literals and environment variables are supported\nmap:\n  max_parallel: 3              # Numeric literal\n  max_parallel: $MAX_PARALLEL  # Environment variable\n</code></pre></p> <p>See Environment Variables for comprehensive documentation.</p>"},{"location":"configuration/best-practices/#protect-sensitive-values","title":"Protect Sensitive Values","text":"<p>Prodigy automatically masks common secret patterns in logs (src/cook/environment/config.rs:78-119).</p> <p>Automatic Masking Patterns: - API keys (contains \"api\" or \"key\") - Tokens (contains \"token\") - Passwords (contains \"password\", \"pwd\", \"secret\") - Auth values (contains \"auth\")</p> <p>Example: <pre><code>env:\n  API_KEY: \"sk-abc123def456\"      # Auto-masked as \"***\" in logs\n  PASSWORD: \"hunter2\"              # Auto-masked\n  AUTH_TOKEN: \"ghp_abcdefg\"        # Auto-masked\n  NORMAL_VAR: \"visible-value\"      # Not masked\n</code></pre></p> <p>Explicit Secret Marking: <pre><code>env:\n  CUSTOM_SECRET:\n    secret: true\n    value: \"my-sensitive-value\"\n</code></pre></p> <p>Best Practices: 1. Never hardcode secrets in workflow files 2. Use environment variable references: <code>${SECRET_NAME}</code> 3. Store secrets in system environment or <code>.env</code> files (excluded from git) 4. Mark custom sensitive fields with <code>secret: true</code> 5. Review logs to verify sensitive values are masked</p>"},{"location":"configuration/best-practices/#use-profiles-for-different-environments","title":"Use Profiles for Different Environments","text":"<p>Profiles allow environment-specific configuration (src/cook/environment/config.rs:146-162):</p> <pre><code>env:\n  API_URL:\n    default: \"http://localhost:3000\"\n    staging: \"https://staging-api.example.com\"\n    prod: \"https://api.example.com\"\n\n  MAX_PARALLEL:\n    default: 2\n    prod: 10\n\n  DEBUG_MODE:\n    default: true\n    prod: false\n</code></pre> <p>Activate a profile: <pre><code>prodigy run workflow.yml --profile prod\n</code></pre></p> <p>Use cases: - Development vs production API endpoints - Resource limits (lower parallelism in dev) - Feature flags (enable debug logging in dev) - Storage backends (local files vs S3 in prod)</p>"},{"location":"configuration/best-practices/#configure-storage-appropriately","title":"Configure Storage Appropriately","text":"<p>Global Storage (Default - Recommended): <pre><code>storage:\n  use_global: true  # Default\n  base_path: \"~/.prodigy\"  # Default\n</code></pre></p> <p>Benefits: - Cross-worktree event aggregation for parallel jobs - Persistent state survives worktree cleanup - Centralized monitoring and debugging - Efficient storage deduplication</p> <p>Local Storage (Deprecated): <pre><code>storage:\n  use_global: false  # \u26a0\ufe0f Deprecated\n  base_path: \".prodigy\"\n</code></pre></p> <p>Warning: Local storage is deprecated (src/storage/config.rs:71-73). Use global storage unless you have specific isolation requirements.</p> <p>Environment Variable Fallbacks (src/storage/config.rs:244-286): <pre><code># Precedence: highest to lowest\nexport PRODIGY_STORAGE_TYPE=\"file\"        # or \"memory\"\nexport PRODIGY_STORAGE_BASE_PATH=\"/data/.prodigy\"\nexport PRODIGY_STORAGE_DIR=\"/tmp/.prodigy\"  # Fallback\nexport PRODIGY_STORAGE_PATH=\"/var/.prodigy\"  # Secondary fallback\n</code></pre></p> <p>See Storage Configuration for detailed options.</p>"},{"location":"configuration/best-practices/#set-appropriate-timeouts","title":"Set Appropriate Timeouts","text":"<p>Configure timeouts based on operation characteristics (src/app/config.rs:10-63):</p> <p>Workflow Timeouts: <pre><code># Default timeout: 30 seconds per command\ncommands:\n  - shell: \"quick-test\"  # Uses default\n  - shell: \"long-build\"\n    timeout_secs: 600    # 10 minutes for long operations\n</code></pre></p> <p>MapReduce Timeouts: <pre><code>map:\n  agent_timeout_secs: 300  # 5 minutes per agent\n  max_parallel: 5\n</code></pre></p> <p>Retry Configuration: <pre><code>retry_config:\n  max_attempts: 3           # Default\n  initial_delay_ms: 1000    # 1 second\n  max_delay_ms: 30000       # 30 seconds cap\n  backoff: exponential      # Backoff strategy\n</code></pre></p> <p>Backoff Strategies (src/storage/config.rs:184-226): - <code>exponential</code>: Delay doubles each retry (2^n * initial_delay) - <code>linear</code>: Delay increases linearly (n * initial_delay) - <code>fibonacci</code>: Delay follows fibonacci sequence</p>"},{"location":"configuration/best-practices/#prefer-simplified-yaml-syntax","title":"Prefer Simplified YAML Syntax","text":"<p>Prodigy supports both simplified and verbose syntax (src/config/mapreduce.rs:287-351).</p> <p>Simplified Syntax (Recommended): <pre><code>map:\n  agent_template:\n    - claude: \"/process ${item.path}\"\n    - shell: \"test -f ${item.path}\"\n</code></pre></p> <p>Verbose Syntax (Backward Compatible): <pre><code>map:\n  agent_template:\n    commands:  # \u26a0\ufe0f Deprecated nested array\n      - claude: \"/process ${item.path}\"\n      - shell: \"test -f ${item.path}\"\n</code></pre></p> <p>Why simplified is better: - Less indentation - Clearer intent - Matches standard workflow syntax - Forward-compatible</p>"},{"location":"configuration/best-practices/#validate-configuration-early","title":"Validate Configuration Early","text":"<p>Type Safety (src/config/mapreduce.rs:354-393): - Workflow YAML is validated at parse time - Clear error messages include context - Type mismatches caught before execution</p> <p>Example Error Message: <pre><code>Error: Environment variable 'MAX_PARALLEL' not found\nContext: Required by map.max_parallel in workflow.yml:12\nResolution order: workflow env \u2192 system environment\n</code></pre></p> <p>Configuration Debugging: <pre><code># View effective configuration after all precedence rules\nprodigy config show\n\n# Validate workflow without running\nprodigy validate workflow.yml\n</code></pre></p>"},{"location":"configuration/best-practices/#common-configuration-patterns","title":"Common Configuration Patterns","text":"<p>Capture Command Output: <pre><code>setup:\n  - shell: \"git rev-parse HEAD\"\n    capture: \"commit_hash\"\n  - shell: \"echo 'Building commit ${commit_hash}'\"\n</code></pre></p> <p>Conditional Execution: <pre><code>map:\n  filter: \"item.priority &gt;= 5\"  # Only process high-priority items\n  sort_by: \"item.priority DESC\"  # Process highest priority first\n</code></pre></p> <p>Error Handling: <pre><code>map:\n  agent_template:\n    - shell: \"risky-operation ${item.id}\"\n      on_failure:\n        - claude: \"/diagnose-failure ${item.id}\"\n</code></pre></p> <p>Resource Limits: <pre><code>map:\n  max_parallel: 10              # Concurrent agents\n  max_items: 100                # Limit total items processed\n  agent_timeout_secs: 300       # Per-agent timeout\n</code></pre></p>"},{"location":"configuration/best-practices/#anti-patterns-to-avoid","title":"Anti-Patterns to Avoid","text":"<p>\u274c Don't: Hardcode Paths <pre><code>commands:\n  - shell: \"cd /Users/alice/project &amp;&amp; make test\"\n</code></pre></p> <p>\u2705 Do: Use Variables <pre><code>env:\n  PROJECT_DIR: \"${PWD}\"\ncommands:\n  - shell: \"cd ${PROJECT_DIR} &amp;&amp; make test\"\n</code></pre></p> <p>\u274c Don't: Expose Secrets <pre><code>env:\n  API_KEY: \"sk-real-key-here\"  # Committed to git!\n</code></pre></p> <p>\u2705 Do: Reference Environment <pre><code>env:\n  API_KEY: \"${CLAUDE_API_KEY}\"  # From system environment\n</code></pre></p> <p>\u274c Don't: Disable Global Storage Unnecessarily <pre><code>storage:\n  use_global: false  # Deprecated and limits functionality\n</code></pre></p> <p>\u2705 Do: Use Default Global Storage <pre><code># No storage configuration needed - global is default\n</code></pre></p> <p>\u274c Don't: Skip Timeouts for Long Operations <pre><code>commands:\n  - shell: \"npm install\"  # May hang indefinitely\n</code></pre></p> <p>\u2705 Do: Set Explicit Timeouts <pre><code>commands:\n  - shell: \"npm install\"\n    timeout_secs: 300\n</code></pre></p>"},{"location":"configuration/best-practices/#troubleshooting-configuration-issues","title":"Troubleshooting Configuration Issues","text":"<p>Configuration Not Found: <pre><code># Check search paths\nprodigy config show --verbose\n\n# Verify file exists and is valid YAML\ncat .prodigy/config.toml\nyamllint workflow.yml\n</code></pre></p> <p>Wrong Precedence: <pre><code># View effective configuration\nprodigy config show\n\n# Check which file is setting a value\nPRODIGY_LOG_LEVEL=debug prodigy run workflow.yml -v\n</code></pre></p> <p>Environment Variable Not Resolved: <pre><code># Check variable is defined\necho $MY_VAR\n\n# Use correct syntax: $VAR or ${VAR}\n# NOT: %VAR% (Windows) or $env:VAR (PowerShell)\n</code></pre></p> <p>Timeout Too Short: <pre><code># Increase timeouts for slow operations\ncommands:\n  - shell: \"cargo build --release\"\n    timeout_secs: 600  # 10 minutes\n</code></pre></p> <p>See Troubleshooting for more configuration issues and solutions.</p>"},{"location":"configuration/best-practices/#advanced-configuration","title":"Advanced Configuration","text":"<p>Custom Cleanup Presets: <pre><code># Aggressive cleanup for CI/CD\ncleanup:\n  preset: \"aggressive\"\n  cleanup_delay_secs: 5\n  max_worktrees_per_job: 20\n  disk_threshold_mb: 512\n</code></pre></p> <p>Custom Storage Backend: <pre><code>storage:\n  type: \"file\"  # or \"memory\" for testing\n  base_path: \"/mnt/fast-disk/.prodigy\"\n  connection_pool_size: 20  # High-concurrency workloads\n</code></pre></p> <p>Workflow Validation: <pre><code>validation:\n  threshold: 80  # Minimum 80% success rate\n  timeout_secs: 600\n  output_schema: \"output-schema.json\"  # Validate JSON outputs\n</code></pre></p>"},{"location":"configuration/best-practices/#summary","title":"Summary","text":"<p>Key Takeaways: 1. Trust the defaults - They're production-tested and sensible 2. Layer your config - Global \u2192 Project \u2192 Workflow hierarchy 3. Parameterize everything - Use environment variables for flexibility 4. Protect secrets - Automatic masking + explicit <code>secret: true</code> 5. Use profiles - Separate dev, staging, prod configurations 6. Global storage - Default and recommended for all use cases 7. Set timeouts - Prevent hanging on long operations 8. Simplified syntax - Clearer and forward-compatible 9. Validate early - Catch errors before execution 10. Monitor effective config - Use <code>prodigy config show</code> for debugging</p>"},{"location":"configuration/best-practices/#see-also","title":"See Also","text":"<ul> <li>Configuration Precedence Rules - Detailed precedence examples</li> <li>Environment Variables - Comprehensive variable documentation</li> <li>Storage Configuration - Storage backend options</li> <li>Global Configuration Structure - GlobalConfig fields</li> <li>Complete Configuration Examples - Real-world examples</li> <li>Troubleshooting - Common configuration issues</li> </ul>"},{"location":"configuration/complete-configuration-examples/","title":"Complete Configuration Examples","text":""},{"location":"configuration/complete-configuration-examples/#complete-configuration-examples","title":"Complete Configuration Examples","text":"<p>This subsection provides comprehensive, production-ready workflow examples demonstrating all major Prodigy configuration features. Each example is extracted from real workflows in the repository and includes detailed annotations explaining configuration choices.</p>"},{"location":"configuration/complete-configuration-examples/#quick-reference","title":"Quick Reference","text":"<p>Complete workflow configurations include:</p> Feature Standard Workflow MapReduce Workflow Basic Structure <code>commands: []</code> <code>mode: mapreduce</code> with <code>setup</code>, <code>map</code>, <code>reduce</code> Environment Variables <code>env:</code>, <code>secrets:</code>, <code>profiles:</code> Same + phase-specific overrides Command Types <code>claude:</code>, <code>shell:</code>, <code>goal_seek:</code>, <code>foreach:</code>, <code>write_file:</code> Same + <code>agent_template</code> Error Handling <code>on_failure:</code>, <code>on_success:</code>, <code>retry:</code> Same + <code>error_policy:</code>, <code>on_item_failure:</code> Validation <code>validate:</code> with <code>threshold</code>, <code>on_incomplete</code> Per-step validation + gap filling Output Capture <code>capture_output:</code>, <code>outputs:</code> <code>capture_outputs:</code> in setup phase Timeouts <code>timeout:</code> per command <code>timeout:</code> per phase + <code>agent_timeout_secs</code> Merge Workflow <code>merge:</code> with custom commands Same with <code>${merge.*}</code> variables"},{"location":"configuration/complete-configuration-examples/#1-complete-standard-workflow-example","title":"1. Complete Standard Workflow Example","text":"<p>This example demonstrates a full standard workflow with all major configuration options.</p> <p>Source: workflows/debtmap.yml (lines 1-56)</p> <pre><code># Sequential workflow for technical debt analysis and remediation\n# Demonstrates: validation, goal-seeking, error handlers, output capture\n\n# Phase 1: Generate coverage data\n- shell: \"just coverage-lcov\"\n  timeout: 300\n\n# Phase 2: Analyze tech debt and capture baseline\n- shell: \"debtmap analyze . --lcov target/coverage/lcov.info --output .prodigy/debtmap-before.json --format json\"\n  capture_output: true\n\n# Phase 3: Create implementation plan with validation\n- claude: \"/prodigy-debtmap-plan --before .prodigy/debtmap-before.json --output .prodigy/IMPLEMENTATION_PLAN.md\"\n  commit_required: true\n  validate:\n    commands:\n      - claude: \"/prodigy-validate-debtmap-plan --before .prodigy/debtmap-before.json --plan .prodigy/IMPLEMENTATION_PLAN.md --output .prodigy/plan-validation.json\"\n    result_file: \".prodigy/plan-validation.json\"\n    threshold: 75  # Must achieve 75% completeness\n    on_incomplete:\n      commands:\n        - claude: \"/prodigy-revise-debtmap-plan --gaps ${validation.gaps} --plan .prodigy/IMPLEMENTATION_PLAN.md\"\n      max_attempts: 3\n      fail_workflow: false\n\n# Phase 4: Execute the plan with comprehensive validation\n- claude: \"/prodigy-debtmap-implement --plan .prodigy/IMPLEMENTATION_PLAN.md\"\n  commit_required: true\n  validate:\n    commands:\n      - shell: \"just coverage-lcov\"\n      - shell: \"debtmap analyze . --lcov target/coverage/lcov.info --output .prodigy/debtmap-after.json --format json\"\n      - shell: \"debtmap compare --before .prodigy/debtmap-before.json --after .prodigy/debtmap-after.json --plan .prodigy/IMPLEMENTATION_PLAN.md --output .prodigy/comparison.json --format json\"\n      - claude: \"/prodigy-validate-debtmap-improvement --comparison .prodigy/comparison.json --output .prodigy/debtmap-validation.json\"\n    result_file: \".prodigy/debtmap-validation.json\"\n    threshold: 75\n    on_incomplete:\n      commands:\n        - claude: \"/prodigy-complete-debtmap-fix --plan .prodigy/IMPLEMENTATION_PLAN.md --validation .prodigy/debtmap-validation.json --attempt ${validation.attempt_number}\"\n          commit_required: true\n        - shell: \"just coverage-lcov\"\n        - shell: \"debtmap analyze . --lcov target/coverage/lcov.info --output .prodigy/debtmap-after.json --format json\"\n        - shell: \"debtmap compare --before .prodigy/debtmap-before.json --after .prodigy/debtmap-after.json --plan .prodigy/IMPLEMENTATION_PLAN.md --output .prodigy/comparison.json --format json\"\n      max_attempts: 5\n      fail_workflow: true\n\n# Phase 5: Verify tests pass with error recovery\n- shell: \"just test\"\n  on_failure:\n    claude: \"/prodigy-debug-test-failure --output ${shell.output}\"\n    max_attempts: 5\n    fail_workflow: true\n\n# Phase 6: Enforce code quality standards\n- shell: \"just fmt-check &amp;&amp; just lint\"\n  on_failure:\n    claude: \"/prodigy-lint ${shell.output}\"\n    max_attempts: 5\n    fail_workflow: true\n</code></pre> <p>Key Features Demonstrated: - Validation with gap filling: <code>validate:</code> block with <code>threshold</code> and <code>on_incomplete</code> handler - Error recovery: <code>on_failure:</code> handlers with <code>max_attempts</code> for automatic fixing - Output capture: Shell output captured and passed to Claude for debugging - Commit control: <code>commit_required: true</code> ensures changes are tracked - Timeouts: Per-command timeout to prevent hanging - Sequential orchestration: Each phase builds on previous results</p> <p>Configuration Details (from src/config/command.rs:WorkflowStepCommand): - <code>commit_required: bool</code> - Whether step must create a git commit (default: false) - <code>timeout: u64</code> - Maximum execution time in seconds - <code>validate: ValidationConfig</code> - Validation specification with threshold and handlers - <code>on_failure: TestDebugConfig</code> - Error handler with max_attempts and fail_workflow - <code>capture_output: bool</code> - Capture command output for use in subsequent steps</p>"},{"location":"configuration/complete-configuration-examples/#2-complete-mapreduce-workflow-example","title":"2. Complete MapReduce Workflow Example","text":"<p>This example demonstrates a production MapReduce workflow with all phases and configuration options.</p> <p>Source: workflows/book-docs-drift.yml (lines 1-101)</p> <pre><code>name: prodigy-book-docs-drift-detection\nmode: mapreduce\n\n# Global environment configuration\nenv:\n  # Project configuration\n  PROJECT_NAME: \"Prodigy\"\n  PROJECT_CONFIG: \".prodigy/book-config.json\"\n  FEATURES_PATH: \".prodigy/book-analysis/features.json\"\n\n  # Book-specific settings\n  BOOK_DIR: \"book\"\n  ANALYSIS_DIR: \".prodigy/book-analysis\"\n  CHAPTERS_FILE: \"workflows/data/prodigy-chapters.json\"\n\n  # Workflow settings\n  MAX_PARALLEL: \"3\"\n\n# Setup phase: Analyze codebase and prepare work items\nsetup:\n  - shell: \"mkdir -p $ANALYSIS_DIR\"\n\n  # Step 1: Analyze codebase features\n  - claude: \"/prodigy-analyze-features-for-book --project $PROJECT_NAME --config $PROJECT_CONFIG\"\n\n  # Step 2: Detect gaps and generate work items\n  - claude: \"/prodigy-detect-documentation-gaps --project $PROJECT_NAME --config $PROJECT_CONFIG --features $FEATURES_PATH --chapters $CHAPTERS_FILE --book-dir $BOOK_DIR\"\n\n# Map phase: Process each documentation subsection in parallel\nmap:\n  input: \"${ANALYSIS_DIR}/flattened-items.json\"\n  json_path: \"$[*]\"\n\n  agent_template:\n    # Step 1: Analyze subsection for drift\n    - claude: \"/prodigy-analyze-subsection-drift --project $PROJECT_NAME --json '${item}' --features $FEATURES_PATH\"\n      commit_required: true\n\n    # Step 2: Fix drift with validation\n    - claude: \"/prodigy-fix-subsection-drift --project $PROJECT_NAME --json '${item}'\"\n      commit_required: true\n      validate:\n        claude: \"/prodigy-validate-doc-fix --project $PROJECT_NAME --json '${item}' --output .prodigy/validation-result.json\"\n        result_file: \".prodigy/validation-result.json\"\n        threshold: 100  # Documentation must meet 100% quality standards\n        on_incomplete:\n          claude: \"/prodigy-complete-doc-fix --project $PROJECT_NAME --json '${item}' --gaps ${validation.gaps}\"\n          max_attempts: 3\n          fail_workflow: false\n          commit_required: true\n\n  max_parallel: ${MAX_PARALLEL}\n\n# Reduce phase: Aggregate results and validate build\nreduce:\n  # Rebuild the book to ensure all chapters compile\n  - shell: \"cd book &amp;&amp; mdbook build\"\n    on_failure:\n      claude: \"/prodigy-fix-book-build-errors --project $PROJECT_NAME\"\n      commit_required: true\n\n  # Clean up temporary analysis files\n  - shell: \"rm -rf ${ANALYSIS_DIR}\"\n  - shell: \"git add -A &amp;&amp; git commit -m 'chore: remove temporary book analysis files for ${PROJECT_NAME}' || true\"\n\n# Error handling policy\nerror_policy:\n  on_item_failure: dlq          # Send failures to Dead Letter Queue\n  continue_on_failure: true     # Don't stop on individual item failures\n  max_failures: 2               # Stop if more than 2 items fail\n  error_collection: aggregate   # Collect errors for batch reporting\n\n# Custom merge workflow\nmerge:\n  commands:\n    - shell: \"git fetch origin\"\n    - claude: \"/prodigy-merge-master --project ${PROJECT_NAME}\"\n    - claude: \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"\n</code></pre> <p>Key Features Demonstrated: - Environment parameterization: All paths and settings in <code>env:</code> block for easy customization - Setup phase: Generate work items before parallel processing - Agent template: Commands execute in isolation per work item - Work item access: <code>${item}</code> variable provides access to current item fields - Parallel execution: <code>max_parallel</code> controls concurrency (can reference env vars) - Validation with gap filling: Automatic quality improvement until threshold met - Error policy: Comprehensive failure handling with DLQ and thresholds - Merge workflow: Custom merge process with branch variables</p> <p>MapReduce Configuration Details (from src/config/mapreduce.rs):</p> <p>SetupPhaseConfig: - <code>commands: Vec&lt;WorkflowStep&gt;</code> - Commands to execute during setup - <code>timeout: Option&lt;String&gt;</code> - Phase timeout (supports env var references like <code>\"$TIMEOUT\"</code>) - <code>capture_outputs: HashMap&lt;String, CaptureConfig&gt;</code> - Variables to capture from setup</p> <p>MapPhaseYaml: - <code>input: String</code> - Path to work items JSON or command to generate items - <code>json_path: String</code> - JSONPath expression to extract items (default: <code>\"\"</code> for array root) - <code>agent_template: AgentTemplate</code> - Commands to execute per item - <code>max_parallel: String</code> - Concurrency limit (supports env vars like <code>\"${MAX_PARALLEL}\"</code>) - <code>filter: Option&lt;String&gt;</code> - Filter expression (e.g., <code>\"item.priority &gt;= 5\"</code>) - <code>sort_by: Option&lt;String&gt;</code> - Sort field with direction (<code>\"item.priority DESC\"</code>) - <code>max_items: Option&lt;usize&gt;</code> - Limit number of items to process - <code>offset: Option&lt;usize&gt;</code> - Skip first N items - <code>agent_timeout_secs: Option&lt;String&gt;</code> - Per-agent timeout (supports env vars)</p> <p>Error Policy (from src/cook/workflow/error_policy.rs:WorkflowErrorPolicy): - <code>on_item_failure: ItemFailureAction</code> - Action on failure: <code>dlq</code>, <code>retry</code>, <code>skip</code>, <code>stop</code> (default: <code>dlq</code>) - <code>continue_on_failure: bool</code> - Continue processing after failures (default: <code>true</code>) - <code>max_failures: Option&lt;usize&gt;</code> - Stop after N failures - <code>failure_threshold: Option&lt;f64&gt;</code> - Stop if failure rate exceeds threshold (0.0 to 1.0) - <code>error_collection: ErrorCollectionStrategy</code> - Collection mode: <code>aggregate</code>, <code>immediate</code>, <code>batched</code> (default: <code>aggregate</code>)</p> <p>Merge Workflow Variables: - <code>${merge.worktree}</code> - Worktree name being merged - <code>${merge.source_branch}</code> - Source branch (worktree branch) - <code>${merge.target_branch}</code> - Target branch (original branch) - <code>${merge.session_id}</code> - Session ID for correlation</p>"},{"location":"configuration/complete-configuration-examples/#3-environment-variables-and-secrets-example","title":"3. Environment Variables and Secrets Example","text":"<p>This example demonstrates comprehensive environment configuration with static variables, dynamic values, secrets, and profiles.</p> <p>Source: workflows/environment-example.yml (lines 1-70)</p> <pre><code># Global environment configuration\nenv:\n  # Static environment variables\n  NODE_ENV: production\n  API_URL: https://api.example.com\n\n  # Dynamic environment variable (computed from command)\n  WORKERS:\n    command: \"nproc 2&gt;/dev/null || echo 4\"\n    cache: true  # Cache the result for workflow duration\n\n  # Conditional environment variable (based on git branch)\n  DEPLOY_ENV:\n    condition: \"${branch} == 'main'\"\n    when_true: \"production\"\n    when_false: \"staging\"\n\n# Secret environment variables (masked in logs)\nsecrets:\n  # Reference to environment variable\n  API_KEY: \"${env:SECRET_API_KEY}\"\n\n# Environment files to load (.env format)\nenv_files:\n  - .env.production\n\n# Environment profiles for different contexts\nprofiles:\n  development:\n    NODE_ENV: development\n    API_URL: http://localhost:3000\n    DEBUG: \"true\"\n\n  testing:\n    NODE_ENV: test\n    API_URL: http://localhost:4000\n    COVERAGE: \"true\"\n\n# Workflow steps demonstrating environment features\ncommands:\n  - name: \"Show environment\"\n    shell: \"echo NODE_ENV=$NODE_ENV API_URL=$API_URL WORKERS=$WORKERS\"\n    capture_output: true\n\n  - name: \"Build frontend\"\n    shell: \"echo 'Building frontend with NODE_ENV='$NODE_ENV\"\n    env:\n      BUILD_TARGET: production  # Step-specific environment override\n      OPTIMIZE: \"true\"\n    working_dir: ./frontend\n\n  - name: \"Run tests\"\n    shell: \"echo 'Running tests in test environment'\"\n    env:\n      PYTHONPATH: \"./src:./tests\"\n      TEST_ENV: \"true\"\n    working_dir: ./backend\n    temporary: true  # Environment restored after this step\n\n  - name: \"Deploy application\"\n    shell: \"echo 'Deploying to '$DEPLOY_ENV' environment'\"\n    working_dir: \"${env.DEPLOY_DIR}\"\n\n  - name: \"Cleanup\"\n    shell: \"echo 'Cleaning up temporary files'\"\n    clear_env: true  # Clear all environment variables except step-specific\n    env:\n      CLEANUP_MODE: \"full\"\n</code></pre> <p>Environment Configuration Details (from src/cook/environment/config.rs):</p> <p>EnvValue Types: - Static: Simple string value - Dynamic: Computed from command with optional caching   - <code>command: String</code> - Command to execute for value   - <code>cache: bool</code> - Cache result (default: false) - Conditional: Value based on expression evaluation   - <code>condition: String</code> - Expression to evaluate   - <code>when_true: String</code> - Value when condition is true   - <code>when_false: String</code> - Value when condition is false</p> <p>Secret Management: - Marked with <code>secret: true</code> or defined in <code>secrets:</code> block - Automatically masked in logs, error messages, and event streams - Supports environment variable references: <code>\"${env:VAR_NAME}\"</code></p> <p>Profile Usage: <pre><code># Activate a profile at runtime\nprodigy run workflow.yml --profile development\nprodigy run workflow.yml --profile testing\n</code></pre></p> <p>Step-Level Environment (from src/config/command.rs:WorkflowStepCommand): - <code>env: HashMap&lt;String, String&gt;</code> - Step-specific environment variables - <code>working_dir: Option&lt;PathBuf&gt;</code> - Working directory for this step - <code>temporary: bool</code> - Restore environment after step (default: false) - <code>clear_env: bool</code> - Clear parent environment before applying step env (default: false)</p>"},{"location":"configuration/complete-configuration-examples/#4-error-handling-and-retry-strategies-example","title":"4. Error Handling and Retry Strategies Example","text":"<p>This example demonstrates comprehensive error handling patterns including retry strategies, backoff configurations, and circuit breakers.</p> <p>Source: workflows/implement-with-tests.yml (lines 1-79) and workflows/debtmap.yml</p> <pre><code># Nested error handling with automatic recovery\ncommands:\n  # Step 1: Implement specification\n  - claude: \"/prodigy-implement-spec $ARG\"\n    analysis:\n      max_cache_age: 300\n\n  # Step 2: Run tests with nested error recovery\n  - shell: \"cargo test\"\n    capture_output: \"test_output\"\n    commit_required: false\n    on_failure:\n      # First attempt: Debug test failures\n      claude: \"/prodigy-debug-test-failures '${test_output}'\"\n      commit_required: true\n      on_success:\n        # Verify fixes work\n        shell: \"cargo test\"\n        commit_required: false\n        on_failure:\n          # Second attempt: Deep analysis if still failing\n          claude: \"/prodigy-fix-test-failures '${shell.output}' --deep-analysis\"\n          commit_required: true\n\n  # Step 3: Run linting\n  - claude: \"/prodigy-lint\"\n    commit_required: false\n\n  # Step 4: Run benchmarks (non-critical)\n  - shell: \"cargo bench --no-run\"\n    commit_required: false\n    on_failure:\n      shell: \"echo 'Skipping benchmarks due to compilation issues'\"\n      commit_required: false\n\n  # Step 5: Final verification with status reporting\n  - shell: \"cargo test --release\"\n    capture_output: \"final_test_results\"\n    commit_required: false\n    on_failure:\n      # Report persistent failures\n      claude: \"/prodigy-report-test-status failed '${final_test_results}' --notify\"\n      commit_required: false\n    on_success:\n      shell: \"echo '\u2705 All tests passing! Implementation complete.'\"\n      commit_required: false\n</code></pre> <p>Error Handler Configuration (from src/config/command.rs:TestDebugConfig): - <code>claude: String</code> - Command to run on failure - <code>max_attempts: u32</code> - Maximum retry attempts (default: 3) - <code>fail_workflow: bool</code> - Stop workflow if max attempts exceeded (default: false) - <code>commit_required: bool</code> - Whether handler must create commits (default: true)</p> <p>Backoff Strategy Types (from src/cook/workflow/error_policy.rs:BackoffStrategy):</p> <pre><code># Fixed delay between retries\nretry:\n  backoff:\n    fixed:\n      delay: 5s\n\n# Linear backoff (delay increases linearly)\nretry:\n  backoff:\n    linear:\n      initial: 1s\n      increment: 2s\n\n# Exponential backoff (default: 2x multiplier)\nretry:\n  backoff:\n    exponential:\n      initial: 1s\n      multiplier: 2.0\n\n# Fibonacci sequence delays\nretry:\n  backoff:\n    fibonacci:\n      initial: 1s\n</code></pre> <p>Circuit Breaker Configuration (from src/cook/workflow/error_policy.rs:CircuitBreakerConfig):</p> <pre><code>error_policy:\n  circuit_breaker:\n    failure_threshold: 5       # Open circuit after 5 failures\n    success_threshold: 3       # Close after 3 successes\n    timeout: 30s              # Time before attempting to close\n    half_open_requests: 3     # Requests allowed in half-open state\n</code></pre>"},{"location":"configuration/complete-configuration-examples/#5-goal-seeking-and-validation-examples","title":"5. Goal-Seeking and Validation Examples","text":"<p>This example demonstrates iterative refinement with validation and automatic gap filling.</p> <p>Source: workflows/goal-seeking-examples.yml (lines 1-129)</p> <pre><code># Example 1: Test Coverage Improvement\n- goal_seek:\n    goal: \"Achieve 90% test coverage\"\n    claude: \"/prodigy-coverage --improve\"\n    validate: \"cargo tarpaulin --print-summary 2&gt;/dev/null | grep 'Coverage' | sed 's/.*Coverage=\\\\([0-9]*\\\\).*/score: \\\\1/'\"\n    threshold: 90\n    max_attempts: 5\n    timeout_seconds: 300\n    fail_on_incomplete: true\n  commit_required: true\n\n# Example 2: Performance Optimization\n- goal_seek:\n    goal: \"Optimize algorithm performance to under 100ms\"\n    claude: \"/optimize-performance --target 100ms\"\n    validate: \"cargo bench --bench main_bench 2&gt;/dev/null | grep 'time:' | awk '{if ($2 &lt; 100) print \\\"score: 95\\\"; else print \\\"score:\\\", int(10000/$2)}'\"\n    threshold: 90\n    max_attempts: 4\n    timeout_seconds: 600\n  commit_required: true\n\n# Example 3: Code Quality with Custom Validation\n- goal_seek:\n    goal: \"Fix all clippy warnings and improve code quality\"\n    claude: \"/fix-clippy-warnings\"\n    validate: |\n      warnings=$(cargo clippy 2&gt;&amp;1 | grep -c warning || echo 0)\n      if [ \"$warnings\" -eq 0 ]; then\n        echo \"score: 100\"\n      else\n        score=$((100 - warnings * 5))\n        echo \"score: $score\"\n      fi\n    threshold: 95\n    max_attempts: 3\n    fail_on_incomplete: false\n  commit_required: true\n\n# Example 4: Multi-stage Goal Seeking\n- name: \"Complete feature implementation with quality checks\"\n  goal_seek:\n    goal: \"Implement user profile feature\"\n    claude: \"/implement-feature user-profile\"\n    validate: \"test -f src/features/user_profile.rs &amp;&amp; echo 'score: 100' || echo 'score: 0'\"\n    threshold: 100\n    max_attempts: 2\n\n- name: \"Add comprehensive tests\"\n  goal_seek:\n    goal: \"Add tests for user profile feature\"\n    claude: \"/add-tests src/features/user_profile.rs\"\n    validate: |\n      test_count=$(grep -c \"#\\\\[test\\\\]\" src/features/user_profile.rs || echo 0)\n      if [ \"$test_count\" -ge 5 ]; then\n        echo \"score: 100\"\n      else\n        score=$((test_count * 20))\n        echo \"score: $score\"\n      fi\n    threshold: 100\n    max_attempts: 3\n\n- name: \"Ensure tests pass\"\n  goal_seek:\n    goal: \"Make all user profile tests pass\"\n    claude: \"/fix-tests user_profile\"\n    validate: \"cargo test user_profile 2&gt;&amp;1 | grep -q 'test result: ok' &amp;&amp; echo 'score: 100' || echo 'score: 0'\"\n    threshold: 100\n    max_attempts: 4\n    fail_on_incomplete: true\n</code></pre> <p>Goal-Seeking Configuration (from src/cook/goal_seek/mod.rs): - <code>goal: String</code> - Human-readable description of the goal - <code>claude: String</code> - Claude command to execute for improvement - <code>validate: String</code> - Shell command that outputs \"score: N\" (0-100) - <code>threshold: u32</code> - Minimum score required for success (0-100) - <code>max_attempts: u32</code> - Maximum refinement iterations - <code>timeout_seconds: u64</code> - Maximum time for all attempts - <code>fail_on_incomplete: bool</code> - Fail workflow if threshold not reached</p> <p>Validation Output Format: The validation command must output a single line with the score: <pre><code>score: 85\n</code></pre></p> <p>The score should be between 0 and 100, where 100 indicates complete success.</p>"},{"location":"configuration/complete-configuration-examples/#6-foreach-parallel-iteration-example","title":"6. Foreach Parallel Iteration Example","text":"<p>This example demonstrates parallel iteration over work items with different input sources and concurrency controls.</p> <p>Configuration Details (from src/config/command.rs:ForeachConfig):</p> <pre><code># Static list input with parallel execution\n- foreach:\n    input: [\"file1.rs\", \"file2.rs\", \"file3.rs\"]\n    parallel: 3  # Process 3 files concurrently\n    do:\n      - claude: \"/lint ${item}\"\n      - shell: \"rustfmt ${item}\"\n    continue_on_error: true  # Don't stop on individual item failures\n\n# Command input (output becomes items)\n- foreach:\n    input:\n      command: \"find src -name '*.rs' -type f\"\n    parallel: 5\n    do:\n      - claude: \"/analyze ${item}\"\n      - shell: \"cargo check --file ${item}\"\n    max_items: 50  # Limit to first 50 files\n\n# Sequential execution (no parallelism)\n- foreach:\n    input: [\"step1\", \"step2\", \"step3\"]\n    parallel: false  # Execute sequentially\n    do:\n      - claude: \"/execute-step ${item}\"\n</code></pre> <p>ForeachConfig Structure: - <code>input: ForeachInput</code> - Source of items (command or static list)   - <code>command: String</code> - Command whose output (one item per line) becomes items   - <code>list: Vec&lt;String&gt;</code> - Static list of items - <code>parallel: ParallelConfig</code> - Concurrency control   - <code>boolean: bool</code> - Enable/disable parallelism (true = default count, false = sequential)   - <code>count: usize</code> - Specific number of concurrent items - <code>do: Vec&lt;WorkflowStepCommand&gt;</code> - Commands to execute per item - <code>continue_on_error: bool</code> - Continue if individual item fails (default: false) - <code>max_items: Option&lt;usize&gt;</code> - Limit number of items to process</p> <p>Item Access: - Use <code>${item}</code> to reference the current item in commands - Each iteration runs in a clean environment - Failures are isolated to individual items</p>"},{"location":"configuration/complete-configuration-examples/#7-write-file-command-example","title":"7. Write File Command Example","text":"<p>This example demonstrates the <code>write_file</code> command for generating files during workflow execution.</p> <p>Configuration Details (from src/config/command.rs:WriteFileConfig):</p> <pre><code># Write plain text file\n- write_file:\n    path: \"reports/summary.txt\"\n    content: |\n      Workflow Summary\n      ================\n      Project: ${PROJECT_NAME}\n      Completed: ${map.successful}/${map.total} items\n      Duration: ${workflow.duration}\n    format: text\n    create_dirs: true  # Create parent directories if needed\n\n# Write JSON file with validation\n- write_file:\n    path: \"config/generated.json\"\n    content: |\n      {\n        \"version\": \"${VERSION}\",\n        \"timestamp\": \"${timestamp}\",\n        \"items_processed\": ${map.total}\n      }\n    format: json  # Validates JSON syntax and pretty-prints\n    mode: \"0644\"\n\n# Write YAML configuration\n- write_file:\n    path: \"config/deploy.yml\"\n    content: |\n      environment: ${DEPLOY_ENV}\n      version: ${VERSION}\n      features:\n        - feature1\n        - feature2\n    format: yaml  # Validates YAML syntax and formats\n    create_dirs: true\n</code></pre> <p>WriteFileConfig Structure: - <code>path: String</code> - File path (supports variable interpolation) - <code>content: String</code> - Content to write (supports variable interpolation) - <code>format: WriteFileFormat</code> - Output format (default: <code>text</code>)   - <code>text</code> - Plain text (no processing)   - <code>json</code> - JSON with validation and pretty-printing   - <code>yaml</code> - YAML with validation and formatting - <code>mode: String</code> - File permissions in octal format (default: <code>\"0644\"</code>) - <code>create_dirs: bool</code> - Create parent directories (default: false)</p>"},{"location":"configuration/complete-configuration-examples/#8-advanced-timeout-configuration-example","title":"8. Advanced Timeout Configuration Example","text":"<p>This example demonstrates timeout configuration at multiple levels.</p> <p>Configuration Details:</p> <pre><code># Global timeout for workflow\ntimeout: 3600  # 1 hour for entire workflow\n\ncommands:\n  # Command-level timeout\n  - shell: \"long-running-task.sh\"\n    timeout: 600  # 10 minutes for this command\n\n# MapReduce with phase-specific timeouts\nsetup:\n  - shell: \"setup-task.sh\"\n  timeout: 300  # 5 minutes for setup phase\n\nmap:\n  agent_template:\n    - claude: \"/process ${item}\"\n      timeout: 180  # 3 minutes per command\n  agent_timeout_secs: 600  # 10 minutes total per agent\n  timeout_config:\n    total_timeout_secs: 3600     # Max time for entire map phase\n    idle_timeout_secs: 300       # Kill agent if idle for 5 minutes\n    per_item_timeout_secs: 180   # Max time per work item\n\nmerge:\n  commands:\n    - claude: \"/merge\"\n  timeout: 600  # 10 minutes for merge phase\n</code></pre> <p>Timeout Hierarchy (most specific wins): 1. Command-level <code>timeout:</code> - Per command 2. Agent-level <code>agent_timeout_secs:</code> - Per MapReduce agent 3. Phase-level <code>timeout:</code> - Per workflow phase (setup, reduce, merge) 4. Global-level <code>timeout:</code> - Entire workflow</p>"},{"location":"configuration/complete-configuration-examples/#cross-references","title":"Cross-References","text":"<p>For more detailed information on specific features:</p> <ul> <li>Workflow Structure: See ../workflow-basics/full-workflow-structure.md</li> <li>Environment Variables: See environment-variables.md</li> <li>Error Handling: See ../workflow-basics/error-handling.md</li> <li>MapReduce Basics: See ../mapreduce/index.md</li> <li>Validation: See ../advanced/implementation-validation.md</li> <li>Goal Seeking: See ../advanced/goal-seeking-operations.md</li> </ul>"},{"location":"configuration/configuration-precedence-rules/","title":"Configuration Precedence Rules","text":""},{"location":"configuration/configuration-precedence-rules/#configuration-precedence-rules","title":"Configuration Precedence Rules","text":"<p>Prodigy loads configuration from multiple sources with a clear precedence hierarchy. Understanding how configuration is merged helps you control which settings take effect.</p>"},{"location":"configuration/configuration-precedence-rules/#precedence-hierarchy","title":"Precedence Hierarchy","text":"<p>From highest to lowest priority:</p> <ol> <li>Project Config (<code>.prodigy/config.yml</code>) - Highest priority</li> <li>Project-specific settings in your repository</li> <li>Located at <code>.prodigy/config.yml</code> in your project directory</li> <li>Overrides all default values</li> <li> <p>Committed to version control (be careful with secrets)</p> </li> <li> <p>Defaults (lowest priority)</p> </li> <li>Built-in default values defined in the code</li> <li>Used when project config doesn't provide a value</li> <li>See source: <code>src/config/mod.rs:88-100</code></li> </ol> <p>Note: Global config (<code>~/.prodigy/config.yml</code>), environment variables (<code>PRODIGY_*</code>), and CLI flag overrides are defined in the code but not currently loaded in production. Only project-level configuration and defaults are active.</p>"},{"location":"configuration/configuration-precedence-rules/#how-settings-are-loaded","title":"How Settings Are Loaded","text":"<p>When Prodigy starts, it builds the final configuration with this process:</p> <ol> <li>Initialize with defaults - Create <code>GlobalConfig</code> with built-in defaults (source: <code>src/config/mod.rs:88-100</code>)</li> <li>Load project config - Read <code>.prodigy/config.yml</code> from project directory (source: <code>src/config/loader.rs:85-104</code>)</li> <li>Merge at field level - Project config values override defaults on a per-field basis (source: <code>src/config/mod.rs:133-154</code>)</li> </ol> <p>Project config overrides default values at the individual field level. Settings not specified in project config inherit from defaults.</p>"},{"location":"configuration/configuration-precedence-rules/#examples","title":"Examples","text":""},{"location":"configuration/configuration-precedence-rules/#example-1-using-defaults","title":"Example 1: Using Defaults","text":"<pre><code># No .prodigy/config.yml file exists\n</code></pre> <p>Result: Prodigy uses all default values: - <code>log_level: \"info\"</code> - <code>auto_commit: true</code> - <code>max_concurrent_specs: 1</code></p> <p>(Source: <code>src/config/mod.rs:88-100</code>)</p>"},{"location":"configuration/configuration-precedence-rules/#example-2-project-config-override","title":"Example 2: Project Config Override","text":"<pre><code># .prodigy/config.yml (project config)\nname: my-project\nclaude_api_key: \"sk-project-key\"\nauto_commit: false\n</code></pre> <p>Result: - <code>claude_api_key: \"sk-project-key\"</code> (from project config) - <code>auto_commit: false</code> (from project config) - <code>log_level: \"info\"</code> (from defaults - not specified in project) - <code>max_concurrent_specs: 1</code> (from defaults - not specified in project)</p> <p>Field-level precedence is implemented via getter methods (source: <code>src/config/mod.rs:133-154</code>): <pre><code>pub fn get_auto_commit(&amp;self) -&gt; bool {\n    self.project\n        .as_ref()\n        .and_then(|p| p.auto_commit)\n        .or(self.global.auto_commit)\n        .unwrap_or(true)  // Default if neither provides value\n}\n</code></pre></p>"},{"location":"configuration/configuration-precedence-rules/#example-3-partial-project-override","title":"Example 3: Partial Project Override","text":"<pre><code># .prodigy/config.yml\nname: my-project\nclaude_api_key: \"sk-abc123\"\n# Other fields not specified\n</code></pre> <p>Result: - <code>claude_api_key: \"sk-abc123\"</code> (from project config) - <code>log_level: \"info\"</code> (from defaults) - <code>auto_commit: true</code> (from defaults) - <code>max_concurrent_specs: 1</code> (from defaults)</p>"},{"location":"configuration/configuration-precedence-rules/#field-level-precedence","title":"Field-Level Precedence","text":"<p>Precedence is applied per field, not per file. Each configuration field is resolved independently using the precedence rules.</p> <pre><code># .prodigy/config.yml (project config)\nname: my-project\nauto_commit: false  # Only override auto_commit\n# Other fields inherited from defaults\n</code></pre> <p>Precedence Logic (source: <code>src/config/mod.rs:133-154</code>): 1. Check if project config has the field \u2192 use it 2. Otherwise, check if global config has the field \u2192 use it 3. Otherwise, use the default value</p> <p>This allows fine-grained configuration: override only what you need, inherit the rest.</p>"},{"location":"configuration/configuration-precedence-rules/#configuration-loading-implementation","title":"Configuration Loading Implementation","text":"<p>The configuration loading happens in these steps (source: <code>src/config/loader.rs</code>):</p> <p>Step 1: Initialize <pre><code>// ConfigLoader::new() - line 23\nlet config = Config::new();  // Creates Config with GlobalConfig defaults\n</code></pre></p> <p>Step 2: Load Project Config (optional) <pre><code>// ConfigLoader::load_project() - line 85\nlet config_path = project_path.join(\".prodigy\").join(\"config.yml\");\nif config_path.exists() {\n    let content = fs::read_to_string(&amp;config_path).await?;\n    let project_config = parse_project_config(&amp;content)?;\n    *config = merge_project_config(config.clone(), project_config);\n}\n</code></pre></p> <p>Step 3: Access with Precedence <pre><code>// Config::get_claude_api_key() - line 133\nself.project\n    .as_ref()\n    .and_then(|p| p.claude_api_key.as_deref())  // Try project first\n    .or(self.global.claude_api_key.as_deref())   // Fall back to global\n</code></pre></p>"},{"location":"configuration/configuration-precedence-rules/#default-values","title":"Default Values","text":"<p>Built-in defaults (source: <code>src/config/mod.rs:88-100</code>):</p> <pre><code>impl Default for GlobalConfig {\n    fn default() -&gt; Self {\n        Self {\n            prodigy_home: get_global_prodigy_dir()\n                .unwrap_or_else(|_| PathBuf::from(\"~/.prodigy\")),\n            default_editor: None,\n            log_level: Some(\"info\".to_string()),\n            claude_api_key: None,\n            max_concurrent_specs: Some(1),\n            auto_commit: Some(true),\n            plugins: None,\n        }\n    }\n}\n</code></pre>"},{"location":"configuration/configuration-precedence-rules/#future-global-config-and-environment-variables","title":"Future: Global Config and Environment Variables","text":"<p>The codebase includes infrastructure for additional configuration sources, but these are not currently loaded in production:</p> <p>Global Config (<code>~/.prodigy/config.yml</code>): - Mentioned in documentation (line 49: <code>src/config/mod.rs</code>) - No loader implementation yet - Would provide user-level defaults across all projects</p> <p>Environment Variables: - Defined in <code>Config::merge_env_vars()</code> (lines 111-131: <code>src/config/mod.rs</code>) - Supports: <code>PRODIGY_CLAUDE_API_KEY</code>, <code>PRODIGY_LOG_LEVEL</code>, <code>PRODIGY_EDITOR</code>, <code>PRODIGY_AUTO_COMMIT</code> - Only called in tests, not in production code - Would override file-based configuration when implemented</p> <p>CLI Flag Overrides: - No implementation yet - Would provide highest-priority overrides for individual runs</p>"},{"location":"configuration/configuration-precedence-rules/#test-coverage","title":"Test Coverage","text":"<p>Configuration precedence behavior is validated through comprehensive tests (source: <code>src/config/loader.rs:113-334</code>):</p> <p>Test: Default Configuration <pre><code>// Line 120: test_new_creates_default_config\n// Verifies GlobalConfig defaults are set correctly\nassert_eq!(config.global.log_level, Some(\"info\".to_string()));\nassert_eq!(config.global.max_concurrent_specs, Some(1));\nassert_eq!(config.global.auto_commit, Some(true));\n</code></pre></p> <p>Test: Project Config Loading <pre><code>// Line 230: test_load_project_config\n// Verifies .prodigy/config.yml is loaded and merged\nlet project = config.project.unwrap();\nassert_eq!(project.name, \"test-project\");\nassert_eq!(project.claude_api_key, Some(\"test-key\".to_string()));\nassert_eq!(project.auto_commit, Some(false));\n</code></pre></p> <p>Test: Field-Level Override <pre><code>// src/config/mod.rs:471 - test shows project overrides global\nconfig.project = Some(ProjectConfig {\n    name: \"test\".into(),\n    claude_api_key: Some(\"project-key\".into()),\n    // ... other fields\n});\nassert_eq!(config.get_claude_api_key(), Some(\"project-key\"));\n</code></pre></p>"},{"location":"configuration/configuration-precedence-rules/#see-also","title":"See Also","text":"<ul> <li>Global Configuration Structure - Complete field reference</li> <li>Default Values Reference - All default values</li> <li>Complete Configuration Examples - Real-world configuration patterns</li> </ul>"},{"location":"configuration/default-values-reference/","title":"Default Values Reference","text":""},{"location":"configuration/default-values-reference/#default-values-reference","title":"Default Values Reference","text":"<p>Complete reference of all default configuration values in Prodigy. These defaults are used when settings are not explicitly configured in global config, project config, or environment variables.</p>"},{"location":"configuration/default-values-reference/#global-configuration-defaults","title":"Global Configuration Defaults","text":"<p>Source: <code>src/config/mod.rs:51-59, 88-100</code></p> Setting Default Value Description <code>prodigy_home</code> <code>~/.prodigy</code> Global storage directory (platform-specific) <code>default_editor</code> None Text editor (falls back to <code>$EDITOR</code>) <code>log_level</code> <code>\"info\"</code> Logging verbosity (<code>trace</code>, <code>debug</code>, <code>info</code>, <code>warn</code>, <code>error</code>) <code>claude_api_key</code> None Claude API key (use environment variable) <code>max_concurrent_specs</code> <code>1</code> Maximum concurrent spec implementations <code>auto_commit</code> <code>true</code> Automatically commit after successful commands <code>plugins.enabled</code> <code>false</code> Enable plugin system <code>plugins.directory</code> <code>~/.prodigy/plugins</code> Plugin directory <code>plugins.auto_load</code> <code>[]</code> Plugins to load on startup"},{"location":"configuration/default-values-reference/#project-configuration-defaults","title":"Project Configuration Defaults","text":"<p>Source: <code>src/config/mod.rs:66-74</code></p> Setting Default Value Description <code>name</code> Required Project identifier (no default) <code>description</code> None Human-readable project description <code>version</code> None Project version <code>spec_dir</code> <code>\"specs\"</code> Directory containing specification files <code>claude_api_key</code> Inherits from global Project-specific API key <code>auto_commit</code> Inherits from global Project-specific auto-commit setting <code>variables</code> <code>{}</code> Project variables for workflows"},{"location":"configuration/default-values-reference/#storage-configuration-defaults","title":"Storage Configuration Defaults","text":"<p>Source: <code>src/storage/config.rs:24-55, 228-241</code></p> Setting Default Value Description <code>backend</code> <code>\"file\"</code> Storage backend type (<code>file</code> or <code>memory</code>) <code>connection_pool_size</code> <code>10</code> Connection pool size (for future backends) <code>timeout</code> <code>30s</code> Default operation timeout <code>enable_locking</code> <code>true</code> Enable distributed locking <code>enable_cache</code> <code>false</code> Enable caching layer"},{"location":"configuration/default-values-reference/#file-storage-defaults","title":"File Storage Defaults","text":"<p>Source: <code>src/storage/config.rs:66-86, 196-198</code></p> Setting Default Value Description <code>base_dir</code> <code>~/.prodigy</code> Base storage directory <code>use_global</code> <code>true</code> Use global storage (recommended) <code>enable_file_locks</code> <code>true</code> Enable file-based locking <code>max_file_size</code> <code>104857600</code> (100MB) Max file size before rotation <code>enable_compression</code> <code>false</code> Compress archived files"},{"location":"configuration/default-values-reference/#memory-storage-defaults","title":"Memory Storage Defaults","text":"<p>Source: <code>src/storage/config.rs:89-111, 200-201</code></p> Setting Default Value Description <code>max_memory</code> <code>1073741824</code> (1GB) Maximum memory usage <code>persist_to_disk</code> <code>false</code> Persist memory storage to disk <code>persistence_path</code> None Path for disk persistence"},{"location":"configuration/default-values-reference/#retry-policy-defaults","title":"Retry Policy Defaults","text":"<p>Source: <code>src/storage/config.rs:114-147, 204-212</code></p> Setting Default Value Description <code>max_retries</code> <code>3</code> Maximum retry attempts <code>initial_delay</code> <code>1s</code> Initial retry delay <code>max_delay</code> <code>30s</code> Maximum retry delay (with backoff) <code>backoff_multiplier</code> <code>2.0</code> Exponential backoff multiplier <code>jitter</code> <code>true</code> Add random jitter to delays"},{"location":"configuration/default-values-reference/#cache-configuration-defaults","title":"Cache Configuration Defaults","text":"<p>Source: <code>src/storage/config.rs:150-173</code></p> Setting Default Value Description <code>max_entries</code> <code>1000</code> Maximum cached entries <code>ttl</code> <code>3600s</code> (1 hour) Cache time-to-live <code>cache_type</code> <code>\"memory\"</code> Cache implementation type"},{"location":"configuration/default-values-reference/#environment-variable-defaults","title":"Environment Variable Defaults","text":"<p>These settings can be overridden by environment variables (see Environment Variables):</p> Environment Variable Corresponding Setting Default <code>PRODIGY_CLAUDE_API_KEY</code> <code>claude_api_key</code> None <code>PRODIGY_LOG_LEVEL</code> <code>log_level</code> <code>\"info\"</code> <code>PRODIGY_EDITOR</code> <code>default_editor</code> None <code>PRODIGY_AUTO_COMMIT</code> <code>auto_commit</code> <code>true</code> <code>PRODIGY_STORAGE_TYPE</code> <code>storage.backend</code> <code>\"file\"</code> <code>PRODIGY_STORAGE_BASE_PATH</code> <code>storage.base_dir</code> <code>~/.prodigy</code> <code>PRODIGY_CLAUDE_STREAMING</code> - <code>true</code> <code>PRODIGY_AUTOMATION</code> - Not set (set by Prodigy)"},{"location":"configuration/default-values-reference/#cli-parameter-defaults","title":"CLI Parameter Defaults","text":"<p>Source: <code>src/cook/command.rs:28-29</code></p> <p>These are CLI-level parameters, not workflow configuration fields:</p> Parameter Default Value Description <code>--max-iterations</code> <code>1</code> Number of workflow iterations to run <code>--path</code> Current directory Repository path to run in"},{"location":"configuration/default-values-reference/#command-metadata-defaults","title":"Command Metadata Defaults","text":"<p>Source: <code>src/config/command.rs:130-154, src/config/mod.rs:363-365</code></p> <p>Applied to individual commands when not specified:</p> Setting Default Value Description <code>retries</code> <code>2</code> Retry attempts for failed commands <code>timeout</code> <code>300</code> Command timeout (seconds) <code>continue_on_error</code> <code>false</code> Continue workflow on command failure <code>commit_required</code> <code>false</code> Whether command must create git commits <code>env</code> <code>{}</code> Environment variables for command"},{"location":"configuration/default-values-reference/#understanding-defaults","title":"Understanding Defaults","text":"<p>How defaults work:</p> <ol> <li>Prodigy starts with built-in defaults</li> <li>Global config (<code>~/.prodigy/config.yml</code>) overrides defaults</li> <li>Project config (<code>.prodigy/config.yml</code>) overrides global config</li> <li>Environment variables override file config</li> <li>CLI flags override everything</li> </ol> <p>Example precedence flow:</p> <pre><code>Built-in default: log_level = \"info\"\n       \u2193\nGlobal config: log_level = \"warn\"  (overrides default)\n       \u2193\nProject config: (not specified, inherits \"warn\")\n       \u2193\nEnvironment: PRODIGY_LOG_LEVEL=debug  (overrides all configs)\n       \u2193\nResult: log_level = \"debug\"\n</code></pre>"},{"location":"configuration/default-values-reference/#practical-example-overriding-storage-defaults","title":"Practical Example: Overriding Storage Defaults","text":"<p>This example shows how to override storage defaults in a project config:</p> <pre><code># .prodigy/config.yml\nname: my-project\n\nstorage:\n  backend: file\n  timeout: 60s  # Override default 30s\n  enable_cache: true  # Override default false\n\n  backend_config:\n    file:\n      base_dir: /custom/storage  # Override default ~/.prodigy\n      max_file_size: 524288000  # 500MB (override default 100MB)\n      enable_compression: true   # Override default false\n\n  cache_config:\n    max_entries: 5000  # Override default 1000\n    ttl: 7200s  # 2 hours (override default 1 hour)\n</code></pre> <p>With this configuration: - Storage timeout increases from 30s \u2192 60s - Caching is enabled (default: disabled) - Files can be 500MB instead of 100MB - Cache holds 5000 entries instead of 1000</p>"},{"location":"configuration/default-values-reference/#see-also","title":"See Also","text":"<ul> <li>Configuration Precedence Rules</li> <li>Global Configuration Structure</li> <li>Project Configuration Structure</li> <li>Storage Configuration</li> <li>Environment Variables</li> </ul>"},{"location":"configuration/environment-variables/","title":"Environment Variables","text":""},{"location":"configuration/environment-variables/#environment-variables","title":"Environment Variables","text":"<p>Prodigy supports two types of environment variables:</p> <ol> <li>System Environment Variables: Standard Unix environment variables that control Prodigy's behavior globally</li> <li>Workflow Environment Variables: Variables defined in workflow YAML files that are available during workflow execution</li> </ol> <p>This page documents both types. For details on how environment variables interact with other configuration sources, see Configuration Precedence Rules.</p>"},{"location":"configuration/environment-variables/#workflow-environment-variables","title":"Workflow Environment Variables","text":"<p>Workflows can define custom environment variables using the <code>env:</code> block. These variables are available to all commands within the workflow and support advanced features like secrets, profiles, and interpolation.</p>"},{"location":"configuration/environment-variables/#basic-syntax","title":"Basic Syntax","text":"<pre><code>name: my-workflow\n\nenv:\n  # Plain variables\n  PROJECT_NAME: \"prodigy\"\n  VERSION: \"1.0.0\"\n  BUILD_DIR: \"target/release\"\n\ncommands:\n  - shell: \"echo Building $PROJECT_NAME version $VERSION\"\n  - shell: \"cargo build --release --target-dir $BUILD_DIR\"\n</code></pre>"},{"location":"configuration/environment-variables/#variable-interpolation","title":"Variable Interpolation","text":"<p>Workflow environment variables can be referenced using two syntaxes:</p> <ul> <li><code>$VAR</code> - Simple variable reference (shell-style)</li> <li><code>${VAR}</code> - Bracketed reference (recommended for clarity and complex expressions)</li> </ul> <pre><code>env:\n  API_URL: \"https://api.example.com\"\n  API_VERSION: \"v2\"\n  ENDPOINT: \"${API_URL}/${API_VERSION}\"\n\ncommands:\n  - shell: \"curl ${ENDPOINT}/status\"\n  - claude: \"/deploy --url $API_URL --version $API_VERSION\"\n</code></pre>"},{"location":"configuration/environment-variables/#secrets-and-sensitive-data","title":"Secrets and Sensitive Data","text":"<p>Mark sensitive values as secrets to automatically mask them in logs and output:</p> <pre><code>env:\n  # Public configuration\n  DATABASE_HOST: \"db.example.com\"\n\n  # Secret configuration (masked in logs)\n  DATABASE_PASSWORD:\n    secret: true\n    value: \"super-secret-password\"\n\n  API_KEY:\n    secret: true\n    value: \"sk-abc123...\"\n\ncommands:\n  - shell: \"psql -h $DATABASE_HOST -p $DATABASE_PASSWORD\"\n  # Output: psql -h db.example.com -p ***\n</code></pre> <p>Security Best Practices: - Always mark API keys, passwords, and tokens as secrets - Never commit secret values to version control - Use environment variable references for secrets: <code>value: \"${PROD_API_KEY}\"</code> - Rotate secrets regularly</p>"},{"location":"configuration/environment-variables/#profiles-for-multiple-environments","title":"Profiles for Multiple Environments","text":"<p>Profiles allow different values for different environments (dev, staging, prod):</p> <pre><code>env:\n  # API endpoints vary by environment\n  API_URL:\n    default: \"http://localhost:3000\"\n    staging: \"https://staging.api.com\"\n    prod: \"https://api.com\"\n\n  # Credentials vary by environment\n  API_KEY:\n    secret: true\n    default: \"dev-key-123\"\n    staging:\n      secret: true\n      value: \"${STAGING_API_KEY}\"  # From system env\n    prod:\n      secret: true\n      value: \"${PROD_API_KEY}\"\n\ncommands:\n  - shell: \"curl -H 'Authorization: Bearer ${API_KEY}' ${API_URL}/health\"\n</code></pre> <p>Activate a profile:</p> <pre><code># Use staging profile\nprodigy run workflow.yml --profile staging\n\n# Use prod profile via environment variable\nexport PRODIGY_PROFILE=prod\nprodigy run workflow.yml\n</code></pre>"},{"location":"configuration/environment-variables/#step-level-environment-overrides","title":"Step-Level Environment Overrides","text":"<p>Individual commands can override workflow environment variables:</p> <pre><code>env:\n  NODE_ENV: \"development\"\n  LOG_LEVEL: \"info\"\n\ncommands:\n  # Uses workflow-level NODE_ENV\n  - shell: \"npm test\"\n\n  # Override for this command only\n  - shell: \"npm run build\"\n    env:\n      NODE_ENV: \"production\"\n      LOG_LEVEL: \"warn\"\n\n  # Back to workflow-level NODE_ENV\n  - shell: \"npm start\"\n</code></pre> <p>Precedence: Step env &gt; Profile env &gt; Workflow env &gt; System env</p>"},{"location":"configuration/environment-variables/#mapreduce-environment-variables","title":"MapReduce Environment Variables","text":"<p>Environment variables work across all MapReduce phases (setup, map, reduce, merge):</p> <pre><code>name: parallel-processing\nmode: mapreduce\n\nenv:\n  MAX_PARALLEL: \"10\"\n  TIMEOUT: \"300\"\n  OUTPUT_DIR: \"/tmp/results\"\n\nsetup:\n  - shell: \"mkdir -p $OUTPUT_DIR\"\n  - shell: \"generate-work-items.sh &gt; items.json\"\n\nmap:\n  input: \"items.json\"\n  json_path: \"$[*]\"\n  max_parallel: ${MAX_PARALLEL}  # Use env var for parallelism\n\n  agent_template:\n    - claude: \"/process ${item.file} --timeout $TIMEOUT\"\n    - shell: \"cp result.json ${OUTPUT_DIR}/${item.name}.json\"\n\nreduce:\n  - shell: \"echo Processed ${map.total} items to $OUTPUT_DIR\"\n</code></pre> <p>Advanced MapReduce Usage: - Use env vars for <code>max_parallel</code>, <code>timeout</code>, <code>agent_timeout_secs</code> - Reference in <code>filter</code> and <code>sort_by</code> expressions - Pass to validation and gap-filling commands</p>"},{"location":"configuration/environment-variables/#environment-files-env","title":"Environment Files (<code>.env</code>)","text":"<p>Load variables from dotenv-format files (not yet implemented in Prodigy, but planned):</p> <pre><code>env:\n  env_files:\n    - \".env\"\n    - \".env.${PRODIGY_PROFILE}\"\n\n# .env file format:\n# PROJECT_NAME=prodigy\n# VERSION=1.0.0\n# API_KEY=sk-abc123\n</code></pre> <p>Note: This feature is planned but not yet available. Use system environment variables as a workaround.</p>"},{"location":"configuration/environment-variables/#complete-workflow-example","title":"Complete Workflow Example","text":"<pre><code>name: deployment-workflow\n\nenv:\n  # Project configuration\n  PROJECT_NAME: \"my-app\"\n  VERSION: \"2.1.0\"\n\n  # Environment-specific settings\n  DEPLOY_TARGET:\n    default: \"dev-server\"\n    staging: \"staging-cluster\"\n    prod: \"prod-cluster\"\n\n  # Secrets (masked in logs)\n  DEPLOY_TOKEN:\n    secret: true\n    default: \"${DEV_TOKEN}\"\n    prod:\n      secret: true\n      value: \"${PROD_TOKEN}\"\n\ncommands:\n  - shell: \"echo Deploying $PROJECT_NAME v$VERSION to $DEPLOY_TARGET\"\n  - shell: \"docker build -t ${PROJECT_NAME}:${VERSION} .\"\n  - shell: \"deploy --target $DEPLOY_TARGET --token $DEPLOY_TOKEN\"\n  # Output: deploy --target prod-cluster --token ***\n</code></pre> <p>Run with:</p> <pre><code># Development deployment\nprodigy run deploy.yml\n\n# Production deployment\nexport PROD_TOKEN=\"secret-prod-token\"\nprodigy run deploy.yml --profile prod\n</code></pre>"},{"location":"configuration/environment-variables/#system-environment-variables","title":"System Environment Variables","text":"<p>System environment variables control Prodigy's global behavior and configuration.</p>"},{"location":"configuration/environment-variables/#claude-api-configuration","title":"Claude API Configuration","text":""},{"location":"configuration/environment-variables/#prodigy_claude_api_key","title":"<code>PRODIGY_CLAUDE_API_KEY</code>","text":"<p>Purpose: Claude API key for AI-powered commands Default: None Overrides: Global and project <code>claude_api_key</code> settings</p> <pre><code>export PRODIGY_CLAUDE_API_KEY=\"sk-ant-api03-...\"\n</code></pre> <p>This is the recommended way to provide API keys (more secure than storing in config files).</p>"},{"location":"configuration/environment-variables/#prodigy_claude_streaming","title":"<code>PRODIGY_CLAUDE_STREAMING</code>","text":"<p>Purpose: Control Claude JSON streaming output Default: <code>true</code> (streaming enabled by default) Valid values: <code>true</code>, <code>false</code></p> <pre><code>export PRODIGY_CLAUDE_STREAMING=false  # Disable streaming\n</code></pre> <p>When <code>false</code>, uses legacy print mode instead of JSON streaming.</p>"},{"location":"configuration/environment-variables/#general-configuration","title":"General Configuration","text":""},{"location":"configuration/environment-variables/#prodigy_log_level","title":"<code>PRODIGY_LOG_LEVEL</code>","text":"<p>Purpose: Logging verbosity Default: <code>info</code> Valid values: <code>trace</code>, <code>debug</code>, <code>info</code>, <code>warn</code>, <code>error</code> Overrides: Global and project <code>log_level</code> settings</p> <pre><code>export PRODIGY_LOG_LEVEL=debug\n</code></pre>"},{"location":"configuration/environment-variables/#prodigy_editor","title":"<code>PRODIGY_EDITOR</code>","text":"<p>Purpose: Default text editor for interactive operations Default: None Overrides: Global <code>default_editor</code> setting Fallback: <code>EDITOR</code> environment variable</p> <pre><code>export PRODIGY_EDITOR=vim\n</code></pre> <p>If neither <code>PRODIGY_EDITOR</code> nor <code>EDITOR</code> is set, Prodigy uses system defaults.</p>"},{"location":"configuration/environment-variables/#editor","title":"<code>EDITOR</code>","text":"<p>Purpose: Standard Unix editor variable (fallback) Default: None Fallback for: <code>PRODIGY_EDITOR</code></p> <pre><code>export EDITOR=nano\n</code></pre> <p>Precedence: <code>PRODIGY_EDITOR</code> takes precedence over <code>EDITOR</code> if both are set.</p>"},{"location":"configuration/environment-variables/#prodigy_auto_commit","title":"<code>PRODIGY_AUTO_COMMIT</code>","text":"<p>Purpose: Automatic commit after successful commands Default: <code>true</code> Valid values: <code>true</code>, <code>false</code> Overrides: Global and project <code>auto_commit</code> settings</p> <pre><code>export PRODIGY_AUTO_COMMIT=false\n</code></pre>"},{"location":"configuration/environment-variables/#storage-configuration","title":"Storage Configuration","text":""},{"location":"configuration/environment-variables/#prodigy_storage_type","title":"<code>PRODIGY_STORAGE_TYPE</code>","text":"<p>Purpose: Storage backend type Default: <code>file</code> Valid values: <code>file</code>, <code>memory</code> Overrides: Storage <code>backend</code> setting</p> <pre><code>export PRODIGY_STORAGE_TYPE=file\n</code></pre>"},{"location":"configuration/environment-variables/#prodigy_storage_base_path","title":"<code>PRODIGY_STORAGE_BASE_PATH</code>","text":"<p>Purpose: Base directory for file storage Default: <code>~/.prodigy</code> Overrides: Storage <code>backend_config.base_dir</code> setting</p> <pre><code>export PRODIGY_STORAGE_BASE_PATH=/custom/storage/path\n</code></pre> <p>Alternative names (deprecated, use <code>PRODIGY_STORAGE_BASE_PATH</code>): - <code>PRODIGY_STORAGE_DIR</code> - <code>PRODIGY_STORAGE_PATH</code></p>"},{"location":"configuration/environment-variables/#workflow-execution","title":"Workflow Execution","text":""},{"location":"configuration/environment-variables/#prodigy_automation","title":"<code>PRODIGY_AUTOMATION</code>","text":"<p>Purpose: Signal automated execution mode Default: Not set Set by: Prodigy when executing workflows</p> <pre><code>export PRODIGY_AUTOMATION=true\n</code></pre> <p>This variable is set automatically by Prodigy during workflow execution. It signals to Claude and other tools that execution is automated (not interactive).</p>"},{"location":"configuration/environment-variables/#prodigy_claude_console_output","title":"<code>PRODIGY_CLAUDE_CONSOLE_OUTPUT</code>","text":"<p>Purpose: Force Claude streaming output regardless of verbosity Default: Not set Valid values: <code>true</code>, <code>false</code></p> <pre><code>export PRODIGY_CLAUDE_CONSOLE_OUTPUT=true\n</code></pre> <p>When set to <code>true</code>, forces JSON streaming output even when verbosity is 0. Useful for debugging specific runs without changing command flags.</p>"},{"location":"configuration/environment-variables/#complete-example","title":"Complete Example","text":"<p>Set up a complete Prodigy environment:</p> <pre><code># API key (recommended method)\nexport PRODIGY_CLAUDE_API_KEY=\"sk-ant-api03-...\"\n\n# Logging\nexport PRODIGY_LOG_LEVEL=info\n\n# Editor\nexport PRODIGY_EDITOR=code\n\n# Behavior\nexport PRODIGY_AUTO_COMMIT=true\n\n# Storage\nexport PRODIGY_STORAGE_TYPE=file\nexport PRODIGY_STORAGE_BASE_PATH=/Users/username/.prodigy\n\n# Development/debugging\nexport PRODIGY_CLAUDE_STREAMING=true\nexport PRODIGY_CLAUDE_CONSOLE_OUTPUT=false\n</code></pre>"},{"location":"configuration/environment-variables/#environment-files","title":"Environment Files","text":"<p>You can use <code>.env</code> files (not committed to version control) to manage environment variables:</p> <pre><code># .env (add to .gitignore)\nPRODIGY_CLAUDE_API_KEY=sk-ant-api03-...\nPRODIGY_LOG_LEVEL=debug\nPRODIGY_AUTO_COMMIT=false\n</code></pre> <p>Load with:</p> <pre><code># Using direnv\neval \"$(cat .env)\"\n\n# Using dotenv tool\ndotenv run prodigy run workflow.yml\n\n# Manually\nexport $(cat .env | xargs)\n</code></pre>"},{"location":"configuration/environment-variables/#security-best-practices","title":"Security Best Practices","text":"<ol> <li>Never commit API keys to version control</li> <li>Use environment variables for secrets (not config files)</li> <li>Use <code>.env</code> files (gitignored) for local development</li> <li>Use secret managers (AWS Secrets Manager, Vault) in production</li> <li>Rotate keys regularly and use project-specific keys when possible</li> </ol> <p>Example <code>.gitignore</code>:</p> <pre><code>.env\n.env.*\n!.env.example\n.prodigy/config.local.yml\n</code></pre>"},{"location":"configuration/environment-variables/#precedence-summary","title":"Precedence Summary","text":"<p>For any given setting, the effective value comes from (highest to lowest):</p> <ol> <li>CLI flags (if applicable)</li> <li>Environment variables \u2190 This level</li> <li>Project config (<code>.prodigy/config.yml</code>)</li> <li>Global config (<code>~/.prodigy/config.yml</code>)</li> <li>Defaults (built-in values)</li> </ol> <p>Example:</p> <pre><code># ~/.prodigy/config.yml\nlog_level: info\n</code></pre> <pre><code># .prodigy/config.yml\nlog_level: warn\n</code></pre> <pre><code>export PRODIGY_LOG_LEVEL=debug  # This wins\n</code></pre> <p>Result: <code>log_level: debug</code></p>"},{"location":"configuration/environment-variables/#checking-environment-variables","title":"Checking Environment Variables","text":"<p>To see which environment variables are active:</p> <pre><code># List all PRODIGY_* variables\nenv | grep PRODIGY_\n\n# Check effective configuration (merges all sources)\nprodigy config show\n</code></pre>"},{"location":"configuration/environment-variables/#see-also","title":"See Also","text":"<ul> <li>Configuration Precedence Rules</li> <li>Global Configuration Structure</li> <li>Project Configuration Structure</li> <li>Storage Configuration</li> </ul>"},{"location":"configuration/global-config/","title":"Global Configuration","text":"<p>Global configuration settings apply across all Prodigy projects and workflows. These settings are stored in your user home directory and can be overridden by project-specific configuration or environment variables.</p>"},{"location":"configuration/global-config/#configuration-structure","title":"Configuration Structure","text":"<p>The global configuration is defined by the <code>GlobalConfig</code> struct, which contains system-wide settings for Prodigy's behavior, external tool integration, and operational parameters.</p> <pre><code>// Source: src/config/mod.rs:45-59\n/// Global configuration settings for Prodigy\n///\n/// These settings apply across all projects and workflows. Can be overridden\n/// by project-specific configuration. Stored in the user's home\n/// directory under ~/.prodigy/config.yml.\npub struct GlobalConfig {\n    pub prodigy_home: PathBuf,\n    pub default_editor: Option&lt;String&gt;,\n    pub log_level: Option&lt;String&gt;,\n    pub claude_api_key: Option&lt;String&gt;,\n    pub max_concurrent_specs: Option&lt;u32&gt;,\n    pub auto_commit: Option&lt;bool&gt;,\n    pub plugins: Option&lt;PluginConfig&gt;,\n}\n</code></pre>"},{"location":"configuration/global-config/#configuration-fields","title":"Configuration Fields","text":"Field Type Description Default <code>prodigy_home</code> <code>PathBuf</code> Directory for Prodigy data, sessions, and state <code>~/.prodigy</code> <code>default_editor</code> <code>Option&lt;String&gt;</code> Default text editor for editing files <code>None</code> (uses <code>$EDITOR</code>) <code>log_level</code> <code>Option&lt;String&gt;</code> Logging verbosity level <code>\"info\"</code> <code>claude_api_key</code> <code>Option&lt;String&gt;</code> Claude API key for authentication <code>None</code> <code>max_concurrent_specs</code> <code>Option&lt;u32&gt;</code> Maximum number of concurrent spec executions <code>1</code> <code>auto_commit</code> <code>Option&lt;bool&gt;</code> Automatically commit changes after successful commands <code>true</code> <code>plugins</code> <code>Option&lt;PluginConfig&gt;</code> Plugin system configuration <code>None</code>"},{"location":"configuration/global-config/#file-locations","title":"File Locations","text":""},{"location":"configuration/global-config/#global-configuration_1","title":"Global Configuration","text":"<p>The global configuration file is stored in the Prodigy data directory:</p> <pre><code>~/.prodigy/config.yml\n</code></pre> <p>The global directory location is determined by platform-specific conventions:</p> <pre><code>// Source: src/config/mod.rs:26-31\npub fn get_global_prodigy_dir() -&gt; Result&lt;PathBuf&gt; {\n    ProjectDirs::from(\"com\", \"prodigy\", \"prodigy\")\n        .map(|dirs| dirs.data_dir().to_path_buf())\n        .ok_or_else(|| anyhow!(\"Could not determine home directory\"))\n}\n</code></pre> <p>Platform-specific paths:</p> LinuxmacOSWindows <pre><code>~/.local/share/prodigy/\n</code></pre> <pre><code>~/Library/Application Support/com.prodigy.prodigy/\n</code></pre> <pre><code>C:\\Users\\&lt;username&gt;\\AppData\\Roaming\\prodigy\\prodigy\\\n</code></pre>"},{"location":"configuration/global-config/#project-configuration","title":"Project Configuration","text":"<p>Project-specific configuration files are stored within the project directory:</p> <pre><code>&lt;project-root&gt;/.prodigy/config.yml\n</code></pre>"},{"location":"configuration/global-config/#configuration-precedence","title":"Configuration Precedence","text":"<p>Prodigy uses a hierarchical configuration system where settings can be overridden at multiple levels. The precedence order from highest to lowest is:</p> <pre><code>graph TD\n    Env[Environment Variables&lt;br/&gt;PRODIGY_*] --&gt; Project[Project Config&lt;br/&gt;.prodigy/config.yml]\n    Project --&gt; Global[Global Config&lt;br/&gt;~/.prodigy/config.yml]\n    Global --&gt; Defaults[Default Values&lt;br/&gt;Hardcoded]\n\n    Env -.-&gt;|Highest Priority| Resolution[Final Value]\n    Project -.-&gt;|Override Global| Resolution\n    Global -.-&gt;|Override Defaults| Resolution\n    Defaults -.-&gt;|Lowest Priority| Resolution\n\n    style Env fill:#e8f5e9\n    style Project fill:#e1f5ff\n    style Global fill:#fff3e0\n    style Defaults fill:#f5f5f5\n    style Resolution fill:#e8eaf6</code></pre> <p>Figure: Configuration precedence hierarchy showing how values are resolved from highest to lowest priority.</p> <p>Precedence Order</p> <p>Configuration values are resolved in this order: Environment Variables \u2192 Project Config \u2192 Global Config \u2192 Defaults. The first defined value wins.</p>"},{"location":"configuration/global-config/#precedence-examples","title":"Precedence Examples","text":"<p>Claude API Key Resolution:</p> <pre><code>// Source: src/config/mod.rs:133-138\npub fn get_claude_api_key(&amp;self) -&gt; Option&lt;&amp;str&gt; {\n    self.project\n        .as_ref()\n        .and_then(|p| p.claude_api_key.as_deref())\n        .or(self.global.claude_api_key.as_deref())\n}\n</code></pre> <ol> <li><code>PRODIGY_CLAUDE_API_KEY</code> environment variable (highest priority)</li> <li><code>claude_api_key</code> in project <code>.prodigy/config.yml</code></li> <li><code>claude_api_key</code> in global <code>~/.prodigy/config.yml</code></li> <li><code>None</code> (no API key configured)</li> </ol> <p>Auto-Commit Behavior:</p> <pre><code>// Source: src/config/mod.rs:140-146\npub fn get_auto_commit(&amp;self) -&gt; bool {\n    self.project\n        .as_ref()\n        .and_then(|p| p.auto_commit)\n        .or(self.global.auto_commit)\n        .unwrap_or(true)\n}\n</code></pre> <ol> <li><code>PRODIGY_AUTO_COMMIT</code> environment variable</li> <li><code>auto_commit</code> in project config</li> <li><code>auto_commit</code> in global config</li> <li><code>true</code> (default)</li> </ol>"},{"location":"configuration/global-config/#environment-variables","title":"Environment Variables","text":"<p>Environment variables provide the highest-priority configuration mechanism, useful for CI/CD environments, temporary overrides, and secrets management.</p> <p>When to Use Environment Variables</p> <p>Use environment variables for:</p> <ul> <li>CI/CD pipelines - Keep secrets out of version control</li> <li>Temporary overrides - Test different settings without modifying config files</li> <li>Multi-environment deployments - Same workflow, different configs per environment</li> <li>Sensitive credentials - API keys and tokens that shouldn't be stored in files</li> </ul>"},{"location":"configuration/global-config/#supported-environment-variables","title":"Supported Environment Variables","text":"<pre><code>// Source: src/config/mod.rs:111-131\npub fn merge_env_vars(&amp;mut self) {\n    if let Ok(api_key) = std::env::var(\"PRODIGY_CLAUDE_API_KEY\") {\n        self.global.claude_api_key = Some(api_key);\n    }\n\n    if let Ok(log_level) = std::env::var(\"PRODIGY_LOG_LEVEL\") {\n        self.global.log_level = Some(log_level);\n    }\n\n    if let Ok(editor) = std::env::var(\"PRODIGY_EDITOR\") {\n        self.global.default_editor = Some(editor);\n    } else if let Ok(editor) = std::env::var(\"EDITOR\") {\n        self.global.default_editor = Some(editor);\n    }\n\n    if let Ok(auto_commit) = std::env::var(\"PRODIGY_AUTO_COMMIT\") {\n        if let Ok(value) = auto_commit.parse::&lt;bool&gt;() {\n            self.global.auto_commit = Some(value);\n        }\n    }\n}\n</code></pre> Variable Purpose Example <code>PRODIGY_CLAUDE_API_KEY</code> Override Claude API key <code>sk-ant-api03-...</code> <code>PRODIGY_LOG_LEVEL</code> Set logging verbosity <code>debug</code>, <code>info</code>, <code>warn</code>, <code>error</code> <code>PRODIGY_EDITOR</code> Override default editor <code>vim</code>, <code>code</code>, <code>nano</code> <code>EDITOR</code> System default editor (fallback) <code>vim</code>, <code>emacs</code> <code>PRODIGY_AUTO_COMMIT</code> Enable/disable auto-commit <code>true</code>, <code>false</code>"},{"location":"configuration/global-config/#environment-variable-usage","title":"Environment Variable Usage","text":"<pre><code># Temporary API key override for a single command\nPRODIGY_CLAUDE_API_KEY=sk-ant-api03-xxx prodigy run workflow.yml\n\n# Enable debug logging for troubleshooting\nPRODIGY_LOG_LEVEL=debug prodigy run workflow.yml\n\n# Disable auto-commit for manual control\nPRODIGY_AUTO_COMMIT=false prodigy run workflow.yml\n</code></pre>"},{"location":"configuration/global-config/#claude-settings","title":"Claude Settings","text":"<p>Claude integration is configured through the <code>claude_api_key</code> field, which can be set at multiple levels.</p>"},{"location":"configuration/global-config/#api-key-configuration","title":"API Key Configuration","text":"<p>Global Configuration:</p> <pre><code># ~/.prodigy/config.yml\nclaude_api_key: sk-ant-api03-xxxxxxxxxxxxx\nlog_level: info\n</code></pre> <p>Project Configuration Override:</p> <pre><code># .prodigy/config.yml\nname: my-project\nclaude_api_key: sk-ant-api03-project-specific-key\n</code></pre> <p>Environment Variable (Recommended for CI/CD):</p> <pre><code>export PRODIGY_CLAUDE_API_KEY=sk-ant-api03-xxxxxxxxxxxxx\n</code></pre> <p>API Key Security</p> <p>Never commit API keys to version control. Use environment variables or secure secret management systems for production environments.</p>"},{"location":"configuration/global-config/#best-practices-for-api-key-management","title":"Best Practices for API Key Management","text":"<p>API Key Storage Strategy</p> <p>Choose the right storage level for your use case:</p> <ol> <li>Development: Use global config file (<code>~/.prodigy/config.yml</code>) - convenient for local work</li> <li>CI/CD: Use environment variables (<code>PRODIGY_CLAUDE_API_KEY</code>) - keeps secrets out of code</li> <li>Multi-tenant: Use project-specific config for different API keys per project</li> <li>Production/Sensitive: Use secret management tools (Vault, AWS Secrets Manager, GitHub Secrets)</li> </ol>"},{"location":"configuration/global-config/#configuration-loading","title":"Configuration Loading","text":"<p>Prodigy uses a multi-stage configuration loading process with clear precedence rules.</p>"},{"location":"configuration/global-config/#loading-hierarchy","title":"Loading Hierarchy","text":"<pre><code>// Source: src/config/loader.rs:31-55\npub async fn load_with_explicit_path(\n    &amp;self,\n    project_path: &amp;Path,\n    explicit_path: Option&lt;&amp;Path&gt;,\n) -&gt; Result&lt;()&gt; {\n    match explicit_path {\n        Some(path) =&gt; {\n            // Load from explicit path, error if not found\n            self.load_from_path(path).await?;\n        }\n        None =&gt; {\n            // Check for .prodigy/workflow.yml\n            let default_path = project_path.join(\".prodigy\").join(\"workflow.yml\");\n            if default_path.exists() {\n                self.load_from_path(&amp;default_path).await?;\n            }\n            // Otherwise use defaults (already set in new())\n        }\n    }\n    Ok(())\n}\n</code></pre> <p>Load Order:</p> <p>Configuration Load Sequence</p> <p>Prodigy loads configuration in this sequence:</p> <ol> <li>Explicit Path: If <code>-f</code> or <code>--file</code> flag provided, load that exact file (error if not found)</li> <li>Project Default: Check for <code>.prodigy/workflow.yml</code> in project directory</li> <li>Global Defaults: Use default <code>GlobalConfig</code> values if no project config exists</li> </ol>"},{"location":"configuration/global-config/#configuration-format","title":"Configuration Format","text":"<p>Prodigy configuration files must use YAML format with the <code>.yml</code> extension.</p> <p>YAML Only</p> <p>Configuration files MUST use <code>.yml</code> extension. Files with <code>.toml</code> or other extensions will be rejected with a validation error.</p> <pre><code>// Source: src/config/loader.rs:65-69\nlet extension = path.extension().and_then(|ext| ext.to_str()).unwrap_or(\"\");\n\nvalidate_config_format(extension)\n    .with_context(|| format!(\"Invalid configuration file: {}\", path.display()))?;\n</code></pre> <p>TOML Deprecated</p> <p>Earlier versions of Prodigy supported TOML format (<code>.toml</code>), but this has been deprecated in favor of YAML for consistency with workflow definitions.</p> <p>Internal Representation</p> <p>While all Prodigy configuration files must be written in YAML format, some internal data structures (like <code>ProjectConfig.variables</code>) use <code>toml::Table</code> for historical reasons and backward compatibility. This is an implementation detail - users should always write configuration in YAML format with the <code>.yml</code> extension.</p>"},{"location":"configuration/global-config/#default-values","title":"Default Values","text":"<p>When no configuration is explicitly provided, Prodigy uses these default values:</p> <pre><code>// Source: src/config/mod.rs:88-99\nimpl Default for GlobalConfig {\n    fn default() -&gt; Self {\n        Self {\n            prodigy_home: get_global_prodigy_dir().unwrap_or_else(|_| PathBuf::from(\"~/.prodigy\")),\n            default_editor: None,\n            log_level: Some(\"info\".to_string()),\n            claude_api_key: None,\n            max_concurrent_specs: Some(1),\n            auto_commit: Some(true),\n            plugins: None,\n        }\n    }\n}\n</code></pre> Setting Default Value Reason <code>prodigy_home</code> Platform-specific (see above) Standard user data directory <code>default_editor</code> <code>None</code> Uses <code>$EDITOR</code> or <code>$PRODIGY_EDITOR</code> <code>log_level</code> <code>\"info\"</code> Balance between visibility and noise <code>claude_api_key</code> <code>None</code> Must be explicitly configured <code>max_concurrent_specs</code> <code>1</code> Sequential execution for safety <code>auto_commit</code> <code>true</code> Automatic change tracking <code>plugins</code> <code>None</code> Plugins disabled by default"},{"location":"configuration/global-config/#plugin-configuration","title":"Plugin Configuration","text":"<p>The plugin system allows extending Prodigy with custom commands and workflows.</p>"},{"location":"configuration/global-config/#plugin-structure","title":"Plugin Structure","text":"<pre><code>// Source: src/config/mod.rs:81-86\npub struct PluginConfig {\n    pub enabled: bool,\n    pub directory: PathBuf,\n    pub auto_load: Vec&lt;String&gt;,\n}\n</code></pre>"},{"location":"configuration/global-config/#enabling-plugins","title":"Enabling Plugins","text":"<pre><code># ~/.prodigy/config.yml\nplugins:\n  enabled: true\n  directory: ~/.prodigy/plugins\n  auto_load:\n    - git-helpers\n    - code-review\n    - custom-validators\n</code></pre> <p>Fields:</p> <ul> <li><code>enabled</code>: Master switch for plugin system</li> <li><code>directory</code>: Location where plugins are stored</li> <li><code>auto_load</code>: List of plugins to load automatically on startup</li> </ul>"},{"location":"configuration/global-config/#complete-configuration-examples","title":"Complete Configuration Examples","text":""},{"location":"configuration/global-config/#minimal-global-configuration","title":"Minimal Global Configuration","text":"<pre><code># ~/.prodigy/config.yml\n# Minimal configuration with only API key\nclaude_api_key: sk-ant-api03-xxxxxxxxxxxxx\n</code></pre>"},{"location":"configuration/global-config/#full-global-configuration","title":"Full Global Configuration","text":"<pre><code># ~/.prodigy/config.yml\n# Complete configuration with all options\nprodigy_home: ~/.prodigy                        # (1)!\ndefault_editor: code                            # (2)!\nlog_level: info                                 # (3)!\nclaude_api_key: sk-ant-api03-xxxxxxxxxxxxx      # (4)!\nmax_concurrent_specs: 1                         # (5)!\nauto_commit: true                               # (6)!\n\nplugins:\n  enabled: true                                 # (7)!\n  directory: ~/.prodigy/plugins                 # (8)!\n  auto_load:                                    # (9)!\n    - git-helpers\n    - code-review\n</code></pre> <ol> <li>Directory for sessions, state, events, and DLQ data</li> <li>Editor for <code>prodigy edit</code> commands (falls back to <code>$EDITOR</code>)</li> <li>Logging verbosity: <code>debug</code>, <code>info</code>, <code>warn</code>, <code>error</code></li> <li>Claude API key for authentication (keep secret!)</li> <li>Maximum concurrent MapReduce agents (start low, increase carefully)</li> <li>Automatically commit changes after successful commands</li> <li>Enable/disable the plugin system</li> <li>Where plugin scripts are stored</li> <li>Plugins to load automatically on startup</li> </ol>"},{"location":"configuration/global-config/#project-configuration-override","title":"Project Configuration Override","text":"<pre><code># &lt;project&gt;/.prodigy/config.yml\n# Project-specific overrides\nname: my-project                                        # (1)!\ndescription: Example project with custom settings       # (2)!\nversion: 1.0.0\n\n# Override global API key for this project\nclaude_api_key: sk-ant-api03-project-specific-key      # (3)!\n\n# Override auto-commit for manual control\nauto_commit: false                                      # (4)!\n\n# Project-specific variables\nvariables:                                              # (5)!\n  deploy_env: staging\n  api_url: https://staging.api.example.com\n</code></pre> <ol> <li>Project identifier used in state and event tracking</li> <li>Human-readable description for documentation</li> <li>Project-specific API key (overrides global config)</li> <li>Disable auto-commit for this project (manual <code>git commit</code> control)</li> <li>Variables available in workflows as <code>${variable_name}</code></li> </ol>"},{"location":"configuration/global-config/#environment-variable-workflow","title":"Environment Variable Workflow","text":"<pre><code>#!/bin/bash\n# CI/CD workflow using environment variables\n\n# Set configuration via environment\nexport PRODIGY_CLAUDE_API_KEY=${CLAUDE_API_KEY_SECRET}\nexport PRODIGY_LOG_LEVEL=debug\nexport PRODIGY_AUTO_COMMIT=true\n\n# Run workflow with environment-based config\nprodigy run .prodigy/workflow.yml\n</code></pre>"},{"location":"configuration/global-config/#related-documentation","title":"Related Documentation","text":"<ul> <li>Project Configuration - Project-specific settings and overrides</li> <li>Workflow Configuration - Workflow definition and structure</li> <li>Environment Variables - Complete environment variable reference</li> <li>Installation - Initial setup and configuration</li> </ul>"},{"location":"configuration/global-configuration-structure/","title":"Global Configuration Structure","text":""},{"location":"configuration/global-configuration-structure/#global-configuration-structure","title":"Global Configuration Structure","text":"<p>Global configuration is stored in <code>~/.prodigy/config.yml</code> in your home directory. These settings apply to all Prodigy projects unless overridden by project-specific configuration.</p>"},{"location":"configuration/global-configuration-structure/#location","title":"Location","text":"<ul> <li>File: <code>~/.prodigy/config.yml</code></li> <li>Created: Automatically on first run with defaults</li> <li>Format: YAML</li> </ul>"},{"location":"configuration/global-configuration-structure/#fields","title":"Fields","text":""},{"location":"configuration/global-configuration-structure/#prodigy_home","title":"<code>prodigy_home</code>","text":"<p>Type: Path Default: <code>~/.prodigy</code> (or platform-specific data directory)</p> <p>Base directory for global Prodigy data including events, DLQ, state, and worktrees.</p> <pre><code>prodigy_home: /Users/username/.prodigy\n</code></pre>"},{"location":"configuration/global-configuration-structure/#default_editor","title":"<code>default_editor</code>","text":"<p>Type: String (optional) Default: None</p> <p>Default text editor for interactive operations. Falls back to <code>EDITOR</code> environment variable if not set.</p> <pre><code>default_editor: vim\n</code></pre>"},{"location":"configuration/global-configuration-structure/#log_level","title":"<code>log_level</code>","text":"<p>Type: String (optional) Default: <code>info</code> Valid values: <code>trace</code>, <code>debug</code>, <code>info</code>, <code>warn</code>, <code>error</code></p> <p>Controls logging verbosity for Prodigy operations.</p> <pre><code>log_level: debug\n</code></pre>"},{"location":"configuration/global-configuration-structure/#claude_api_key","title":"<code>claude_api_key</code>","text":"<p>Type: String (optional) Default: None</p> <p>Claude API key for AI-powered commands. Can be overridden by project config or <code>PRODIGY_CLAUDE_API_KEY</code> environment variable.</p> <pre><code>claude_api_key: \"sk-ant-api03-...\"\n</code></pre> <p>Security Note: Store API keys in environment variables or project config (not committed to version control) rather than global config.</p>"},{"location":"configuration/global-configuration-structure/#max_concurrent_specs","title":"<code>max_concurrent_specs</code>","text":"<p>Type: Integer (optional) Default: <code>1</code></p> <p>Maximum number of concurrent spec implementations to run in parallel.</p> <pre><code>max_concurrent_specs: 3\n</code></pre>"},{"location":"configuration/global-configuration-structure/#auto_commit","title":"<code>auto_commit</code>","text":"<p>Type: Boolean (optional) Default: <code>true</code></p> <p>Whether to automatically commit changes after successful command execution.</p> <pre><code>auto_commit: false\n</code></pre>"},{"location":"configuration/global-configuration-structure/#storage","title":"<code>storage</code>","text":"<p>Type: Object (optional) Default: File storage in <code>~/.prodigy</code></p> <p>Storage backend configuration for events, DLQ, state, and worktrees. See Storage Configuration for details.</p> <pre><code>storage:\n  backend: file\n  backend_config:\n    base_dir: ~/.prodigy\n    repository_grouping: true\n</code></pre> <p>Storage Fields: - <code>backend</code>: Storage type (<code>file</code> or <code>memory</code>) - <code>backend_config.base_dir</code>: Base directory for file storage - <code>backend_config.repository_grouping</code>: Group data by repository name (default: true)</p> <p>See Storage Configuration for complete documentation.</p>"},{"location":"configuration/global-configuration-structure/#plugins","title":"<code>plugins</code>","text":"<p>Type: Object (optional) Default: None</p> <p>Plugin system configuration. See Plugin Configuration below.</p>"},{"location":"configuration/global-configuration-structure/#complete-example","title":"Complete Example","text":"<pre><code># ~/.prodigy/config.yml\nprodigy_home: /Users/username/.prodigy\ndefault_editor: code\nlog_level: info\nclaude_api_key: \"sk-ant-api03-...\"\nmax_concurrent_specs: 2\nauto_commit: true\n\nstorage:\n  backend: file\n  backend_config:\n    base_dir: /Users/username/.prodigy\n    repository_grouping: true\n\nplugins:\n  enabled: true\n  directory: /Users/username/.prodigy/plugins\n  auto_load:\n    - github-integration\n    - slack-notifications\n</code></pre>"},{"location":"configuration/global-configuration-structure/#plugin-configuration","title":"Plugin Configuration","text":"<p>The <code>plugins</code> field controls the plugin system:</p>"},{"location":"configuration/global-configuration-structure/#enabled","title":"<code>enabled</code>","text":"<p>Type: Boolean Default: <code>false</code></p> <p>Enable or disable the plugin system.</p> <pre><code>plugins:\n  enabled: true\n</code></pre>"},{"location":"configuration/global-configuration-structure/#directory","title":"<code>directory</code>","text":"<p>Type: Path Default: <code>~/.prodigy/plugins</code></p> <p>Directory to search for plugins.</p> <pre><code>plugins:\n  directory: /custom/plugin/path\n</code></pre>"},{"location":"configuration/global-configuration-structure/#auto_load","title":"<code>auto_load</code>","text":"<p>Type: Array of strings Default: <code>[]</code></p> <p>List of plugin names to automatically load on startup.</p> <pre><code>plugins:\n  auto_load:\n    - plugin-name-1\n    - plugin-name-2\n</code></pre>"},{"location":"configuration/global-configuration-structure/#creating-global-config","title":"Creating Global Config","text":"<p>If the file doesn't exist, create it manually:</p> <pre><code>mkdir -p ~/.prodigy\ncat &gt; ~/.prodigy/config.yml &lt;&lt; 'EOF'\nlog_level: info\nauto_commit: true\nEOF\n</code></pre>"},{"location":"configuration/global-configuration-structure/#relationship-to-project-config","title":"Relationship to Project Config","text":"<ul> <li>Global config applies to all projects</li> <li>Project config (<code>.prodigy/config.yml</code>) overrides global config per field</li> <li>Settings not specified in project config are inherited from global config</li> <li>See Configuration Precedence Rules for details</li> </ul>"},{"location":"configuration/global-configuration-structure/#see-also","title":"See Also","text":"<ul> <li>Project Configuration Structure</li> <li>Configuration Precedence Rules</li> <li>Environment Variables</li> <li>Default Values Reference</li> </ul>"},{"location":"configuration/migration-guide-toml-to-yaml/","title":"Migration Guide: TOML to YAML","text":""},{"location":"configuration/migration-guide-toml-to-yaml/#migration-guide-toml-to-yaml","title":"Migration Guide: TOML to YAML","text":"<p>Prodigy has migrated from TOML to YAML for all configuration files. This guide helps you migrate existing configurations.</p>"},{"location":"configuration/migration-guide-toml-to-yaml/#quick-migration","title":"Quick Migration","text":"<p>Before (TOML): <pre><code># .prodigy/config.toml\nname = \"my-project\"\ndescription = \"My project\"\n\n[variables]\nPROJECT_ROOT = \"/app\"\nVERSION = \"1.0.0\"\n\n[claude]\napi_key = \"sk-ant-api03-...\"\nmodel = \"claude-3-sonnet\"\n</code></pre></p> <p>After (YAML): <pre><code># .prodigy/config.yml\nname: my-project\ndescription: My project\n\nvariables:\n  PROJECT_ROOT: /app\n  VERSION: 1.0.0\n\nclaude:\n  api_key: sk-ant-api03-...\n  model: claude-3-sonnet\n</code></pre></p>"},{"location":"configuration/migration-guide-toml-to-yaml/#key-syntax-differences","title":"Key Syntax Differences","text":"Feature TOML YAML Assignment <code>key = value</code> <code>key: value</code> Sections <code>[section]</code> <code>section:</code> (nested with indentation) Strings <code>\"quoted\"</code> or <code>'quoted'</code> Usually unquoted (quote if contains <code>:</code>, <code>#</code>, etc.) Indentation Doesn't matter Critical - use 2 spaces per level Comments <code># comment</code> <code># comment</code> (same) Booleans <code>true</code>, <code>false</code> <code>true</code>, <code>false</code> (same) Arrays <code>[1, 2, 3]</code> <code>[1, 2, 3]</code> or newline-separated with <code>-</code> Nested tables <code>[table.subtable]</code> Indented structure"},{"location":"configuration/migration-guide-toml-to-yaml/#configuration-file-locations","title":"Configuration File Locations","text":"<p>Both global and project configurations have moved from <code>.toml</code> to <code>.yml</code>:</p> Type Old Location New Location Global Config <code>~/.prodigy/config.toml</code> <code>~/.prodigy/config.yml</code> Project Config <code>.prodigy/config.toml</code> <code>.prodigy/config.yml</code>"},{"location":"configuration/migration-guide-toml-to-yaml/#global-configuration-migration","title":"Global Configuration Migration","text":"<p>Before (<code>~/.prodigy/config.toml</code>): <pre><code>prodigy_home = \"/Users/username/.prodigy\"\ndefault_editor = \"vim\"\nlog_level = \"info\"\nauto_commit = true\nmax_concurrent_specs = 2\n\n[storage]\nbackend = \"file\"\n\n[storage.backend_config]\nbase_dir = \"/Users/username/.prodigy\"\nrepository_grouping = true\n\n[plugins]\nenabled = true\ndirectory = \"/Users/username/.prodigy/plugins\"\nauto_load = [\"plugin1\", \"plugin2\"]\n</code></pre></p> <p>After (<code>~/.prodigy/config.yml</code>): <pre><code>prodigy_home: /Users/username/.prodigy\ndefault_editor: vim\nlog_level: info\nauto_commit: true\nmax_concurrent_specs: 2\n\nstorage:\n  backend: file\n  backend_config:\n    base_dir: /Users/username/.prodigy\n    repository_grouping: true\n\nplugins:\n  enabled: true\n  directory: /Users/username/.prodigy/plugins\n  auto_load:\n    - plugin1\n    - plugin2\n</code></pre></p>"},{"location":"configuration/migration-guide-toml-to-yaml/#project-configuration-migration","title":"Project Configuration Migration","text":"<p>Before (<code>.prodigy/config.toml</code>): <pre><code>name = \"my-project\"\ndescription = \"A sample project\"\n\n[variables]\nPROJECT_ROOT = \"/app\"\nVERSION = \"1.0.0\"\n\n[claude]\napi_key = \"sk-ant-api03-...\"\nmodel = \"claude-3-sonnet\"\n\n[storage]\nbackend = \"file\"\n</code></pre></p> <p>After (<code>.prodigy/config.yml</code>): <pre><code>name: my-project\ndescription: A sample project\n\nvariables:\n  PROJECT_ROOT: /app\n  VERSION: \"1.0.0\"\n\nclaude:\n  api_key: sk-ant-api03-...\n  model: claude-3-sonnet\n\nstorage:\n  backend: file\n</code></pre></p>"},{"location":"configuration/migration-guide-toml-to-yaml/#array-syntax-migration","title":"Array Syntax Migration","text":"<p>TOML and YAML handle arrays differently:</p> <p>TOML: <pre><code>auto_load = [\"plugin1\", \"plugin2\", \"plugin3\"]\n</code></pre></p> <p>YAML (inline style): <pre><code>auto_load: [plugin1, plugin2, plugin3]\n</code></pre></p> <p>YAML (block style - recommended): <pre><code>auto_load:\n  - plugin1\n  - plugin2\n  - plugin3\n</code></pre></p>"},{"location":"configuration/migration-guide-toml-to-yaml/#nested-structure-migration","title":"Nested Structure Migration","text":"<p>TOML uses dotted keys or table headers for nesting. YAML uses indentation.</p> <p>TOML: <pre><code>[storage]\nbackend = \"file\"\n\n[storage.backend_config]\nbase_dir = \"/Users/username/.prodigy\"\nrepository_grouping = true\n</code></pre></p> <p>YAML: <pre><code>storage:\n  backend: file\n  backend_config:\n    base_dir: /Users/username/.prodigy\n    repository_grouping: true\n</code></pre></p>"},{"location":"configuration/migration-guide-toml-to-yaml/#string-quoting-rules","title":"String Quoting Rules","text":"<p>YAML is more relaxed about string quoting:</p> <p>When quotes are NOT needed: <pre><code>name: my-project\npath: /usr/local/bin\nurl: https://example.com\n</code></pre></p> <p>When quotes ARE needed: <pre><code># Contains colon\nmessage: \"Error: something failed\"\n\n# Contains hash (would be a comment)\ntag: \"#important\"\n\n# Starts with special character\nvalue: \"@username\"\n\n# Looks like a number but should be string\nversion: \"1.0\"\n\n# Contains YAML reserved words\nstatus: \"yes\"  # Would be boolean without quotes\n</code></pre></p>"},{"location":"configuration/migration-guide-toml-to-yaml/#boolean-values","title":"Boolean Values","text":"<p>Both use the same boolean syntax:</p> <pre><code>auto_commit: true\nenabled: false\n</code></pre> <p>Note: In YAML, these are also booleans: <code>yes</code>, <code>no</code>, <code>on</code>, <code>off</code>, <code>true</code>, <code>false</code>. To use them as strings, quote them: <code>\"yes\"</code>, <code>\"no\"</code>.</p>"},{"location":"configuration/migration-guide-toml-to-yaml/#comments","title":"Comments","text":"<p>Both use <code>#</code> for comments:</p> <pre><code># This is a comment\nlog_level: info  # Inline comment\n</code></pre>"},{"location":"configuration/migration-guide-toml-to-yaml/#migrating-workflows","title":"Migrating Workflows","text":"<p>Workflows were always YAML, so no migration is needed. However, if you referenced TOML config in workflows, update the file extension:</p> <p>Before: <pre><code>commands:\n  - shell: \"cat .prodigy/config.toml\"\n</code></pre></p> <p>After: <pre><code>commands:\n  - shell: \"cat .prodigy/config.yml\"\n</code></pre></p>"},{"location":"configuration/migration-guide-toml-to-yaml/#migration-checklist","title":"Migration Checklist","text":"<ul> <li> Rename <code>~/.prodigy/config.toml</code> \u2192 <code>~/.prodigy/config.yml</code></li> <li> Rename <code>.prodigy/config.toml</code> \u2192 <code>.prodigy/config.yml</code></li> <li> Convert TOML syntax to YAML syntax</li> <li> Update section headers <code>[section]</code> to <code>section:</code> with indentation</li> <li> Convert assignments <code>key = value</code> to <code>key: value</code></li> <li> Fix array syntax (use <code>-</code> for lists)</li> <li> Ensure consistent 2-space indentation</li> <li> Quote strings with special characters</li> <li> Test configuration with <code>prodigy config show</code></li> <li> Update any workflow references to config files</li> </ul>"},{"location":"configuration/migration-guide-toml-to-yaml/#validation","title":"Validation","text":"<p>After migration, validate your configuration:</p> <pre><code># Check effective configuration (merges global + project)\nprodigy config show\n\n# Validate YAML syntax\nyamllint .prodigy/config.yml\n\n# Run a simple workflow to verify settings work\nprodigy run test-workflow.yml\n</code></pre>"},{"location":"configuration/migration-guide-toml-to-yaml/#troubleshooting","title":"Troubleshooting","text":"<p>\"Invalid YAML syntax\" error: - Check indentation (must be 2 spaces, no tabs) - Ensure colons have space after them (<code>key: value</code>, not <code>key:value</code>) - Quote strings with special characters</p> <p>\"Configuration not found\" error: - Ensure file is named <code>config.yml</code> (not <code>config.yaml</code> or <code>config.toml</code>) - Check file is in correct location (<code>.prodigy/</code> or <code>~/.prodigy/</code>)</p> <p>\"Invalid field\" warnings: - Remove deprecated TOML-specific fields - Check spelling of field names - Refer to Configuration Structure for valid fields</p>"},{"location":"configuration/migration-guide-toml-to-yaml/#backward-compatibility","title":"Backward Compatibility","text":"<p>TOML support is deprecated and may be removed in future versions. Migration to YAML is strongly recommended.</p> <p>If you must support both formats temporarily: 1. Keep both <code>config.toml</code> and <code>config.yml</code> 2. YAML takes precedence if both exist 3. Migrate fully before upgrading to newer Prodigy versions</p>"},{"location":"configuration/migration-guide-toml-to-yaml/#see-also","title":"See Also","text":"<ul> <li>Global Configuration Structure</li> <li>Project Configuration Structure</li> <li>Configuration Precedence Rules</li> <li>YAML Specification</li> </ul>"},{"location":"configuration/project-configuration-structure/","title":"Project Configuration Structure","text":""},{"location":"configuration/project-configuration-structure/#project-configuration-structure","title":"Project Configuration Structure","text":"<p>Project configuration is stored in <code>.prodigy/config.yml</code> within your project repository. These settings override global configuration for this specific project and are typically committed to version control (except for secrets).</p>"},{"location":"configuration/project-configuration-structure/#location","title":"Location","text":"<ul> <li>File: <code>.prodigy/config.yml</code> (in project root)</li> <li>Created: Manually or via <code>prodigy init</code></li> <li>Format: YAML</li> <li>Version Control: Committed to git (recommended, except for secrets)</li> </ul>"},{"location":"configuration/project-configuration-structure/#fields","title":"Fields","text":""},{"location":"configuration/project-configuration-structure/#name-required","title":"<code>name</code> (required)","text":"<p>Type: String Default: None (required field)</p> <p>Project identifier used in logs, events, and UI.</p> <pre><code>name: my-project\n</code></pre>"},{"location":"configuration/project-configuration-structure/#description","title":"<code>description</code>","text":"<p>Type: String (optional) Default: None</p> <p>Human-readable project description.</p> <pre><code>description: \"AI-powered code analysis tool\"\n</code></pre>"},{"location":"configuration/project-configuration-structure/#version","title":"<code>version</code>","text":"<p>Type: String (optional) Default: None</p> <p>Project version (semantic versioning recommended).</p> <pre><code>version: \"1.2.3\"\n</code></pre>"},{"location":"configuration/project-configuration-structure/#spec_dir","title":"<code>spec_dir</code>","text":"<p>Type: Path (optional) Default: <code>specs</code></p> <p>Directory containing Prodigy specification files.</p> <pre><code>spec_dir: custom/specs\n</code></pre>"},{"location":"configuration/project-configuration-structure/#claude_api_key","title":"<code>claude_api_key</code>","text":"<p>Type: String (optional) Default: None (inherits from global config or environment)</p> <p>Project-specific Claude API key. Overrides global config and is overridden by environment variable.</p> <pre><code>claude_api_key: \"sk-ant-api03-...\"\n</code></pre> <p>Security Warning: Do NOT commit API keys to version control. Use environment variables or <code>.prodigy/config.local.yml</code> (gitignored) instead.</p>"},{"location":"configuration/project-configuration-structure/#auto_commit","title":"<code>auto_commit</code>","text":"<p>Type: Boolean (optional) Default: Inherits from global config (default: <code>true</code>)</p> <p>Whether to automatically commit changes after successful command execution.</p> <pre><code>auto_commit: false\n</code></pre>"},{"location":"configuration/project-configuration-structure/#variables","title":"<code>variables</code>","text":"<p>Type: Object (optional) Default: <code>{}</code></p> <p>Project-specific variables available in workflows and commands.</p> <pre><code>variables:\n  deploy_branch: production\n  test_timeout: 300\n  feature_flags:\n    new_ui: true\n    beta_features: false\n</code></pre> <p>These variables can be referenced in workflows using <code>${variable_name}</code> syntax.</p> <p>Note: For workflow-level environment variables (with secrets, profiles, and step-level overrides), use the <code>env:</code> block in workflow files instead. See Environment Variables for details.</p>"},{"location":"configuration/project-configuration-structure/#storage","title":"<code>storage</code>","text":"<p>Type: Object (optional) Default: Inherits from global config</p> <p>Project-specific storage configuration. See Storage Configuration for details.</p> <pre><code>storage:\n  backend: file\n  backend_config:\n    base_dir: /custom/project/storage\n</code></pre>"},{"location":"configuration/project-configuration-structure/#complete-example","title":"Complete Example","text":"<pre><code># .prodigy/config.yml\nname: prodigy\ndescription: \"Workflow orchestration tool for Claude Code\"\nversion: \"0.1.0\"\nspec_dir: specs\n\nauto_commit: true\n\nvariables:\n  default_branch: master\n  test_suite: full\n  timeout_seconds: 600\n</code></pre>"},{"location":"configuration/project-configuration-structure/#secrets-management","title":"Secrets Management","text":"<p>For sensitive values like API keys, use one of these approaches:</p>"},{"location":"configuration/project-configuration-structure/#option-1-environment-variables-recommended","title":"Option 1: Environment Variables (Recommended)","text":"<pre><code># .prodigy/config.yml (committed)\nname: my-project\n# No claude_api_key here\n</code></pre> <pre><code># Set in environment\nexport PRODIGY_CLAUDE_API_KEY=\"sk-ant-api03-...\"\n</code></pre>"},{"location":"configuration/project-configuration-structure/#option-2-local-config-file-not-committed","title":"Option 2: Local Config File (Not Committed)","text":"<p>Create <code>.prodigy/config.local.yml</code> and add it to <code>.gitignore</code>:</p> <pre><code># .prodigy/config.local.yml (gitignored)\nclaude_api_key: \"sk-ant-api03-...\"\n</code></pre> <pre><code># .gitignore\n.prodigy/config.local.yml\n</code></pre>"},{"location":"configuration/project-configuration-structure/#option-3-secret-management-service","title":"Option 3: Secret Management Service","text":"<p>Use a secret management service (AWS Secrets Manager, HashiCorp Vault, etc.) and retrieve the key at runtime via environment variables.</p>"},{"location":"configuration/project-configuration-structure/#relationship-to-global-config","title":"Relationship to Global Config","text":"<p>Project config overrides global config on a per-field basis:</p> <ul> <li>Fields specified in project config replace global config values</li> <li>Fields NOT specified in project config inherit global config values</li> <li>See Configuration Precedence Rules</li> </ul> <p>Example:</p> <pre><code># ~/.prodigy/config.yml (global)\nlog_level: info\nauto_commit: true\nmax_concurrent_specs: 1\n</code></pre> <pre><code># .prodigy/config.yml (project)\nname: my-project\nlog_level: debug  # Override global\n# auto_commit and max_concurrent_specs inherited from global\n</code></pre> <p>Result: Project uses <code>log_level: debug</code> but inherits <code>auto_commit: true</code> and <code>max_concurrent_specs: 1</code> from global config.</p>"},{"location":"configuration/project-configuration-structure/#project-variables-in-workflows","title":"Project Variables in Workflows","text":"<p>Variables defined in project config are available in workflows:</p> <pre><code># .prodigy/config.yml\nname: my-project\nvariables:\n  environment: staging\n  api_url: https://staging.api.example.com\n</code></pre> <pre><code># .prodigy/workflow.yml\ncommands:\n  - name: deploy\n    args: [\"${environment}\"]\n    options:\n      api_url: \"${api_url}\"\n</code></pre>"},{"location":"configuration/project-configuration-structure/#creating-project-config","title":"Creating Project Config","text":"<p>Initialize a new project:</p> <pre><code>cd my-project\nmkdir -p .prodigy\ncat &gt; .prodigy/config.yml &lt;&lt; 'EOF'\nname: my-project\nauto_commit: true\nEOF\n</code></pre> <p>Or use the init command (if available):</p> <pre><code>prodigy init\n</code></pre>"},{"location":"configuration/project-configuration-structure/#see-also","title":"See Also","text":"<ul> <li>Global Configuration Structure</li> <li>Configuration Precedence Rules</li> <li>Workflow Configuration</li> <li>Environment Variables</li> </ul>"},{"location":"configuration/related-documentation/","title":"Related Documentation","text":""},{"location":"configuration/related-documentation/#related-documentation","title":"Related Documentation","text":"<p>This section provides links to related configuration and feature documentation.</p>"},{"location":"configuration/related-documentation/#core-configuration","title":"Core Configuration","text":"<ul> <li>Global Configuration Structure - System-wide settings in <code>~/.prodigy/config.yml</code></li> <li>Project Configuration Structure - Project settings in <code>.prodigy/config.yml</code></li> <li>Environment Variables - System and workflow environment variables</li> <li>Configuration Precedence Rules - How settings override each other</li> <li>Default Values Reference - Built-in default values for all settings</li> <li>Storage Configuration - Data storage backends and options</li> </ul>"},{"location":"configuration/related-documentation/#workflow-features","title":"Workflow Features","text":"<ul> <li>Workflow Configuration - Workflow file structure and options</li> <li>Environment Variables - Workflow Section - Using <code>env:</code> blocks in workflows</li> <li>Best Practices - Configuration and workflow best practices</li> </ul>"},{"location":"configuration/related-documentation/#migration-and-troubleshooting","title":"Migration and Troubleshooting","text":"<ul> <li>Migration Guide: TOML to YAML - Migrate from old TOML format</li> <li>Troubleshooting - Common configuration issues and solutions</li> </ul>"},{"location":"configuration/related-documentation/#advanced-topics","title":"Advanced Topics","text":"<ul> <li>Git Context (Advanced) - Pattern filtering and format modifiers for git variables</li> <li>MapReduce Worktree Architecture - Understanding worktree isolation in MapReduce workflows</li> </ul>"},{"location":"configuration/related-documentation/#feature-documentation","title":"Feature Documentation","text":"<ul> <li>MapReduce - Parallel workflow execution</li> <li>Variables - Variable interpolation and usage</li> <li>Command Types - Available command types (claude, shell, goal_seek, etc.)</li> <li>Error Handling - Error handling strategies and retry policies</li> </ul>"},{"location":"configuration/related-documentation/#reference-material","title":"Reference Material","text":"<ul> <li>Complete Configuration Examples - Real-world configuration examples</li> <li>YAML Specification - Official YAML documentation</li> </ul>"},{"location":"configuration/storage-configuration/","title":"Storage Configuration","text":""},{"location":"configuration/storage-configuration/#storage-configuration","title":"Storage Configuration","text":"<p>Storage configuration controls where and how Prodigy stores data including events, state, DLQ (Dead Letter Queue), and worktrees. By default, Prodigy uses global storage (<code>~/.prodigy/</code>) to enable cross-worktree data sharing and centralized management.</p>"},{"location":"configuration/storage-configuration/#storage-backend-types","title":"Storage Backend Types","text":""},{"location":"configuration/storage-configuration/#file-storage-default","title":"File Storage (Default)","text":"<p>Stores data as files in the filesystem. This is the recommended backend for production use.</p> <pre><code>storage:\n  backend: file\n  backend_config:\n    base_dir: /Users/username/.prodigy\n    use_global: true  # Default: use global storage\n    enable_file_locks: true\n    max_file_size: 104857600  # 100MB\n    enable_compression: false\n</code></pre>"},{"location":"configuration/storage-configuration/#memory-storage","title":"Memory Storage","text":"<p>Stores data in memory. Useful for testing but data is lost when the process exits.</p> <pre><code>storage:\n  backend: memory\n  backend_config:\n    max_memory: 104857600  # 100MB\n    persist_to_disk: false\n</code></pre>"},{"location":"configuration/storage-configuration/#file-storage-configuration","title":"File Storage Configuration","text":""},{"location":"configuration/storage-configuration/#base_dir","title":"<code>base_dir</code>","text":"<p>Type: Path Default: <code>~/.prodigy</code></p> <p>Base directory for all storage operations.</p> <pre><code>storage:\n  backend: file\n  backend_config:\n    base_dir: /custom/storage/path\n</code></pre>"},{"location":"configuration/storage-configuration/#use_global","title":"<code>use_global</code>","text":"<p>Type: Boolean Default: <code>true</code></p> <p>IMPORTANT: Global storage is the default and recommended setting. When <code>true</code>, Prodigy stores all data in <code>~/.prodigy/</code> organized by repository:</p> <pre><code>~/.prodigy/\n\u251c\u2500\u2500 events/{repo_name}/{job_id}/\n\u251c\u2500\u2500 dlq/{repo_name}/{job_id}/\n\u251c\u2500\u2500 state/{repo_name}/mapreduce/jobs/{job_id}/\n\u2514\u2500\u2500 worktrees/{repo_name}/{session_id}/\n</code></pre> <p>Benefits of global storage: - Cross-worktree event aggregation: Multiple worktrees share event logs - Persistent state: Job checkpoints survive worktree cleanup - Centralized monitoring: All job data in one location - Efficient storage: Deduplication across worktrees</p> <p>When <code>false</code> (deprecated, local storage): <pre><code>.prodigy/  # In project directory\n\u251c\u2500\u2500 events/\n\u251c\u2500\u2500 dlq/\n\u2514\u2500\u2500 state/\n</code></pre></p> <p>Recommendation: Always use <code>use_global: true</code> (the default).</p>"},{"location":"configuration/storage-configuration/#enable_file_locks","title":"<code>enable_file_locks</code>","text":"<p>Type: Boolean Default: <code>true</code></p> <p>Enable file-based locking to prevent concurrent access conflicts.</p> <pre><code>storage:\n  backend: file\n  backend_config:\n    enable_file_locks: true\n</code></pre>"},{"location":"configuration/storage-configuration/#max_file_size","title":"<code>max_file_size</code>","text":"<p>Type: Integer (bytes) Default: <code>104857600</code> (100MB)</p> <p>Maximum file size before rotation.</p> <pre><code>storage:\n  backend: file\n  backend_config:\n    max_file_size: 209715200  # 200MB\n</code></pre>"},{"location":"configuration/storage-configuration/#enable_compression","title":"<code>enable_compression</code>","text":"<p>Type: Boolean Default: <code>false</code></p> <p>Enable compression for archived files.</p> <pre><code>storage:\n  backend: file\n  backend_config:\n    enable_compression: true\n</code></pre>"},{"location":"configuration/storage-configuration/#connection-and-performance","title":"Connection and Performance","text":""},{"location":"configuration/storage-configuration/#connection_pool_size","title":"<code>connection_pool_size</code>","text":"<p>Type: Integer Default: <code>10</code></p> <p>Connection pool size for backend operations (future database backends).</p> <pre><code>storage:\n  connection_pool_size: 20\n</code></pre>"},{"location":"configuration/storage-configuration/#timeout","title":"<code>timeout</code>","text":"<p>Type: Duration Default: <code>30s</code></p> <p>Default timeout for storage operations.</p> <pre><code>storage:\n  timeout: 60s  # Format: 60s, 1m, 1h\n</code></pre>"},{"location":"configuration/storage-configuration/#enable_locking","title":"<code>enable_locking</code>","text":"<p>Type: Boolean Default: <code>true</code></p> <p>Enable distributed locking for concurrent access control.</p> <pre><code>storage:\n  enable_locking: true\n</code></pre>"},{"location":"configuration/storage-configuration/#retry-policy","title":"Retry Policy","text":"<p>Configure automatic retries for transient failures:</p> <pre><code>storage:\n  retry_policy:\n    max_retries: 3\n    initial_delay: 1s\n    max_delay: 30s\n    backoff_multiplier: 2.0\n    jitter: true\n</code></pre>"},{"location":"configuration/storage-configuration/#max_retries","title":"<code>max_retries</code>","text":"<p>Type: Integer Default: <code>3</code></p> <p>Maximum number of retry attempts.</p>"},{"location":"configuration/storage-configuration/#initial_delay","title":"<code>initial_delay</code>","text":"<p>Type: Duration Default: <code>1s</code></p> <p>Initial delay before first retry.</p>"},{"location":"configuration/storage-configuration/#max_delay","title":"<code>max_delay</code>","text":"<p>Type: Duration Default: <code>30s</code></p> <p>Maximum delay between retries (with exponential backoff).</p>"},{"location":"configuration/storage-configuration/#backoff_multiplier","title":"<code>backoff_multiplier</code>","text":"<p>Type: Float Default: <code>2.0</code></p> <p>Multiplier for exponential backoff (delay doubles each retry).</p>"},{"location":"configuration/storage-configuration/#jitter","title":"<code>jitter</code>","text":"<p>Type: Boolean Default: <code>true</code></p> <p>Add random jitter to retry delays to avoid thundering herd.</p>"},{"location":"configuration/storage-configuration/#cache-configuration","title":"Cache Configuration","text":"<p>Optional caching layer for improved performance:</p> <pre><code>storage:\n  enable_cache: true\n  cache_config:\n    max_entries: 1000\n    ttl: 1h\n    cache_type: memory\n</code></pre>"},{"location":"configuration/storage-configuration/#enable_cache","title":"<code>enable_cache</code>","text":"<p>Type: Boolean Default: <code>false</code></p> <p>Enable in-memory caching.</p>"},{"location":"configuration/storage-configuration/#max_entries","title":"<code>max_entries</code>","text":"<p>Type: Integer Default: <code>1000</code></p> <p>Maximum number of cached entries.</p>"},{"location":"configuration/storage-configuration/#ttl","title":"<code>ttl</code>","text":"<p>Type: Duration Default: <code>1h</code></p> <p>Cache time-to-live.</p>"},{"location":"configuration/storage-configuration/#cache_type","title":"<code>cache_type</code>","text":"<p>Type: String Default: <code>memory</code> Valid values: <code>memory</code></p> <p>Cache implementation type (currently only memory is supported).</p>"},{"location":"configuration/storage-configuration/#complete-example","title":"Complete Example","text":"<pre><code>storage:\n  backend: file\n  connection_pool_size: 10\n  timeout: 30s\n  enable_locking: true\n  enable_cache: false\n\n  backend_config:\n    base_dir: /Users/username/.prodigy\n    use_global: true\n    enable_file_locks: true\n    max_file_size: 104857600\n    enable_compression: false\n\n  retry_policy:\n    max_retries: 3\n    initial_delay: 1s\n    max_delay: 30s\n    backoff_multiplier: 2.0\n    jitter: true\n\n  cache_config:\n    max_entries: 1000\n    ttl: 1h\n    cache_type: memory\n</code></pre>"},{"location":"configuration/storage-configuration/#environment-variable-configuration","title":"Environment Variable Configuration","text":"<p>Storage can be configured via environment variables:</p> <pre><code># Backend type (file or memory)\nexport PRODIGY_STORAGE_TYPE=file\n\n# Base directory path\nexport PRODIGY_STORAGE_BASE_PATH=/custom/path\n\n# Alternative variable names (deprecated)\nexport PRODIGY_STORAGE_DIR=/custom/path\nexport PRODIGY_STORAGE_PATH=/custom/path\n</code></pre> <p>Environment variables override file-based configuration. See Environment Variables for complete list.</p>"},{"location":"configuration/storage-configuration/#storage-directory-structure","title":"Storage Directory Structure","text":"<p>With global storage enabled (default):</p> <pre><code>~/.prodigy/\n\u251c\u2500\u2500 events/\n\u2502   \u2514\u2500\u2500 {repo_name}/\n\u2502       \u2514\u2500\u2500 {job_id}/\n\u2502           \u2514\u2500\u2500 events-{timestamp}.jsonl\n\u251c\u2500\u2500 dlq/\n\u2502   \u2514\u2500\u2500 {repo_name}/\n\u2502       \u2514\u2500\u2500 {job_id}/\n\u2502           \u2514\u2500\u2500 items.json\n\u251c\u2500\u2500 state/\n\u2502   \u2514\u2500\u2500 {repo_name}/\n\u2502       \u2514\u2500\u2500 mapreduce/\n\u2502           \u2514\u2500\u2500 jobs/\n\u2502               \u2514\u2500\u2500 {job_id}/\n\u2502                   \u251c\u2500\u2500 setup-checkpoint.json\n\u2502                   \u251c\u2500\u2500 map-checkpoint-{timestamp}.json\n\u2502                   \u2514\u2500\u2500 reduce-checkpoint-v1-{timestamp}.json\n\u251c\u2500\u2500 worktrees/\n\u2502   \u2514\u2500\u2500 {repo_name}/\n\u2502       \u2514\u2500\u2500 session-{session_id}/\n\u2514\u2500\u2500 orphaned_worktrees/\n    \u2514\u2500\u2500 {repo_name}/\n        \u2514\u2500\u2500 {job_id}.json\n</code></pre>"},{"location":"configuration/storage-configuration/#migration-from-local-storage","title":"Migration from Local Storage","text":"<p>If you have existing local storage (<code>.prodigy/</code> in project directory), migrate to global storage:</p> <ol> <li>Automatic: Set <code>use_global: true</code> (default) - Prodigy creates new global storage</li> <li>Manual Migration: Copy existing data to global storage structure</li> <li>Coexistence: Old local storage is ignored once global storage is active</li> </ol> <p>Note: Global storage is the default and recommended approach. Local storage is deprecated.</p>"},{"location":"configuration/storage-configuration/#troubleshooting","title":"Troubleshooting","text":"<p>Issue: \"Failed to acquire storage lock\" Solution: Check for stuck processes, wait for lock release, or disable locking temporarily</p> <p>Issue: \"Storage directory not writable\" Solution: Check permissions on <code>~/.prodigy</code> directory</p> <p>Issue: \"Disk space full\" Solution: Clean up old events/DLQ data, increase <code>max_file_size</code>, enable compression</p>"},{"location":"configuration/storage-configuration/#see-also","title":"See Also","text":"<ul> <li>Environment Variables</li> <li>Default Values Reference</li> <li>Troubleshooting</li> </ul>"},{"location":"configuration/troubleshooting/","title":"Troubleshooting","text":""},{"location":"configuration/troubleshooting/#troubleshooting","title":"Troubleshooting","text":"<p>Common configuration issues and their solutions.</p>"},{"location":"configuration/troubleshooting/#configuration-file-issues","title":"Configuration File Issues","text":""},{"location":"configuration/troubleshooting/#configuration-file-not-found","title":"\"Configuration file not found\"","text":"<p>Symptoms: <pre><code>Error: Configuration file not found at ~/.prodigy/config.yml\n</code></pre></p> <p>Causes: - Config file doesn't exist - Wrong file extension (<code>.yaml</code> instead of <code>.yml</code>, or <code>.toml</code> instead of <code>.yml</code>) - File is in the wrong location</p> <p>Solutions: 1. Create the file:    <pre><code>mkdir -p ~/.prodigy\ncat &gt; ~/.prodigy/config.yml &lt;&lt; 'EOF'\nlog_level: info\nauto_commit: true\nEOF\n</code></pre></p> <ol> <li> <p>Check file extension:    <pre><code>ls -la ~/.prodigy/config.*\n# Should show config.yml, not config.yaml or config.toml\n</code></pre></p> </li> <li> <p>Verify location:    <pre><code># Global config\nls ~/.prodigy/config.yml\n\n# Project config\nls .prodigy/config.yml\n</code></pre></p> </li> </ol>"},{"location":"configuration/troubleshooting/#invalid-yaml-syntax","title":"\"Invalid YAML syntax\"","text":"<p>Symptoms: <pre><code>Error: Failed to parse configuration: invalid YAML syntax at line 10\n</code></pre></p> <p>Causes: - Incorrect indentation (must use 2 spaces, no tabs) - Missing space after colon (<code>key:value</code> instead of <code>key: value</code>) - Unquoted strings with special characters - Mixing TOML and YAML syntax</p> <p>Solutions: 1. Check indentation (must be 2 spaces):    <pre><code># \u2717 Wrong (tabs or 4 spaces)\nstorage:\n    backend: file\n\n# \u2713 Correct (2 spaces)\nstorage:\n  backend: file\n</code></pre></p> <ol> <li> <p>Add space after colons:    <pre><code># \u2717 Wrong\nlog_level:debug\n\n# \u2713 Correct\nlog_level: debug\n</code></pre></p> </li> <li> <p>Quote strings with special characters:    <pre><code># \u2717 Wrong\nmessage: Error: something failed\n\n# \u2713 Correct\nmessage: \"Error: something failed\"\n</code></pre></p> </li> <li> <p>Use a YAML validator:    <pre><code>yamllint ~/.prodigy/config.yml\n</code></pre></p> </li> </ol>"},{"location":"configuration/troubleshooting/#unknown-field-in-configuration","title":"\"Unknown field in configuration\"","text":"<p>Symptoms: <pre><code>Warning: Unknown field 'unknown_setting' in configuration\n</code></pre></p> <p>Causes: - Typo in field name - Using deprecated field name - Field from old TOML format</p> <p>Solutions: 1. Check spelling against Global Configuration Structure or Project Configuration Structure</p> <ol> <li> <p>Remove deprecated fields:    <pre><code># \u2717 Deprecated TOML-style\n[storage]\nbackend = \"file\"\n\n# \u2713 Correct YAML\nstorage:\n  backend: file\n</code></pre></p> </li> <li> <p>Update field names from old versions</p> </li> </ol>"},{"location":"configuration/troubleshooting/#environment-variable-issues","title":"Environment Variable Issues","text":""},{"location":"configuration/troubleshooting/#environment-variable-not-resolving","title":"\"Environment variable not resolving\"","text":"<p>Symptoms: <pre><code>Error: Variable 'API_KEY' not found\n</code></pre></p> <p>Causes: - Variable not defined in any configuration source - Incorrect variable syntax in workflow - Profile not activated</p> <p>Solutions: 1. Check variable is defined:    <pre><code># System env\necho $PRODIGY_API_KEY\n\n# Workflow env (check workflow.yml)\ngrep API_KEY workflow.yml\n</code></pre></p> <ol> <li> <p>Use correct syntax:    <pre><code># \u2717 Wrong\ncommand: \"curl $API_KEY\"\n\n# \u2713 Correct\ncommand: \"curl ${API_KEY}\"\n</code></pre></p> </li> <li> <p>Activate profile if using profile-specific values:    <pre><code>prodigy run workflow.yml --profile prod\n</code></pre></p> </li> <li> <p>Check precedence chain: Step env &gt; Profile env &gt; Workflow env &gt; System env</p> </li> </ol>"},{"location":"configuration/troubleshooting/#secret-not-being-masked-in-logs","title":"\"Secret not being masked in logs\"","text":"<p>Symptoms: <pre><code>Output: curl -H 'Authorization: Bearer sk-abc123...'\n</code></pre></p> <p>Causes: - Secret not marked with <code>secret: true</code> in workflow env block - Using system env vars (not masked automatically)</p> <p>Solutions: <pre><code># Mark as secret in workflow\nenv:\n  API_KEY:\n    secret: true\n    value: \"${PROD_API_KEY}\"  # From system env\n</code></pre></p> <p>Note: Only workflow env vars marked as <code>secret: true</code> are masked. System environment variables are not automatically masked.</p>"},{"location":"configuration/troubleshooting/#storage-issues","title":"Storage Issues","text":""},{"location":"configuration/troubleshooting/#storage-directory-not-writable","title":"\"Storage directory not writable\"","text":"<p>Symptoms: <pre><code>Error: Failed to write to storage: Permission denied\n</code></pre></p> <p>Causes: - Insufficient permissions on <code>~/.prodigy</code> directory - Directory owned by different user - Disk full</p> <p>Solutions: 1. Check permissions:    <pre><code>ls -ld ~/.prodigy\n# Should show: drwxr-xr-x username username\n</code></pre></p> <ol> <li> <p>Fix ownership:    <pre><code>sudo chown -R $USER:$USER ~/.prodigy\nchmod -R u+rwX ~/.prodigy\n</code></pre></p> </li> <li> <p>Check disk space:    <pre><code>df -h ~/.prodigy\n</code></pre></p> </li> </ol>"},{"location":"configuration/troubleshooting/#failed-to-acquire-storage-lock","title":"\"Failed to acquire storage lock\"","text":"<p>Symptoms: <pre><code>Error: Failed to acquire storage lock after 30s\n</code></pre></p> <p>Causes: - Another Prodigy process holding the lock - Stale lock from crashed process - File locking disabled but concurrent access occurring</p> <p>Solutions: 1. Check for running processes:    <pre><code>ps aux | grep prodigy\n</code></pre></p> <ol> <li> <p>Remove stale lock (if no processes running):    <pre><code>rm ~/.prodigy/storage.lock\n</code></pre></p> </li> <li> <p>Wait for lock release (if process is running)</p> </li> <li> <p>Disable locking temporarily (not recommended for production):    <pre><code>storage:\n  enable_locking: false\n</code></pre></p> </li> </ol>"},{"location":"configuration/troubleshooting/#workflow-configuration-issues","title":"Workflow Configuration Issues","text":""},{"location":"configuration/troubleshooting/#workflow-variables-not-interpolating","title":"\"Workflow variables not interpolating\"","text":"<p>Symptoms: <pre><code>Output: Deploying ${PROJECT_NAME} to ${ENVIRONMENT}\n</code></pre></p> <p>Causes: - Incorrect variable syntax - Variable not defined in workflow or config - Using project config variables instead of workflow env</p> <p>Solutions: 1. Use correct syntax:    <pre><code># \u2713 Workflow env vars\nenv:\n  PROJECT_NAME: myapp\ncommands:\n  - shell: \"echo ${PROJECT_NAME}\"\n\n# \u2717 Project config variables (different namespace)\n# .prodigy/config.yml\nvariables:\n  PROJECT_NAME: myapp  # Not available in workflows\n</code></pre></p> <ol> <li> <p>Define variable in workflow <code>env:</code> block or as system env</p> </li> <li> <p>Check variable exists:    <pre><code>prodigy run workflow.yml -vv  # Verbose mode shows variable resolution\n</code></pre></p> </li> </ol>"},{"location":"configuration/troubleshooting/#mapreduce-items-not-found","title":"\"MapReduce items not found\"","text":"<p>Symptoms: <pre><code>Error: No items found at JSONPath: $.items[*]\n</code></pre></p> <p>Causes: - Incorrect JSONPath expression - Input file not generated in setup phase - JSON structure doesn't match path</p> <p>Solutions: 1. Validate JSONPath:    <pre><code>cat items.json | jq '.items[0]'\n</code></pre></p> <ol> <li> <p>Check setup phase output:    <pre><code>setup:\n  - shell: \"generate-items.sh &gt; items.json\"\n  - shell: \"cat items.json\"  # Verify file exists and has content\n</code></pre></p> </li> <li> <p>Test JSONPath expression:    <pre><code># Use jq to test\ncat items.json | jq '$[*]'  # Root array\ncat items.json | jq '.items[*]'  # Nested array\n</code></pre></p> </li> </ol>"},{"location":"configuration/troubleshooting/#validation-always-fails","title":"\"Validation always fails\"","text":"<p>Symptoms: <pre><code>Error: Validation failed: completion_percentage 85 below threshold 100\n</code></pre></p> <p>Causes: - Threshold set too high (default: 100) - Validation command not returning expected format - Expected schema mismatch</p> <p>Solutions: 1. Adjust threshold:    <pre><code>validate:\n  threshold: 80  # Accept 80% instead of 100%\n</code></pre></p> <ol> <li> <p>Check validation output format:    <pre><code># Validation must output:\n{\n  \"completion_percentage\": 85,\n  \"status\": \"incomplete\",\n  \"gaps\": [\"Missing feature X\", \"Incomplete test Y\"]\n}\n</code></pre></p> </li> <li> <p>Test validation command manually:    <pre><code># Run validation command outside workflow\n./validate-script.sh\n</code></pre></p> </li> </ol>"},{"location":"configuration/troubleshooting/#api-and-authentication-issues","title":"API and Authentication Issues","text":""},{"location":"configuration/troubleshooting/#claude-api-key-not-recognized","title":"\"Claude API key not recognized\"","text":"<p>Symptoms: <pre><code>Error: Invalid Claude API key\n</code></pre></p> <p>Causes: - API key not set - Key set in wrong location - Invalid key format - Key expired or revoked</p> <p>Solutions: 1. Check key is set (precedence order):    <pre><code># Highest precedence: Environment variable\necho $PRODIGY_CLAUDE_API_KEY\n\n# Project config\ngrep claude_api_key .prodigy/config.yml\n\n# Global config\ngrep claude_api_key ~/.prodigy/config.yml\n</code></pre></p> <ol> <li> <p>Verify key format (should start with <code>sk-ant-</code>):    <pre><code>echo $PRODIGY_CLAUDE_API_KEY | head -c 10\n# Should show: sk-ant-api\n</code></pre></p> </li> <li> <p>Use environment variable (recommended):    <pre><code>export PRODIGY_CLAUDE_API_KEY=\"sk-ant-api03-...\"\n</code></pre></p> </li> <li> <p>Verify key is valid at Anthropic Console</p> </li> </ol>"},{"location":"configuration/troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"configuration/troubleshooting/#workflow-running-slowly","title":"\"Workflow running slowly\"","text":"<p>Causes: - Excessive parallelism exhausting resources - Large work items in MapReduce - Slow validation commands</p> <p>Solutions: 1. Reduce parallelism:    <pre><code>map:\n  max_parallel: 3  # Reduce from 10\n</code></pre></p> <ol> <li> <p>Add timeout limits:    <pre><code>commands:\n  - shell: \"long-running-command\"\n    timeout: 300  # 5 minutes\n</code></pre></p> </li> <li> <p>Enable caching (if available):    <pre><code>storage:\n  enable_cache: true\n</code></pre></p> </li> </ol>"},{"location":"configuration/troubleshooting/#storage-growing-too-large","title":"\"Storage growing too large\"","text":"<p>Causes: - Old events and DLQ data accumulating - Large checkpoint files - No compression enabled</p> <p>Solutions: 1. Clean up old data:    <pre><code># Remove old events (older than 30 days)\nfind ~/.prodigy/events -type f -mtime +30 -delete\n\n# Clean DLQ for completed jobs\nprodigy dlq clean --completed\n</code></pre></p> <ol> <li> <p>Enable compression:    <pre><code>storage:\n  backend_config:\n    enable_compression: true\n</code></pre></p> </li> <li> <p>Reduce file retention:    <pre><code>storage:\n  backend_config:\n    max_file_size: 52428800  # 50MB instead of 100MB\n</code></pre></p> </li> </ol>"},{"location":"configuration/troubleshooting/#debugging-tools","title":"Debugging Tools","text":""},{"location":"configuration/troubleshooting/#check-effective-configuration","title":"Check Effective Configuration","text":"<p>View the merged configuration from all sources:</p> <pre><code>prodigy config show\n</code></pre>"},{"location":"configuration/troubleshooting/#verbose-logging","title":"Verbose Logging","text":"<p>Enable detailed logging:</p> <pre><code># Verbose mode (shows Claude streaming)\nprodigy run workflow.yml -v\n\n# Debug mode (shows variable resolution)\nprodigy run workflow.yml -vv\n\n# Trace mode (shows all internal operations)\nprodigy run workflow.yml -vvv\n</code></pre> <p>Or set log level in config:</p> <pre><code>log_level: debug  # trace, debug, info, warn, error\n</code></pre>"},{"location":"configuration/troubleshooting/#validate-configuration-files","title":"Validate Configuration Files","text":"<pre><code># Validate YAML syntax\nyamllint ~/.prodigy/config.yml\nyamllint .prodigy/config.yml\n\n# Validate workflow syntax\nprodigy validate workflow.yml\n</code></pre>"},{"location":"configuration/troubleshooting/#check-claude-logs","title":"Check Claude Logs","text":"<p>View Claude execution logs for debugging:</p> <pre><code># Latest log\nprodigy logs --latest\n\n# Tail live log\nprodigy logs --latest --tail\n\n# View specific log\ncat ~/.local/state/claude/logs/session-abc123.json | jq\n</code></pre>"},{"location":"configuration/troubleshooting/#common-error-messages","title":"Common Error Messages","text":"Error Message Likely Cause Solution \"Configuration file not found\" Missing config file Create <code>~/.prodigy/config.yml</code> or <code>.prodigy/config.yml</code> \"Invalid YAML syntax\" Syntax error in YAML Check indentation, colons, quotes \"Unknown field\" Typo or deprecated field Check docs for correct field names \"Variable not found\" Undefined variable Define in workflow <code>env:</code> or system env \"Storage lock timeout\" Concurrent access Wait or remove stale lock \"Permission denied\" Insufficient permissions Fix ownership/permissions on <code>~/.prodigy</code> \"JSONPath not found\" Wrong path or missing data Verify JSON structure and path \"API key invalid\" Wrong or expired key Check key format and validity"},{"location":"configuration/troubleshooting/#getting-help","title":"Getting Help","text":"<p>If you can't resolve an issue:</p> <ol> <li>Check logs: Use <code>-vvv</code> for maximum verbosity</li> <li>Verify config: Run <code>prodigy config show</code></li> <li>Check docs: See Configuration Structure and Workflow Basics</li> <li>File an issue: Prodigy GitHub Issues with:</li> <li>Error message (redact secrets)</li> <li>Relevant config snippets</li> <li>Prodigy version (<code>prodigy --version</code>)</li> <li>Operating system</li> </ol>"},{"location":"configuration/troubleshooting/#see-also","title":"See Also","text":"<ul> <li>Configuration Precedence Rules</li> <li>Environment Variables</li> <li>Storage Configuration</li> <li>Default Values Reference</li> </ul>"},{"location":"configuration/workflow-configuration/","title":"Workflow Configuration","text":""},{"location":"configuration/workflow-configuration/#workflow-configuration","title":"Workflow Configuration","text":"<p>Workflow configuration defines the sequence of commands to execute and their execution environment. Workflows are the core of Prodigy's automation capabilities.</p>"},{"location":"configuration/workflow-configuration/#overview","title":"Overview","text":"<p>Workflows are defined in <code>.prodigy/workflow.yml</code> files and specify: - Commands to execute in sequence - Iteration and loop control - Error handling behavior - Environment variables - Timeout and retry settings</p> <p>For detailed workflow syntax and MapReduce capabilities, see the Workflow Basics chapter.</p>"},{"location":"configuration/workflow-configuration/#basic-workflow-structure","title":"Basic Workflow Structure","text":"<pre><code># .prodigy/workflow.yml\ncommands:\n  - prodigy-code-review\n  - prodigy-lint\n  - prodigy-test\n</code></pre>"},{"location":"configuration/workflow-configuration/#advanced-workflow-features","title":"Advanced Workflow Features","text":""},{"location":"configuration/workflow-configuration/#structured-commands","title":"Structured Commands","text":"<p>Commands can include arguments and options:</p> <pre><code>commands:\n  - name: prodigy-implement-spec\n    args: [\"${SPEC_ID}\"]\n    options:\n      focus: performance\n  - name: prodigy-code-review\n    options:\n      severity: high\n</code></pre>"},{"location":"configuration/workflow-configuration/#iteration-control","title":"Iteration Control","text":"<pre><code>max_iterations: 5\ncommands:\n  - prodigy-fix-issues\n  - prodigy-test\n</code></pre>"},{"location":"configuration/workflow-configuration/#error-handling","title":"Error Handling","text":"<pre><code>commands:\n  - name: prodigy-risky-operation\n    metadata:\n      continue_on_error: true\n      retries: 3\n      timeout: 600\n</code></pre>"},{"location":"configuration/workflow-configuration/#configuration-location","title":"Configuration Location","text":"<p>Workflows can be specified in multiple ways:</p> <ol> <li>Explicit path: <code>prodigy run /path/to/workflow.yml</code></li> <li>Project default: <code>.prodigy/workflow.yml</code> in project directory</li> <li>Embedded in config: Nested under <code>workflow:</code> in config files</li> </ol>"},{"location":"configuration/workflow-configuration/#environment-variables-in-workflows","title":"Environment Variables in Workflows","text":"<p>Workflows have a dedicated <code>env:</code> block for defining environment variables with advanced features like secrets, profiles, and step-level overrides:</p> <pre><code># .prodigy/workflow.yml\nname: deployment\n\nenv:\n  # Plain variables\n  ENVIRONMENT: staging\n  API_URL: https://staging.api.com\n\n  # Secret variables (masked in logs)\n  API_KEY:\n    secret: true\n    value: \"${STAGING_API_KEY}\"  # From system env\n\n  # Profile-specific values\n  DEPLOY_TARGET:\n    default: dev-server\n    staging: staging-cluster\n    prod: prod-cluster\n\ncommands:\n  - shell: \"deploy --env ${ENVIRONMENT} --url ${API_URL} --key ${API_KEY}\"\n    # Output: deploy --env staging --url https://staging.api.com --key ***\n</code></pre> <p>Key Features: - Secrets: Automatically masked in all output (<code>secret: true</code>) - Profiles: Different values for dev/staging/prod environments - Step Overrides: Override variables for specific commands - Interpolation: Reference system environment variables</p> <p>See Environment Variables - Workflow Section for complete documentation.</p> <p>Note: Project config <code>variables</code> are separate from workflow <code>env</code> and serve different purposes: - Workflow <code>env:</code>: Runtime environment variables, supports secrets and profiles - Config <code>variables:</code>: Project metadata and settings (deprecated for workflow use)</p>"},{"location":"configuration/workflow-configuration/#mapreduce-workflows","title":"MapReduce Workflows","text":"<p>For parallel processing of large datasets, use MapReduce mode:</p> <pre><code>name: process-all-files\nmode: mapreduce\n\nmap:\n  input: items.json\n  json_path: \"$.files[*]\"\n  agent_template:\n    - claude: \"/process-file '${item.path}'\"\n  max_parallel: 10\n\nreduce:\n  - claude: \"/summarize ${map.results}\"\n</code></pre> <p>See the MapReduce Workflows chapter for complete documentation.</p>"},{"location":"configuration/workflow-configuration/#workflow-precedence","title":"Workflow Precedence","text":"<p>When multiple workflow sources exist, Prodigy uses this precedence:</p> <ol> <li>Explicit path via <code>prodigy run workflow.yml</code> (highest)</li> <li><code>.prodigy/workflow.yml</code> in project directory</li> <li>Default workflow configuration (lowest)</li> </ol>"},{"location":"configuration/workflow-configuration/#see-also","title":"See Also","text":"<ul> <li>Workflow Basics - Complete workflow syntax and features</li> <li>MapReduce Workflows - Parallel execution patterns</li> <li>Project Configuration Structure - Project variables for workflows</li> <li>Environment Variables - Using environment variables in workflows</li> </ul>"},{"location":"environment/","title":"Environment Configuration","text":"<p>Prodigy provides flexible environment configuration for workflows, allowing you to manage environment variables, secrets, profiles, and step-specific settings. This chapter explains the user-facing configuration options available in workflow YAML files.</p>"},{"location":"environment/#architecture-overview","title":"Architecture Overview","text":"<p>Prodigy uses a two-layer architecture for environment management:</p> <ol> <li>WorkflowConfig: User-facing YAML configuration with <code>env</code>, <code>secrets</code>, <code>profiles</code>, and <code>env_files</code> fields</li> <li>EnvironmentConfig: Internal runtime configuration that extends workflow config with additional features</li> </ol> <p>This chapter documents the WorkflowConfig layer - the fields you write in workflow YAML files (<code>env</code>, <code>secrets</code>, <code>env_files</code>, <code>profiles</code>). The EnvironmentConfig is Prodigy's internal runtime that processes these YAML fields and adds internal-only features like dynamic command-based values and conditional expressions.</p> <p>Internal vs. User-Facing Capabilities:</p> <p>The internal <code>EnvironmentConfig</code> supports richer environment value types through the <code>EnvValue</code> enum: - <code>Static</code>: Simple string values (what WorkflowConfig exposes) - <code>Dynamic</code>: Values from command output (internal only) - <code>Conditional</code>: Expression-based values (internal only)</p> <p>In workflow YAML, the <code>env</code> field only supports static string values (<code>HashMap&lt;String, String&gt;</code>). The Dynamic and Conditional variants are internal runtime features not exposed in workflow configuration.</p> <p>Note on Internal Features: The <code>EnvironmentConfig</code> runtime layer includes a <code>StepEnvironment</code> struct with fields like <code>env</code>, <code>working_dir</code>, <code>clear_env</code>, and <code>temporary</code>. These are internal implementation details not exposed in <code>WorkflowStepCommand</code> YAML syntax. Per-command environment changes must use shell syntax (e.g., <code>ENV=value command</code>).</p>"},{"location":"environment/#global-environment-variables","title":"Global Environment Variables","text":"<p>Define static environment variables that apply to all commands in your workflow:</p> <pre><code># Global environment variables (static strings only)\nenv:\n  NODE_ENV: production\n  PORT: \"3000\"\n  API_URL: https://api.example.com\n  DEBUG: \"false\"\n\ncommands:\n  - shell: \"echo $NODE_ENV\"  # Uses global environment\n</code></pre> <p>Important: The <code>env</code> field at the workflow level only supports static string values. Dynamic or conditional environment variables are handled internally by the runtime but are not directly exposed in workflow YAML.</p> <p>Environment Inheritance: Parent process environment variables are always inherited by default. All global environment variables are merged with the parent environment.</p>"},{"location":"environment/#variable-interpolation","title":"Variable Interpolation","text":"<p>Prodigy supports two syntaxes for referencing environment variables in workflows:</p>"},{"location":"environment/#simple-syntax-var","title":"Simple Syntax: <code>$VAR</code>","text":"<p>The simple <code>$VAR</code> syntax works for basic variable references:</p> <pre><code>env:\n  API_URL: https://api.example.com\n  PORT: \"3000\"\n\ncommands:\n  - shell: \"curl $API_URL/health\"\n  - shell: \"echo Server running on port $PORT\"\n</code></pre> <p>Use <code>$VAR</code> when: - Variable name is standalone (not adjacent to other text) - You're passing to shell commands - Simple, clear usage without ambiguity</p>"},{"location":"environment/#bracketed-syntax-var","title":"Bracketed Syntax: <code>${VAR}</code>","text":"<p>The bracketed <code>${VAR}</code> syntax is preferred for clarity and is required in some cases:</p> <pre><code>env:\n  PROJECT: my-app\n  VERSION: \"1.0.0\"\n  ENVIRONMENT: prod\n\ncommands:\n  - shell: \"deploy-${PROJECT}-${VERSION}.sh\"  # Required: adjacent to text\n  - shell: \"echo Deploying ${PROJECT} v${VERSION}\"  # Preferred: explicit\n  - shell: \"mkdir -p /var/log/${PROJECT}/${ENVIRONMENT}\"  # Required: in paths\n</code></pre> <p>Use <code>${VAR}</code> when: - Variable is adjacent to other text (e.g., <code>${VAR}-suffix</code>, <code>prefix-${VAR}</code>) - Variable is part of a path (e.g., <code>config/${VAR}/file.json</code>) - Complex expressions or nested usage - You want explicit, unambiguous references (recommended)</p>"},{"location":"environment/#when-var-is-required","title":"When <code>${VAR}</code> is Required","text":"<p>1. Adjacent to text: <pre><code>env:\n  NAME: api\n  VERSION: \"2.1\"\n\ncommands:\n  # Wrong - Shell interprets as variable named \"NAME_VERSION\"\n  - shell: \"echo $NAME_VERSION\"\n\n  # Correct - Explicitly separates variables\n  - shell: \"echo ${NAME}_${VERSION}\"\n</code></pre></p> <p>2. In file paths: <pre><code>env:\n  ENVIRONMENT: staging\n  PROJECT: my-app\n\ncommands:\n  # Preferred - Clear variable boundaries\n  - shell: \"cp config.json /etc/${PROJECT}/${ENVIRONMENT}/config.json\"\n</code></pre></p> <p>3. In URLs and complex strings: <pre><code>env:\n  API_BASE: https://api.example.com\n  VERSION: v1\n\ncommands:\n  # Required - Variable within URL\n  - shell: \"curl ${API_BASE}/${VERSION}/users\"\n</code></pre></p>"},{"location":"environment/#interpolation-in-different-contexts","title":"Interpolation in Different Contexts","text":"<p>Shell commands: Both syntaxes work, but <code>${VAR}</code> is safer: <pre><code>env:\n  DATABASE_URL: postgresql://localhost:5432/app\n  TIMEOUT: \"30\"\n\ncommands:\n  - shell: \"psql $DATABASE_URL\"           # Simple case: $VAR works\n  - shell: \"timeout ${TIMEOUT} ./app\"     # Preferred: ${VAR} is explicit\n</code></pre></p> <p>Claude commands: Use <code>${VAR}</code> for consistency: <pre><code>env:\n  SPEC_FILE: spec-123.md\n  PROJECT_NAME: my-project\n\ncommands:\n  - claude: \"/implement-spec ${SPEC_FILE} --project ${PROJECT_NAME}\"\n</code></pre></p> <p>File paths: Always use <code>${VAR}</code> in paths: <pre><code>env:\n  OUTPUT_DIR: /tmp/results\n  TIMESTAMP: \"20240101\"\n\ncommands:\n  - shell: \"mkdir -p ${OUTPUT_DIR}/${TIMESTAMP}\"\n  - write_file:\n      path: ${OUTPUT_DIR}/${TIMESTAMP}/report.json\n      content: \"...\"\n</code></pre></p> <p>MapReduce configurations: Combine with MapReduce variables like <code>${item}</code>: <pre><code>env:\n  MAX_WORKERS: \"10\"\n  OUTPUT_PATH: /results\n\nmap:\n  max_parallel: ${MAX_WORKERS}\n  agent_template:\n    - shell: \"process ${item.file} --output ${OUTPUT_PATH}/${item.id}.result\"\n</code></pre></p>"},{"location":"environment/#escaping-variables","title":"Escaping Variables","text":"<p>If you need a literal <code>$</code> character, use shell escaping:</p> <pre><code>commands:\n  # Using single quotes (no interpolation)\n  - shell: 'echo \"Price: $100\"'\n\n  # Using double quotes with escape\n  - shell: \"echo \\\"Price: \\\\$100\\\"\"\n\n  # Double $$ for literal $ in some contexts\n  - shell: \"echo Price: $$100\"\n</code></pre>"},{"location":"environment/#both-acceptable","title":"Both acceptable","text":"<ul> <li>shell: \"echo Port: $PORT\"</li> <li>shell: \"echo Port: ${PORT}\" <pre><code>**Complex case (requires `${VAR}`):**\n```yaml\nenv:\n  PROJECT: api\n  VERSION: \"1.0\"\n  ENVIRONMENT: prod\n\n# Required - variables adjacent to text and in paths\n- shell: \"deploy-${PROJECT}-v${VERSION}.sh --env ${ENVIRONMENT}\"\n- shell: \"cp /src/config.${ENVIRONMENT}.json /etc/${PROJECT}/config.json\"\n</code></pre></li> </ul> <p>Recommended approach (always use <code>${VAR}</code>): <pre><code>env:\n  DATABASE: myapp\n  USER: admin\n  HOST: localhost\n\ncommands:\n  - shell: \"psql -h ${HOST} -U ${USER} -d ${DATABASE}\"\n  - shell: \"backup-${DATABASE}-$(date +%Y%m%d).sql\"\n</code></pre></p>"},{"location":"environment/#additional-topics","title":"Additional Topics","text":""},{"location":"environment/#environment-configuration-subsections","title":"Environment Configuration Subsections","text":"<ul> <li>MapReduce Environment Variables - Environment variables specific to MapReduce workflows</li> <li>Environment Files - Using .env files for configuration</li> <li>Secrets Management - Handling sensitive data securely</li> <li>Environment Profiles - Profile-based configuration for different environments</li> <li>Per-Command Environment Overrides - Step-level environment customization</li> <li>Environment Precedence - Understanding variable resolution order</li> <li>Best Practices - Recommended patterns and approaches</li> <li>Common Patterns - Real-world usage examples</li> </ul>"},{"location":"environment/#related-chapters","title":"Related Chapters","text":"<ul> <li>MapReduce Workflows - Parallel processing with environment configuration</li> <li>Variables and Interpolation - Understanding variable syntax and usage</li> <li>Configuration - Overall workflow and project configuration</li> <li>Workflow Configuration - Complete workflow file structure</li> </ul>"},{"location":"environment/best-practices/","title":"Best Practices","text":""},{"location":"environment/best-practices/#best-practices","title":"Best Practices","text":"<p>This section provides guidelines for effective environment variable management, secrets handling, and profile organization in Prodigy workflows.</p>"},{"location":"environment/best-practices/#environment-variable-usage","title":"Environment Variable Usage","text":"<p>When to use environment variables: - Configuration values that change between environments (dev/staging/prod) - Parameterization for reusable workflows - Non-sensitive API endpoints and URLs - Timeouts, limits, and resource constraints - Project-specific paths and file locations</p> <p>When to use profiles instead: - Environment-specific configuration sets (dev/staging/prod) - Multiple related values that change together - When you need to switch entire configuration contexts</p> <p>When to use env_files instead: - Loading many variables from external sources - Sharing configuration across multiple workflows - Managing .env files in version control (without secrets) - Local development overrides (.env.local)</p> <p>When to use step-level env overrides: - Command-specific configuration that overrides global/profile values - Per-step resource limits or timeouts - Temporary environment changes for individual commands - Testing different configurations in the same workflow</p> <p>See Per-Command Environment Overrides for detailed step-level env documentation.</p>"},{"location":"environment/best-practices/#naming-conventions","title":"Naming Conventions","text":"<p>Follow these naming conventions for clarity:</p> <ol> <li>Use UPPERCASE with underscores: <code>API_URL</code>, <code>MAX_WORKERS</code>, <code>DATABASE_URL</code></li> <li>Be descriptive: <code>AGENT_TIMEOUT</code> not <code>TIMEOUT</code>, <code>API_BASE_URL</code> not <code>URL</code></li> <li>Use prefixes for grouping: <code>DB_HOST</code>, <code>DB_PORT</code>, <code>DB_NAME</code> or <code>REDIS_HOST</code>, <code>REDIS_PORT</code></li> <li>Avoid abbreviations: <code>PROJECT_NAME</code> not <code>PROJ_NM</code>, <code>ENVIRONMENT</code> not <code>ENV</code></li> </ol> <p>Examples:</p> <pre><code>env:\n  # Good - Clear and descriptive\n  PROJECT_NAME: my-project\n  API_BASE_URL: https://api.example.com\n  MAX_PARALLEL_WORKERS: \"10\"\n  CLAUDE_COMMAND_TIMEOUT: \"600\"\n\n  # Avoid - Unclear or abbreviated\n  PROJ: my-project\n  URL: https://api.example.com\n  MAX: \"10\"\n  TIMEOUT: \"600\"\n</code></pre>"},{"location":"environment/best-practices/#secrets-management-guidelines","title":"Secrets Management Guidelines","text":"<p>Critical security practices:</p> <ol> <li>NEVER commit secrets to version control</li> <li>Add <code>.env.secrets</code>, <code>.env.local</code> to <code>.gitignore</code></li> <li>Use secret providers (vault, aws-secrets) for production</li> <li> <p>Rotate secrets regularly</p> </li> <li> <p>Always use the secrets block for sensitive values <pre><code>secrets:\n  # Simple format - loads from environment variable\n  API_KEY: \"my-secret-value\"  # Masked in all logs\n\n  # Provider format - explicitly specify source\n  DATABASE_PASSWORD:\n    provider: env\n    key: DB_PASSWORD  # Retrieves from $DB_PASSWORD env var\n</code></pre></p> </li> <li> <p>Use external secret providers</p> </li> </ol> <p>Currently implemented providers (src/cook/environment/secret_store.rs:35-46):    - <code>env</code> - Load from environment variable (EnvSecretProvider)    - <code>file</code> - Load from file path (FileSecretProvider)</p> <pre><code>secrets:\n  # Load from environment variable\n  API_KEY:\n    provider: env\n    key: SECRET_API_KEY\n\n  # Load from file\n  DATABASE_PASSWORD:\n    provider: file\n    key: /etc/secrets/db-password\n</code></pre> <p>Planned providers (not yet implemented):    - <code>vault</code> - HashiCorp Vault integration (declared in config.rs:106)    - <code>aws</code> - AWS Secrets Manager (declared in config.rs:108)    - <code>custom</code> - Custom provider support (declared in config.rs:110)</p> <ol> <li>Minimize secret exposure</li> <li>Only expose secrets to commands that need them</li> <li>Use short-lived tokens when possible</li> <li> <p>Use <code>file</code> provider to avoid secrets in environment variables</p> </li> <li> <p>Verify secret masking</p> </li> <li>Check logs to ensure secrets are masked (appear as <code>***</code>)</li> <li>Test with <code>-v</code> verbose mode to confirm masking works</li> </ol> <p>Secret organization pattern:</p> <pre><code># Good - Organized with file-based secrets\nenv_files:\n  - .env              # Non-sensitive config\n  - .env.local        # Local overrides (gitignored)\n\nsecrets:\n  # Production secrets from files (managed by deployment system)\n  DATABASE_PASSWORD:\n    provider: file\n    key: /run/secrets/db-password\n\n  API_KEY:\n    provider: file\n    key: /run/secrets/api-key\n\n  # Development secrets from environment (for local testing)\n  DEV_API_KEY:\n    provider: env\n    key: DEV_API_KEY  # Loaded from environment, still masked in logs\n</code></pre> <p>Source: Secret provider implementations in src/cook/environment/secret_store.rs:34-148</p>"},{"location":"environment/best-practices/#profile-organization-strategies","title":"Profile Organization Strategies","text":"<p>Multi-environment structure:</p> <p>Use profiles to manage dev/staging/prod configurations:</p> <pre><code>env:\n  # Common values shared across all environments\n  PROJECT_NAME: my-project\n  VERSION: \"1.0.0\"\n\nprofiles:\n  dev:\n    API_URL: http://localhost:3000\n    MAX_WORKERS: \"2\"\n    TIMEOUT: \"60\"\n    DEBUG: \"true\"\n\n  staging:\n    API_URL: https://staging-api.example.com\n    MAX_WORKERS: \"10\"\n    TIMEOUT: \"30\"\n    DEBUG: \"true\"\n\n  prod:\n    API_URL: https://api.example.com\n    MAX_WORKERS: \"20\"\n    TIMEOUT: \"30\"\n    DEBUG: \"false\"\n</code></pre> <p>Activate with: <code>prodigy run workflow.yml --profile prod</code></p> <p>Base + override pattern:</p> <pre><code>env:\n  # Base/default values\n  API_URL: http://localhost:3000\n  MAX_WORKERS: \"5\"\n  CACHE_ENABLED: \"true\"\n\nprofiles:\n  prod:\n    # Only override what changes in prod\n    API_URL: https://api.production.com\n    MAX_WORKERS: \"20\"\n    # CACHE_ENABLED inherits \"true\" from base\n</code></pre>"},{"location":"environment/best-practices/#avoiding-common-pitfalls","title":"Avoiding Common Pitfalls","text":"<p>1. Variable name conflicts:</p> <pre><code># Problem - Confusing overlap with shell variables\nenv:\n  PATH: /custom/bin  # Conflicts with system PATH\n  HOME: /app         # Conflicts with system HOME\n\n# Solution - Use prefixed names\nenv:\n  APP_PATH: /custom/bin\n  APP_HOME: /app\n</code></pre> <p>2. Precedence confusion:</p> <p>Remember the order (highest to lowest priority): 1. Step-level env - Per-command overrides (see Per-Command Environment Overrides) 2. Profile values - When profile is active via <code>--profile</code> flag 3. Global <code>env</code> field - Workflow-level environment variables 4. Environment files - Later files override earlier ones (from <code>env_files</code>) 5. Parent process environment - Inherited from shell (if <code>inherit: true</code>)</p> <p>Source: Implementation in src/cook/environment/manager.rs:88-156</p> <p>For detailed explanation of precedence rules, see Environment Precedence.</p> <p>3. Forgetting to mark secrets:</p> <pre><code># Problem - Secret exposed in logs\nenv_files:\n  - .env.secrets  # Contains API_KEY=sk-abc123\n\ncommands:\n  - shell: \"curl -H 'Authorization: Bearer $API_KEY' ...\"\n  # API_KEY appears in logs!\n\n# Solution - Use secrets block to mask in logs\nsecrets:\n  API_KEY:\n    provider: env\n    key: SECRET_API_KEY  # Retrieves from environment, masked in all output\n</code></pre> <p>4. Hardcoding environment-specific values:</p> <pre><code># Problem - Not reusable across environments\ncommands:\n  - shell: \"curl https://api.production.com/data\"\n  - shell: \"timeout 30 ./process.sh\"\n\n# Solution - Use environment variables\nenv:\n  API_URL: https://api.production.com\n  TIMEOUT: \"30\"\n\ncommands:\n  - shell: \"curl $API_URL/data\"\n  - shell: \"timeout $TIMEOUT ./process.sh\"\n</code></pre>"},{"location":"environment/best-practices/#documentation-practices","title":"Documentation Practices","text":"<p>Document your environment configuration:</p> <ol> <li> <p>List required variables in README or workflow comments:    <pre><code># Required environment variables:\n# - API_URL: Base URL for API endpoints\n# - MAX_WORKERS: Number of parallel workers (default: 5)\n# - TIMEOUT: Command timeout in seconds (default: 30)\n\nenv:\n  API_URL: https://api.example.com\n  MAX_WORKERS: \"5\"\n  TIMEOUT: \"30\"\n</code></pre></p> </li> <li> <p>Provide example .env file:    <pre><code># .env.example (safe to commit)\nAPI_URL=https://api.example.com\nMAX_WORKERS=5\nTIMEOUT=30\n\n# Copy to .env and fill in values:\n# cp .env.example .env\n</code></pre></p> </li> <li> <p>Document profile usage:    <pre><code># Profiles available:\n# - dev: Local development (low resource usage)\n# - staging: Staging environment (medium resources)\n# - prod: Production environment (high resources)\n#\n# Usage: prodigy run workflow.yml --profile prod\n</code></pre></p> </li> </ol>"},{"location":"environment/best-practices/#testing-environment-configurations","title":"Testing Environment Configurations","text":"<p>Validate your environment setup:</p> <ol> <li>Test with dry-run mode (if available)</li> <li> <p>Verify variable interpolation:    <pre><code>commands:\n  - shell: \"echo 'API_URL: $API_URL'\"\n  - shell: \"echo 'MAX_WORKERS: $MAX_WORKERS'\"\n  - shell: \"echo 'TIMEOUT: $TIMEOUT'\"\n</code></pre></p> </li> <li> <p>Test each profile:    <pre><code>prodigy run workflow.yml --profile dev\nprodigy run workflow.yml --profile staging\nprodigy run workflow.yml --profile prod\n</code></pre></p> </li> <li> <p>Verify secret masking:</p> </li> <li>Run with <code>-v</code> verbose mode</li> <li>Check that secrets appear as <code>***</code> in output</li> <li>Verify secrets don't leak in error messages</li> </ol>"},{"location":"environment/best-practices/#environment-variable-composition","title":"Environment Variable Composition","text":"<p>Layered configuration strategy:</p> <pre><code># Layer 1: Base configuration (committed)\nenv_files:\n  - .env\n\n# Layer 2: Local overrides (gitignored)\nenv_files:\n  - .env.local\n\n# Layer 3: Global workflow values\nenv:\n  PROJECT_NAME: my-project\n  VERSION: \"1.0.0\"\n\n# Layer 4: Profile-specific overrides\nprofiles:\n  prod:\n    MAX_WORKERS: \"20\"\n    TIMEOUT: \"30\"\n\n# Layer 5: Step-level overrides (per-command)\ncommands:\n  - shell: \"process-data\"\n    env:\n      TIMEOUT: \"60\"  # Override for this command only\n\n# Layer 6: Secrets (separate management)\nsecrets:\n  API_KEY:\n    provider: file\n    key: /run/secrets/api-key\n</code></pre> <p>Source: Precedence implementation in src/cook/environment/manager.rs:88-156</p> <p>Benefits: - Clear separation of concerns - Easy to understand precedence - Secure secret handling - Flexible environment switching - Local development friendly</p>"},{"location":"environment/best-practices/#performance-considerations","title":"Performance Considerations","text":"<p>Use environment variables to optimize performance:</p> <ol> <li> <p>Parameterize resource limits:    <pre><code>env:\n  MAX_WORKERS: \"10\"      # Tune based on CPU cores\n  MEMORY_LIMIT: \"8G\"     # Tune based on available RAM\n  TIMEOUT: \"300\"         # Tune based on expected duration\n</code></pre></p> </li> <li> <p>Environment-specific optimizations:    <pre><code>profiles:\n  dev:\n    MAX_WORKERS: \"2\"     # Low resource usage for local dev\n    CACHE_ENABLED: \"false\"  # Disable caching for faster iteration\n\n  prod:\n    MAX_WORKERS: \"20\"    # High throughput for production\n    CACHE_ENABLED: \"true\"   # Enable caching for performance\n</code></pre></p> </li> <li> <p>Avoid expensive operations in variable expansion:    <pre><code># Problem - Runs command on every variable access\nenv:\n  TIMESTAMP: \"$(date +%s)\"  # Evaluated once, not per use\n\n# Better - Capture once if dynamic value needed\ncommands:\n  - shell: \"date +%s\"\n    capture_output: TIMESTAMP\n</code></pre></p> </li> </ol>"},{"location":"environment/best-practices/#security-checklist","title":"Security Checklist","text":"<p>Before deploying workflows to production:</p> <ul> <li> All secrets defined in <code>secrets</code> block for masking</li> <li> Secret files in <code>.gitignore</code></li> <li> Production secrets use <code>file</code> provider (vault/aws planned for future)</li> <li> Verified secrets masked in logs with <code>-v</code> mode</li> <li> No hardcoded credentials in workflow YAML</li> <li> Environment variables documented in README</li> <li> <code>.env.example</code> provided for team</li> <li> Tested all profiles work correctly</li> <li> No sensitive data in environment variable names</li> <li> Secrets rotated regularly</li> <li> File-based secrets have restricted permissions (chmod 600)</li> </ul> <p>See also: - Environment Precedence for understanding resolution order - Secrets Management for detailed secret handling - Environment Profiles for profile configuration - Common Patterns for practical examples</p>"},{"location":"environment/common-patterns/","title":"Common Patterns","text":""},{"location":"environment/common-patterns/#common-patterns","title":"Common Patterns","text":"<p>This section provides practical patterns and realistic examples for environment configuration in Prodigy workflows. All examples are validated against the actual implementation and real workflow files.</p> <p>Source References: Examples based on: - src/config/workflow.rs:12-39 (WorkflowConfig structure) - src/cook/environment/config.rs:12-144 (Environment configuration types) - workflows/mapreduce-env-example.yml (Complete working example)</p>"},{"location":"environment/common-patterns/#multi-environment-deployment-pattern","title":"Multi-Environment Deployment Pattern","text":"<p>Use profiles to manage different deployment environments with environment-specific configurations.</p> <p>Profile Support: Profiles provide environment-specific variable overrides (src/cook/environment/config.rs:116-124). Activate with <code>--profile &lt;name&gt;</code> flag.</p> <pre><code>name: multi-env-deployment\nmode: mapreduce\n\nenv:\n  # Shared across all environments\n  PROJECT_NAME: my-service\n  VERSION: \"2.1.0\"\n  LOG_LEVEL: info\n\nprofiles:\n  dev:\n    API_URL: http://localhost:3000\n    DATABASE_URL: postgresql://localhost:5432/dev\n    MAX_WORKERS: \"2\"\n    CACHE_TTL: \"60\"\n    DEBUG: \"true\"\n\n  staging:\n    API_URL: https://staging-api.example.com\n    DATABASE_URL: postgresql://staging-db:5432/app\n    MAX_WORKERS: \"10\"\n    CACHE_TTL: \"300\"\n    DEBUG: \"true\"\n\n  prod:\n    API_URL: https://api.example.com\n    DATABASE_URL: postgresql://prod-db:5432/app\n    MAX_WORKERS: \"20\"\n    CACHE_TTL: \"3600\"\n    DEBUG: \"false\"\n\nsecrets:\n  # Secrets from environment variables (supported)\n  DATABASE_PASSWORD:\n    provider: env\n    key: DB_PASSWORD\n\ncommands:\n  - shell: \"echo 'Deploying $PROJECT_NAME v$VERSION to $PROFILE environment'\"\n  - shell: \"echo 'API: $API_URL, Workers: $MAX_WORKERS'\"\n  - shell: \"deploy.sh --env $PROFILE --version $VERSION\"\n</code></pre> <p>Usage: <pre><code>prodigy run deploy.yml --profile dev\nprodigy run deploy.yml --profile staging\nprodigy run deploy.yml --profile prod\n</code></pre></p>"},{"location":"environment/common-patterns/#secrets-management-pattern","title":"Secrets Management Pattern","text":"<p>Combine env files with secret providers for secure credential management:</p> <p>Currently Supported Providers (src/cook/environment/secret_store.rs:40-41): - <code>env</code> - Environment variables - <code>file</code> - File-based secrets</p> <p>Planned Providers (defined but not yet implemented): - <code>vault</code> - HashiCorp Vault integration - <code>aws</code> - AWS Secrets Manager - <code>custom</code> - Custom provider support</p> <pre><code>name: secure-workflow\n\n# Load non-sensitive config from files\nenv_files:\n  - .env              # Base configuration (committed)\n  - .env.local        # Local overrides (gitignored)\n\nenv:\n  SERVICE_NAME: payment-processor\n  REGION: us-west-2\n\nsecrets:\n  # Secrets from environment variables (currently supported)\n  AWS_ACCESS_KEY:\n    provider: env\n    key: AWS_ACCESS_KEY_ID\n\n  AWS_SECRET_KEY:\n    provider: env\n    key: AWS_SECRET_ACCESS_KEY\n\n  # Secrets from files (currently supported)\n  DATABASE_URL:\n    provider: file\n    key: /path/to/secrets/database_url.txt\n\n  # Simple secret reference (loaded from env)\n  THIRD_PARTY_API_KEY: \"${THIRD_PARTY_API_KEY}\"\n\ncommands:\n  - shell: \"aws s3 ls --region $REGION\"  # Uses AWS credentials\n  - shell: \"psql $DATABASE_URL -c 'SELECT version()'\"\n  - shell: \"curl -H 'Authorization: Bearer ***' https://api.example.com\"\n</code></pre> <p>.env file (committed): <pre><code>SERVICE_NAME=payment-processor\nREGION=us-west-2\nLOG_LEVEL=info\n</code></pre></p> <p>.env.local file (gitignored): <pre><code>THIRD_PARTY_API_KEY=sk-local-dev-key\nDATABASE_URL=postgresql://localhost:5432/dev\n</code></pre></p>"},{"location":"environment/common-patterns/#profile-based-configuration-strategy","title":"Profile-Based Configuration Strategy","text":"<p>Base configuration with environment-specific overrides:</p> <pre><code>name: profile-based-config\n\nenv:\n  # Base configuration (defaults)\n  PROJECT_NAME: analytics-pipeline\n  MAX_WORKERS: \"5\"\n  BATCH_SIZE: \"100\"\n  TIMEOUT: \"300\"\n  RETRY_ATTEMPTS: \"3\"\n  CACHE_ENABLED: \"true\"\n\nprofiles:\n  # Local development - minimal resources\n  dev:\n    MAX_WORKERS: \"2\"\n    BATCH_SIZE: \"10\"\n    TIMEOUT: \"600\"        # Longer timeout for debugging\n    CACHE_ENABLED: \"false\"  # Disable caching for fresh data\n\n  # CI/CD environment - controlled resources\n  ci:\n    MAX_WORKERS: \"4\"\n    BATCH_SIZE: \"50\"\n    TIMEOUT: \"300\"\n    RETRY_ATTEMPTS: \"1\"  # Fail fast in CI\n\n  # Production - optimized for throughput\n  prod:\n    MAX_WORKERS: \"20\"\n    BATCH_SIZE: \"500\"\n    TIMEOUT: \"180\"       # Strict timeout\n    RETRY_ATTEMPTS: \"5\"  # More retries for resilience\n    CACHE_ENABLED: \"true\"\n\nmap:\n  input: data.json\n  max_parallel: ${MAX_WORKERS}\n  agent_timeout_secs: ${TIMEOUT}\n\n  agent_template:\n    - shell: \"process-batch.sh --size $BATCH_SIZE --retries $RETRY_ATTEMPTS\"\n</code></pre>"},{"location":"environment/common-patterns/#environment-variable-composition-pattern","title":"Environment Variable Composition Pattern","text":"<p>Layer configuration from multiple sources with clear precedence (src/cook/environment/config.rs:12-36):</p> <p>Note: env_files paths are static and don't support variable interpolation. Use profiles to handle environment-specific file loading.</p> <pre><code>name: layered-config\n\n# Layer 1: Base defaults and local overrides\nenv_files:\n  - .env              # Base configuration\n  - .env.local        # Local overrides (if exists)\n\n# Layer 4: Global workflow values (override env files)\nenv:\n  WORKFLOW_VERSION: \"3.0.0\"\n  EXECUTION_MODE: standard\n\n# Layer 5: Profile values (override everything when active)\nprofiles:\n  prod:\n    EXECUTION_MODE: high-performance\n    MAX_WORKERS: \"50\"\n\n# Layer 6: Secrets (separate layer for security)\nsecrets:\n  API_TOKEN:\n    provider: vault\n    key: secret/data/api-token\n\ncommands:\n  - shell: \"echo 'Mode: $EXECUTION_MODE, Workers: $MAX_WORKERS'\"\n</code></pre> <p>Precedence Order (highest to lowest): 1. Profile variables (when profile is active) 2. Global <code>env</code> variables 3. Variables from <code>env_files</code> (later files override earlier) 4. Inherited system environment variables</p> <p>File structure: <pre><code>.env                  # Base: MAX_WORKERS=5, API_URL=http://localhost\n.env.local           # Local: MAX_WORKERS=2 (overrides .env)\n</code></pre></p>"},{"location":"environment/common-patterns/#cicd-integration-pattern","title":"CI/CD Integration Pattern","text":"<p>Use environment variables to make workflows portable across CI/CD systems.</p> <p>Conditional Execution: Commands support the <code>when</code> field for conditional execution based on environment variables (src/config/command.rs:388).</p> <pre><code>name: ci-cd-workflow\n\nenv:\n  # CI/CD environment detection\n  CI_MODE: \"${CI:-false}\"                    # GitHub Actions, GitLab CI set CI=true\n  BUILD_NUMBER: \"${BUILD_NUMBER:-local}\"     # Jenkins BUILD_NUMBER\n  COMMIT_SHA: \"${GITHUB_SHA:-unknown}\"       # GitHub Actions\n  BRANCH_NAME: \"${BRANCH_NAME:-main}\"        # Can be set by CI\n\n  # Resource limits for CI\n  MAX_WORKERS: \"${CI_MAX_WORKERS:-5}\"\n  TIMEOUT: \"${CI_TIMEOUT:-300}\"\n\n  # Paths\n  ARTIFACT_DIR: \"${WORKSPACE:-./artifacts}\"\n  CACHE_DIR: \"${CACHE_DIR:-./cache}\"\n\ncommands:\n  - shell: \"echo 'CI Mode: $CI_MODE, Build: $BUILD_NUMBER'\"\n  - shell: \"echo 'Branch: $BRANCH_NAME, Commit: $COMMIT_SHA'\"\n\n  - shell: \"cargo build --release\"\n    when: \"${CI_MODE} == 'true'\"\n\n  - shell: \"cargo test --all\"\n    timeout: ${TIMEOUT}\n\n  - shell: \"mkdir -p $ARTIFACT_DIR\"\n  - shell: \"cp target/release/app $ARTIFACT_DIR/\"\n</code></pre> <p>GitHub Actions example: <pre><code>- name: Run Prodigy workflow\n  env:\n    CI_MAX_WORKERS: 10\n    CI_TIMEOUT: 600\n  run: prodigy run workflow.yml\n</code></pre></p> <p>Jenkins example: <pre><code>environment {\n  CI_MAX_WORKERS = '10'\n  CI_TIMEOUT = '600'\n}\nsteps {\n  sh 'prodigy run workflow.yml'\n}\n</code></pre></p>"},{"location":"environment/common-patterns/#local-development-pattern","title":"Local Development Pattern","text":"<p>Optimize for local development with overridable defaults:</p> <pre><code>name: dev-friendly-workflow\n\nenv_files:\n  - .env                # Base config (committed)\n  - .env.local          # Personal settings (gitignored)\n\nenv:\n  # Development defaults\n  API_URL: http://localhost:3000\n  DATABASE_URL: postgresql://localhost:5432/dev\n  REDIS_URL: redis://localhost:6379\n\n  # Resource limits for local dev\n  MAX_WORKERS: \"2\"\n  TIMEOUT: \"60\"\n\n  # Feature flags\n  ENABLE_CACHING: \"false\"\n  ENABLE_ANALYTICS: \"false\"\n  DEBUG_MODE: \"true\"\n\nprofiles:\n  # Personal override for more powerful dev machines\n  high-perf:\n    MAX_WORKERS: \"8\"\n    ENABLE_CACHING: \"true\"\n\ncommands:\n  - shell: \"echo 'Development mode: Debug=$DEBUG_MODE'\"\n  - shell: \"docker-compose up -d\"\n    when: \"${DATABASE_URL} =~ 'localhost'\"\n\n  - shell: \"cargo run --bin migrate\"\n  - shell: \"cargo test\"\n</code></pre> <p>.env.local.example (committed as template): <pre><code># Copy to .env.local and customize for your machine\n\n# Override API endpoint for local backend\n# API_URL=http://localhost:8080\n\n# Use more workers if you have powerful CPU\n# MAX_WORKERS=4\n\n# Enable features for testing\n# ENABLE_CACHING=true\n# ENABLE_ANALYTICS=true\n</code></pre></p>"},{"location":"environment/common-patterns/#template-parameterization-pattern","title":"Template Parameterization Pattern","text":"<p>Create reusable workflows parameterized with environment variables:</p> <pre><code>name: reusable-test-workflow\n\nenv:\n  # Required parameters (set by caller or env)\n  PROJECT_NAME: \"${PROJECT_NAME}\"           # Must be provided\n  TEST_SUITE: \"${TEST_SUITE:-all}\"         # Default: all\n  COVERAGE_THRESHOLD: \"${COVERAGE_THRESHOLD:-80}\"\n\n  # Optional customization\n  PARALLEL_JOBS: \"${PARALLEL_JOBS:-5}\"\n  TIMEOUT: \"${TIMEOUT:-300}\"\n  REPORT_FORMAT: \"${REPORT_FORMAT:-json}\"\n\ncommands:\n  - shell: \"echo 'Testing $PROJECT_NAME: $TEST_SUITE suite'\"\n\n  - shell: \"cargo test --workspace\"\n    when: \"${TEST_SUITE} == 'all'\"\n    timeout: ${TIMEOUT}\n\n  - shell: \"cargo test --package $PROJECT_NAME\"\n    when: \"${TEST_SUITE} == 'unit'\"\n\n  - shell: \"cargo tarpaulin --out $REPORT_FORMAT --output-dir coverage\"\n    capture_output: coverage_result\n    capture_format: json\n\n  - shell: |\n      COVERAGE=$(echo '$coverage_result' | jq '.coverage')\n      if (( $(echo \"$COVERAGE &lt; $COVERAGE_THRESHOLD\" | bc -l) )); then\n        echo \"Coverage $COVERAGE% below threshold $COVERAGE_THRESHOLD%\"\n        exit 1\n      fi\n</code></pre> <p>Usage: <pre><code># With environment variables\nPROJECT_NAME=my-app TEST_SUITE=unit prodigy run test-workflow.yml\n\n# Or with .env file\necho \"PROJECT_NAME=my-app\" &gt; .env\necho \"TEST_SUITE=integration\" &gt;&gt; .env\necho \"COVERAGE_THRESHOLD=90\" &gt;&gt; .env\nprodigy run test-workflow.yml\n</code></pre></p>"},{"location":"environment/common-patterns/#regional-configuration-pattern","title":"Regional Configuration Pattern","text":"<p>Deploy to different regions with region-specific settings:</p> <pre><code>name: multi-region-deployment\n\nenv:\n  SERVICE_NAME: api-gateway\n  VERSION: \"1.2.0\"\n\nprofiles:\n  us-west:\n    REGION: us-west-2\n    API_ENDPOINT: https://api-usw.example.com\n    S3_BUCKET: my-app-usw-artifacts\n    MAX_INSTANCES: \"10\"\n\n  us-east:\n    REGION: us-east-1\n    API_ENDPOINT: https://api-use.example.com\n    S3_BUCKET: my-app-use-artifacts\n    MAX_INSTANCES: \"20\"\n\n  eu-west:\n    REGION: eu-west-1\n    API_ENDPOINT: https://api-euw.example.com\n    S3_BUCKET: my-app-euw-artifacts\n    MAX_INSTANCES: \"15\"\n\nsecrets:\n  # AWS credentials from environment (currently supported)\n  AWS_ACCESS_KEY:\n    provider: env\n    key: AWS_ACCESS_KEY_ID\n  AWS_SECRET_KEY:\n    provider: env\n    key: AWS_SECRET_ACCESS_KEY\n\ncommands:\n  - shell: \"echo 'Deploying to $REGION'\"\n  - shell: \"aws s3 cp ./artifact.zip s3://$S3_BUCKET/$VERSION/ --region $REGION\"\n  - shell: \"deploy-to-region.sh --region $REGION --instances $MAX_INSTANCES\"\n</code></pre> <p>Usage: <pre><code>prodigy run deploy.yml --profile us-west\nprodigy run deploy.yml --profile eu-west\n</code></pre></p>"},{"location":"environment/common-patterns/#feature-flag-pattern","title":"Feature Flag Pattern","text":"<p>Use environment variables to control feature availability:</p> <pre><code>name: feature-flag-workflow\n\nenv:\n  # Feature flags\n  ENABLE_NEW_PIPELINE: \"${ENABLE_NEW_PIPELINE:-false}\"\n  ENABLE_EXPERIMENTAL: \"${ENABLE_EXPERIMENTAL:-false}\"\n  ENABLE_BETA_FEATURES: \"${ENABLE_BETA_FEATURES:-false}\"\n\n  # Version-based features\n  MIN_VERSION: \"2.0.0\"\n  CURRENT_VERSION: \"2.1.0\"\n\nprofiles:\n  canary:\n    ENABLE_EXPERIMENTAL: \"true\"\n\n  beta:\n    ENABLE_BETA_FEATURES: \"true\"\n\n  production:\n    ENABLE_NEW_PIPELINE: \"true\"\n    ENABLE_EXPERIMENTAL: \"false\"\n    ENABLE_BETA_FEATURES: \"false\"\n\ncommands:\n  - shell: \"run-legacy-pipeline.sh\"\n    when: \"${ENABLE_NEW_PIPELINE} == 'false'\"\n\n  - shell: \"run-new-pipeline.sh\"\n    when: \"${ENABLE_NEW_PIPELINE} == 'true'\"\n\n  - shell: \"run-experimental-features.sh\"\n    when: \"${ENABLE_EXPERIMENTAL} == 'true'\"\n\n  - shell: \"validate-version.sh --min $MIN_VERSION --current $CURRENT_VERSION\"\n</code></pre>"},{"location":"environment/common-patterns/#complete-real-world-example","title":"Complete Real-World Example","text":"<p>Combining multiple patterns for a production-ready workflow:</p> <pre><code>name: production-data-pipeline\nmode: mapreduce\n\n# Layer 1: Base configuration\nenv_files:\n  - .env\n  - .env.${ENVIRONMENT}\n\n# Layer 2: Global settings\nenv:\n  PROJECT_NAME: data-pipeline\n  VERSION: \"3.0.0\"\n  ENVIRONMENT: \"${ENVIRONMENT:-dev}\"\n\n# Layer 3: Environment profiles\nprofiles:\n  dev:\n    DATA_SOURCE: s3://dev-data-bucket\n    OUTPUT_PATH: s3://dev-results-bucket\n    MAX_WORKERS: \"5\"\n    BATCH_SIZE: \"100\"\n    ENABLE_MONITORING: \"false\"\n\n  staging:\n    DATA_SOURCE: s3://staging-data-bucket\n    OUTPUT_PATH: s3://staging-results-bucket\n    MAX_WORKERS: \"15\"\n    BATCH_SIZE: \"500\"\n    ENABLE_MONITORING: \"true\"\n\n  prod:\n    DATA_SOURCE: s3://prod-data-bucket\n    OUTPUT_PATH: s3://prod-results-bucket\n    MAX_WORKERS: \"50\"\n    BATCH_SIZE: \"1000\"\n    ENABLE_MONITORING: \"true\"\n\n# Layer 4: Secrets\nsecrets:\n  # Currently supported: env and file providers\n  AWS_ACCESS_KEY:\n    provider: env\n    key: AWS_ACCESS_KEY_ID\n\n  DATABASE_URL:\n    provider: file\n    key: /secrets/${ENVIRONMENT}/database_url.txt\n\nsetup:\n  - shell: \"echo 'Starting $PROJECT_NAME v$VERSION in $ENVIRONMENT'\"\n  - shell: \"aws s3 ls $DATA_SOURCE --region us-west-2\"\n  - shell: \"generate-work-items.sh --source $DATA_SOURCE --batch-size $BATCH_SIZE &gt; items.json\"\n\nmap:\n  input: items.json\n  max_parallel: ${MAX_WORKERS}\n\n  agent_template:\n    - shell: \"process-batch.sh --input ${item.path} --output $OUTPUT_PATH/${item.id}.result\"\n    - shell: \"validate-result.sh $OUTPUT_PATH/${item.id}.result\"\n      on_failure:\n        shell: \"log-failure.sh ${item.id}\"\n\nreduce:\n  - shell: \"echo 'Processed ${map.successful}/${map.total} batches'\"\n  - shell: \"aggregate-results.sh --input $OUTPUT_PATH --output $OUTPUT_PATH/summary.json\"\n  - shell: \"send-metrics.sh --completed ${map.successful} --failed ${map.failed}\"\n    when: \"${ENABLE_MONITORING} == 'true'\"\n\nmerge:\n  commands:\n    - shell: \"cargo test\"\n    - shell: \"validate-deployment.sh --env $ENVIRONMENT\"\n</code></pre> <p>See also: - Best Practices for guidelines on environment management - Environment Profiles for profile configuration details - Secrets Management for secure credential handling - MapReduce Environment Variables for MapReduce-specific usage</p>"},{"location":"environment/environment-files/","title":"Environment Files","text":""},{"location":"environment/environment-files/#environment-files","title":"Environment Files","text":"<p>Load environment variables from <code>.env</code> files:</p> <pre><code># Environment files to load\nenv_files:\n  - .env\n  - .env.local\n  - config/.env.production\n\ncommands:\n  - shell: \"echo $DATABASE_URL\"\n</code></pre> <p>Environment File Format:</p> <p>Environment files use the standard <code>.env</code> format with <code>KEY=value</code> pairs:</p> <pre><code># .env file example\nDATABASE_URL=postgresql://localhost:5432/mydb\nREDIS_HOST=localhost\nREDIS_PORT=6379\n\n# Comments are supported (lines starting with #)\nAPI_KEY=secret-key-here\n\n# Empty lines are ignored\n\n# Multi-line values use quotes\nPRIVATE_KEY=\"-----BEGIN PRIVATE KEY-----\nMIIEvQIBADANBg...\n-----END PRIVATE KEY-----\"\n</code></pre> <p>Quote Handling:</p> <p>Prodigy automatically strips both single and double quotes from the start and end of values during parsing:</p> <pre><code># These all produce the same value: myvalue\nKEY1=myvalue\nKEY2=\"myvalue\"\nKEY3='myvalue'\n\n# For multi-line values, quotes are required but will be stripped\nPRIVATE_KEY=\"-----BEGIN PRIVATE KEY-----\nMIIEvQIBADANBg...\n-----END PRIVATE KEY-----\"\n# Resulting value does NOT include the surrounding quotes\n</code></pre> <p>Source: <code>src/cook/environment/manager.rs:200-207</code></p> <p>Parsing Rules:</p> <ul> <li>Lines starting with <code>#</code> are treated as comments and skipped</li> <li>Empty lines are ignored</li> <li>Each line must contain an <code>=</code> character to be valid</li> <li>Key is everything before the first <code>=</code> (trimmed)</li> <li>Value is everything after the first <code>=</code> (trimmed)</li> <li>Surrounding quotes (<code>\"</code> or <code>'</code>) are automatically removed from values</li> </ul> <p>Source: <code>src/cook/environment/manager.rs:190-211</code></p> <p>Loading Order and Precedence:</p> <p>Environment files are loaded in order (if they exist), with later files overriding earlier files. Missing files are silently skipped with debug logging. This enables layered configuration:</p> <pre><code>env_files:\n  - .env                # Base configuration\n  - .env.local          # Local overrides (gitignored)\n  - .env.production     # Environment-specific settings\n</code></pre> <p>Example override behavior:</p> <pre><code># .env (base)\nDATABASE_URL=postgresql://localhost:5432/dev\nAPI_TIMEOUT=30\n\n# .env.production (overrides)\nDATABASE_URL=postgresql://prod-server:5432/app\n# API_TIMEOUT remains 30 from base file\n</code></pre> <p>Precedence order (highest to lowest): 1. Global <code>env</code> field in workflow YAML 2. Later files in <code>env_files</code> list 3. Earlier files in <code>env_files</code> list 4. Parent process environment</p> <p>File Paths and Resolution:</p> <p>Environment file paths can be: - Absolute paths: <code>/etc/myapp/.env</code> - Relative paths: Resolved relative to the workflow file location (e.g., <code>.env</code>, <code>config/.env.production</code>)</p> <p>Source: <code>src/cook/environment/manager.rs:182-215</code></p> <p>Error Handling:</p> <p>Prodigy handles environment files with the following behavior:</p> <ul> <li>Missing files: Silently skipped with debug logging (<code>\"Environment file not found: {path}\"</code>). This allows optional configuration files and environment-specific files that may not exist in all contexts.</li> <li>File read errors: Will halt workflow execution with an error (e.g., permission denied)</li> <li>Invalid syntax: Will halt workflow execution with an error (e.g., malformed <code>.env</code> format)</li> </ul> <p>Source: <code>src/cook/environment/manager.rs:184-186</code></p> <p>Example with missing file:</p> <pre><code>env_files:\n  - .env                    # Always exists (base config)\n  - .env.local              # May not exist (personal overrides)\n  - .env.${ENVIRONMENT}     # May not exist (environment-specific)\n</code></pre> <p>In this example, if <code>.env.local</code> doesn't exist, Prodigy logs a debug message and continues. Only <code>.env</code> needs to exist. This is useful for: - Personal configuration files that are gitignored - Environment-specific files (<code>.env.production</code>, <code>.env.staging</code>) - Optional feature flags or overrides</p> <p>Troubleshooting:</p> <p>To verify which env files are being loaded, run Prodigy with verbose logging:</p> <pre><code># Enable debug logging to see which files are loaded/skipped\nRUST_LOG=debug prodigy run workflow.yml\n</code></pre> <p>You'll see messages like: <pre><code>DEBUG prodigy::cook::environment - Environment file not found: .env.local\nINFO  prodigy::cook::environment - Loaded environment from: .env\n</code></pre></p> <p>Common Syntax Errors:</p> <p>These will cause workflow execution to halt:</p> <pre><code># INVALID - No = character\nINVALID_LINE\n\n# VALID - Value can be empty\nEMPTY_VALUE=\n\n# INVALID - Unbalanced quotes (opening quote without closing)\nBAD_QUOTE=\"unclosed value\n\n# VALID - Quotes must match\nGOOD_QUOTE_1=\"value with spaces\"\nGOOD_QUOTE_2='single quoted value'\n\n# INVALID - Mixed quotes\nMIXED_QUOTES=\"value'\n\n# VALID - Embedded quotes of opposite type\nEMBEDDED=\"value with 'single' quotes inside\"\n</code></pre>"},{"location":"environment/environment-files/#integration-with-profiles-and-secrets","title":"Integration with Profiles and Secrets","text":"<p>Environment files work seamlessly with other environment features like profiles and secrets management.</p> <p>Combining env_files with profiles:</p> <pre><code># Base configuration in .env file\nenv_files:\n  - .env\n\n# Profile-specific overrides\nprofiles:\n  dev:\n    API_URL: http://localhost:3000\n    DEBUG: \"true\"\n  prod:\n    API_URL: https://api.production.com\n    DEBUG: \"false\"\n\ncommands:\n  - shell: \"echo $API_URL\"  # Uses profile value if active, otherwise .env value\n</code></pre> <p>The precedence order is: 1. Profile-specific values (if profile active) 2. Global <code>env</code> field values 3. Environment file values (later files override earlier) 4. Parent process environment</p> <p>Loading secrets from env_files:</p> <p>Environment files can contain secrets, but you must explicitly mark them as secrets in the workflow configuration:</p> <pre><code># .env.secrets file\nAPI_KEY=sk-abc123xyz\nDATABASE_PASSWORD=secret-password\n\n# Workflow configuration\nenv_files:\n  - .env.secrets\n\nsecrets:\n  # Retrieve from environment variable (loaded from .env.secrets)\n  API_KEY: \"${env:API_KEY}\"\n  DATABASE_PASSWORD: \"${env:DATABASE_PASSWORD}\"\n</code></pre> <p>Note: Variables loaded from env_files are NOT automatically masked. You must explicitly mark them as secrets in the <code>secrets</code> section for masking in logs.</p> <p>Complete integration example:</p> <pre><code># Layered configuration strategy\nenv_files:\n  - .env                # Base configuration\n  - .env.local          # Local overrides (gitignored)\n  - .env.${ENVIRONMENT} # Environment-specific (e.g., .env.production)\n\nenv:\n  PROJECT_NAME: my-project\n  VERSION: \"1.0.0\"\n\nsecrets:\n  # Secrets loaded from env files, masked in logs\n  API_KEY: \"${env:API_KEY}\"\n  DATABASE_URL: \"${env:DATABASE_URL}\"\n\nprofiles:\n  dev:\n    MAX_WORKERS: \"2\"\n    TIMEOUT: \"60\"\n  prod:\n    MAX_WORKERS: \"20\"\n    TIMEOUT: \"30\"\n\ncommands:\n  - shell: \"echo 'Project: $PROJECT_NAME v$VERSION'\"\n  - shell: \"echo 'Workers: $MAX_WORKERS, Timeout: $TIMEOUT'\"\n  - shell: \"curl -H 'Authorization: Bearer ***' $API_URL\"  # API_KEY masked\n</code></pre> <p>Best practices for organizing env files:</p> <ol> <li>.env: Base configuration, safe to commit (no secrets)</li> <li>.env.local: Personal overrides, add to .gitignore</li> <li>.env.production / .env.staging / .env.dev: Environment-specific, may contain encrypted secrets</li> <li>.env.secrets: Sensitive values, NEVER commit, always in .gitignore</li> </ol> <p>Precedence example:</p> <pre><code># .env (base)\nAPI_URL=http://localhost:3000\nMAX_WORKERS=5\nTIMEOUT=30\n\n# .env.production (overrides)\nAPI_URL=https://api.production.com\nMAX_WORKERS=20\n</code></pre> <pre><code>env_files:\n  - .env\n  - .env.production  # Overrides API_URL and MAX_WORKERS\n\nenv:\n  TIMEOUT: \"60\"      # Overrides TIMEOUT from both files\n\nprofiles:\n  prod:\n    MAX_WORKERS: \"50\"  # Overrides MAX_WORKERS when --profile prod used\n</code></pre> <p>Final values when running with <code>--profile prod</code>: - <code>API_URL</code>: <code>https://api.production.com</code> (from .env.production) - <code>MAX_WORKERS</code>: <code>50</code> (from prod profile - highest precedence) - <code>TIMEOUT</code>: <code>60</code> (from global env field - overrides files)</p>"},{"location":"environment/environment-files/#see-also","title":"See Also","text":"<ul> <li>Environment Precedence - Detailed precedence rules for all environment sources</li> <li>Environment Profiles - Profile-specific environment configuration</li> <li>Secrets Management - How to securely handle secrets loaded from env files</li> </ul>"},{"location":"environment/environment-precedence/","title":"Environment Precedence","text":""},{"location":"environment/environment-precedence/#environment-precedence","title":"Environment Precedence","text":"<p>Environment variables are resolved with a clear precedence order, ensuring predictable behavior when the same variable is defined in multiple locations.</p> <p>Source: Implementation in <code>src/cook/environment/manager.rs:95-137</code></p>"},{"location":"environment/environment-precedence/#precedence-order","title":"Precedence Order","text":"<p>Environment variables are applied in the following order (later sources override earlier ones):</p> <ol> <li>Parent environment - Inherited from the parent process</li> <li>Environment files - Loaded from <code>env_files</code> (later files override earlier)</li> <li>Global <code>env</code> - Defined at workflow level in YAML</li> <li>Active profile - Applied if a profile is set (internal infrastructure)</li> <li>Step-specific <code>env</code> - Per-step environment variables</li> <li>Secrets - Loaded from secrets configuration (applied after step env)</li> <li>Shell-level overrides - Using <code>ENV=value command</code> syntax</li> </ol> <p>Source: Precedence implementation in <code>src/cook/environment/manager.rs:95-137</code></p>"},{"location":"environment/environment-precedence/#implementation-details","title":"Implementation Details","text":""},{"location":"environment/environment-precedence/#parent-environment-inheritance","title":"Parent Environment Inheritance","text":"<p>By default, workflows inherit all environment variables from the parent process. You can disable this with <code>inherit: false</code> in the environment configuration.</p> <pre><code># Disable parent environment inheritance\ninherit: false\n\nenv:\n  # Only these variables will be available\n  NODE_ENV: production\n</code></pre> <p>Source: Parent environment loading at <code>src/cook/environment/manager.rs:98-102</code></p>"},{"location":"environment/environment-precedence/#environment-files-precedence","title":"Environment Files Precedence","text":"<p>When multiple environment files are specified, later files override earlier ones. This allows layering of configuration (base + environment-specific).</p> <pre><code>env_files:\n  - .env              # Base configuration\n  - .env.production   # Overrides base values\n</code></pre> <p>Source: Environment file loading at <code>src/cook/environment/manager.rs:107-109</code></p>"},{"location":"environment/environment-precedence/#global-environment","title":"Global Environment","text":"<p>The global <code>env</code> block at the workflow level overrides both parent environment and environment files.</p> <pre><code>env:\n  NODE_ENV: production  # Overrides .env files and parent environment\n  API_URL: https://api.example.com\n</code></pre> <p>Source: Global environment application at <code>src/cook/environment/manager.rs:112-115</code></p>"},{"location":"environment/environment-precedence/#profile-infrastructure","title":"Profile Infrastructure","text":"<p>Prodigy includes internal profile infrastructure that can activate different environment configurations. However, this feature is not currently exposed via CLI flags.</p> <pre><code># Profile infrastructure exists but no --profile CLI flag available\nprofiles:\n  development:\n    NODE_ENV: development\n    API_URL: http://localhost:3000\n</code></pre> <p>Source: Profile application at <code>src/cook/environment/manager.rs:118-120</code>; No CLI flag in <code>src/cli/args.rs</code></p>"},{"location":"environment/environment-precedence/#step-specific-environment","title":"Step-Specific Environment","text":"<p>Note: The YAML command syntax (<code>WorkflowStepCommand</code>) does not expose step-level environment configuration. However, the internal runtime (<code>StepEnvironment</code>) supports it for future extensibility.</p> <p>Source: - <code>StepEnvironment</code> struct at <code>src/cook/environment/config.rs:128-144</code> - Step environment application at <code>src/cook/environment/manager.rs:123-127</code></p>"},{"location":"environment/environment-precedence/#secrets-loading","title":"Secrets Loading","text":"<p>Secrets are loaded AFTER step-specific environment variables, ensuring they cannot be accidentally overridden by step configuration.</p> <pre><code>secrets:\n  API_KEY: \"${env:SECRET_API_KEY}\"\n\n# This takes precedence over global env, env_files, and step env\n</code></pre> <p>Source: Secrets application at <code>src/cook/environment/manager.rs:130-137</code></p>"},{"location":"environment/environment-precedence/#shell-level-overrides","title":"Shell-Level Overrides","text":"<p>Shell syntax provides the highest precedence override, applied at command execution time.</p> <pre><code>- shell: \"NODE_ENV=test echo $NODE_ENV\"  # Prints: test\n</code></pre> <p>This override is handled by the shell itself and takes precedence over all Prodigy environment sources.</p>"},{"location":"environment/environment-precedence/#complete-example","title":"Complete Example","text":"<p>Here's a comprehensive example demonstrating all precedence levels:</p> <pre><code># Parent environment: NODE_ENV=local (inherited by default)\n\nenv_files:\n  - .env  # Contains: NODE_ENV=development, API_URL=http://localhost:3000\n\nenv:\n  NODE_ENV: production      # Overrides .env file and parent\n  API_URL: https://api.prod.example.com  # Overrides .env file\n\nsecrets:\n  API_KEY: \"${env:SECRET_API_KEY}\"  # Loaded after global env\n\ncommands:\n  - shell: \"echo $NODE_ENV\"          # Prints: production (from global env)\n  - shell: \"echo $API_URL\"           # Prints: https://api.prod.example.com\n  - shell: \"echo $API_KEY\"           # Prints: *** (masked, from secrets)\n\n  # Override using shell syntax (highest precedence)\n  - shell: \"NODE_ENV=staging echo $NODE_ENV\"  # Prints: staging\n</code></pre> <p>Source: Real-world example from <code>workflows/environment-example.yml</code></p>"},{"location":"environment/environment-precedence/#precedence-resolution-flow","title":"Precedence Resolution Flow","text":"<p>When resolving an environment variable, Prodigy follows this flow:</p> <pre><code>1. Start with parent environment (if inherit: true, default)\n2. Apply each env_file in order (later files override)\n3. Apply global env block (overrides files and parent)\n4. Apply active profile if set (internal feature)\n5. Apply step-specific env (runtime capability)\n6. Apply secrets (highest Prodigy-level precedence)\n7. Shell overrides apply at execution time (highest overall)\n</code></pre> <p>Result: The last value set wins, creating a predictable override chain.</p>"},{"location":"environment/environment-precedence/#debugging-precedence-issues","title":"Debugging Precedence Issues","text":"<p>When troubleshooting which environment source is active:</p> <pre><code># Print all environment variables to debug precedence\n- shell: \"env | sort\"\n  capture_output: true\n\n# Print specific variable to verify its value\n- shell: \"echo NODE_ENV=$NODE_ENV\"\n</code></pre> <p>Common debugging scenarios: - Variable not set: Check if parent environment is being inherited - Wrong value: Check which precedence level last set the variable - Secrets not working: Verify secrets are loaded after step environment - Override not applying: Ensure shell syntax is correct</p>"},{"location":"environment/environment-precedence/#related-topics","title":"Related Topics","text":"<ul> <li>Environment Files - Loading configuration from files</li> <li>Secrets Management - Secure handling of sensitive values</li> <li>Environment Profiles - Profile infrastructure details</li> <li>Per-Command Overrides - Command-level environment control</li> <li>Best Practices - Environment configuration recommendations</li> </ul>"},{"location":"environment/environment-profiles/","title":"Environment Profiles","text":""},{"location":"environment/environment-profiles/#environment-profiles","title":"Environment Profiles","text":"<p>Environment profiles allow you to define named sets of environment variables for different execution contexts (development, staging, production, etc.). Each profile contains environment variables that are applied when the profile is activated.</p> <p>Source: Profile infrastructure implemented in <code>src/cook/environment/config.rs</code> (EnvironmentConfig struct) and <code>src/cook/environment/manager.rs</code> (profile application logic).</p>"},{"location":"environment/environment-profiles/#defining-profiles","title":"Defining Profiles","text":"<p>Profiles use a flat structure where environment variables are defined directly at the profile level (not nested under an <code>env:</code> key):</p> <pre><code># Define multiple profiles for different environments\nprofiles:\n  development:\n    description: \"Development environment with debug enabled\"\n    NODE_ENV: development\n    DEBUG: \"true\"\n    API_URL: http://localhost:3000\n    LOG_LEVEL: debug\n\n  staging:\n    description: \"Staging environment for QA\"\n    NODE_ENV: staging\n    DEBUG: \"true\"\n    API_URL: https://staging.api.example.com\n    LOG_LEVEL: info\n\n  production:\n    description: \"Production environment configuration\"\n    NODE_ENV: production\n    DEBUG: \"false\"\n    API_URL: https://api.example.com\n    LOG_LEVEL: error\n\n# Global environment variables (apply to all profiles)\nenv:\n  APP_NAME: \"my-app\"\n  VERSION: \"1.0.0\"\n\ncommands:\n  - shell: \"npm run build\"\n</code></pre> <p>Source: Profile structure defined in <code>src/cook/environment/config.rs</code> (EnvProfile type).</p> <p>Profile Structure Details: - description (optional): Human-readable description of the profile's purpose - Environment variables: Direct key-value pairs at the profile level - All variable values must be strings in YAML</p>"},{"location":"environment/environment-profiles/#activating-profiles","title":"Activating Profiles","text":"<p>Design Note: The profile activation infrastructure is architecturally complete in the codebase. The <code>EnvironmentConfig</code> struct has an <code>active_profile</code> field (<code>src/cook/environment/config.rs:33-35</code>), and the <code>EnvironmentManager</code> applies active profiles during environment setup (<code>src/cook/environment/manager.rs:118-120</code>). Comprehensive integration tests demonstrate profile activation (<code>tests/environment_workflow_test.rs:63-132</code>).</p> <p>Current Implementation Status: As of this documentation, the CLI wiring for profile activation (<code>--profile</code> flag and <code>PRODIGY_PROFILE</code> environment variable) is not yet connected to the argument parser. The profile application infrastructure exists and is tested, but requires the active profile to be set programmatically rather than via command-line arguments.</p> <p>Intended Usage (when CLI wiring is complete):</p> <pre><code># Activate profile via command line flag\nprodigy run workflow.yml --profile production\n\n# Activate profile via environment variable\nexport PRODIGY_PROFILE=staging\nprodigy run workflow.yml\n</code></pre> <p>Current Workaround: Profiles can be activated programmatically in tests or by directly setting the <code>active_profile</code> field when constructing <code>EnvironmentConfig</code> objects.</p>"},{"location":"environment/environment-profiles/#common-use-cases","title":"Common Use Cases","text":"<p>Profiles are ideal for managing environment-specific configuration:</p> <ol> <li> <p>Different API Endpoints <pre><code>profiles:\n  development:\n    API_URL: http://localhost:3000\n    AUTH_URL: http://localhost:4000\n\n  production:\n    API_URL: https://api.example.com\n    AUTH_URL: https://auth.example.com\n</code></pre></p> </li> <li> <p>Environment-Specific Credentials <pre><code>profiles:\n  development:\n    DB_HOST: localhost\n    DB_NAME: myapp_dev\n    DB_USER: dev_user\n\n  production:\n    DB_HOST: prod-db.example.com\n    DB_NAME: myapp_prod\n    DB_USER: prod_user\n</code></pre></p> </li> <li> <p>Deployment Target Configuration <pre><code>profiles:\n  aws:\n    CLOUD_PROVIDER: aws\n    REGION: us-east-1\n    DEPLOY_COMMAND: \"aws deploy\"\n\n  gcp:\n    CLOUD_PROVIDER: gcp\n    REGION: us-central1\n    DEPLOY_COMMAND: \"gcloud deploy\"\n</code></pre></p> </li> </ol>"},{"location":"environment/environment-profiles/#environment-variable-precedence","title":"Environment Variable Precedence","text":"<p>When a profile is active, environment variables are resolved in this order (highest to lowest precedence):</p> <ol> <li>Step-level environment - Variables defined in individual command <code>env:</code> blocks</li> <li>Active profile environment - Variables from the activated profile</li> <li>Global environment - Variables from top-level <code>env:</code> block</li> <li>System environment - Variables inherited from the shell</li> </ol> <p>Source: Precedence chain implemented in <code>src/cook/environment/manager.rs</code> and tested in <code>tests/environment_workflow_test.rs</code>.</p> <p>For detailed information on precedence rules, see Environment Precedence.</p>"},{"location":"environment/environment-profiles/#profile-best-practices","title":"Profile Best Practices","text":"<p>Define sensible defaults: <pre><code>profiles:\n  development:\n    description: \"Local development with debug enabled\"\n    DEBUG: \"true\"\n    LOG_LEVEL: debug\n\n  production:\n    description: \"Production environment with minimal logging\"\n    DEBUG: \"false\"\n    LOG_LEVEL: error\n</code></pre></p> <p>Combine with env_files for secrets: <pre><code>profiles:\n  production:\n    API_URL: https://api.example.com\n    DEBUG: \"false\"\n\nenv_files:\n  - path: .env.production\n    required: true  # Contains secrets like API_KEY\n</code></pre></p> <p>See Environment Files for more on combining profiles with environment files.</p> <p>Override profile values at step level: <pre><code>profiles:\n  production:\n    LOG_LEVEL: error\n\ncommands:\n  - shell: \"run-diagnostics.sh\"\n    env:\n      LOG_LEVEL: debug  # Override for this step only\n</code></pre></p> <p>See Per-Command Environment Overrides for step-level overrides.</p>"},{"location":"environment/environment-profiles/#troubleshooting","title":"Troubleshooting","text":"<p>Profile not applied: - Verify profile name matches exactly (case-sensitive) - Check that profile is defined in <code>profiles:</code> section - Confirm profile activation method is used correctly</p> <p>Variables not resolved: - Ensure variable names are correct in profile definition - Check precedence - higher-precedence sources may override profile values - Verify string values are quoted in YAML if they contain special characters</p> <p>See Also: - Environment Precedence - Understanding variable resolution order - Environment Files - Loading variables from external files - Best Practices - Recommended patterns for environment configuration</p>"},{"location":"environment/mapreduce-environment-variables/","title":"MapReduce Environment Variables","text":""},{"location":"environment/mapreduce-environment-variables/#mapreduce-environment-variables","title":"MapReduce Environment Variables","text":"<p>In MapReduce workflows, environment variables provide powerful parameterization across all phases (setup, map, reduce, and merge). This enables workflows to be reusable across different projects and configurations.</p>"},{"location":"environment/mapreduce-environment-variables/#overview","title":"Overview","text":"<p>Environment variables in MapReduce workflows are available in all execution phases: - Setup phase: Initialize environment, generate configuration - Map phase: Parameterize agent templates, configure timeouts and parallelism - Reduce phase: Aggregate results, format output - Merge phase: Control merge behavior, validation</p>"},{"location":"environment/mapreduce-environment-variables/#setup-phase-environment-variables","title":"Setup Phase Environment Variables","text":"<p>Environment variables are fully available in setup phase commands:</p> <pre><code>env:\n  PROJECT_NAME: prodigy\n  DATA_SOURCE: https://api.example.com/items\n  TIMEOUT: \"30\"\n\nsetup:\n  - shell: \"curl $DATA_SOURCE &gt; items.json\"\n  - shell: \"echo 'Processing $PROJECT_NAME workflow'\"\n  - shell: \"mkdir -p output/$PROJECT_NAME\"\n</code></pre> <p>Use cases in setup: - Configure data sources and API endpoints - Set project-specific paths - Initialize environment-specific settings</p>"},{"location":"environment/mapreduce-environment-variables/#map-phase-environment-variables","title":"Map Phase Environment Variables","text":"<p>Environment variables can be used throughout the map phase configuration:</p> <pre><code>env:\n  MAX_WORKERS: \"10\"\n  AGENT_TIMEOUT: \"600\"\n  PROJECT_DIR: /path/to/project\n\nmap:\n  input: items.json\n  max_parallel: ${MAX_WORKERS}     # Parameterize parallelism\n  agent_timeout_secs: ${AGENT_TIMEOUT}\n\n  agent_template:\n    - claude: \"/process-item '${item.name}' --project $PROJECT_NAME\"\n    - shell: \"test -f $PROJECT_DIR/${item.file}\"\n    - shell: \"timeout ${AGENT_TIMEOUT} ./analyze.sh ${item.path}\"\n</code></pre> <p>Parameterizing max_parallel and agent_timeout_secs:</p> <p>Both <code>max_parallel</code> and <code>agent_timeout_secs</code> accept numeric values or environment variable references. These are resolved at configuration parse time (when the workflow is loaded), not at execution time.</p> <pre><code>env:\n  MAX_WORKERS: \"5\"   # Can be overridden per environment\n  AGENT_TIMEOUT: \"600\"\n\nmap:\n  max_parallel: ${MAX_WORKERS}           # Resolved to 5 at parse time\n  agent_timeout_secs: ${AGENT_TIMEOUT}   # Resolved to 600 at parse time\n  # OR\n  max_parallel: 10                       # Static value\n  agent_timeout_secs: 300                # Static value\n</code></pre> <p>Environment Variable Resolution Timing:</p> <ul> <li>Parse time (when workflow is loaded): <code>max_parallel</code> and <code>agent_timeout_secs</code> are resolved using <code>resolve_env_or_parse</code> method</li> <li>Execution time (when commands run): Command interpolation happens (e.g., <code>${item.name}</code>, <code>$PROJECT_NAME</code>)</li> </ul> <p>This distinction matters because parse-time resolution fails fast if environment variables are undefined, while execution-time interpolation happens dynamically.</p> <p>Source: src/config/mapreduce.rs:527-540 (to_map_phase method with resolve_env_or_parse)</p> <p>Integration with MapReduce-specific variables:</p> <p>Environment variables work seamlessly with MapReduce variables like <code>${item}</code> and <code>${map.results}</code>:</p> <pre><code>env:\n  OUTPUT_DIR: /tmp/results\n  CONFIG_FILE: config.json\n\nmap:\n  agent_template:\n    - shell: \"process --config $CONFIG_FILE --input ${item.path} --output $OUTPUT_DIR/${item.id}.json\"\n</code></pre>"},{"location":"environment/mapreduce-environment-variables/#reduce-phase-environment-variables","title":"Reduce Phase Environment Variables","text":"<p>Use environment variables in reduce commands to parameterize aggregation:</p> <pre><code>env:\n  OUTPUT_PATH: results/summary.json\n  MIN_SUCCESS_RATE: \"80\"\n\nreduce:\n  - shell: \"echo 'Processed ${map.successful}/${map.total} items'\"\n  - write_file:\n      path: \"${OUTPUT_PATH}\"\n      content: \"${map.results}\"\n      format: json\n  - shell: |\n      SUCCESS_RATE=$((${map.successful} * 100 / ${map.total}))\n      if [ $SUCCESS_RATE -lt $MIN_SUCCESS_RATE ]; then\n        echo \"Warning: Success rate below threshold\"\n        exit 1\n      fi\n</code></pre> <p>Important: write_file format field</p> <p>The <code>format</code> field in <code>write_file</code> accepts enum literals only (<code>text</code>, <code>json</code>, or <code>yaml</code>), not environment variable interpolation. Only the <code>path</code> and <code>content</code> fields support variable interpolation.</p> <p>Source: src/config/command.rs:303-314 (WriteFileFormat enum)</p>"},{"location":"environment/mapreduce-environment-variables/#merge-phase-environment-variables","title":"Merge Phase Environment Variables","text":"<p>Environment variables are available in merge workflows alongside merge-specific variables:</p> <pre><code>env:\n  CI_MODE: \"true\"\n  TEST_TIMEOUT: \"300\"\n\nmerge:\n  commands:\n    - shell: \"git fetch origin\"\n    - shell: \"timeout $TEST_TIMEOUT cargo test\"\n    - shell: |\n        if [ \"$CI_MODE\" = \"true\" ]; then\n          git merge --no-edit ${merge.source_branch}\n        else\n          claude \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"\n        fi\n</code></pre>"},{"location":"environment/mapreduce-environment-variables/#complete-example-parameterized-mapreduce-workflow","title":"Complete Example: Parameterized MapReduce Workflow","text":"<pre><code>name: parameterized-processing\nmode: mapreduce\n\nenv:\n  # Project configuration\n  PROJECT_NAME: my-project\n  VERSION: \"1.0.0\"\n\n  # Execution parameters\n  MAX_WORKERS: \"10\"\n  AGENT_TIMEOUT: \"600\"\n\n  # Paths\n  INPUT_FILE: items.json\n  OUTPUT_DIR: results\n  CONFIG_PATH: config/settings.json\n\n  # Thresholds\n  MIN_COVERAGE: \"80\"\n\nsetup:\n  - shell: \"echo 'Starting $PROJECT_NAME v$VERSION'\"\n  - shell: \"mkdir -p $OUTPUT_DIR\"\n  - shell: \"generate-items.sh &gt; $INPUT_FILE\"\n\nmap:\n  input: ${INPUT_FILE}\n  max_parallel: ${MAX_WORKERS}\n  agent_timeout_secs: ${AGENT_TIMEOUT}\n\n  agent_template:\n    - claude: \"/process '${item.name}' --project $PROJECT_NAME --config $CONFIG_PATH\"\n    - shell: \"test -f $OUTPUT_DIR/${item.id}.result\"\n\nreduce:\n  - shell: \"echo 'Project: $PROJECT_NAME, Version: $VERSION'\"\n  - shell: \"echo 'Results: ${map.successful}/${map.total} succeeded'\"\n  - write_file:\n      path: \"${OUTPUT_DIR}/summary.json\"\n      content: |\n        {\n          \"project\": \"$PROJECT_NAME\",\n          \"version\": \"$VERSION\",\n          \"total\": ${map.total},\n          \"successful\": ${map.successful},\n          \"failed\": ${map.failed}\n        }\n      format: json\n\nmerge:\n  commands:\n    - shell: \"cargo test --timeout $AGENT_TIMEOUT\"\n    - claude: \"/validate-merge --project $PROJECT_NAME\"\n</code></pre>"},{"location":"environment/per-command-environment-overrides/","title":"Per-Command Environment Overrides","text":""},{"location":"environment/per-command-environment-overrides/#per-command-environment-overrides","title":"Per-Command Environment Overrides","text":"<p>IMPORTANT: WorkflowStepCommand does NOT have an <code>env</code> field. All per-command environment changes must use shell syntax.</p> <p>Note: The legacy Command struct (structured format) has an <code>env</code> field via CommandMetadata, but the modern WorkflowStepCommand format does not. For workflows using the modern <code>claude:</code>/<code>shell:</code> syntax, use shell-level environment syntax (<code>ENV=value command</code>).</p> <p>You can override environment variables for individual commands using shell environment syntax:</p> <pre><code>env:\n  RUST_LOG: info\n  API_URL: \"https://api.example.com\"\n\n# Steps go directly in the workflow\n- shell: \"cargo run\"  # Uses RUST_LOG=info from global env\n\n# Override environment for this command only using shell syntax\n- shell: \"RUST_LOG=debug cargo run --verbose\"\n\n# Change directory and set environment in shell\n- shell: \"cd frontend &amp;&amp; PATH=./node_modules/.bin:$PATH npm run build\"\n</code></pre>"},{"location":"environment/per-command-environment-overrides/#shell-based-environment-techniques","title":"Shell-Based Environment Techniques","text":""},{"location":"environment/per-command-environment-overrides/#basic-overrides","title":"Basic Overrides","text":"<ul> <li>Single variable override: <code>ENV_VAR=value command</code></li> <li>Multiple variables: <code>VAR1=value1 VAR2=value2 command</code></li> <li>Change directory: <code>cd path &amp;&amp; command</code></li> <li>Combine both: <code>cd path &amp;&amp; ENV_VAR=value command</code></li> </ul>"},{"location":"environment/per-command-environment-overrides/#advanced-shell-patterns","title":"Advanced Shell Patterns","text":"<p>You can combine shell environment overrides with redirection and other shell features:</p> <pre><code># Redirect output while overriding environment\n- shell: \"RUST_LOG=trace cargo test &gt; test-output.txt 2&gt;&amp;1\"\n\n# Pipe commands with environment overrides\n- shell: \"COLUMNS=120 cargo fmt --check | tee fmt-report.txt\"\n\n# Multiple environment variables with complex shell operations\n- shell: |\n    RUST_BACKTRACE=1 RUST_LOG=debug cargo run \\\n      --release \\\n      --bin my-app &gt; app.log 2&gt;&amp;1 || echo \"Build failed\"\n</code></pre> <p>Source: Implementation in <code>src/config/command.rs:320-401</code> (WorkflowStepCommand definition)</p>"},{"location":"environment/per-command-environment-overrides/#interaction-with-environment-precedence","title":"Interaction with Environment Precedence","text":"<p>Shell-level environment overrides take highest precedence and apply only to the specific command where they're defined. These overrides shadow:</p> <ul> <li>Global <code>env</code> variables</li> <li>Profile-specific environment</li> <li><code>.env</code> file values</li> <li>System environment variables</li> </ul> <p>For detailed precedence rules, see Environment Precedence.</p>"},{"location":"environment/per-command-environment-overrides/#future-plans-stepenvironment-struct","title":"Future Plans: StepEnvironment Struct","text":"<p>A <code>StepEnvironment</code> struct exists in the internal runtime (defined in <code>src/cook/environment/config.rs:126-144</code>) with support for:</p> <ul> <li><code>env</code>: HashMap of environment variables</li> <li><code>working_dir</code>: Optional working directory override</li> <li><code>clear_env</code>: Clear parent environment before applying step env</li> <li><code>temporary</code>: Restore environment after step execution</li> </ul> <p>This struct may be exposed in future versions to provide more structured per-step environment control directly in YAML syntax, eliminating the need for shell-based workarounds. However, currently it is not exposed in WorkflowStepCommand, so all per-command environment changes must use shell syntax as demonstrated above.</p> <p>Source: <code>src/cook/environment/config.rs:126-144</code></p>"},{"location":"environment/per-command-environment-overrides/#troubleshooting","title":"Troubleshooting","text":""},{"location":"environment/per-command-environment-overrides/#environment-variable-not-taking-effect","title":"Environment Variable Not Taking Effect","text":"<p>Problem: Shell environment override doesn't apply to command</p> <p>Cause: Quote escaping or shell evaluation order issues</p> <p>Solution: Use proper quoting and verify variable expansion: <pre><code># \u2713 Correct\n- shell: 'API_URL=\"https://example.com\" ./script.sh'\n\n# \u2717 Incorrect (quotes broken)\n- shell: \"API_URL=\"https://example.com\" ./script.sh\"\n</code></pre></p>"},{"location":"environment/per-command-environment-overrides/#variable-expansion-issues","title":"Variable Expansion Issues","text":"<p>Problem: Variable contains shell special characters (<code>$</code>, <code>\\</code>, etc.)</p> <p>Solution: Use single quotes to prevent shell expansion: <pre><code># If variable value is literal (no shell expansion needed)\n- shell: \"PASSWORD='$ecr3t!' ./deploy.sh\"\n</code></pre></p>"},{"location":"environment/per-command-environment-overrides/#debugging-environment-resolution","title":"Debugging Environment Resolution","text":"<p>To debug which environment values are active:</p> <pre><code># Print all environment variables\n- shell: \"env | sort\"\n\n# Check specific variable resolution\n- shell: 'echo \"RUST_LOG is: $RUST_LOG\"'\n\n# Verify override works\n- shell: 'RUST_LOG=trace sh -c \"echo RUST_LOG is: $RUST_LOG\"'\n</code></pre>"},{"location":"environment/per-command-environment-overrides/#see-also","title":"See Also","text":"<ul> <li>Environment Precedence - How environment variables are resolved</li> <li>Environment Profiles - Named environment configurations</li> <li>Secrets Management - Handling sensitive values</li> </ul>"},{"location":"environment/secrets-management/","title":"Secrets Management","text":""},{"location":"environment/secrets-management/#secrets-management","title":"Secrets Management","text":"<p>Prodigy provides secure secret management through the <code>secrets</code> field in <code>EnvironmentConfig</code>. Secrets are environment variables that are masked in logs and output for security.</p>"},{"location":"environment/secrets-management/#configuration-structure","title":"Configuration Structure","text":"<p>Secrets are defined in the workflow-level <code>secrets:</code> block using the <code>SecretValue</code> type, which supports two variants:</p> <p>Source: <code>src/cook/environment/config.rs:84-96</code></p> <pre><code># Simple secret reference (SecretValue::Simple)\nsecrets:\n  API_KEY: \"${env:SECRET_API_KEY}\"\n\n# Provider-based secrets (SecretValue::Provider)\nsecrets:\n  DATABASE_URL:\n    provider: env\n    key: \"DB_CONNECTION_STRING\"\n\n  SSH_KEY:\n    provider: file\n    key: \"~/.ssh/deploy_key\"\n    version: \"v1\"  # Optional version field\n</code></pre>"},{"location":"environment/secrets-management/#secretvalue-types","title":"SecretValue Types","text":"<p>The <code>SecretValue</code> enum has two variants defined in <code>src/cook/environment/config.rs:84-96</code>:</p>"},{"location":"environment/secrets-management/#1-simple-string-reference","title":"1. Simple String Reference","text":"<p>The <code>Simple</code> variant allows direct string references, typically using environment variable interpolation:</p> <pre><code>secrets:\n  # Reference environment variable directly\n  API_TOKEN: \"${env:GITHUB_TOKEN}\"\n\n  # Reference another env var with fallback\n  DATABASE_PASS: \"${env:DB_PASSWORD}\"\n</code></pre> <p>Resolution: Simple values are resolved by looking up the referenced environment variable name (<code>src/cook/environment/manager.rs:316-319</code>).</p>"},{"location":"environment/secrets-management/#2-provider-based-secrets","title":"2. Provider-Based Secrets","text":"<p>The <code>Provider</code> variant supports structured secret resolution with different providers:</p> <pre><code>secrets:\n  # Environment variable provider\n  API_KEY:\n    provider: env\n    key: \"SECRET_API_KEY\"\n\n  # File-based secret\n  SSH_PRIVATE_KEY:\n    provider: file\n    key: \"/etc/secrets/ssh_key\"\n\n  # Custom provider (extensible)\n  CUSTOM_SECRET:\n    provider: custom\n    key: \"my-secret-id\"\n</code></pre> <p>Source: Example from <code>workflows/mapreduce-env-example.yml:23-26</code></p>"},{"location":"environment/secrets-management/#supported-secret-providers","title":"Supported Secret Providers","text":"<p>Prodigy defines five secret providers in the <code>SecretProvider</code> enum (<code>src/cook/environment/config.rs:98-112</code>):</p> Provider Status Description Source Reference <code>env</code> \u2705 Implemented Reads from environment variables <code>manager.rs:322-323</code> <code>file</code> \u2705 Implemented Reads from filesystem <code>manager.rs:324-329</code> <code>vault</code> \ud83d\udd2e Planned HashiCorp Vault integration <code>config.rs:107</code> <code>aws</code> \ud83d\udd2e Planned AWS Secrets Manager <code>config.rs:109</code> <code>custom</code> \u2699\ufe0f Extensible Custom provider via SecretStore <code>config.rs:111</code> <p>Important: Only <code>env</code> and <code>file</code> providers are fully implemented in <code>EnvironmentManager.resolve_secret()</code> (<code>src/cook/environment/manager.rs:313-337</code>). Vault and AWS providers are defined in the enum but delegate to <code>SecretStore</code> for implementation, which currently returns \"not found\" errors.</p>"},{"location":"environment/secrets-management/#secret-resolution-flow","title":"Secret Resolution Flow","text":"<p>Secrets are resolved during environment setup with the following flow (<code>src/cook/environment/manager.rs:128-136</code>):</p> <pre><code>1. EnvironmentConfig.secrets loaded from YAML\n   \u2193\n2. For each secret, EnvironmentManager.resolve_secret() is called\n   \u2193\n3. Resolution strategy based on SecretValue variant:\n   - Simple: Look up in std::env\n   - Provider(Env): Look up in std::env\n   - Provider(File): Read from filesystem\n   - Provider(Other): Delegate to SecretStore\n   \u2193\n4. Resolved value added to environment HashMap\n   \u2193\n5. Secret key tracked in EnvironmentContext.secrets Vec for masking\n</code></pre> <p>Source: <code>src/cook/environment/manager.rs:128-136</code></p>"},{"location":"environment/secrets-management/#secret-masking","title":"Secret Masking","text":"<p>Secret values are masked in logs and command output to prevent accidental exposure:</p> <pre><code>secrets:\n  API_TOKEN:\n    provider: env\n    key: \"GITHUB_TOKEN\"\n\ncommands:\n  - shell: \"curl -H 'Authorization: Bearer $API_TOKEN' https://api.github.com\"\n</code></pre> <p>Output (masked): <pre><code>$ curl -H 'Authorization: Bearer ***' https://api.github.com\n</code></pre></p> <p>How it works: The <code>EnvironmentContext</code> struct tracks secret keys in a <code>Vec&lt;String&gt;</code> field (<code>src/cook/environment/manager.rs:23-30</code>). When commands are executed, output is scanned and secret values are replaced with <code>***</code>.</p>"},{"location":"environment/secrets-management/#secretstore-architecture","title":"SecretStore Architecture","text":"<p>For extensibility, Prodigy provides a <code>SecretStore</code> system that supports custom secret providers (<code>src/cook/environment/secret_store.rs:26-107</code>):</p> <p>Built-in Providers: - <code>EnvSecretProvider</code> - Environment variable lookup (<code>secret_store.rs:120-131</code>) - <code>FileSecretProvider</code> - File-based secrets (<code>secret_store.rs:134-148</code>)</p> <p>Custom Providers:</p> <p>You can add custom secret providers by implementing the <code>SecretProvider</code> trait:</p> <pre><code>// Example custom provider (for reference)\n#[async_trait::async_trait]\npub trait SecretProvider: Send + Sync {\n    async fn get_secret(&amp;self, key: &amp;str) -&gt; Result&lt;String&gt;;\n    async fn has_secret(&amp;self, key: &amp;str) -&gt; bool;\n}\n</code></pre> <p>Source: <code>src/cook/environment/secret_store.rs:110-117</code></p> <p>Custom providers can be registered with <code>SecretStore.add_provider()</code> (<code>secret_store.rs:79-81</code>).</p>"},{"location":"environment/secrets-management/#real-world-examples","title":"Real-World Examples","text":""},{"location":"environment/secrets-management/#example-1-simple-environment-variable-secrets","title":"Example 1: Simple Environment Variable Secrets","text":"<p>From <code>workflows/environment-example.yml:21-23</code>:</p> <pre><code>secrets:\n  API_KEY: \"${env:SECRET_API_KEY}\"\n\ncommands:\n  - shell: \"echo 'Deploying with API key: $API_KEY'\"  # Value masked in logs\n</code></pre>"},{"location":"environment/secrets-management/#example-2-provider-based-secrets-in-mapreduce","title":"Example 2: Provider-Based Secrets in MapReduce","text":"<p>From <code>workflows/mapreduce-env-example.yml:23-26</code>:</p> <pre><code>secrets:\n  API_TOKEN:\n    provider: env\n    key: \"GITHUB_TOKEN\"\n\nreduce:\n  - shell: \"curl -H 'Authorization: Bearer $API_TOKEN' https://api.github.com/repos/notify\"\n</code></pre>"},{"location":"environment/secrets-management/#example-3-file-based-secrets","title":"Example 3: File-Based Secrets","text":"<pre><code>secrets:\n  DATABASE_PASSWORD:\n    provider: file\n    key: \"/run/secrets/db_password\"  # Docker secrets pattern\n\n  SSH_DEPLOY_KEY:\n    provider: file\n    key: \"~/.ssh/deploy_key\"\n\ncommands:\n  - shell: \"psql postgresql://user:$DATABASE_PASSWORD@localhost/db\"\n  - shell: \"ssh -i $SSH_DEPLOY_KEY deploy@server 'systemctl restart app'\"\n</code></pre>"},{"location":"environment/secrets-management/#integration-with-environment-configuration","title":"Integration with Environment Configuration","text":"<p>Secrets are part of the global <code>EnvironmentConfig</code> structure and work alongside other environment features:</p> <pre><code># Global environment configuration\nenv:\n  NODE_ENV: production\n  API_URL: https://api.example.com\n\n# Secrets (masked in logs)\nsecrets:\n  API_KEY:\n    provider: env\n    key: \"SECRET_API_KEY\"\n\n# Environment profiles\nprofiles:\n  production:\n    NODE_ENV: production\n  development:\n    NODE_ENV: development\n\ncommands:\n  - shell: \"curl -H 'X-API-Key: $API_KEY' $API_URL/deploy\"\n</code></pre> <p>Source: Structure from <code>src/cook/environment/config.rs:11-36</code></p>"},{"location":"environment/secrets-management/#security-best-practices","title":"Security Best Practices","text":"<ol> <li>Never commit secrets to version control</li> <li>Use environment variables or secret files</li> <li> <p>Add secret files to <code>.gitignore</code></p> </li> <li> <p>Use the <code>secrets:</code> field for sensitive data</p> </li> <li>Ensures masking in logs and output</li> <li> <p>Prevents accidental exposure in error messages</p> </li> <li> <p>Prefer environment variables or secure files</p> </li> <li>Only <code>env</code> and <code>file</code> providers are currently implemented</li> <li> <p>Vault and AWS providers are planned for future releases</p> </li> <li> <p>Use profiles for environment-specific secrets <pre><code>profiles:\n  production:\n    DB_HOST: \"prod-db.example.com\"\n  development:\n    DB_HOST: \"localhost\"\n\nsecrets:\n  DB_PASSWORD:\n    provider: env\n    key: \"DATABASE_PASSWORD\"\n</code></pre></p> </li> <li> <p>Test secret resolution in development</p> </li> <li>Verify secrets load correctly before deploying</li> <li>Use different secret sources per profile</li> </ol>"},{"location":"environment/secrets-management/#troubleshooting","title":"Troubleshooting","text":""},{"location":"environment/secrets-management/#issue-secret-not-found-in-environment","title":"Issue: \"Secret not found in environment\"","text":"<p>Cause: Secret key doesn't exist in environment variables</p> <p>Solution: <pre><code># Verify environment variable exists\necho $SECRET_API_KEY\n\n# Set the variable before running workflow\nexport SECRET_API_KEY=\"your-secret-value\"\nprodigy run workflow.yml\n</code></pre></p>"},{"location":"environment/secrets-management/#issue-failed-to-read-secret-file","title":"Issue: \"Failed to read secret file\"","text":"<p>Cause: Secret file doesn't exist or insufficient permissions</p> <p>Solution: <pre><code># Verify file exists and is readable\nls -la ~/.ssh/deploy_key\n\n# Fix permissions if needed\nchmod 600 ~/.ssh/deploy_key\n</code></pre></p>"},{"location":"environment/secrets-management/#issue-secrets-not-masked-in-output","title":"Issue: Secrets not masked in output","text":"<p>Cause: Secret not defined in <code>secrets:</code> field</p> <p>Solution: Move sensitive variables from <code>env:</code> to <code>secrets:</code> block:</p> <pre><code># Before (NOT masked)\nenv:\n  API_KEY: \"${env:SECRET_API_KEY}\"\n\n# After (masked)\nsecrets:\n  API_KEY: \"${env:SECRET_API_KEY}\"\n</code></pre>"},{"location":"environment/secrets-management/#related-documentation","title":"Related Documentation","text":"<ul> <li>Environment Variables - General environment variable configuration</li> <li>Environment Profiles - Profile-based configuration</li> <li>Environment Precedence - How environment values are resolved</li> <li>Best Practices - Environment configuration best practices</li> </ul>"},{"location":"environment/secrets-management/#implementation-references","title":"Implementation References","text":"<ul> <li>Configuration Types: <code>src/cook/environment/config.rs:84-112</code></li> <li>Secret Resolution: <code>src/cook/environment/manager.rs:313-337</code></li> <li>Secret Store: <code>src/cook/environment/secret_store.rs:26-107</code></li> <li>Environment Setup: <code>src/cook/environment/manager.rs:128-136</code></li> <li>Test Examples: <code>tests/environment_workflow_test.rs:19-59</code></li> <li>Workflow Examples: <code>workflows/environment-example.yml</code>, <code>workflows/mapreduce-env-example.yml</code></li> </ul>"},{"location":"mapreduce/","title":"MapReduce Workflows","text":""},{"location":"mapreduce/#quick-start","title":"Quick Start","text":"<p>Want to get started with MapReduce? Here's a minimal working example:</p> <pre><code>name: my-first-mapreduce\nmode: mapreduce\n\n# Generate work items\nsetup:\n  - shell: \"echo '[{\\\"id\\\": 1, \\\"name\\\": \\\"task-1\\\"}, {\\\"id\\\": 2, \\\"name\\\": \\\"task-2\\\"}]' &gt; items.json\"\n\n# Process items in parallel\nmap:\n  input: \"items.json\"\n  json_path: \"$[*]\"\n  agent_template:\n    - shell: \"echo Processing ${item.name}\"\n  max_parallel: 5\n\n# Aggregate results\nreduce:\n  - shell: \"echo Completed ${map.successful}/${map.total} items\"\n</code></pre> <p>Run it: <pre><code>prodigy run workflow.yml\n</code></pre></p> <p>That's it! Now let's explore the full capabilities.</p>"},{"location":"mapreduce/#complete-structure","title":"Complete Structure","text":"<pre><code>name: parallel-processing\nmode: mapreduce\n\n# Optional setup phase\nsetup:\n  - shell: \"generate-work-items.sh\"\n  - shell: \"debtmap analyze . --output items.json\"\n\n# Map phase: Process items in parallel\nmap:\n  # Input source (JSON file or command)\n  input: \"items.json\"\n\n  # JSONPath expression to extract items\n  json_path: \"$.items[*]\"\n\n  # Agent template (commands run for each item)\n  # Modern syntax: Commands directly under agent_template\n  agent_template:\n    - claude: \"/process '${item}'\"\n    - shell: \"test ${item.path}\"\n      on_failure:\n        claude: \"/fix-issue '${item}'\"\n\n  # DEPRECATED: Nested 'commands' syntax (still supported)\n  # agent_template:\n  #   commands:\n  #     - claude: \"/process '${item}'\"\n\n  # Maximum parallel agents (can use environment variables)\n  max_parallel: 10  # or max_parallel: \"$MAX_WORKERS\"\n\n  # Optional: Filter items\n  filter: \"item.score &gt;= 5\"\n\n  # Optional: Sort items\n  sort_by: \"item.priority DESC\"\n\n  # Optional: Limit number of items\n  max_items: 100\n\n  # Optional: Skip items\n  offset: 10\n\n  # Optional: Deduplicate by field\n  distinct: \"item.id\"\n\n  # Optional: Agent timeout in seconds\n  agent_timeout_secs: 300\n\n  # Optional: Advanced timeout configuration (alternative to agent_timeout_secs)\n  # timeout_config:\n  #   default: \"5m\"\n  #   per_command: \"2m\"\n\n# Reduce phase: Aggregate results\n# Modern syntax: Commands directly under reduce\nreduce:\n  - claude: \"/summarize ${map.results}\"\n  - shell: \"echo 'Processed ${map.successful}/${map.total} items'\"\n\n# DEPRECATED: Nested 'commands' syntax (still supported)\n# reduce:\n#   commands:\n#     - claude: \"/summarize ${map.results}\"\n\n# Optional: Custom merge workflow (supports two formats)\nmerge:\n  # Simple array format\n  - shell: \"git fetch origin\"\n  - claude: \"/merge-worktree ${merge.source_branch}\"\n  - shell: \"cargo test\"\n\n# OR full format with timeout\n# merge:\n#   commands:\n#     - shell: \"git fetch origin\"\n#     - claude: \"/merge-worktree ${merge.source_branch}\"\n#   timeout: 600  # Timeout in seconds\n\n# Error handling policy\nerror_policy:\n  on_item_failure: dlq  # dlq, retry, skip, stop, or custom handler name\n  continue_on_failure: true\n  max_failures: 5\n  failure_threshold: 0.2  # 20% failure rate\n  error_collection: aggregate  # aggregate, immediate, or batched:N\n\n  # Circuit breaker configuration\n  circuit_breaker:\n    failure_threshold: 5      # Open circuit after N failures\n    success_threshold: 2      # Close circuit after N successes\n    timeout: \"60s\"           # Duration before attempting half-open (humantime format: \"1s\", \"1m\", \"5m\")\n    half_open_requests: 3    # Test requests in half-open state\n\n  # Retry configuration with backoff\n  retry_config:\n    max_attempts: 3\n    # BackoffStrategy is an enum - use one variant:\n    backoff:\n      exponential:\n        initial: \"1s\"\n        multiplier: 2.0\n\n# Convenience fields (alternative to nested error_policy)\n# These top-level fields map to error_policy for simpler syntax\non_item_failure: dlq\ncontinue_on_failure: true\nmax_failures: 5\n</code></pre>"},{"location":"mapreduce/#additional-topics","title":"Additional Topics","text":"<p>See also: - Environment Variables in Configuration - Backoff Strategies - Error Collection Strategies - Setup Phase (Advanced) - Global Storage Architecture - Event Tracking - Checkpoint and Resume - Dead Letter Queue (DLQ) - Workflow Format Comparison</p>"},{"location":"mapreduce/backoff-strategies/","title":"Backoff Strategies","text":""},{"location":"mapreduce/backoff-strategies/#backoff-strategies","title":"Backoff Strategies","text":"<p>Backoff strategies control the delay between retry attempts when a MapReduce agent fails. Prodigy supports multiple backoff algorithms to handle different failure patterns and workload characteristics.</p> <p>Source: <code>BackoffStrategy</code> enum in <code>src/cook/retry_v2.rs:73-90</code></p>"},{"location":"mapreduce/backoff-strategies/#overview","title":"Overview","text":"<p>The <code>retry_config.backoff</code> field configures which backoff algorithm to use. All strategies work with the following <code>RetryConfig</code> fields:</p> <ul> <li>initial_delay: Base delay for calculations (default: <code>1s</code>)</li> <li>max_delay: Maximum cap on any calculated delay (default: <code>30s</code>)</li> <li>jitter: Add randomness to prevent thundering herd (default: <code>false</code>)</li> </ul> <p>Default Strategy: If no backoff is specified, Prodigy uses Exponential backoff with base 2.0.</p> <p>Source: <code>src/cook/retry_v2.rs:92-97</code></p>"},{"location":"mapreduce/backoff-strategies/#available-strategies","title":"Available Strategies","text":""},{"location":"mapreduce/backoff-strategies/#fixed-backoff","title":"Fixed Backoff","text":"<p>Uses a constant delay between all retry attempts.</p> <p>Simple form (uses <code>initial_delay</code>): <pre><code>retry_config:\n  backoff: fixed\n  initial_delay: 2s\n</code></pre></p> <p>Explicit form (error_policy implementation): <pre><code>retry_config:\n  backoff:\n    fixed:\n      delay: 5s\n</code></pre></p> <p>Behavior: - Retry 1: 2s delay - Retry 2: 2s delay - Retry 3: 2s delay</p> <p>Use cases: - Fast retries for transient errors - Quick recovery for flaky tests - Network requests with predictable timeouts</p> <p>Source: <code>src/cook/retry_v2.rs:75</code> (retry_v2), <code>src/cook/workflow/error_policy.rs:110</code> (error_policy)</p> <p>Test: <code>src/cook/retry_v2.rs:583-594</code></p>"},{"location":"mapreduce/backoff-strategies/#linear-backoff","title":"Linear Backoff","text":"<p>Increases delay linearly on each retry: <code>initial_delay + (attempt * increment)</code>.</p> <p>retry_v2 form: <pre><code>retry_config:\n  backoff:\n    linear:\n      increment: 2s\n  initial_delay: 1s\n</code></pre></p> <p>error_policy form: <pre><code>retry_config:\n  backoff:\n    linear:\n      initial: 1s\n      increment: 2s\n</code></pre></p> <p>Behavior (retry_v2 example): - Retry 1: 1s (initial_delay) - Retry 2: 3s (1s + 2s) - Retry 3: 5s (1s + 4s)</p> <p>Use cases: - Database connection retries - Resource allocation failures - Predictable backpressure</p> <p>Source: <code>src/cook/retry_v2.rs:77-80</code> (retry_v2), <code>src/cook/workflow/error_policy.rs:112-115</code> (error_policy)</p> <p>Test: <code>src/cook/retry_v2.rs:597-610</code></p>"},{"location":"mapreduce/backoff-strategies/#exponential-backoff-default","title":"Exponential Backoff (Default)","text":"<p>Increases delay exponentially: <code>initial_delay * (base ^ attempt)</code>.</p> <p>retry_v2 form (uses <code>base</code>): <pre><code>retry_config:\n  backoff:\n    exponential:\n      base: 2.0\n  initial_delay: 1s\n  max_delay: 100s\n</code></pre></p> <p>error_policy form (uses <code>multiplier</code>): <pre><code>retry_config:\n  backoff:\n    exponential:\n      initial: 1s\n      multiplier: 2.0\n</code></pre></p> <p>Behavior (base 2.0): - Retry 1: 1s - Retry 2: 2s (1s * 2^1) - Retry 3: 4s (1s * 2^2) - Retry 4: 8s (1s * 2^3) - Retry 5: 16s (capped at max_delay if exceeded)</p> <p>Use cases: - API rate limiting - Network failures - Most general-purpose retry scenarios - Recommended for MapReduce workloads</p> <p>Source: <code>src/cook/retry_v2.rs:82-85</code> (retry_v2), <code>src/cook/workflow/error_policy.rs:117</code> (error_policy)</p> <p>Test: <code>src/cook/retry_v2.rs:613-626</code></p>"},{"location":"mapreduce/backoff-strategies/#fibonacci-backoff","title":"Fibonacci Backoff","text":"<p>Uses the Fibonacci sequence (0, 1, 1, 2, 3, 5, 8, 13...) multiplied by <code>initial_delay</code>.</p> <p>retry_v2 form: <pre><code>retry_config:\n  backoff: fibonacci\n  initial_delay: 1s\n</code></pre></p> <p>error_policy form: <pre><code>retry_config:\n  backoff:\n    fibonacci:\n      initial: 1s\n</code></pre></p> <p>Behavior: - Retry 1: 1s (fib(1) = 1) - Retry 2: 1s (fib(2) = 1) - Retry 3: 2s (fib(3) = 2) - Retry 4: 3s (fib(4) = 3) - Retry 5: 5s (fib(5) = 5) - Retry 6: 8s (fib(6) = 8)</p> <p>Use cases: - Distributed systems (natural backpressure) - Parallel MapReduce jobs (reduces retry storms) - Gradual recovery from cascading failures</p> <p>Implementation: The Fibonacci sequence is calculated using iterative approach to avoid stack overflow for large attempt numbers.</p> <p>Source: <code>src/cook/retry_v2.rs:87</code>, <code>src/cook/retry_v2.rs:424-440</code> (fibonacci function)</p> <p>Test: <code>src/cook/retry_v2.rs:629-642</code></p>"},{"location":"mapreduce/backoff-strategies/#custom-backoff-retry_v2-only","title":"Custom Backoff (retry_v2 only)","text":"<p>Specify an explicit list of delays for each retry attempt.</p> <p>Note: Only available in the retry_v2 implementation, not in error_policy.</p> <pre><code>retry_config:\n  backoff:\n    custom:\n      delays:\n        - 500ms\n        - 1s\n        - 2s\n        - 5s\n        - 10s\n</code></pre> <p>Behavior: - Retry 1: 500ms (delays[0]) - Retry 2: 1s (delays[1]) - Retry 3: 2s (delays[2]) - Retry 4: 5s (delays[3]) - Retry 5: 10s (delays[4]) - Retry 6+: max_delay (if attempt exceeds array length)</p> <p>Use cases: - Fine-tuned retry patterns - Known failure recovery timelines - Testing specific delay sequences</p> <p>Edge cases: - Empty <code>delays</code> array: Uses <code>max_delay</code> for all attempts (fallback behavior) - Attempt exceeds array length: Uses <code>max_delay</code> (fallback behavior)</p> <p>This ensures predictable behavior when custom delays are exhausted or misconfigured, preventing undefined delay calculations.</p> <p>Source: <code>src/cook/retry_v2.rs:88-89</code>, <code>src/cook/retry_v2.rs:298-301</code> (calculation logic)</p>"},{"location":"mapreduce/backoff-strategies/#mapreduce-integration","title":"MapReduce Integration","text":"<p>Backoff strategies are configured in the <code>retry_config</code> block, which can be part of: 1. WorkflowErrorPolicy at workflow level (applies to all items) 2. TimeoutConfig in map phase (more specific control)</p> <p>Complete MapReduce Example:</p> <pre><code>name: resilient-mapreduce-job\nmode: mapreduce\n\nmap:\n  input: \"items.json\"\n  json_path: \"$.items[*]\"\n\n  # Workflow-level error policy\n  error_policy:\n    retry_config:\n      max_attempts: 5\n      backoff:\n        fibonacci:\n          initial: 1s\n      jitter: true\n      jitter_factor: 0.3\n    continue_on_failure: true\n\n  agent_template:\n    - claude: \"/process-item '${item.id}'\"\n    - shell: \"validate ${item.output}\"\n      on_failure:\n        claude: \"/fix-validation-error\"\n\n  max_parallel: 10\n</code></pre> <p>Source: <code>src/cook/workflow/error_policy.rs:160</code> (retry_config field in WorkflowErrorPolicy)</p>"},{"location":"mapreduce/backoff-strategies/#retryconfig-fields","title":"RetryConfig Fields","text":"<p>The <code>RetryConfig</code> structure controls all retry behavior:</p> <pre><code>retry_config:\n  attempts: 3              # Maximum retry attempts (default: 3)\n  backoff: exponential     # Strategy (default: exponential with base 2.0)\n  initial_delay: 1s        # Base delay (default: 1s)\n  max_delay: 30s           # Delay cap (default: 30s)\n  jitter: true             # Add randomness (default: false)\n  jitter_factor: 0.3       # Jitter amount 0.0-1.0 (default: 0.3)\n</code></pre> <p>How fields interact: 1. Backoff strategy calculates base delay using <code>initial_delay</code> 2. Calculated delay is capped by <code>max_delay</code> 3. If <code>jitter: true</code>, random offset is added: <code>delay \u00b1 (delay * jitter_factor)</code> 4. Final delay is applied before next retry attempt</p> <p>Source: <code>src/cook/retry_v2.rs:16-52</code></p> <p>Defaults: <code>src/cook/retry_v2.rs:443-461</code></p>"},{"location":"mapreduce/backoff-strategies/#implementation-variants","title":"Implementation Variants","text":"<p>Prodigy has two BackoffStrategy implementations:</p>"},{"location":"mapreduce/backoff-strategies/#retry_v2rs-recommended","title":"retry_v2.rs (Recommended)","text":"<ul> <li>Location: <code>src/cook/retry_v2.rs:73-90</code></li> <li>Features: Includes <code>Custom</code> backoff strategy</li> <li>Exponential parameter: <code>base</code> (default 2.0)</li> <li>Simple syntax: Most strategies use <code>backoff: strategy_name</code> form</li> <li>More comprehensive: Full RetryConfig with jitter, retry_budget, error matchers</li> </ul>"},{"location":"mapreduce/backoff-strategies/#error_policyrs-legacy","title":"error_policy.rs (Legacy)","text":"<ul> <li>Location: <code>src/cook/workflow/error_policy.rs:108-120</code></li> <li>Features: No <code>Custom</code> strategy</li> <li>Exponential parameter: <code>multiplier</code> instead of <code>base</code></li> <li>Explicit syntax: All strategies require nested configuration with <code>initial</code> delay</li> <li>Simpler: Minimal RetryConfig with max_attempts and backoff only</li> </ul> <p>Which to use: Both are valid, but retry_v2 provides more features and better defaults. In MapReduce workflows, the error_policy implementation is used at the workflow level.</p>"},{"location":"mapreduce/backoff-strategies/#best-practices","title":"Best Practices","text":"<p>Choosing a Strategy:</p> Scenario Recommended Strategy Reasoning Parallel MapReduce jobs Fibonacci Reduces retry storms, natural backpressure API rate limiting Exponential Quickly backs off from rate limits Database retries Linear Predictable recovery for connection pools Fast transient errors Fixed Quick recovery without overloading Custom requirements Custom (retry_v2) Fine-grained control for known patterns <p>Jitter Usage: - Enable jitter for MapReduce workloads to prevent thundering herd - Disable jitter for testing and debugging (predictable delays)</p> <p>Delay Configuration: - Set <code>initial_delay</code> based on typical failure recovery time - Set <code>max_delay</code> to prevent excessively long waits - For MapReduce: Consider agent timeout interaction (delays count toward timeout)</p> <p>Timeout Interaction: Backoff delays count toward the agent's overall timeout. If you configure: - Agent timeout: 300s (5 minutes) - Max retries: 5 - Fibonacci backoff with initial_delay: 10s</p> <p>Total delay could be: 10s + 10s + 20s + 30s + 50s = 120s of retries, leaving 180s for actual work.</p>"},{"location":"mapreduce/backoff-strategies/#troubleshooting","title":"Troubleshooting","text":"<p>Delays too long: - Reduce <code>initial_delay</code> or <code>base</code>/<code>multiplier</code> - Lower <code>max_delay</code> cap - Consider switching from Exponential to Linear or Fibonacci</p> <p>Delays too short: - Increase <code>initial_delay</code> - Increase <code>base</code>/<code>multiplier</code> for Exponential - Switch from Fixed to Linear or Exponential</p> <p>Max delay being ignored: - Verify <code>max_delay</code> is set correctly in RetryConfig - Check that backoff calculation doesn't skip max_delay check - All strategies respect max_delay cap (see source for verification)</p> <p>Custom delays edge cases: - Empty <code>delays</code> array: Defaults to <code>max_delay</code> for all retries - Attempt &gt; array length: Uses <code>max_delay</code> (not initial_delay) - Invalid duration format: Fails YAML parsing (use humantime format: <code>1s</code>, <code>500ms</code>, etc.)</p> <p>Humantime format errors: Valid formats: <code>1s</code>, <code>500ms</code>, <code>2m</code>, <code>1h30m</code>, <code>100ms</code> Invalid formats: <code>1 second</code>, <code>500</code>, <code>2 minutes</code></p>"},{"location":"mapreduce/backoff-strategies/#see-also","title":"See Also","text":"<ul> <li>Basic Retry Configuration - Full RetryConfig documentation</li> <li>Dead Letter Queue (DLQ) - What happens when retries are exhausted</li> <li>Checkpoint and Resume - How retry state is preserved for resume</li> <li>Timeout Configuration - Interaction between backoff and timeouts</li> <li>Error Collection Strategies - Overall error handling in MapReduce</li> </ul>"},{"location":"mapreduce/checkpoint-and-resume/","title":"Checkpoint and Resume","text":""},{"location":"mapreduce/checkpoint-and-resume/#checkpoint-and-resume","title":"Checkpoint and Resume","text":"<p>Prodigy provides comprehensive checkpoint and resume capabilities for MapReduce workflows, ensuring work can be recovered from any point of failure. Checkpoints are automatically created during workflow execution, preserving all state needed to continue from where you left off. This enables resilient workflows that can survive interruptions, crashes, or planned pauses without losing progress.</p>"},{"location":"mapreduce/checkpoint-and-resume/#checkpoint-behavior","title":"Checkpoint Behavior","text":"<p>Checkpoints are automatically created at strategic points during workflow execution:</p> <pre><code>flowchart TD\n    Start[Start MapReduce Workflow] --&gt; Setup[Setup Phase Executes]\n    Setup --&gt; SetupCheck{Setup&lt;br/&gt;Complete?}\n    SetupCheck --&gt;|Success| SaveSetup[Save Setup Checkpoint&lt;br/&gt;setup-checkpoint.json]\n    SetupCheck --&gt;|Failure| Failed[Workflow Failed]\n\n    SaveSetup --&gt; MapStart[Map Phase Begins]\n    MapStart --&gt; ProcessItems[Process Work Items]\n    ProcessItems --&gt; MapCheck{Checkpoint&lt;br/&gt;Trigger?}\n\n    MapCheck --&gt;|100 items OR&lt;br/&gt;5 minutes| SaveMap[Save Map Checkpoint&lt;br/&gt;map-checkpoint-timestamp.json]\n    MapCheck --&gt;|Continue| ProcessItems\n\n    ProcessItems --&gt; MapDone{All Items&lt;br/&gt;Processed?}\n    MapDone --&gt;|No| MapCheck\n    MapDone --&gt;|Yes| ReduceStart[Reduce Phase Begins]\n\n    SaveMap --&gt; ProcessItems\n\n    ReduceStart --&gt; ReduceCmd[Execute Reduce Command]\n    ReduceCmd --&gt; SaveReduce[Save Reduce Checkpoint&lt;br/&gt;reduce-checkpoint-v1-timestamp.json]\n    SaveReduce --&gt; ReduceNext{More&lt;br/&gt;Commands?}\n\n    ReduceNext --&gt;|Yes| ReduceCmd\n    ReduceNext --&gt;|No| Complete[Workflow Complete]\n\n    style SaveSetup fill:#e1f5ff\n    style SaveMap fill:#fff3e0\n    style SaveReduce fill:#f3e5f5\n    style Failed fill:#ffebee\n    style Complete fill:#e8f5e9</code></pre> <p>Figure: Checkpoint creation flow across MapReduce workflow phases.</p> <p>Setup Phase Checkpointing: - Checkpoint created after successful setup completion - Preserves setup output, generated artifacts, and environment state - Stored as <code>setup-checkpoint.json</code> - Resume restarts setup from beginning (idempotent operations recommended)</p> <p>Map Phase Checkpointing: - Checkpoints created after processing configurable number of work items - Tracks completed, in-progress, and pending work items - Stores agent results and failure details for recovery - Resume continues from last successful checkpoint - In-progress items are moved back to pending on resume - Stored as <code>map-checkpoint-{timestamp}.json</code></p> <p>Reduce Phase Checkpointing: - Checkpoint created after each reduce command execution - Tracks completed steps, step results, variables, and map results - Enables resume from any point in reduce phase execution - Resume continues from last completed step - Stored as <code>reduce-checkpoint-v1-{timestamp}.json</code></p>"},{"location":"mapreduce/checkpoint-and-resume/#checkpoint-interval-configuration","title":"Checkpoint Interval Configuration","text":"<p>Prodigy controls when checkpoints are created through configurable intervals. The checkpoint strategy differs between workflow types:</p> <p>Standard Workflow Checkpoints (src/cook/workflow/checkpoint.rs:20): - Default interval: 60 seconds between checkpoints - Configurable: Use <code>.with_interval(Duration)</code> in checkpoint manager builder - Decision logic: Compares elapsed time since last checkpoint</p> <p>MapReduce Checkpoints (src/cook/execution/mapreduce/checkpoint/types.rs:242-264): - Default <code>interval_items</code>: 100 work items per checkpoint - Default <code>interval_duration</code>: 300 seconds (5 minutes) per checkpoint - Dual triggers: Checkpoint created when either item count OR duration threshold is reached - Retention policy:   - <code>max_checkpoints</code>: Keep 10 most recent checkpoints   - <code>max_age</code>: Retain checkpoints for 7 days (604,800 seconds)   - <code>keep_final</code>: Always preserve final checkpoint</p> <p>Example MapReduce checkpoint configuration:</p> <pre><code>name: my-workflow\nmode: mapreduce\n\ncheckpoint:\n  interval_items: 50      # (1)!\n  interval_duration: 600  # (2)!\n  max_checkpoints: 15     # (3)!\n  max_age: 1209600        # (4)!\n</code></pre> <ol> <li>Item-based trigger: Create checkpoint every 50 processed items (default: 100)</li> <li>Time-based trigger: Create checkpoint every 10 minutes / 600 seconds (default: 300s / 5 minutes)</li> <li>Retention count: Keep 15 most recent checkpoints (default: 10)</li> <li>Retention duration: Keep checkpoints for 14 days / 1,209,600 seconds (default: 604,800s / 7 days)</li> </ol> <p>Checkpoint Strategy</p> <p>These intervals balance checkpoint overhead against recovery granularity:</p> <ul> <li>More frequent checkpoints (lower values): Finer-grained resume, but higher I/O overhead</li> <li>Less frequent checkpoints (higher values): Lower overhead, but more work to re-process on resume</li> <li>Sweet spot: Checkpoint every 50-100 items or 5-10 minutes for most workflows</li> </ul>"},{"location":"mapreduce/checkpoint-and-resume/#resume-commands","title":"Resume Commands","text":"<p>MapReduce jobs can be resumed using either session IDs or job IDs:</p> <pre><code># Resume using session ID\nprodigy resume session-mapreduce-1234567890\n\n# Resume using job ID\nprodigy resume-job mapreduce-1234567890\n\n# Unified resume command (auto-detects ID type)\nprodigy resume mapreduce-1234567890\n</code></pre> <p>Session-Job Mapping:</p> <p>The <code>SessionJobMapping</code> structure provides bidirectional mapping between session and job identifiers (src/storage/session_job_mapping.rs:14-26):</p> <ul> <li>Storage location: <code>~/.prodigy/state/{repo_name}/mappings/</code></li> <li>Mapping fields:</li> <li><code>session_id</code>: Unique identifier for the workflow session</li> <li><code>job_id</code>: MapReduce job identifier</li> <li><code>workflow_name</code>: Name of the workflow for easier identification</li> <li><code>created_at</code>: Timestamp when the mapping was created</li> <li>Created: Automatically when MapReduce workflow starts</li> <li>Purpose: Enables resume with either session ID or job ID</li> </ul> <p>Flexible Resume IDs</p> <p>This bidirectional mapping allows you to resume a workflow using whichever identifier is more convenient:</p> <ul> <li>Use session ID if you have the session information</li> <li>Use job ID if you're tracking MapReduce job execution</li> <li>The unified <code>prodigy resume</code> command auto-detects which ID type you provided</li> </ul>"},{"location":"mapreduce/checkpoint-and-resume/#state-preservation","title":"State Preservation","text":"<p>All critical state is preserved across resume operations:</p> <p>Variables and Context: - Workflow variables preserved across resume - Captured outputs from setup and reduce phases - Environment variables maintained - Map results available to reduce phase after resume</p> <p>Work Item State: - Completed items: Preserved with full results - In-progress items: Moved back to pending on resume - Failed items: Tracked with retry counts and error details - Pending items: Continue processing from where left off</p> <p>Agent State: - Active agent information preserved - Resource allocation tracked - Worktree paths recorded for cleanup</p>"},{"location":"mapreduce/checkpoint-and-resume/#resume-strategies","title":"Resume Strategies","text":"<p>Based on checkpoint state and phase, different resume strategies apply:</p> <ul> <li>Setup Phase: Restart setup from beginning (idempotent operations recommended)</li> <li>Map Phase: Continue from last checkpoint, re-process in-progress items</li> <li>Reduce Phase: Continue from last completed step</li> <li>Validate and Continue: Verify checkpoint integrity before resuming</li> </ul> <p>Setup Phase Idempotency</p> <p>Setup phase commands are re-executed from the beginning on resume. Design setup commands to be idempotent (safe to run multiple times):</p> <p>\u2705 Good practices:</p> <ul> <li>Use <code>mkdir -p</code> instead of <code>mkdir</code> (won't fail if directory exists)</li> <li>Check for file existence before downloading</li> <li>Use atomic file operations (write to temp, then move)</li> </ul> <p>\u274c Avoid:</p> <ul> <li>Commands that fail if run twice (e.g., <code>mkdir</code> without <code>-p</code>)</li> <li>Appending to files without checking for duplicates</li> <li>Side effects that can't be safely repeated</li> </ul>"},{"location":"mapreduce/checkpoint-and-resume/#storage-structure","title":"Storage Structure","text":"<p>Checkpoints are stored in a structured directory hierarchy:</p> <pre><code>~/.prodigy/state/{repo_name}/mapreduce/jobs/{job_id}/\n\u251c\u2500\u2500 setup-checkpoint.json           # Setup phase results\n\u251c\u2500\u2500 map-checkpoint-{timestamp}.json  # Map phase progress\n\u251c\u2500\u2500 reduce-checkpoint-v1-{timestamp}.json  # Reduce phase progress\n\u2514\u2500\u2500 job-state.json                  # Overall job state\n</code></pre>"},{"location":"mapreduce/checkpoint-and-resume/#checkpoint-file-structure","title":"Checkpoint File Structure","text":"<p>Checkpoint files contain JSON-serialized state for recovery. Here's what each checkpoint type stores:</p> <p>MapReduce Checkpoint (src/cook/execution/mapreduce/checkpoint/types.rs:48-64):</p> <pre><code>{\n  \"metadata\": {\n    \"checkpoint_id\": \"ckpt-1704556800\",\n    \"version\": 1,\n    \"phase\": \"Map\",\n    \"created_at\": \"2025-01-11T12:00:00Z\",\n    \"items_processed\": 150,\n    \"items_total\": 500\n  },\n  \"work_items\": {\n    \"pending\": [\"item-151\", \"item-152\", \"...\"],\n    \"in_progress\": [],\n    \"completed\": [\"item-1\", \"item-2\", \"...\"],\n    \"failed\": []\n  },\n  \"agent_state\": {\n    \"active_agents\": [],\n    \"agent_assignments\": {},\n    \"agent_results\": {\n      \"item-1\": {\"status\": \"success\", \"commits\": [\"abc123\"]},\n      \"item-2\": {\"status\": \"success\", \"commits\": [\"def456\"]}\n    },\n    \"resource_allocation\": {\n      \"max_parallel\": 10,\n      \"current_agents\": 0\n    }\n  },\n  \"variables\": {\n    \"workflow_vars\": {\"PROJECT_NAME\": \"prodigy\"},\n    \"captured_vars\": {},\n    \"environment_vars\": {},\n    \"item_vars\": {}\n  },\n  \"error_state\": {\n    \"error_count\": 0,\n    \"dlq_items\": [],\n    \"error_threshold_reached\": false\n  },\n  \"reason\": \"Interval\",\n  \"checksum\": \"sha256:abc123...\"\n}\n</code></pre> <p>Workflow Checkpoint (src/cook/workflow/checkpoint.rs:26-57):</p> <pre><code>{\n  \"workflow_id\": \"workflow-1704556800\",\n  \"version\": 1,\n  \"execution_state\": {\n    \"current_step_index\": 3,\n    \"status\": \"Running\",\n    \"started_at\": \"2025-01-11T12:00:00Z\",\n    \"updated_at\": \"2025-01-11T12:05:00Z\"\n  },\n  \"completed_steps\": [\n    {\n      \"step_index\": 0,\n      \"command\": \"shell: cargo build\",\n      \"status\": \"Success\",\n      \"duration\": {\"secs\": 45, \"nanos\": 0},\n      \"completed_at\": \"2025-01-11T12:01:00Z\"\n    }\n  ],\n  \"variable_state\": {\n    \"BUILD_OUTPUT\": \"/target/release/prodigy\",\n    \"VERSION\": \"1.0.0\"\n  },\n  \"workflow_hash\": \"sha256:def789...\"\n}\n</code></pre> <p>Key Fields: - metadata/execution_state: Current progress and timestamps - work_items: Work item status tracking (MapReduce only) - agent_state: Agent results and resource allocation (MapReduce only) - variables/variable_state: Preserved workflow variables for resume - completed_steps: Audit trail of successfully completed steps - checksum/workflow_hash: Integrity verification</p> <p>These structures enable Prodigy to reconstruct exact execution state during resume operations.</p>"},{"location":"mapreduce/checkpoint-and-resume/#concurrent-resume-protection","title":"Concurrent Resume Protection","text":"<p>Prodigy prevents multiple resume processes from running on the same session/job simultaneously using an RAII-based locking mechanism:</p> <pre><code>stateDiagram-v2\n    [*] --&gt; Unlocked: No active resume\n    Unlocked --&gt; Acquiring: Resume command starts\n\n    Acquiring --&gt; Locked: Lock acquired successfully\n    Acquiring --&gt; Blocked: Lock held by another process\n\n    Blocked --&gt; StaleCheck: Check if process is running\n    StaleCheck --&gt; Unlocked: Process dead, clean stale lock\n    StaleCheck --&gt; Blocked: Process alive, wait or error\n\n    Locked --&gt; Processing: Resume executing\n    Processing --&gt; Locked: Checkpoint updates\n\n    Processing --&gt; Releasing: Resume completes/fails\n    Releasing --&gt; Unlocked: Lock released (RAII)\n\n    note right of Locked\n        Lock file contains:\n        - PID\n        - Hostname\n        - Timestamp\n        - Job ID\n    end note\n\n    note right of StaleCheck\n        Platform-specific check:\n        - Unix: kill -0 PID\n        - Windows: tasklist\n    end note\n\n    note right of Releasing\n        RAII ensures release\n        even on panic/error\n    end note\n\n    style Locked fill:#fff3e0\n    style Blocked fill:#ffebee\n    style Unlocked fill:#e8f5e9\n    style StaleCheck fill:#e1f5ff</code></pre> <p>Figure: Resume lock lifecycle showing acquisition, stale detection, and automatic cleanup.</p> <p>Lock Behavior: - Resume automatically acquires exclusive lock before starting - Lock creation is atomic - fails if another process holds the lock - Lock automatically released when resume completes or fails (RAII pattern) - Stale locks (from crashed processes) are automatically detected and cleaned up</p> <p>Lock Metadata: Lock files contain: - Process ID (PID) of the holding process - Hostname where the process is running - Timestamp when lock was acquired - Job/session ID being locked</p> <p>Lock Storage: <pre><code>~/.prodigy/resume_locks/\n\u251c\u2500\u2500 session-abc123.lock\n\u251c\u2500\u2500 mapreduce-xyz789.lock\n\u2514\u2500\u2500 ...\n</code></pre></p> <p>Error Messages: If a resume is blocked by an active lock:</p> <pre><code>$ prodigy resume &lt;job_id&gt;\nError: Resume already in progress for job &lt;job_id&gt;\nLock held by: PID 12345 on hostname (acquired 2025-01-11 10:30:00 UTC)\nPlease wait for the other process to complete, or use --force to override.\n</code></pre> <p>Stale Lock Detection: - Platform-specific process existence check (Unix: <code>kill -0</code>, Windows: <code>tasklist</code>) - If holding process is no longer running, lock is automatically removed - New resume attempt succeeds after stale lock cleanup</p> <p>Safety Guarantees: - Data Corruption Prevention: Only one process can modify job state at a time - No Duplicate Work: Work items cannot be processed by multiple agents concurrently - Consistent State: Checkpoint updates are serialized - Automatic Cleanup: RAII pattern ensures locks are released even on errors - Cross-Host Safety: Hostname in lock prevents conflicts across machines</p>"},{"location":"mapreduce/checkpoint-and-resume/#example-resume-workflow","title":"Example Resume Workflow","text":"<p>Here's a typical workflow for resuming an interrupted MapReduce job:</p> <p>Typical Resume Scenario</p> <p>Scenario: Your laptop battery died during a MapReduce workflow's reduce phase.</p> <p>Recovery steps:</p> <ol> <li>Workflow interrupted during reduce phase (laptop battery died, terminal killed)</li> <li>Find job ID with <code>prodigy sessions list</code> or <code>prodigy resume-job list</code>:    <pre><code>$ prodigy sessions list\nsession-mapreduce-1234567890  |  analyze-codebase  |  Paused  |  2025-01-11 12:00\n</code></pre></li> <li>Resume execution using either ID:    <pre><code>prodigy resume session-mapreduce-1234567890\n# or\nprodigy resume mapreduce-1234567890\n</code></pre></li> <li>Prodigy loads checkpoint from <code>~/.prodigy/state/{repo_name}/mapreduce/jobs/{job_id}/</code></li> <li>Reconstructs execution state with all variables, work items, and progress</li> <li>Continues from last completed step in reduce phase (skips already-completed reduce commands)</li> </ol>"},{"location":"mapreduce/checkpoint-and-resume/#best-practices","title":"Best Practices","text":"<p>Designing Resumable Workflows:</p> <p>Idempotent Setup Commands</p> <p>Make setup commands idempotent (safe to run multiple times):</p> <ul> <li>Use <code>|| true</code> for commands that may fail on re-run</li> <li>Check for existing files/directories before creating</li> <li>Use atomic operations (write to temp file, then rename)</li> <li>Avoid appending to files without deduplication</li> </ul> <p>Testing Resume Behavior</p> <p>Validate your workflow handles interruptions correctly:</p> <ol> <li>Start workflow with small <code>max_items</code> for testing</li> <li>Interrupt with <code>Ctrl+C</code> during different phases (setup, map, reduce)</li> <li>Verify resume continues from correct checkpoint</li> <li>Check that work items aren't duplicated or skipped</li> </ol> <ul> <li>Avoid side effects that can't be safely repeated</li> <li>Use descriptive work item IDs for easier debugging (helps identify what was processing during failure)</li> </ul> <p>Troubleshooting Resume Issues: - Check checkpoint files exist: <code>ls ~/.prodigy/state/{repo_name}/mapreduce/jobs/{job_id}/</code> - List available sessions: <code>prodigy sessions list</code> (shows session IDs, job IDs, and status) - List available checkpoints: <code>prodigy checkpoints list</code> - Show detailed checkpoint info: <code>prodigy checkpoints show &lt;job_id&gt;</code> - Validate checkpoint integrity: <code>prodigy checkpoints validate &lt;checkpoint_id&gt;</code> - Review event logs: <code>prodigy events &lt;job_id&gt;</code> - Check for stale locks: <code>ls ~/.prodigy/resume_locks/</code> - Clean old checkpoints: <code>prodigy checkpoints clean --all</code></p>"},{"location":"mapreduce/checkpoint-and-resume/#troubleshooting-stale-locks","title":"Troubleshooting Stale Locks","text":"<p>Resume locks can occasionally become \"stale\" if a resume process crashes or is forcefully terminated before releasing its lock. Prodigy includes automatic detection and cleanup, but here's how to troubleshoot persistent lock issues:</p> <p>Identifying Stale Locks:</p> <p>When a resume is blocked by a lock, you'll see this error message:</p> <pre><code>$ prodigy resume mapreduce-1234567890\nError: Resume already in progress for job mapreduce-1234567890\nLock held by: PID 12345 on hostname (acquired 2025-01-11 10:30:00 UTC)\nPlease wait for the other process to complete, or use --force to override.\n</code></pre> <p>Automatic Cleanup (src/cook/execution/resume_lock.rs:96-120):</p> <p>Prodigy automatically detects stale locks using platform-specific process existence checks:</p> <ul> <li>Unix/Linux/macOS: Uses <code>kill -0 &lt;PID&gt;</code> to check if process is running</li> <li>Windows: Uses <code>tasklist</code> to verify process existence</li> <li>Cross-host detection: Compares hostname in lock file to current system</li> </ul> <p>If the holding process is no longer running, the lock is automatically removed and resume proceeds.</p> <p>Manual Verification Steps:</p> <ol> <li> <p>Check if the process is actually running:    <pre><code># Extract PID from error message (e.g., 12345)\nps aux | grep 12345\n</code></pre></p> </li> <li> <p>Examine the lock file:    <pre><code>cat ~/.prodigy/resume_locks/mapreduce-1234567890.lock\n</code></pre></p> </li> </ol> <p>Lock file contains:    <pre><code>{\n  \"job_id\": \"mapreduce-1234567890\",\n  \"process_id\": 12345,\n  \"hostname\": \"my-laptop\",\n  \"acquired_at\": \"2025-01-11T10:30:00Z\"\n}\n</code></pre></p> <ol> <li> <p>Verify process on the same host:    <pre><code># If hostname matches your current system\nps -p 12345\n\n# If process doesn't exist, the lock is stale\n# Retry resume - it will auto-clean the stale lock\n</code></pre></p> </li> <li> <p>Cross-host lock scenario:</p> </li> <li>If <code>hostname</code> in lock file differs from your current system, the process may be running elsewhere</li> <li>Verify the other system is actually running the resume</li> <li>If other system crashed or is offline, the lock is stale</li> </ol> <p>Manual Lock Removal (last resort):</p> <p>If automatic cleanup fails (rare), you can manually remove the lock:</p> <pre><code># \u26a0\ufe0f Only do this if you're certain the process is dead!\nrm ~/.prodigy/resume_locks/mapreduce-1234567890.lock\n\n# Then retry resume\nprodigy resume mapreduce-1234567890\n</code></pre> <p>Prevention Tips:</p> <ul> <li>Always let resume processes complete naturally (don't use <code>kill -9</code>)</li> <li>Use <code>Ctrl+C</code> for graceful interruption (triggers RAII cleanup)</li> <li>The RAII pattern ensures locks are released even on panic or early exit</li> <li>Stale locks are rare in normal operation</li> </ul> <p>When Automatic Cleanup Fails:</p> <p>Test coverage (tests/concurrent_resume_test.rs:77-105) validates stale lock detection. If you encounter persistent stale locks:</p> <ol> <li>Verify the lock file has valid JSON structure</li> <li>Check file permissions on <code>~/.prodigy/resume_locks/</code></li> <li>Confirm platform-specific process check is working (<code>kill -0</code> on Unix, <code>tasklist</code> on Windows)</li> <li>Report issue with lock file contents and platform details</li> </ol> <p>Available checkpoint commands (src/cli/args.rs:363-413):</p> <p>Checkpoint Management Commands</p> <ul> <li><code>prodigy checkpoints list</code> - List all available checkpoints</li> <li><code>prodigy checkpoints show &lt;job_id&gt;</code> - Show detailed checkpoint information</li> <li><code>prodigy checkpoints validate &lt;checkpoint_id&gt;</code> - Verify checkpoint integrity</li> <li><code>prodigy checkpoints clean</code> - Delete checkpoints for completed workflows</li> </ul> <p>Use these commands to inspect checkpoint state, verify integrity before resume, and clean up old checkpoints to free disk space.</p>"},{"location":"mapreduce/checkpoint-and-resume/#see-also","title":"See Also","text":"<ul> <li>Dead Letter Queue (DLQ) - Managing failed work items during resume operations</li> <li>Event Tracking - Understanding events logged during checkpoint creation and resume</li> <li>Global Storage Architecture - Storage locations for checkpoints and state files</li> </ul>"},{"location":"mapreduce/dead-letter-queue-dlq/","title":"Dead Letter Queue (DLQ)","text":""},{"location":"mapreduce/dead-letter-queue-dlq/#dead-letter-queue-dlq","title":"Dead Letter Queue (DLQ)","text":"<p>The Dead Letter Queue (DLQ) captures persistently failing work items for analysis and retry while allowing MapReduce jobs to continue processing other items. Instead of blocking the entire workflow when individual items fail, the DLQ provides fault tolerance and enables debugging of failure patterns.</p>"},{"location":"mapreduce/dead-letter-queue-dlq/#overview","title":"Overview","text":"<p>When a map agent fails to process a work item after exhausting retry attempts, the item is automatically sent to the DLQ. This allows the MapReduce job to complete successfully while preserving all failure information for later investigation and reprocessing.</p> <p>The DLQ integrates with MapReduce through the <code>on_item_failure</code> policy, which defaults to <code>dlq</code> for MapReduce workflows. Alternative policies include <code>retry</code> (immediate retry), <code>skip</code> (ignore failures), <code>stop</code> (halt workflow), and <code>custom</code> (user-defined handling).</p> <pre><code>flowchart TD\n    Start[Work Item Processing] --&gt; Execute[Execute Agent Command]\n    Execute --&gt; Success{Success?}\n    Success --&gt;|Yes| Next[Process Next Item]\n    Success --&gt;|No| Retry{Retry&lt;br/&gt;Attempts&lt;br/&gt;Left?}\n\n    Retry --&gt;|Yes| Backoff[Exponential Backoff]\n    Backoff --&gt; Execute\n\n    Retry --&gt;|No| Policy{on_item_failure&lt;br/&gt;Policy}\n\n    Policy --&gt;|dlq| CreateDLQ[Create DeadLetteredItem]\n    Policy --&gt;|stop| Halt[Halt Workflow]\n    Policy --&gt;|skip| Skip[Mark Skipped]\n    Policy --&gt;|custom| Custom[Run Custom Handler]\n\n    CreateDLQ --&gt; SaveDLQ[Save to DLQ Storage]\n    SaveDLQ --&gt; Next\n    Skip --&gt; Next\n    Custom --&gt; Next\n\n    Halt --&gt; End[Workflow Failed]\n    Next --&gt; End\n\n    style Success fill:#e8f5e9\n    style Policy fill:#fff3e0\n    style CreateDLQ fill:#e1f5ff\n    style SaveDLQ fill:#e1f5ff\n    style Halt fill:#ffebee</code></pre> <p>Figure: DLQ failure flow showing retry logic and policy-based handling.</p>"},{"location":"mapreduce/dead-letter-queue-dlq/#storage-structure","title":"Storage Structure","text":"<p>DLQ data is stored in the global Prodigy directory using this structure:</p> <pre><code>~/.prodigy/dlq/{repo_name}/{job_id}/mapreduce/dlq/{job_id}/\n\u251c\u2500\u2500 items/                  # Individual item files\n\u2502   \u251c\u2500\u2500 item-123.json      # DeadLetteredItem for item-123\n\u2502   \u251c\u2500\u2500 item-456.json      # DeadLetteredItem for item-456\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 index.json             # DLQ index metadata\n</code></pre> <p>For example: <pre><code>~/.prodigy/dlq/prodigy/mapreduce-1234567890/mapreduce/dlq/mapreduce-1234567890/\n\u251c\u2500\u2500 items/\n\u2502   \u2514\u2500\u2500 item-123.json\n\u2514\u2500\u2500 index.json\n</code></pre></p> <p>Storage Implementation: - <code>GlobalStorage</code> provides the base path: <code>~/.prodigy/dlq/{repo_name}/{job_id}</code> (src/storage/global.rs:47) - <code>DLQStorage</code> adds subdirectories: <code>mapreduce/dlq/{job_id}</code> (src/cook/execution/dlq.rs:529)   - Note: The redundant <code>mapreduce/dlq/{job_id}</code> subdirectory structure is maintained for backward compatibility with earlier DLQ implementations. This ensures existing DLQ data remains accessible after upgrades. - Each failed item is stored as a separate JSON file in the <code>items/</code> directory (src/cook/execution/dlq.rs:536) - File naming: <code>{item_id}.json</code> (e.g., <code>item-123.json</code>) - <code>index.json</code> maintains a list of all item IDs and metadata for fast lookups (src/cook/execution/dlq.rs:608-637) - Items are loaded into memory on-demand for operations</p> <p>This global storage architecture enables: - Cross-worktree access: Multiple worktrees working on the same job share DLQ data - Persistent state: DLQ survives worktree cleanup - Centralized monitoring: All failures accessible from a single location - Scalability: Individual item files prevent loading entire DLQ into memory</p>"},{"location":"mapreduce/dead-letter-queue-dlq/#dlq-item-structure","title":"DLQ Item Structure","text":"<p>Each failed item in the DLQ is stored as a <code>DeadLetteredItem</code> with comprehensive failure information:</p> <pre><code>{\n  \"item_id\": \"item-123\",                    // (1)!\n  \"item_data\": { \"file\": \"src/main.rs\", \"priority\": 5 },  // (2)!\n  \"first_attempt\": \"2025-01-11T10:25:00Z\",  // (3)!\n  \"last_attempt\": \"2025-01-11T10:30:00Z\",\n  \"failure_count\": 3,                       // (4)!\n  \"failure_history\": [                      // (5)!\n    {\n      \"attempt_number\": 1,\n      \"timestamp\": \"2025-01-11T10:30:00Z\",\n      \"error_type\": { \"CommandFailed\": { \"exit_code\": 101 } },\n      \"error_message\": \"cargo test failed with exit code 101\",\n      \"stack_trace\": \"thread 'main' panicked at src/main.rs:42...\",\n      \"agent_id\": \"agent-1\",\n      \"step_failed\": \"shell: cargo test\",\n      \"duration_ms\": 45000,\n      \"json_log_location\": \"/Users/user/.local/state/claude/logs/session-abc123.json\"\n    }\n  ],\n  \"error_signature\": \"CommandFailed::cargo test failed with exit\",  // (6)!\n  \"reprocess_eligible\": true,               // (7)!\n  \"manual_review_required\": false,\n  \"worktree_artifacts\": {                   // (8)!\n    \"worktree_path\": \"/Users/user/.prodigy/worktrees/prodigy/agent-1\",\n    \"branch_name\": \"agent-1\",\n    \"uncommitted_changes\": \"src/main.rs modified\",\n    \"error_logs\": \"error.log contents...\"\n  }\n}\n</code></pre> <ol> <li>Unique identifier for the work item</li> <li>Original work item data from input JSON</li> <li>When the item first failed (used for age-based filtering)</li> <li>Number of retry attempts made</li> <li>Complete history of all failure attempts with detailed error information</li> <li>Simplified error pattern for grouping similar failures</li> <li>Whether item can be automatically retried</li> <li>Preserved state from failed agent's execution environment</li> </ol> <p>Source: <code>DeadLetteredItem</code> struct (src/cook/execution/dlq.rs:32-43)</p>"},{"location":"mapreduce/dead-letter-queue-dlq/#key-fields","title":"Key Fields","text":"<p>Source: <code>DeadLetteredItem</code> struct (src/cook/execution/dlq.rs:32-43)</p> <ul> <li><code>item_id</code>: Unique identifier for the work item</li> <li><code>item_data</code>: Original work item data from input JSON</li> <li><code>first_attempt</code>: Timestamp of first failure attempt (DateTime) <li><code>last_attempt</code>: Timestamp of most recent failure attempt (DateTime) <li><code>failure_count</code>: Number of failed attempts (u32)</li> <li><code>failure_history</code>: Array of <code>FailureDetail</code> objects capturing each attempt</li> <li><code>error_signature</code>: Simplified error pattern for grouping similar failures</li> <li><code>reprocess_eligible</code>: Whether item can be retried automatically</li> <li><code>manual_review_required</code>: Whether item needs human intervention</li> <li><code>worktree_artifacts</code>: Captured state from failed agent's worktree (Optional)</li>"},{"location":"mapreduce/dead-letter-queue-dlq/#failure-detail-fields","title":"Failure Detail Fields","text":"<p>Each attempt in <code>failure_history</code> is a <code>FailureDetail</code> object (src/cook/execution/dlq.rs:46-59):</p> <pre><code># Source: src/cook/execution/dlq.rs:46-59 (FailureDetail struct)\n{\n  \"attempt_number\": 1,\n  \"timestamp\": \"2025-01-11T10:30:00Z\",\n  \"error_type\": { \"CommandFailed\": { \"exit_code\": 101 } },\n  \"error_message\": \"cargo test failed with exit code 101\",\n  \"stack_trace\": \"thread 'main' panicked at src/main.rs:42...\",\n  \"agent_id\": \"agent-1\",\n  \"step_failed\": \"claude: /process '${item}'\",\n  \"duration_ms\": 45000,\n  \"json_log_location\": \"/path/to/claude-session.json\"\n}\n</code></pre> <p>Field Details: - <code>attempt_number</code>: Sequential attempt counter (u32) - <code>timestamp</code>: When this attempt occurred (DateTime) - <code>error_type</code>: Error classification (see Error Types below) - <code>error_message</code>: Human-readable error description - <code>stack_trace</code>: Optional detailed stack trace - <code>agent_id</code>: ID of the agent that failed - <code>step_failed</code>: Command string that failed (e.g., \"shell: cargo test\") - <code>duration_ms</code>: How long the attempt ran before failing (u64) - <code>json_log_location</code>: Path to Claude JSON log for debugging (Optional, see Best Practices for Debugging)"},{"location":"mapreduce/dead-letter-queue-dlq/#error-types","title":"Error Types","text":"<p>The <code>error_type</code> field uses the <code>ErrorType</code> enum (src/cook/execution/dlq.rs:62-71):</p> Execution ErrorsGit ErrorsSystem Errors <p><code>Timeout</code>: Agent execution exceeded timeout limit</p> <p><code>CommandFailed { exit_code: i32 }</code>: Shell or Claude command returned non-zero exit code</p> <p>Example: <pre><code>{ \"CommandFailed\": { \"exit_code\": 101 } }\n</code></pre></p> <p><code>ValidationFailed</code>: Post-execution validation command failed</p> <p><code>WorktreeError</code>: Git worktree operation failed (creation, cleanup)</p> <p><code>MergeConflict</code>: Merge back to parent worktree failed</p> <p>Common when multiple agents modify the same files.</p> <p><code>ResourceExhausted</code>: System resources (memory, disk) exhausted</p> <p><code>Unknown</code>: Unclassified error</p> <p>Review <code>error_message</code> and <code>stack_trace</code> for details.</p>"},{"location":"mapreduce/dead-letter-queue-dlq/#worktree-artifacts","title":"Worktree Artifacts","text":"<p>The <code>WorktreeArtifacts</code> structure captures the agent's execution environment (src/cook/execution/dlq.rs:74-80):</p> <ul> <li><code>worktree_path</code>: Path to agent's isolated worktree (PathBuf)</li> <li><code>branch_name</code>: Git branch created for agent (String)</li> <li><code>uncommitted_changes</code>: Files modified but not committed (Option) <li><code>error_logs</code>: Captured error output (Option) <p>Note: Both <code>uncommitted_changes</code> and <code>error_logs</code> are stored as optional strings, not arrays. If present, they contain descriptive text about the changes/logs rather than file lists.</p> <p>These artifacts are preserved for debugging and can be accessed after failure.</p>"},{"location":"mapreduce/dead-letter-queue-dlq/#dlq-commands","title":"DLQ Commands","text":"<p>Planned Feature - Stub Implementation</p> <p>The DLQ CLI commands are currently implemented as stubs that only print status messages. Full functionality is planned for a future release.</p> <p>Current Status:</p> <ul> <li>Command definitions exist in src/cli/args.rs:577-677</li> <li>Stub implementations in src/cli/commands/dlq.rs:1-74</li> <li>Commands will print confirmation messages but do not execute actual operations</li> </ul> <p>See DLQ Integration for current workflow-level DLQ functionality.</p> <p>Prodigy provides comprehensive CLI commands for managing the DLQ. The following sections describe the planned command interface.</p>"},{"location":"mapreduce/dead-letter-queue-dlq/#list-command","title":"List Command","text":"<p>View DLQ items across jobs:</p> <pre><code># List all DLQ items\nprodigy dlq list\n\n# List items for specific job\nprodigy dlq list --job-id mapreduce-1234567890\n\n# List only retry-eligible items\nprodigy dlq list --eligible\n\n# Limit results\nprodigy dlq list --limit 10\n</code></pre>"},{"location":"mapreduce/dead-letter-queue-dlq/#inspect-command","title":"Inspect Command","text":"<p>Examine detailed information for a specific item:</p> <pre><code># Inspect item by ID\nprodigy dlq inspect item-123\n\n# Inspect item in specific job\nprodigy dlq inspect item-123 --job-id mapreduce-1234567890\n</code></pre> <p>Output includes: - Full item data - Complete failure history with all attempts - Error messages and stack traces - JSON log locations for debugging - Worktree artifacts</p>"},{"location":"mapreduce/dead-letter-queue-dlq/#analyze-command","title":"Analyze Command","text":"<p>Analyze failure patterns across items:</p> <pre><code># Analyze all failures\nprodigy dlq analyze\n\n# Analyze specific job\nprodigy dlq analyze --job-id mapreduce-1234567890\n\n# Export analysis results\nprodigy dlq analyze --export analysis.json\n</code></pre> <p>Analysis output includes: - <code>pattern_groups</code>: Failures grouped by error type and message similarity - <code>error_distribution</code>: Histogram of error types - <code>temporal_distribution</code>: Failures over time</p>"},{"location":"mapreduce/dead-letter-queue-dlq/#retry-command","title":"Retry Command","text":"<p>\u26a0\ufe0f STUB: This command is not yet implemented. Description below shows planned interface.</p> <p>Reprocess failed items:</p> <pre><code># Source: src/cli/args.rs:640-659\n# Retry all failed items for a workflow\nprodigy dlq retry &lt;workflow_id&gt;\n\n# Control parallelism (default: 10)\nprodigy dlq retry &lt;workflow_id&gt; --parallel 10\n\n# Filter items to retry\nprodigy dlq retry &lt;workflow_id&gt; --filter 'item.priority &gt;= 5'\n\n# Limit retry attempts per item (default: 3)\nprodigy dlq retry &lt;workflow_id&gt; --max-retries 2\n\n# Force retry even if not eligible\nprodigy dlq retry &lt;workflow_id&gt; --force\n</code></pre> <p>Command Parameters: - <code>&lt;workflow_id&gt;</code>: The workflow/job identifier to retry items from (e.g., \"mapreduce-1234567890\") - <code>--parallel</code>: Number of concurrent retry workers (default: 10, src/cli/args.rs:653) - <code>--max-retries</code>: Maximum retry attempts per item (default: 3, src/cli/args.rs:649) - <code>--filter</code>: Expression to filter which items to retry (default: all eligible items, src/cli/args.rs:645) - <code>--force</code>: Force retry of items marked as not eligible (default: false, src/cli/args.rs:657)</p> <p>Supported Flags: <code>--parallel</code>, <code>--max-retries</code>, <code>--filter</code>, <code>--force</code></p>"},{"location":"mapreduce/dead-letter-queue-dlq/#retry-behavior","title":"Retry Behavior","text":"<p>The retry functionality is designed to handle large-scale DLQ reprocessing:</p> <ul> <li>Streaming support: Items are processed incrementally to avoid memory issues with large DLQs</li> <li>Parallel execution: Uses <code>--parallel</code> flag (default: 10) to control concurrent workers</li> <li>State updates:</li> <li>Successful items are removed from DLQ</li> <li>Failed items remain with updated <code>failure_history</code></li> <li><code>failure_count</code> is incremented</li> <li>Correlation tracking: Maintains original item IDs and correlation metadata</li> <li>Interruption safe: Supports stopping and resuming retry operations</li> </ul> <p>Implementation Note: The DLQ reprocessing logic uses the <code>DeadLetterQueue::reprocess</code> method (src/cook/execution/dlq.rs:202-233) which filters for <code>reprocess_eligible</code> items before attempting retry.</p>"},{"location":"mapreduce/dead-letter-queue-dlq/#export-command","title":"Export Command","text":"<p>Export DLQ items for external analysis:</p> <pre><code># Export to JSON (default)\nprodigy dlq export dlq-items.json\n\n# Export to CSV\nprodigy dlq export dlq-items.csv --format csv\n\n# Export specific job\nprodigy dlq export output.json --job-id mapreduce-1234567890\n</code></pre>"},{"location":"mapreduce/dead-letter-queue-dlq/#stats-command","title":"Stats Command","text":"<p>View DLQ statistics:</p> <pre><code># Overall DLQ statistics\nprodigy dlq stats\n\n# Stats for specific workflow\nprodigy dlq stats --workflow-id my-workflow\n</code></pre> <p>Output includes: - Total items in DLQ - Items by error type - Average retry count - Retry-eligible vs manual review required - Temporal distribution</p>"},{"location":"mapreduce/dead-letter-queue-dlq/#clear-command","title":"Clear Command","text":"<p>Remove items from DLQ:</p> <pre><code># Clear all items for a workflow (prompts for confirmation)\nprodigy dlq clear my-workflow\n\n# Skip confirmation prompt\nprodigy dlq clear my-workflow --yes\n</code></pre>"},{"location":"mapreduce/dead-letter-queue-dlq/#purge-command","title":"Purge Command","text":"<p>Clean up old DLQ items:</p> <pre><code># Purge items older than 30 days\nprodigy dlq purge --older-than-days 30\n\n# Purge for specific job\nprodigy dlq purge --older-than-days 30 --job-id mapreduce-1234567890\n\n# Skip confirmation\nprodigy dlq purge --older-than-days 30 --yes\n</code></pre>"},{"location":"mapreduce/dead-letter-queue-dlq/#debugging-with-dlq","title":"Debugging with DLQ","text":""},{"location":"mapreduce/dead-letter-queue-dlq/#accessing-claude-json-logs","title":"Accessing Claude JSON Logs","text":"<p>Each <code>FailureDetail</code> includes a <code>json_log_location</code> field pointing to the Claude Code JSON log for that execution. This log contains: - Complete conversation history - All tool invocations and results - Error details and stack traces - Token usage statistics</p> <pre><code># View JSON log from DLQ item\ncat $(prodigy dlq inspect item-123 | jq -r '.failure_history[0].json_log_location')\n\n# Pretty-print with jq\ncat /path/to/session.json | jq '.'\n</code></pre> <p>For more details on Claude JSON logs, see Best Practices for Debugging and Retry Metrics and Observability.</p>"},{"location":"mapreduce/dead-letter-queue-dlq/#common-debugging-workflow","title":"Common Debugging Workflow","text":"<p>Step-by-Step Debugging</p> <p>Follow this workflow to diagnose and fix failures systematically:</p> <ol> <li> <p>List failed items:    <pre><code>prodigy dlq list --job-id mapreduce-1234567890\n</code></pre></p> </li> <li> <p>Inspect specific failure:    <pre><code>prodigy dlq inspect item-123\n</code></pre></p> </li> <li> <p>Examine Claude logs:    <pre><code>cat /path/to/claude-session.json | jq '.messages[-3:]'\n</code></pre></p> </li> <li> <p>Analyze failure patterns:    <pre><code>prodigy dlq analyze --job-id mapreduce-1234567890\n</code></pre></p> </li> <li> <p>Fix underlying issue (code bug, config error, etc.)</p> </li> <li> <p>Retry failed items:    <pre><code>prodigy dlq retry mapreduce-1234567890\n</code></pre></p> </li> </ol>"},{"location":"mapreduce/dead-letter-queue-dlq/#integration-with-mapreduce","title":"Integration with MapReduce","text":"<p>The DLQ is tightly integrated with MapReduce workflows through the <code>on_item_failure</code> policy:</p> <pre><code># Source: Workflow configuration (see src/config/mapreduce.rs for MapConfig schema)\nname: my-workflow\nmode: mapreduce\n\nmap:\n  input: \"items.json\"\n  json_path: \"$.items[*]\"\n\n  # Default policy: send failures to DLQ\n  on_item_failure: dlq\n\n  agent_template:\n    - claude: \"/process '${item}'\"\n</code></pre>"},{"location":"mapreduce/dead-letter-queue-dlq/#available-policies","title":"Available Policies","text":"<ul> <li><code>dlq</code> (default): Failed items sent to DLQ, job continues</li> <li><code>retry</code>: Immediate retry with exponential backoff</li> <li><code>skip</code>: Ignore failures, mark as skipped, continue</li> <li><code>stop</code>: Halt entire workflow on first failure</li> <li><code>custom</code>: User-defined failure handler</li> </ul>"},{"location":"mapreduce/dead-letter-queue-dlq/#failure-flow","title":"Failure Flow","text":"<pre><code>Work Item Processing\n       \u2193\n   Command Failed\n       \u2193\n  Retry Attempts (if configured)\n       \u2193\n   Still Failing?\n       \u2193\non_item_failure: dlq\n       \u2193\nCreate DeadLetteredItem\n       \u2193\nSave to ~/.prodigy/dlq/{repo}/{job_id}/mapreduce/dlq/{job_id}/items/{item_id}.json\n       \u2193\nContinue Processing Other Items\n</code></pre>"},{"location":"mapreduce/dead-letter-queue-dlq/#best-practices","title":"Best Practices","text":""},{"location":"mapreduce/dead-letter-queue-dlq/#when-to-retry-vs-manual-fix","title":"When to Retry vs Manual Fix","text":"<p>Choosing the Right Approach</p> <p>Before retrying failed items, analyze the failure patterns with <code>prodigy dlq analyze</code> to determine if the issue is transient (safe to retry) or systematic (requires code changes first).</p> <p>Automatic Retry (via <code>prodigy dlq retry</code>): - Transient failures (network timeouts, resource contention) - Flaky tests or intermittent issues - Items that may succeed with more resources (<code>--max-parallel 1</code>)</p> <p>Manual Fix (code changes, then retry): - Logic errors in processing code - Invalid assumptions about item data - Missing dependencies or configuration - Systematic failures affecting multiple items</p>"},{"location":"mapreduce/dead-letter-queue-dlq/#dlq-management","title":"DLQ Management","text":"<ol> <li>Monitor regularly: Use <code>prodigy dlq stats</code> to track failure rates</li> <li>Analyze patterns: Run <code>prodigy dlq analyze</code> to identify systematic issues</li> <li>Clean up old items: Periodically run <code>prodigy dlq purge</code> to remove resolved failures</li> <li>Set review flags: Mark <code>manual_review_required: true</code> for items needing human investigation</li> </ol>"},{"location":"mapreduce/dead-letter-queue-dlq/#performance-considerations","title":"Performance Considerations","text":"<p>Resource Management</p> <p>When retrying large DLQs, start with lower parallelism (e.g., <code>--max-parallel 5</code>) to avoid overwhelming system resources. Monitor the first few retries before increasing parallelism.</p> <ul> <li>Large DLQs: Retry command uses streaming to handle thousands of items efficiently</li> <li>Parallelism: Tune <code>--max-parallel</code> based on failure type (CPU-bound vs I/O-bound)</li> <li>Filtering: Use <code>--filter</code> to target specific subsets of failures</li> </ul>"},{"location":"mapreduce/dead-letter-queue-dlq/#advanced-dlq-storage-internals","title":"Advanced: DLQ Storage Internals","text":"<p>For advanced debugging or direct file access, understanding the DLQ storage structure can be helpful.</p>"},{"location":"mapreduce/dead-letter-queue-dlq/#index-file-structure","title":"Index File Structure","text":"<p>The <code>index.json</code> file provides metadata about the DLQ (src/cook/execution/dlq.rs:608-637):</p> <pre><code># Source: src/cook/execution/dlq.rs:608-637 (index structure)\n{\n  \"job_id\": \"mapreduce-1234567890\",\n  \"item_count\": 3,\n  \"item_ids\": [\"item-123\", \"item-456\", \"item-789\"],\n  \"updated_at\": \"2025-01-11T10:30:00Z\"\n}\n</code></pre> <p>This index is automatically updated when items are added or removed from the DLQ.</p>"},{"location":"mapreduce/dead-letter-queue-dlq/#direct-file-access","title":"Direct File Access","text":"<p>To inspect DLQ items directly:</p> <pre><code># List all DLQ items for a job\nls ~/.prodigy/dlq/prodigy/mapreduce-1234567890/mapreduce/dlq/mapreduce-1234567890/items/\n\n# View a specific item\ncat ~/.prodigy/dlq/prodigy/mapreduce-1234567890/mapreduce/dlq/mapreduce-1234567890/items/item-123.json | jq\n\n# Count total items\njq '.item_count' ~/.prodigy/dlq/prodigy/mapreduce-1234567890/mapreduce/dlq/mapreduce-1234567890/index.json\n</code></pre> <p>Note: Direct file access is provided for debugging. Always use the Prodigy CLI commands for production operations to ensure data consistency.</p>"},{"location":"mapreduce/dead-letter-queue-dlq/#cross-references","title":"Cross-References","text":"<ul> <li>Checkpoint and Resume: DLQ state preserved in checkpoints</li> <li>Event Tracking: DLQ operations emit trackable events</li> <li>Error Handling: Broader error handling strategies</li> <li>Worktree Architecture: Agent isolation and artifact preservation</li> <li>Best Practices for Debugging: Using JSON logs and DLQ for debugging failed items</li> <li>Retry Metrics and Observability: Monitoring retry behavior and failures</li> </ul>"},{"location":"mapreduce/environment-variables-in-configuration/","title":"Environment Variables in Configuration","text":""},{"location":"mapreduce/environment-variables-in-configuration/#environment-variables-in-configuration","title":"Environment Variables in Configuration","text":"<p>MapReduce workflows support comprehensive environment variable configuration, enabling parameterized workflows with secrets management, multi-environment deployment, and secure credential handling.</p>"},{"location":"mapreduce/environment-variables-in-configuration/#configuration-fields","title":"Configuration Fields","text":"<p>MapReduce workflows support four types of environment configuration:</p> <p>1. Basic Environment Variables (<code>env</code>)</p> <p>Define static environment variables available throughout the workflow:</p> <pre><code>env:\n  MAX_WORKERS: \"10\"\n  AGENT_TIMEOUT: \"300\"\n  PROJECT_NAME: \"my-project\"\n</code></pre> <p>Source: <code>src/config/workflow.rs:11-39</code> - WorkflowConfig struct with <code>env: HashMap&lt;String, String&gt;</code> field</p> <p>2. Secrets Configuration</p> <p>Secrets are defined in a dedicated <code>secrets:</code> block (separate from <code>env:</code>). Secret values are automatically masked in logs, output, events, and checkpoints.</p> <p>Source: <code>src/config/workflow.rs:24-26</code> - WorkflowConfig with <code>secrets: HashMap&lt;String, SecretValue&gt;</code> field</p> <p>Simple Secret Syntax (Recommended for most cases):</p> <pre><code>secrets:\n  # Reference to environment variable\n  API_KEY: \"${env:SECRET_API_KEY}\"\n\n  # Direct secret value (not recommended - use env refs instead)\n  DB_PASSWORD: \"my-secret-password\"\n</code></pre> <p>Provider-Based Secret Syntax:</p> <pre><code>secrets:\n  API_TOKEN:\n    provider: env\n    key: \"GITHUB_TOKEN\"\n\n  AWS_SECRET:\n    provider: aws\n    key: \"prod/api/credentials\"\n    version: \"v1\"  # Optional version\n</code></pre> <p>Source: <code>src/cook/environment/config.rs:86-96</code> - SecretValue enum with Simple and Provider variants</p> <p>Supported Secret Providers (defined in <code>src/cook/environment/config.rs:99-112</code>): - <code>env</code> - Environment variable reference (fully supported) - <code>file</code> - File-based secrets (fully supported) - <code>vault</code> - HashiCorp Vault integration (planned) - <code>aws</code> - AWS Secrets Manager (planned) - <code>custom</code> - Custom provider support (planned)</p> <p>Real-World Examples:</p> <p>From <code>workflows/mapreduce-env-example.yml:22-26</code>: <pre><code>secrets:\n  API_TOKEN:\n    provider: env\n    key: \"GITHUB_TOKEN\"\n</code></pre></p> <p>From <code>workflows/environment-example.yml:20-23</code>: <pre><code>secrets:\n  API_KEY: \"${env:SECRET_API_KEY}\"\n</code></pre></p> <p>3. Environment Files (<code>env_files</code>)</p> <p>Load environment variables from <code>.env</code> files:</p> <pre><code>env_files:\n  - \".env\"\n  - \".env.local\"\n</code></pre> <p>Source: <code>src/config/workflow.rs:11-39</code> - WorkflowConfig with <code>env_files: Vec&lt;PathBuf&gt;</code> field</p> <p>Files are loaded in order, with later files overriding earlier ones. Standard <code>.env</code> format: <code>KEY=value</code></p> <p>4. Profiles (<code>profiles</code>)</p> <p>Environment-specific configurations for different deployment targets:</p> <pre><code>profiles:\n  dev:\n    API_URL: \"http://localhost:3000\"\n    TIMEOUT: \"60\"\n    MAX_WORKERS: \"5\"\n\n  prod:\n    API_URL: \"https://api.prod.com\"\n    TIMEOUT: \"30\"\n    MAX_WORKERS: \"20\"\n</code></pre> <p>Source: <code>src/cook/environment/config.rs:115-124</code> - EnvProfile struct</p> <p>Activate a profile with: <pre><code>prodigy run workflow.yml --profile prod\n</code></pre></p> <p>Profile Structure: Each profile contains a HashMap of environment variables and an optional description field for documentation.</p>"},{"location":"mapreduce/environment-variables-in-configuration/#variable-interpolation","title":"Variable Interpolation","text":"<p>Environment variables can be referenced using two syntaxes: - <code>$VAR</code> - Simple variable reference (shell-style) - <code>${VAR}</code> - Bracketed reference for clarity and complex expressions</p> <p>Source: Verified in <code>tests/mapreduce_env_execution_test.rs:157-189</code> - both syntaxes are fully supported</p> <p>Use <code>${VAR}</code> when: - Variable name is followed by alphanumeric characters - Embedding in strings or paths - Preference for explicit syntax</p> <p>Interpolation in Environment Values:</p> <p>Environment variables can reference other environment variables or system environment variables within their values:</p> <pre><code>env:\n  BASE_URL: \"https://api.example.com\"\n  API_ENDPOINT: \"${BASE_URL}/v1/data\"\n  FULL_PATH: \"${PROJECT_DIR}/output/${REPORT_FORMAT}\"\n</code></pre> <p>Source: <code>src/cook/environment/config.rs:39-60</code> - EnvValue supports Static, Dynamic, and Conditional variants</p> <p>Real-World Composition Example:</p> <p>From <code>workflows/mapreduce-env-example.yml:78</code>: <pre><code>env:\n  PROJECT_NAME: \"my-project\"\n  OUTPUT_DIR: \"output\"\n  REPORT_FORMAT: \"json\"\n\nreduce:\n  # Composing multiple env vars in a single path\n  - shell: \"cp summary.$REPORT_FORMAT $OUTPUT_DIR/${PROJECT_NAME}-summary.$REPORT_FORMAT\"\n</code></pre></p> <p>This demonstrates environment variable composition across different workflow variables to build complex paths dynamically.</p> <p>Supported Fields: - <code>max_parallel</code> - Control parallelism dynamically - <code>agent_timeout_secs</code> - Adjust timeouts per environment - <code>setup.timeout</code> - Configure setup phase timeouts - <code>merge.timeout</code> - Control merge operation timeouts - Any string field in your workflow (commands, paths, etc.)</p> <p>Example from tests (<code>tests/mapreduce_env_execution_test.rs:10-52</code>): <pre><code>env:\n  MAX_PARALLEL: \"5\"\n  TIMEOUT_SECONDS: \"900\"\n\nmap:\n  max_parallel: ${MAX_PARALLEL}\n  agent_timeout_secs: ${TIMEOUT_SECONDS}\n</code></pre></p>"},{"location":"mapreduce/environment-variables-in-configuration/#environment-precedence","title":"Environment Precedence","text":"<p>When the same variable is defined in multiple places, Prodigy uses the following precedence order (highest to lowest):</p> <ol> <li>Command-line environment variables - Set when invoking prodigy</li> <li>Step-level environment - Variables defined in individual command <code>env:</code> blocks</li> <li>Active profile values - Variables from the selected profile</li> <li>Global env block - Base workflow environment variables</li> <li>Environment files - Variables loaded from .env files (later files override earlier)</li> <li>System environment - Parent process environment (if <code>inherit: true</code>)</li> </ol> <p>Source: <code>src/cook/environment/config.rs:11-36</code> - EnvironmentConfig structure and <code>tests/environment_workflow_test.rs:225-256</code> - precedence verification</p> <p>Example: <pre><code># Command-line overrides profile, which overrides base env\nMAX_WORKERS=10 prodigy run workflow.yml --profile prod\n</code></pre></p>"},{"location":"mapreduce/environment-variables-in-configuration/#complete-example","title":"Complete Example","text":"<p>Here's a complete MapReduce workflow demonstrating all environment features:</p> <pre><code>name: configurable-mapreduce\nmode: mapreduce\n\n# Basic environment variables\nenv:\n  PROJECT_NAME: \"data-pipeline\"\n  VERSION: \"1.0.0\"\n  OUTPUT_DIR: \"output\"\n\n# Secrets (masked in logs)\nsecrets:\n  API_KEY: \"${env:SECRET_API_KEY}\"\n\n# Environment files\nenv_files:\n  - \".env\"\n\n# Multi-environment profiles\nprofiles:\n  dev:\n    MAX_WORKERS: \"5\"\n    API_URL: \"http://localhost:3000\"\n    DEBUG_MODE: \"true\"\n\n  prod:\n    MAX_WORKERS: \"20\"\n    API_URL: \"https://api.prod.com\"\n    DEBUG_MODE: \"false\"\n\nsetup:\n  timeout: 300\n  commands:\n    - shell: \"echo Processing $PROJECT_NAME v$VERSION\"\n    - shell: \"mkdir -p $OUTPUT_DIR\"\n    - shell: \"curl -H 'Authorization: Bearer ${API_KEY}' $API_URL/init\"\n\nmap:\n  input: \"items.json\"\n  json_path: \"$[*]\"\n  max_parallel: ${MAX_WORKERS}\n  agent_template:\n    - claude: \"/process ${item} --project $PROJECT_NAME\"\n    - shell: \"curl -H 'Authorization: Bearer ${API_KEY}' $API_URL/items\"\n\nreduce:\n  - claude: \"/summarize ${map.results} --project $PROJECT_NAME\"\n  - shell: \"cp summary.json $OUTPUT_DIR/${PROJECT_NAME}-summary.json\"\n</code></pre> <p>Source: Based on <code>workflows/mapreduce-env-example.yml:1-94</code> with structure validated against <code>src/config/mapreduce.rs:15-82</code></p>"},{"location":"mapreduce/environment-variables-in-configuration/#running-with-different-configurations","title":"Running with Different Configurations","text":"<pre><code># Development environment\nprodigy run workflow.yml --profile dev\n\n# Production environment\nprodigy run workflow.yml --profile prod\n\n# Override specific variables (command-line takes precedence)\nMAX_WORKERS=10 prodigy run workflow.yml --profile prod\n\n# Local development with .env file\necho \"MAX_WORKERS=3\" &gt; .env\nprodigy run workflow.yml\n</code></pre> <p>Precedence Note: Command-line environment variables override profile settings, which override the base env block.</p> <p>Source: Precedence behavior verified in <code>src/cook/environment/manager.rs:43-52</code> and <code>tests/mapreduce_env_execution_test.rs:225-256</code></p>"},{"location":"mapreduce/environment-variables-in-configuration/#step-level-environment-overrides","title":"Step-Level Environment Overrides","text":"<p>Individual commands can override environment variables:</p> <pre><code>map:\n  agent_template:\n    - shell: \"process-item.sh\"\n      env:\n        CUSTOM_VAR: \"value\"\n        PATH: \"/custom/bin:${PATH}\"\n</code></pre> <p>Source: <code>src/cook/environment/config.rs:127-144</code> - StepEnvironment struct with <code>env</code>, <code>working_dir</code>, <code>clear_env</code>, and <code>temporary</code> fields</p> <p>Step-level variables inherit from global <code>env</code> and active <code>profiles</code>, with step-level values taking precedence.</p> <p>Advanced Step Environment Features (<code>tests/environment_workflow_test.rs:42-70</code>): - <code>clear_env: true</code> - Start with clean environment except step-specific vars - <code>temporary: true</code> - Restore environment after step execution - <code>working_dir</code> - Set working directory for the step</p>"},{"location":"mapreduce/environment-variables-in-configuration/#run-with-debug-output-to-see-environment-resolution","title":"Run with debug output to see environment resolution","text":"<p>prodigy run workflow.yml -vv --profile dev</p>"},{"location":"mapreduce/environment-variables-in-configuration/#output-will-show","title":"Output will show:","text":""},{"location":"mapreduce/environment-variables-in-configuration/#-password-as-redacted","title":"- PASSWORD as REDACTED","text":""},{"location":"mapreduce/environment-variables-in-configuration/#-api_key-as-redacted","title":"- API_KEY as REDACTED","text":""},{"location":"mapreduce/environment-variables-in-configuration/#-normal-variables-displayed-in-full","title":"- Normal variables displayed in full","text":"<p>```</p>"},{"location":"mapreduce/environment-variables-in-configuration/#cross-references","title":"Cross-References","text":"<p>For comprehensive environment variable documentation, see: - Variables Chapter - Complete guide to environment variables and interpolation - Environment Configuration - Detailed environment management patterns - MapReduce Environment Variables - MapReduce-specific environment features - Secrets Management - Security best practices for secrets - Environment Profiles - Multi-environment deployment patterns - Setup Phase - Using environment variables in setup commands - MapReduce Index - Main MapReduce workflow documentation</p>"},{"location":"mapreduce/error-collection-strategies/","title":"Error Collection Strategies","text":""},{"location":"mapreduce/error-collection-strategies/#error-collection-strategies","title":"Error Collection Strategies","text":"<p>The <code>error_collection</code> field controls how errors are reported during workflow execution.</p>"},{"location":"mapreduce/error-collection-strategies/#syntax-flexibility","title":"Syntax Flexibility","text":"<p>Error collection can be configured in two ways for backward compatibility:</p> <p>Top-level convenience syntax (recommended for simple workflows): <pre><code>name: my-workflow\nmode: mapreduce\n\nerror_collection: aggregate  # Top-level field\n\nmap:\n  # ... map configuration\n</code></pre></p> <p>Nested under error_policy block (recommended when using other error policy features): <pre><code>name: my-workflow\nmode: mapreduce\n\nerror_policy:\n  error_collection: aggregate\n  continue_on_failure: true\n  max_failures: 10\n  # ... other error policy fields\n\nmap:\n  # ... map configuration\n</code></pre></p> <p>Both syntaxes are fully supported. Use the top-level syntax for simplicity, or the nested syntax when configuring multiple error policy fields together.</p>"},{"location":"mapreduce/error-collection-strategies/#available-strategies","title":"Available Strategies","text":"<p>Aggregate (default): <pre><code>error_collection: aggregate\n</code></pre> - Collects all errors in memory and reports at workflow end - Errors are stored as they occur but not logged - Full error list displayed when workflow completes - Use for: Final summary reports, batch processing where individual failures don't need immediate attention - Trade-off: Low noise, but you won't see errors until completion</p> <p>Immediate: <pre><code>error_collection: immediate\n</code></pre> - Logs each error as soon as it happens via <code>warn!</code> level logging - No error collection in memory - Errors visible in real-time during execution - Use for: Debugging, development, real-time monitoring - Trade-off: More verbose output, but immediate visibility</p> <p>Batched: <pre><code>error_collection: batched:10\n</code></pre> - Collects errors in memory until batch size is reached - When N errors collected, logs the entire batch via <code>warn!</code> level logging and automatically clears the buffer - Use for: Progress updates without spam, monitoring long-running jobs - Trade-off: Balance between noise and visibility (e.g., <code>batched:10</code> reports every 10 failures) - Implementation: Buffer is cleared using <code>drain(..)</code> after each batch is logged (src/cook/workflow/error_policy.rs:593)</p>"},{"location":"mapreduce/error-collection-strategies/#complete-example","title":"Complete Example","text":"<p>Combining error collection with other error policy features:</p> <pre><code>name: data-processing\nmode: mapreduce\n\nerror_policy:\n  # Report errors in batches of 5\n  error_collection: batched:5\n\n  # Send failed items to DLQ instead of failing workflow\n  on_item_failure: dlq\n\n  # Continue processing even if items fail\n  continue_on_failure: true\n\n  # Stop if failure rate exceeds 30%\n  failure_threshold: 0.3\n\nmap:\n  input: \"items.json\"\n  json_path: \"$.items[*]\"\n  agent_template:\n    - claude: \"/process '${item}'\"\n</code></pre> <p>Note: If <code>error_collection</code> is not specified, the default behavior is <code>aggregate</code>.</p> <p>See also: Error Handling, Dead Letter Queue</p>"},{"location":"mapreduce/event-tracking/","title":"Event Tracking","text":""},{"location":"mapreduce/event-tracking/#event-tracking","title":"Event Tracking","text":"<p>All MapReduce execution events are logged to <code>~/.prodigy/events/{repo_name}/{job_id}/</code> for debugging and monitoring. Prodigy provides comprehensive event tracking with correlation IDs, metadata enrichment, buffering, and powerful CLI tools for querying and analysis.</p>"},{"location":"mapreduce/event-tracking/#event-types","title":"Event Types","text":"<p>Prodigy tracks 24 event types across different categories:</p>"},{"location":"mapreduce/event-tracking/#job-lifecycle-events","title":"Job Lifecycle Events","text":"<ul> <li><code>JobStarted</code> - Job begins with config and total items</li> <li><code>JobCompleted</code> - Job finishes with success/failure counts and duration</li> <li><code>JobFailed</code> - Job fails with error and partial results count</li> <li><code>JobPaused</code> - Job paused with checkpoint version</li> <li><code>JobResumed</code> - Job resumed from checkpoint with pending items</li> </ul>"},{"location":"mapreduce/event-tracking/#agent-lifecycle-events","title":"Agent Lifecycle Events","text":"<ul> <li><code>AgentStarted</code> - Agent begins processing work item (includes item_id, worktree, and attempt number)</li> <li><code>AgentProgress</code> - Agent reports progress percentage and current step</li> <li><code>AgentCompleted</code> - Agent finishes successfully with job_id, agent_id, duration, commits (Vec), and optional json_log_location <li><code>AgentFailed</code> - Agent fails with error and retry eligibility</li> <li><code>AgentRetrying</code> - Agent retries with attempt number and backoff delay in milliseconds</li>"},{"location":"mapreduce/event-tracking/#checkpoint-events","title":"Checkpoint Events","text":"<ul> <li><code>CheckpointCreated</code> - Checkpoint saved with version and completed agent count</li> <li><code>CheckpointLoaded</code> - Checkpoint loaded for resume</li> <li><code>CheckpointFailed</code> - Checkpoint operation failed</li> </ul>"},{"location":"mapreduce/event-tracking/#worktree-events","title":"Worktree Events","text":"<ul> <li><code>WorktreeCreated</code> - Git worktree created for agent (includes branch name)</li> <li><code>WorktreeMerged</code> - Agent changes merged to target branch</li> <li><code>WorktreeCleaned</code> - Worktree removed after agent completion</li> </ul>"},{"location":"mapreduce/event-tracking/#performance-monitoring-events","title":"Performance Monitoring Events","text":"<ul> <li><code>QueueDepthChanged</code> - Work queue status (pending/active/completed counts)</li> <li><code>MemoryPressure</code> - Resource usage monitoring (used vs limit in MB)</li> </ul>"},{"location":"mapreduce/event-tracking/#dead-letter-queue-events","title":"Dead Letter Queue Events","text":"<ul> <li><code>DLQItemAdded</code> - Failed item added to DLQ with error signature</li> <li><code>DLQItemRemoved</code> - Item removed from DLQ (successful retry)</li> <li><code>DLQItemsReprocessed</code> - Batch reprocessing of DLQ items</li> <li><code>DLQItemsEvicted</code> - Old DLQ items evicted per retention policy</li> <li><code>DLQAnalysisGenerated</code> - Error pattern analysis completed</li> </ul>"},{"location":"mapreduce/event-tracking/#claude-observability-events","title":"Claude Observability Events","text":"<ul> <li><code>ClaudeToolInvoked</code> - Claude tool use with name, ID, and parameters</li> <li><code>ClaudeTokenUsage</code> - Token consumption (input/output/cache tokens)</li> <li><code>ClaudeSessionStarted</code> - Claude session initialized with model and available tools</li> <li><code>ClaudeMessage</code> - Claude message with content and JSON log location</li> </ul>"},{"location":"mapreduce/event-tracking/#event-record-structure","title":"Event Record Structure","text":"<p>Each event is wrapped in an <code>EventRecord</code> with rich metadata:</p> <pre><code>{\n  \"id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"timestamp\": \"2024-01-01T12:00:00Z\",\n  \"correlation_id\": \"workflow-abc123\",\n  \"event\": {\n    \"event_type\": \"agent_completed\",\n    \"job_id\": \"mapreduce-xyz789\",\n    \"agent_id\": \"agent-1\",\n    \"duration\": { \"secs\": 45, \"nanos\": 0 },\n    \"commits\": [\"a1b2c3d\"],\n    \"json_log_location\": \"/home/user/.local/state/claude/logs/session-xyz.json\"\n  },\n  \"metadata\": {\n    \"host\": \"worker-01\",\n    \"pid\": 12345,\n    \"thread\": \"tokio-runtime-worker\"\n  }\n}\n</code></pre> <p>Note: The <code>event_type</code> field is serialized using serde's tagged enum format (<code>#[serde(tag = \"event_type\")]</code>), which means it appears as a sibling field alongside other event-specific fields within the <code>event</code> object.</p> <p>Fields: - <code>id</code> - Unique UUID for this event - <code>timestamp</code> - When the event occurred (UTC) - <code>correlation_id</code> - Links related events across agents and phases - <code>event</code> - The actual event data (varies by type) - <code>metadata</code> - Runtime context (host, process ID, thread). This field is extensible and supports custom fields via the <code>log_with_metadata</code> method for application-specific tracking needs.</p> <p>Source: EventRecord definition in src/cook/execution/events/event_logger.rs:17-25</p>"},{"location":"mapreduce/event-tracking/#event-storage","title":"Event Storage","text":"<p>Location: <code>~/.prodigy/events/{repo_name}/{job_id}/events-{timestamp}.jsonl</code></p> <p>Format: Events are stored in JSONL (JSON Lines) format with one event per line.</p> <p>Buffering: - Events are buffered in memory before being written to disk - Default buffer size: 1000 events - Default flush interval: 5 seconds - Background flush task runs automatically - Buffer size and flush interval are configurable</p> <p>Source: EventLogger configuration in src/cook/execution/events/event_logger.rs:44-45</p> <p>File Rotation: - Log files automatically rotate at 100MB (configurable via <code>with_rotation()</code>) - Rotated files are moved with timestamp suffix: <code>events-{timestamp}.jsonl.rotated</code> - Compression support is implemented but currently moves files without actual compression (marked as TODO in implementation) - Cross-worktree event aggregation for parallel jobs</p> <p>Source: Rotation implementation in src/cook/execution/events/event_writer.rs:rotation_size field and rotate_if_needed() method</p>"},{"location":"mapreduce/event-tracking/#correlation-ids","title":"Correlation IDs","text":"<p>Each workflow run has a unique <code>correlation_id</code> that links all related events:</p> <pre><code># All events from the same workflow share correlation_id\n# Makes it easy to trace execution flow across agents\n</code></pre> <p>Use correlation IDs to: - Trace work item through multiple retries - Link agent execution to parent job - Track checkpoint saves and resumes - Debug cross-agent dependencies</p>"},{"location":"mapreduce/event-tracking/#viewing-events-with-cli","title":"Viewing Events with CLI","text":"<p>Note on Event File Paths: The CLI commands use <code>.prodigy/events/mapreduce_events.jsonl</code> as the default path for backward compatibility. However, MapReduce workflows using global storage write events to <code>~/.prodigy/events/{repo_name}/{job_id}/events-{timestamp}.jsonl</code>. Use the <code>--file</code> flag to specify the global storage path when querying events from MapReduce jobs.</p> <p>Source: CLI default paths in src/cli/events/mod.rs (all subcommands use <code>default_value = \".prodigy/events/mapreduce_events.jsonl\"</code>)</p>"},{"location":"mapreduce/event-tracking/#list-events","title":"List Events","text":"<pre><code># List all events for a job (local storage)\nprodigy events ls --job-id &lt;job_id&gt;\n\n# List events from global storage\nprodigy events ls --job-id &lt;job_id&gt; --file ~/.prodigy/events/{repo_name}/{job_id}/events-{timestamp}.jsonl\n\n# Filter by event type\nprodigy events ls --job-id &lt;job_id&gt; --event-type agent_completed\n\n# Filter by agent\nprodigy events ls --job-id &lt;job_id&gt; --agent-id agent-1\n\n# Recent events only (last N minutes)\nprodigy events ls --job-id &lt;job_id&gt; --since 30\n\n# Limit results\nprodigy events ls --job-id &lt;job_id&gt; --limit 50\n</code></pre>"},{"location":"mapreduce/event-tracking/#event-statistics","title":"Event Statistics","text":"<pre><code># Show statistics grouped by event type\nprodigy events stats\n\n# Group by job ID\nprodigy events stats --group-by job_id\n\n# Group by agent ID\nprodigy events stats --group-by agent_id\n</code></pre>"},{"location":"mapreduce/event-tracking/#search-events","title":"Search Events","text":"<pre><code># Search by pattern (regex supported)\nprodigy events search \"error|failed\"\n\n# Search in specific fields only\nprodigy events search \"timeout\" --fields error,description\n</code></pre>"},{"location":"mapreduce/event-tracking/#follow-events-live","title":"Follow Events Live","text":"<pre><code># Stream events in real-time (tail -f style)\nprodigy events follow --job-id &lt;job_id&gt;\n\n# Filter while following\nprodigy events follow --job-id &lt;job_id&gt; --event-type agent_failed\n</code></pre>"},{"location":"mapreduce/event-tracking/#clean-old-events","title":"Clean Old Events","text":"<pre><code># Preview cleanup (dry run)\nprodigy events clean --older-than 30d --dry-run\n\n# Keep only recent events\nprodigy events clean --max-events 10000\n\n# Size-based retention\nprodigy events clean --max-size 100MB\n\n# Archive instead of delete\nprodigy events clean --older-than 7d --archive --archive-path /backup/events\n</code></pre> <p>Note: Cleanup operations require user confirmation unless running in automation mode (<code>PRODIGY_AUTOMATION=true</code>) or using <code>--dry-run</code> to preview changes.</p> <p>Source: Cleanup confirmation logic in src/cli/events/mod.rs:591-627</p>"},{"location":"mapreduce/event-tracking/#debugging-with-events","title":"Debugging with Events","text":"<p>Common debugging scenarios:</p> <p>Track failed agent: <pre><code># Find all events for failed agent\nprodigy events ls --job-id &lt;job_id&gt; --agent-id &lt;agent_id&gt;\n\n# Check Claude JSON log from AgentCompleted event\ncat &lt;json_log_location&gt;\n</code></pre></p> <p>Analyze performance: <pre><code># Monitor queue depth changes\nprodigy events ls --event-type queue_depth_changed\n\n# Check memory pressure events\nprodigy events ls --event-type memory_pressure\n</code></pre></p> <p>Debug checkpoint issues: <pre><code># Find checkpoint events\nprodigy events ls --event-type checkpoint_created\nprodigy events ls --event-type checkpoint_failed\n</code></pre></p> <p>Review DLQ patterns: <pre><code># See DLQ additions\nprodigy events ls --event-type dlq_item_added\n\n# Check error pattern analysis\nprodigy events ls --event-type dlq_analysis_generated\n</code></pre></p> <p>Trace specific workflow run: <pre><code># Filter events by correlation_id to trace entire workflow execution\nprodigy events search \"&lt;correlation_id&gt;\"\n\n# Find correlation_id from recent job\nprodigy events ls --job-id &lt;job_id&gt; --limit 1\n</code></pre></p>"},{"location":"mapreduce/event-tracking/#troubleshooting","title":"Troubleshooting","text":"<p>Events not being written: - Check event file permissions in <code>~/.prodigy/events/{repo_name}/{job_id}/</code> - Verify directory exists and is writable - Check disk space availability - Review buffer configuration if events are delayed</p> <p>Missing events: - Events may be buffered (default: 5 second flush interval) - Check if logger was properly shut down (ensures buffer flush) - Verify event type filter isn't excluding events</p>"},{"location":"mapreduce/event-tracking/#cross-references","title":"Cross-References","text":"<ul> <li>See Checkpoint and Resume for checkpoint events</li> <li>See Dead Letter Queue (DLQ) for DLQ event details</li> <li>See Retry Metrics and Observability for retry-specific monitoring</li> </ul>"},{"location":"mapreduce/global-storage-architecture/","title":"Global Storage Architecture","text":""},{"location":"mapreduce/global-storage-architecture/#global-storage-architecture","title":"Global Storage Architecture","text":"<p>MapReduce workflows use a global storage architecture located in <code>~/.prodigy/</code> (not <code>.prodigy/</code> in your project directory). This architecture replaced the legacy local storage system and is now the default for all Prodigy workflows.</p>"},{"location":"mapreduce/global-storage-architecture/#benefits","title":"Benefits","text":"<p>Global storage provides several key advantages for MapReduce workflows:</p> <ul> <li> <p>Cross-worktree event aggregation: When multiple worktrees process the same MapReduce job, all agents write to <code>~/.prodigy/events/{repo_name}/{job_id}/</code>, enabling unified monitoring across all parallel agents without manual log aggregation.</p> </li> <li> <p>Persistent state management: Job checkpoints and state files stored in <code>~/.prodigy/state/</code> survive worktree cleanup. You can delete agent worktrees after completion while preserving full job history and resume capability.</p> </li> <li> <p>Centralized monitoring: All job data is accessible from a single location (<code>~/.prodigy/</code>), making it easy to track multiple concurrent jobs, review historical executions, and debug failures across different worktrees and repositories.</p> </li> <li> <p>Efficient storage: Shared event logs, DLQ data, and checkpoints are deduplicated across worktrees, reducing disk usage when multiple agents process the same job.</p> </li> </ul>"},{"location":"mapreduce/global-storage-architecture/#directory-structure","title":"Directory Structure","text":"<p>The global storage directory is organized by repository name to isolate data between different projects:</p> <pre><code>~/.prodigy/\n\u251c\u2500\u2500 events/                    # Event logs for all MapReduce jobs\n\u2502   \u2514\u2500\u2500 {repo_name}/          # Events for specific repository\n\u2502       \u2514\u2500\u2500 {job_id}/         # Events for specific job\n\u2502           \u2514\u2500\u2500 events-{timestamp}.jsonl\n\u251c\u2500\u2500 dlq/                      # Dead Letter Queue for failed work items\n\u2502   \u2514\u2500\u2500 {repo_name}/          # DLQ for specific repository\n\u2502       \u2514\u2500\u2500 {job_id}/         # Failed items for specific job\n\u2502           \u2514\u2500\u2500 items/        # Individual failed item files\n\u251c\u2500\u2500 state/                    # State and checkpoints for jobs\n\u2502   \u2514\u2500\u2500 {repo_name}/          # State for specific repository\n\u2502       \u251c\u2500\u2500 mapreduce/        # MapReduce job state\n\u2502       \u2502   \u2514\u2500\u2500 jobs/         # Individual job directories\n\u2502       \u2502       \u2514\u2500\u2500 {job_id}/ # Job-specific state and checkpoints\n\u2502       \u251c\u2500\u2500 checkpoints/      # Legacy checkpoint storage\n\u2502       \u2514\u2500\u2500 mappings/         # Session-to-job ID mappings\n\u251c\u2500\u2500 worktrees/                # Git worktrees for sessions\n\u2502   \u2514\u2500\u2500 {repo_name}/          # Worktrees for specific repository\n\u2502       \u2514\u2500\u2500 session-{id}/     # Session-specific worktree\n\u251c\u2500\u2500 sessions/                 # Unified session tracking\n\u2502   \u2514\u2500\u2500 {session-id}.json    # Session metadata and status\n\u251c\u2500\u2500 resume_locks/             # Concurrent resume protection\n\u2502   \u2514\u2500\u2500 {job_id}.lock        # Lock file for job resume\n\u2514\u2500\u2500 logs/                     # Claude execution logs\n    \u2514\u2500\u2500 {repo_name}/          # Logs for specific repository\n        \u2514\u2500\u2500 {timestamp}/      # Log files by timestamp\n</code></pre>"},{"location":"mapreduce/global-storage-architecture/#storage-components","title":"Storage Components","text":""},{"location":"mapreduce/global-storage-architecture/#events","title":"Events","text":"<p>Event logs capture the complete lifecycle of MapReduce jobs in JSONL format (newline-delimited JSON):</p> <ul> <li>Path pattern: <code>~/.prodigy/events/{repo_name}/{job_id}/events-{timestamp}.jsonl</code></li> <li>Content: Agent lifecycle events (started, completed, failed), work item processing status, checkpoint saves, Claude messages, and error details</li> <li>Usage: Real-time monitoring, debugging agent failures, auditing job execution</li> <li>Cross-reference: See Event Tracking for detailed event types and usage</li> </ul>"},{"location":"mapreduce/global-storage-architecture/#dlq-dead-letter-queue","title":"DLQ (Dead Letter Queue)","text":"<p>Failed work items are stored with full context for retry and debugging:</p> <ul> <li>Path pattern: <code>~/.prodigy/dlq/{repo_name}/{job_id}/items/{item-id}.json</code></li> <li>Content: Original work item data, failure reason, retry count, timestamps, error context</li> <li>Usage: Review failed items, retry with <code>prodigy dlq retry</code>, analyze failure patterns</li> <li>Cross-reference: See Dead Letter Queue for DLQ operations and retry strategies</li> </ul>"},{"location":"mapreduce/global-storage-architecture/#state","title":"State","text":"<p>Job state and checkpoints enable resume and recovery:</p> <ul> <li>Path pattern: <code>~/.prodigy/state/{repo_name}/mapreduce/jobs/{job_id}/</code></li> <li>Content: Setup checkpoints, map phase progress, reduce phase state, job metadata</li> <li>Additional storage: Session-job ID mappings stored in <code>state/{repo_name}/mappings/</code> for bidirectional resume lookup (session ID \u2194 job ID)</li> <li>Usage: Resume interrupted jobs, track execution progress, recover from failures</li> <li>Cross-reference: See Checkpoint and Resume for checkpoint structure and resume behavior</li> </ul> <p>The state directory includes session-job ID mappings (src/storage/session_job_mapping.rs) that enable resume operations to work with either session IDs or job IDs, providing flexibility in workflow recovery.</p>"},{"location":"mapreduce/global-storage-architecture/#worktrees","title":"Worktrees","text":"<p>Isolated git worktrees for parallel execution:</p> <ul> <li>Path pattern: <code>~/.prodigy/worktrees/{repo_name}/session-{id}/</code></li> <li>Content: Git worktree for session/agent execution, temporary files, execution artifacts</li> <li>Usage: Isolated execution environment, parallel agent processing, clean separation from main repo</li> <li>Cross-reference: See MapReduce Worktree Architecture for worktree isolation details</li> </ul>"},{"location":"mapreduce/global-storage-architecture/#sessions","title":"Sessions","text":"<p>Unified session tracking for all workflow executions (src/unified_session/):</p> <ul> <li>Path pattern: <code>~/.prodigy/sessions/{session-id}.json</code></li> <li>Content: Session status (Running, Paused, Completed, Failed), timing data, workflow metadata, checkpoint references, execution progress</li> <li>Session types: Workflow sessions and MapReduce sessions with phase-specific data</li> <li>Usage: Track active sessions, resume interrupted workflows, monitor execution time, correlate with job IDs</li> <li>Integration: Works with session-job mappings for flexible resume operations</li> </ul> <p>The unified session system provides a single source of truth for all workflow executions, whether standard workflows or MapReduce jobs. Each session file contains complete metadata about the execution state, progress, and timing information.</p>"},{"location":"mapreduce/global-storage-architecture/#resume-locks","title":"Resume Locks","text":"<p>Concurrent resume protection to prevent conflicts:</p> <ul> <li>Path pattern: <code>~/.prodigy/resume_locks/{job_id}.lock</code></li> <li>Content: Process ID, hostname, acquisition timestamp, job/session ID</li> <li>Usage: Prevent multiple resume processes on same job, automatic stale lock cleanup</li> </ul>"},{"location":"mapreduce/global-storage-architecture/#repository-isolation","title":"Repository Isolation","text":"<p>Storage is automatically organized by repository name (extracted from your project path) to enable multiple projects to use global storage without conflicts.</p> <p>The repository name is determined using the <code>extract_repo_name()</code> function (src/storage/mod.rs:42-59), which: 1. Canonicalizes the project path to resolve symlinks 2. Extracts the final path component as the repository name 3. Example: <code>/path/to/my-project</code> \u2192 <code>my-project</code></p> <p>Key isolation features: - All storage paths include <code>{repo_name}</code> to isolate data between repositories - You can work on multiple Prodigy projects simultaneously without storage collisions - Each repository has independent events, DLQ, state, and worktrees - Repository names are consistent even when accessing via symlinks (due to canonicalization)</p>"},{"location":"mapreduce/global-storage-architecture/#configuration","title":"Configuration","text":""},{"location":"mapreduce/global-storage-architecture/#prodigy_home-environment-variable","title":"PRODIGY_HOME Environment Variable","text":"<p>The default global storage location (<code>~/.prodigy/</code>) can be overridden using the <code>PRODIGY_HOME</code> environment variable:</p> <pre><code># Use custom storage location\nexport PRODIGY_HOME=/mnt/fast-storage/prodigy\nprodigy run workflow.yml\n\n# Useful for testing with isolated storage\nexport PRODIGY_HOME=/tmp/prodigy-test\nprodigy run test-workflow.yml\n</code></pre> <p>This is particularly useful for: - Testing: Isolate test runs from production data - Custom deployments: Use specific storage locations (network mounts, SSDs, etc.) - Multi-user systems: Separate storage per user or team - CI/CD: Use temporary storage that's cleaned up after runs</p>"},{"location":"mapreduce/global-storage-architecture/#examples","title":"Examples","text":""},{"location":"mapreduce/global-storage-architecture/#inspecting-global-storage","title":"Inspecting Global Storage","text":"<p>Check disk usage of global storage: <pre><code>du -sh ~/.prodigy/*\n# Output:\n# 150M    /Users/you/.prodigy/events\n# 25M     /Users/you/.prodigy/dlq\n# 80M     /Users/you/.prodigy/state\n# 200M    /Users/you/.prodigy/worktrees\n</code></pre></p> <p>Find all data for a specific job: <pre><code># Find events\nls ~/.prodigy/events/my-repo/mapreduce-20250111_120000/\n\n# Check for DLQ items\nls ~/.prodigy/dlq/my-repo/mapreduce-20250111_120000/items/\n\n# View checkpoints\nls ~/.prodigy/state/my-repo/mapreduce/jobs/mapreduce-20250111_120000/\n</code></pre></p> <p>List all repositories using global storage: <pre><code>ls ~/.prodigy/events/\n# Output:\n# my-project/\n# another-repo/\n# test-project/\n</code></pre></p>"},{"location":"mapreduce/global-storage-architecture/#storage-maintenance","title":"Storage Maintenance","text":"<p>Clean up old job data: <pre><code># Remove old event logs (older than 30 days)\nfind ~/.prodigy/events -type d -mtime +30 -exec rm -rf {} +\n\n# Clear processed DLQ items for a workflow\nprodigy dlq clear &lt;workflow_id&gt;\n\n# Purge old DLQ items (older than N days)\nprodigy dlq purge --older-than-days 30\n\n# Remove orphaned worktrees after failed cleanup\nprodigy worktree clean-orphaned &lt;job_id&gt;\n</code></pre></p> <p>Monitor storage and sessions: <pre><code># List active sessions and their status\nprodigy sessions list\n\n# Show details about a specific session\nprodigy sessions show &lt;session_id&gt;\n\n# View DLQ statistics for a workflow\nprodigy dlq stats --workflow-id &lt;workflow_id&gt;\n\n# Analyze failure patterns in DLQ\nprodigy dlq analyze --job-id &lt;job_id&gt;\n\n# Check resume locks (detect stuck jobs)\nls ~/.prodigy/resume_locks/\n\n# List all repositories using global storage\nls ~/.prodigy/events/\n</code></pre></p> <p>Available DLQ Commands (src/cli/args.rs:578-675): - <code>list</code> - List items in the Dead Letter Queue - <code>inspect</code> - Inspect a specific DLQ item - <code>analyze</code> - Analyze failure patterns - <code>export</code> - Export DLQ items to a file - <code>purge</code> - Purge old items from the DLQ - <code>retry</code> - Retry failed items - <code>stats</code> - Show DLQ statistics - <code>clear</code> - Clear processed items from DLQ</p> <p>Note: The <code>health_check()</code> method exists in the GlobalStorage implementation (src/storage/global.rs:115) but is not currently exposed as a CLI command. It's used internally for programmatic health verification.</p>"},{"location":"mapreduce/overview/","title":"MapReduce Workflows Overview","text":"<p>MapReduce workflows enable parallel processing of large datasets by distributing work items across isolated agents. This powerful execution mode allows you to process hundreds or thousands of items concurrently, making it ideal for bulk operations, data migrations, and large-scale code transformations.</p>"},{"location":"mapreduce/overview/#what-is-mapreduce-mode","title":"What is MapReduce Mode?","text":"<p>MapReduce is an execution mode in Prodigy that follows the classic MapReduce pattern:</p> <ol> <li>Setup Phase (optional): Prepare work items and initialize the environment</li> <li>Map Phase: Process work items in parallel across isolated agents</li> <li>Reduce Phase: Aggregate results and perform final operations</li> </ol> <p>Each phase executes in an isolated git worktree, ensuring your main repository remains untouched during execution. Agent results are automatically merged, and the final changes are presented for your approval before merging to your original branch.</p>"},{"location":"mapreduce/overview/#when-to-use-mapreduce","title":"When to Use MapReduce","text":"<p>Use MapReduce workflows when:</p> <ul> <li>Processing 10+ independent items (e.g., updating copyright headers in 500 files)</li> <li>Each item takes significant time (e.g., running linting on each module)</li> <li>Operations can be parallelized safely (no shared state dependencies)</li> <li>You need fault isolation per item (one failure doesn't block others)</li> </ul> <p>Use standard workflows for:</p> <ul> <li>Sequential operations that depend on previous results</li> <li>Single-item processing</li> <li>Workflows requiring shared mutable state</li> <li>Operations that must execute in a specific order</li> </ul> <p>Performance Sweet Spot</p> <p>MapReduce excels when you have 10-1000 work items that each take 10 seconds to 5 minutes to process. For smaller workloads, the overhead of parallelization may not be worthwhile.</p>"},{"location":"mapreduce/overview/#the-three-phases","title":"The Three Phases","text":"<pre><code>graph TD\n    Start[Workflow Start] --&gt; Setup[Setup Phase&lt;br/&gt;Generate work items]\n    Setup --&gt; Extract[Extract Items&lt;br/&gt;Apply filter, sort]\n    Extract --&gt; Map[Map Phase&lt;br/&gt;Parallel Processing]\n\n    Map --&gt; A1[Agent 1&lt;br/&gt;Item 0]\n    Map --&gt; A2[Agent 2&lt;br/&gt;Item 1]\n    Map --&gt; A3[Agent 3&lt;br/&gt;Item 2]\n    Map --&gt; AN[Agent N&lt;br/&gt;Item N]\n\n    A1 --&gt;|Success| Merge[Merge Results]\n    A2 --&gt;|Success| Merge\n    A3 --&gt;|Success| Merge\n    AN --&gt;|Success| Merge\n\n    A1 --&gt;|Failure| DLQ[Dead Letter Queue]\n    A2 --&gt;|Failure| DLQ\n    A3 --&gt;|Failure| DLQ\n    AN --&gt;|Failure| DLQ\n\n    Merge --&gt; Reduce[Reduce Phase&lt;br/&gt;Aggregate &amp; Report]\n    Reduce --&gt; Complete[Workflow Complete]\n\n    DLQ -.-&gt;|Retry| Map\n\n    style Setup fill:#e1f5ff\n    style Map fill:#fff3e0\n    style Reduce fill:#f3e5f5\n    style DLQ fill:#ffebee\n    style Complete fill:#e8f5e9</code></pre> <p>Figure: MapReduce execution flow showing the three phases, parallel agent processing, and error handling through the Dead Letter Queue.</p>"},{"location":"mapreduce/overview/#setup-phase","title":"Setup Phase","text":"<p>The optional setup phase prepares your environment and generates work items:</p> <pre><code># Source: workflows/mapreduce-example.yml:5-6\nsetup:\n  - shell: \"debtmap analyze . --output debt_items.json\"\n</code></pre> <p>Key capabilities:</p> <ul> <li>Execute commands before parallel processing begins</li> <li>Generate JSON files containing work items</li> <li>Capture outputs for use in later phases</li> <li>Initialize databases, fetch data, or prepare resources</li> </ul> <p>The setup phase runs once in the MapReduce worktree and creates a checkpoint for resume.</p>"},{"location":"mapreduce/overview/#map-phase","title":"Map Phase","text":"<p>The map phase distributes work items to parallel agents:</p> <pre><code># Source: workflows/mapreduce-example.yml:9-27\nmap:\n  input: debt_items.json                # (1)!\n  json_path: \"$.debt_items[*]\"          # (2)!\n\n  agent_template:                        # (3)!\n    - claude: \"/fix-issue ${item.description} --file ${item.location.file}\"\n    - shell: \"cargo test\"\n      on_failure:                        # (4)!\n        claude: \"/debug-test ${shell.output}\"\n\n  max_parallel: 10                       # (5)!\n  filter: \"severity == 'high' || severity == 'critical'\"  # (6)!\n  sort_by: \"priority\"                    # (7)!\n\n1. JSON file containing work items (generated in setup phase)\n2. JSONPath expression to extract items from the JSON structure\n3. Template commands executed by each agent for its work item\n4. Fallback commands executed if the previous command fails\n5. Maximum number of agents running concurrently (controls parallelism)\n6. Only process items matching this condition (reduces workload)\n7. Process items in this order (e.g., high-priority items first)\n</code></pre> <p>Key capabilities:</p> <ul> <li>JSONPath extraction: Extract work items from JSON files</li> <li>Parallel execution: Run multiple agents concurrently (controlled by <code>max_parallel</code>)</li> <li>Agent isolation: Each agent runs in its own git worktree</li> <li>Filtering: Process only items matching criteria</li> <li>Sorting: Control processing order</li> <li>Error handling: Failed items route to Dead Letter Queue (DLQ)</li> </ul> <p>Each agent executes the <code>agent_template</code> commands with access to <code>${item}</code> variables. Agents commit their changes and merge back to the MapReduce worktree automatically.</p>"},{"location":"mapreduce/overview/#reduce-phase","title":"Reduce Phase","text":"<p>The reduce phase aggregates results after all map agents complete:</p> <pre><code># Source: workflows/mapreduce-example.yml:30-39\nreduce:\n  - claude: \"/summarize-fixes ${map.results}\"\n    capture_output: true\n\n  - shell: \"echo 'Fixed ${map.successful}/${map.total} items'\"\n\n  - claude: \"/generate-report --fixed ${map.successful} --failed ${map.failed}\"\n</code></pre> <p>Key capabilities:</p> <ul> <li>Result aggregation: Access to <code>${map.results}</code>, <code>${map.successful}</code>, <code>${map.failed}</code>, <code>${map.total}</code></li> <li>Sequential execution: Commands run in order</li> <li>Report generation: Summarize outcomes, generate artifacts</li> <li>Validation: Verify all items were processed correctly</li> </ul> <p>The reduce phase runs once in the MapReduce worktree with full visibility into map phase results.</p>"},{"location":"mapreduce/overview/#work-items-and-distribution","title":"Work Items and Distribution","text":""},{"location":"mapreduce/overview/#what-are-work-items","title":"What are Work Items?","text":"<p>Work items are individual units of work extracted from JSON files. Each item is processed by exactly one agent in the map phase.</p> <p>Example work items:</p> <pre><code>[\n  {\"id\": 1, \"file\": \"src/main.rs\", \"issue\": \"unused import\"},\n  {\"id\": 2, \"file\": \"src/lib.rs\", \"issue\": \"missing docs\"},\n  {\"id\": 3, \"file\": \"tests/test.rs\", \"issue\": \"deprecated API\"}\n]\n</code></pre>"},{"location":"mapreduce/overview/#work-distribution-features","title":"Work Distribution Features","text":"<p>JSONPath Extraction:</p> <pre><code>map:\n  input: \"items.json\"\n  json_path: \"$.items[*]\"  # Extract array of items\n</code></pre> <p>Filtering:</p> <pre><code>filter: \"item.severity &gt;= 5 &amp;&amp; item.status == 'open'\"\n</code></pre> <p>Sorting:</p> <pre><code>sort_by: \"item.priority DESC\"  # Process high-priority items first\n</code></pre> <p>Pagination:</p> <pre><code>max_items: 100   # Limit to first 100 items\noffset: 50       # Skip first 50 items\n</code></pre> <p>Deduplication:</p> <pre><code>distinct: \"item.id\"  # Remove duplicate items by ID field\n</code></pre>"},{"location":"mapreduce/overview/#parallel-execution-and-isolation","title":"Parallel Execution and Isolation","text":""},{"location":"mapreduce/overview/#how-parallel-execution-works","title":"How Parallel Execution Works","text":"<p>When the map phase begins, Prodigy:</p> <ol> <li>Extracts work items from the input JSON</li> <li>Applies filtering, sorting, and deduplication</li> <li>Creates a pool of agent slots (limited by <code>max_parallel</code>)</li> <li>Assigns work items to available agents</li> <li>Each agent runs in an isolated git worktree</li> <li>Agents execute independently and commit their changes</li> <li>Completed agents merge back to the MapReduce worktree</li> <li>Failed items are routed to the Dead Letter Queue</li> </ol>"},{"location":"mapreduce/overview/#agent-isolation","title":"Agent Isolation","text":"<p>Each agent runs in its own git worktree branched from the MapReduce worktree:</p> <pre><code>graph TD\n    Main[Original Branch&lt;br/&gt;main/feature-xyz] --&gt; Parent[MapReduce Worktree&lt;br/&gt;session-abc123]\n\n    Parent --&gt; A1[Agent Worktree 1&lt;br/&gt;agent-1-item-1]\n    Parent --&gt; A2[Agent Worktree 2&lt;br/&gt;agent-2-item-2]\n    Parent --&gt; A3[Agent Worktree 3&lt;br/&gt;agent-3-item-3]\n    Parent --&gt; AN[Agent Worktree N&lt;br/&gt;agent-N-item-N]\n\n    A1 --&gt;|Merge Changes| Parent\n    A2 --&gt;|Merge Changes| Parent\n    A3 --&gt;|Merge Changes| Parent\n    AN --&gt;|Merge Changes| Parent\n\n    Parent -.-&gt;|User Approval| Main\n\n    style Main fill:#e8f5e9\n    style Parent fill:#e1f5ff\n    style A1 fill:#fff3e0\n    style A2 fill:#fff3e0\n    style A3 fill:#fff3e0\n    style AN fill:#fff3e0</code></pre> <p>Figure: Worktree isolation architecture showing how agents branch from the parent MapReduce worktree and merge their changes back independently.</p> <p>Benefits of isolation:</p> <ul> <li>No conflicts: Agents can't interfere with each other</li> <li>Independent failures: One agent failure doesn't affect others</li> <li>Clean merges: Each agent's changes merge independently</li> <li>Resource safety: No shared mutable state</li> </ul> <p>Cleanup Failure Handling</p> <p>If worktree cleanup fails after agent completion, the agent's work is still preserved. Failed cleanups are tracked in an orphaned worktree registry and can be retried with: <pre><code>prodigy worktree clean-orphaned &lt;job-id&gt;\n</code></pre> See Spec 136 for details.</p>"},{"location":"mapreduce/overview/#controlling-parallelism","title":"Controlling Parallelism","text":"<pre><code># Static parallelism\nmax_parallel: 10\n\n# Dynamic parallelism from environment\nmax_parallel: \"$MAX_WORKERS\"\n</code></pre> <p>Resource Limits</p> <p>Each agent consumes system resources (CPU, memory, disk I/O). Start with conservative <code>max_parallel</code> values and increase based on system capacity. A good starting point is 5-10 agents.</p>"},{"location":"mapreduce/overview/#example-bulk-code-updates","title":"Example: Bulk Code Updates","text":"<p>When to Use This Pattern</p> <p>This example demonstrates a common MapReduce use case: applying the same transformation to many files independently. Use this pattern for bulk updates like copyright headers, import statements, formatting changes, or license headers where each file can be processed in isolation.</p> <p>Here's a complete example updating copyright headers across a codebase:</p> <pre><code># Source: workflows/mapreduce-example.yml (adapted)\nname: update-copyright-headers\nmode: mapreduce\n\nsetup:\n  - shell: |\n      # Generate list of source files\n      find src -name \"*.rs\" -type f | jq -R -s -c 'split(\"\\n\")[:-1] | map({file: .})' &gt; files.json\n\nmap:\n  input: files.json\n  json_path: \"$[*]\"\n  max_parallel: 20\n\n  agent_template:\n    - shell: |\n        # Update copyright header\n        sed -i '1s/Copyright 2023/Copyright 2024/' ${item.file}\n        git add ${item.file}\n        git commit -m \"Update copyright in ${item.file}\"\n\nreduce:\n  - shell: |\n      echo \"Updated copyright headers in ${map.successful}/${map.total} files\"\n\n  - shell: |\n      if [ ${map.failed} -gt 0 ]; then\n        echo \"Failed files: ${map.failed}\"\n        echo \"Check DLQ: prodigy dlq show &lt;job-id&gt;\"\n      fi\n</code></pre>"},{"location":"mapreduce/overview/#error-handling-and-recovery","title":"Error Handling and Recovery","text":""},{"location":"mapreduce/overview/#dead-letter-queue-dlq","title":"Dead Letter Queue (DLQ)","text":"<p>Failed work items are automatically sent to the DLQ with:</p> <ul> <li>Original work item data</li> <li>Failure reason and error message</li> <li>Timestamp and retry count</li> <li>Claude JSON log location for debugging (via <code>json_log_location</code> field)</li> </ul> <p>Debugging with JSON Logs</p> <p>Each DLQ item includes a <code>json_log_location</code> field pointing to the Claude execution log. Use this to inspect the complete conversation, tool invocations, and error context: <pre><code># View the log location\nprodigy dlq show &lt;job-id&gt; | jq '.items[].failure_history[].json_log_location'\n\n# Inspect the log file\ncat ~/.local/state/claude/logs/session-xyz.json | jq\n</code></pre></p> <p>Retry failed items:</p> <pre><code>prodigy dlq show &lt;job-id&gt;\nprodigy dlq retry &lt;job-id&gt; --max-parallel 5\n</code></pre>"},{"location":"mapreduce/overview/#checkpoint-and-resume","title":"Checkpoint and Resume","text":"<p>MapReduce workflows create checkpoints after each phase:</p> <ul> <li>Setup checkpoint: Setup phase results and artifacts</li> <li>Map checkpoint: Work item states and agent results</li> <li>Reduce checkpoint: Completed reduce steps and variables</li> </ul> <p>Resume an interrupted workflow:</p> <pre><code>prodigy resume &lt;session-id&gt;\n# or\nprodigy resume-job &lt;job-id&gt;\n</code></pre> <p>All completed work is preserved. In-progress items restart from the beginning.</p> <p>Concurrent Resume Protection</p> <p>Resume operations are protected from concurrent execution using automatic lock management. If another process is already resuming the same job, you'll receive an error with details about the lock holder. Stale locks (from crashed processes) are automatically detected and cleaned up. See Spec 140 for details.</p>"},{"location":"mapreduce/overview/#learn-more","title":"Learn More","text":"<p>Ready to dive deeper? Explore these topics:</p> <ul> <li>Work Distribution - JSONPath, filtering, sorting, and deduplication</li> <li>Checkpoint and Resume - State management and recovery</li> <li>Dead Letter Queue (DLQ) - Error handling and retry strategies</li> <li>Event Tracking - Monitoring and debugging with event logs</li> <li>Environment Variables - Parameterization and secrets</li> <li>Global Storage Architecture - Cross-repository state management</li> </ul>"},{"location":"mapreduce/overview/#next-steps","title":"Next Steps","text":"<ol> <li>Try the Quick Start - Run the minimal example in MapReduce Workflows</li> <li>Understand Your Use Case - Identify work items and parallelization opportunities</li> <li>Start Small - Begin with <code>max_parallel: 5</code> and a subset of items</li> <li>Monitor and Tune - Use event tracking and DLQ to optimize performance</li> <li>Scale Up - Increase parallelism based on system capacity</li> </ol> <p>Best Practice</p> <p>Always test MapReduce workflows with a small subset of items first (using <code>max_items: 10</code>) before running on the full dataset. This helps identify issues early and validates your workflow logic.</p>"},{"location":"mapreduce/setup-phase-advanced/","title":"Setup Phase (Advanced)","text":""},{"location":"mapreduce/setup-phase-advanced/#setup-phase-advanced","title":"Setup Phase (Advanced)","text":"<p>The setup phase runs once before the map phase begins, executing in the parent worktree. It's used to initialize the environment, generate work items, download data, or prepare configuration.</p>"},{"location":"mapreduce/setup-phase-advanced/#execution-context","title":"Execution Context","text":"<p>The setup phase: - Runs once before map phase begins - Executes in parent worktree, providing isolation from main repository (not the main repository itself) - Creates checkpoint after successful completion, stored at <code>~/.prodigy/state/{repo_name}/mapreduce/jobs/{job_id}/setup-checkpoint.json</code> - Preserves setup outputs, environment state, and captured variables for resume capability - Outputs available to map and reduce phases via captured variables</p> <p>This isolation ensures the main repository remains untouched while setup operations prepare the environment for parallel processing. All setup operations execute in the parent worktree, providing safety for initialization tasks.</p>"},{"location":"mapreduce/setup-phase-advanced/#common-use-cases","title":"Common Use Cases","text":"<p>The setup phase is typically used for:</p> <ul> <li>Generate work items - Create JSON arrays of items to process in parallel</li> <li>Initialize environment - Install dependencies, configure tools, set up databases</li> <li>Download data - Fetch datasets, clone repositories, pull artifacts</li> <li>Prepare configuration - Generate configs, resolve templates, validate settings</li> </ul>"},{"location":"mapreduce/setup-phase-advanced/#configuration-formats","title":"Configuration Formats","text":"<p>The setup phase supports two formats: simple array OR full configuration object.</p> <pre><code># Simple array format\nsetup:\n  - shell: \"prepare-data.sh\"\n  - shell: \"analyze-codebase.sh\"\n\n# Full configuration format with timeout and capture\nsetup:\n  commands:\n    - shell: \"prepare-data.sh\"\n    - shell: \"analyze-codebase.sh\"\n\n  # Timeout for entire setup phase\n  # Accepts numeric seconds (300) or string with env var (\"${SETUP_TIMEOUT}\")\n  timeout: 300  # or timeout: \"${SETUP_TIMEOUT}\"\n\n  # Capture outputs from setup commands\n  capture_outputs:\n    # Simple format (shorthand - captures stdout with defaults)\n    # Use for basic stdout capture without JSON extraction\n    file_count: 0  # Capture stdout from command at index 0\n\n    # Detailed CaptureConfig format\n    # Use for JSON extraction, size limits, or custom sources\n    analysis_result:\n      command_index: 1\n      source: stdout           # stdout, stderr, both, combined (see below)\n      json_path: \"$.result\"    # Extract JSON field\n      pattern: \"^Result: (.+)$\" # Optional: Extract using regex capture group\n      max_size: 1048576        # Max bytes (1MB)\n      default: \"{}\"            # Fallback if extraction fails\n      multiline: preserve      # preserve, join, first_line, last_line, array (see below)\n</code></pre> <p>Setup Phase Fields: - <code>commands</code> - Array of commands to execute (or use simple array format at top level) - <code>timeout</code> - Timeout for entire setup phase in seconds (numeric or environment variable) - <code>capture_outputs</code> - Map of variable names to command outputs (Simple or Detailed format)</p> <p>CaptureConfig Fields (Detailed Format): - <code>command_index</code> - Which command's output to capture (0-based index) - <code>source</code> - Which output stream to capture (see Capture Source Options below) - <code>pattern</code> - Optional regex pattern with capture group to extract specific text - <code>json_path</code> - Optional JSON path to extract field from JSON output (e.g., \"$.result.value\") - <code>max_size</code> - Maximum bytes to capture (default: 1MB) - <code>default</code> - Fallback value if capture/extraction fails - <code>multiline</code> - How to handle multi-line output (see Multiline Options below)</p>"},{"location":"mapreduce/setup-phase-advanced/#capture-source-options","title":"Capture Source Options","text":"<p>The <code>source</code> field controls which command output streams to capture (src/cook/execution/variable_capture.rs:96-103):</p> <ul> <li><code>stdout</code> (default) - Capture standard output only</li> <li><code>stderr</code> - Capture standard error only</li> <li><code>both</code> - Capture both streams with labels: <code>\"stdout:\\n{}\\nstderr:\\n{}\"</code></li> <li><code>combined</code> - Capture both streams concatenated without labels (interleaved order)</li> </ul> <p>Use Cases: - <code>stdout</code> - For normal command output (logs, data, results) - <code>stderr</code> - For error messages or diagnostic output - <code>both</code> - When you need to see both streams separately with clear labels - <code>combined</code> - When you need chronological order of all output (e.g., debugging)</p> <p>Example from tests (src/cook/execution/variable_capture_test.rs:210): <pre><code>error_log:\n  command_index: 0\n  source: stderr    # Capture error messages\n  default: \"No errors\"\n</code></pre></p>"},{"location":"mapreduce/setup-phase-advanced/#multiline-options","title":"Multiline Options","text":"<p>The <code>multiline</code> field controls how multi-line output is processed (src/cook/execution/variable_capture.rs:114-129):</p> Option Behavior Use Case <code>preserve</code> (default) Keep all lines with newlines Full output needed <code>join</code> Join lines with spaces Create single-line summary <code>first_line</code> Take only first line Extract header or title <code>last_line</code> Take only last line Get final result or status <code>array</code> Return as JSON array of lines Process each line separately <p>Examples from tests (src/cook/execution/variable_capture_test.rs:103-145):</p> <pre><code># Extract first line (e.g., version number)\nversion:\n  command_index: 0\n  source: stdout\n  multiline: first_line\n\n# Extract last line (e.g., final status)\nstatus:\n  command_index: 1\n  source: stdout\n  multiline: last_line\n\n# Join all lines (e.g., single-line summary)\nsummary:\n  command_index: 2\n  source: stdout\n  multiline: join\n\n# Array format (e.g., list of files)\nfiles:\n  command_index: 3\n  source: stdout\n  multiline: array\n</code></pre>"},{"location":"mapreduce/setup-phase-advanced/#pattern-extraction","title":"Pattern Extraction","text":"<p>Use the <code>pattern</code> field to extract specific text using regex capture groups (src/cook/execution/variable_capture.rs:29):</p> <p>Example from tests (src/cook/execution/variable_capture_test.rs:35): <pre><code>capture_outputs:\n  VERSION:\n    command_index: 0\n    source: stdout\n    pattern: \"version (\\\\d+\\\\.\\\\d+\\\\.\\\\d+)\"  # Extract semantic version\n    default: \"0.0.0\"\n</code></pre></p> <p>The pattern uses regex capture groups. The first capture group <code>(...)</code> is extracted as the variable value.</p>"},{"location":"mapreduce/setup-phase-advanced/#best-practices","title":"Best Practices","text":"<p>Idempotent Operations: - Design setup commands to be safe to run multiple times - Use conditional checks before creating resources - Clean up stale artifacts before generating new ones</p> <p>Timeout Sizing: - Set generous timeouts for network operations (downloads, API calls) - Use environment variables (<code>${SETUP_TIMEOUT}</code>) for flexibility across environments - Consider total time for all setup commands, not individual commands</p> <p>Output Capture Patterns: - Use simple format (<code>command_index: number</code>) for basic text capture - Use detailed <code>CaptureConfig</code> when extracting JSON fields or limiting size - Always provide <code>default</code> value for robust error handling</p> <p>Setup Command Failures: - If a setup command fails, the entire workflow stops (src/cook/execution/mapreduce/phases/setup.rs:39-43) - Design setup commands to fail fast with clear error messages - Use <code>default</code> values in <code>capture_outputs</code> to handle optional data gracefully - Test setup commands independently before workflow execution</p>"},{"location":"mapreduce/setup-phase-advanced/#see-also","title":"See Also","text":"<ul> <li>Environment Variables in Configuration - Using env vars in timeout and commands</li> <li>Checkpoint and Resume - How setup checkpoints enable resume</li> <li>Dead Letter Queue (DLQ) - Handling setup phase failures</li> <li>Global Storage Architecture - Understanding checkpoint storage locations</li> <li>MapReduce Overview - Complete workflow format and phase documentation</li> </ul>"},{"location":"mapreduce/work-distribution/","title":"Work Distribution","text":"<p>Work distribution is the process of extracting, filtering, sorting, and distributing work items to parallel agents in MapReduce workflows. The data pipeline provides powerful capabilities for selecting and organizing work items from various input sources.</p>"},{"location":"mapreduce/work-distribution/#overview","title":"Overview","text":"<p>The work distribution system processes data through a multi-stage pipeline:</p> <ol> <li>Input Source - Load data from JSON files or command output</li> <li>JSONPath Extraction - Extract work items from nested structures</li> <li>Filtering - Select items matching criteria</li> <li>Sorting - Order items by priority or other fields</li> <li>Deduplication - Remove duplicate items</li> <li>Pagination - Apply offset and limit for testing or batching</li> </ol> <p>Each stage is optional and can be configured independently to build the exact work distribution strategy you need.</p> <pre><code>flowchart LR\n    Input[Input Source&lt;br/&gt;JSON file or command] --&gt; JSONPath[JSONPath Extraction&lt;br/&gt;$.items[*]]\n    JSONPath --&gt; Filter[Filtering&lt;br/&gt;score &gt;= 5]\n    Filter --&gt; Sort[Sorting&lt;br/&gt;priority DESC]\n    Sort --&gt; Dedup[Deduplication&lt;br/&gt;distinct: id]\n    Dedup --&gt; Offset[Offset&lt;br/&gt;skip first N]\n    Offset --&gt; Limit[Limit&lt;br/&gt;take M items]\n    Limit --&gt; Agents[Distribute to&lt;br/&gt;Parallel Agents]\n\n    style Input fill:#e1f5ff\n    style JSONPath fill:#fff3e0\n    style Filter fill:#f3e5f5\n    style Sort fill:#e8f5e9\n    style Dedup fill:#fff3e0\n    style Offset fill:#f3e5f5\n    style Limit fill:#e1f5ff\n    style Agents fill:#ffebee</code></pre> <p>Figure: Work distribution pipeline showing data flow from input source through transformation stages to parallel agents.</p>"},{"location":"mapreduce/work-distribution/#input-sources","title":"Input Sources","text":"<p>Work items can be loaded from two types of input sources:</p>"},{"location":"mapreduce/work-distribution/#json-files","title":"JSON Files","text":"<p>The most common approach is to use a JSON file containing work items. Specify the file path using the <code>input</code> field in the map phase:</p> <pre><code># Source: workflows/mapreduce-example.yml\nmap:\n  input: debt_items.json\n  json_path: \"$.debt_items[*]\"\n</code></pre>"},{"location":"mapreduce/work-distribution/#command-output","title":"Command Output","text":"<p>You can also generate work items dynamically using a command:</p> <pre><code># Source: workflows/documentation-drift-mapreduce.yml\nsetup:\n  - shell: |\n      cat &gt; .prodigy/doc-areas.json &lt;&lt; 'EOF'\n      {\n        \"areas\": [\n          {\"name\": \"README\", \"pattern\": \"README.md\", \"priority\": \"high\"}\n        ]\n      }\n      EOF\n\nmap:\n  input: .prodigy/doc-areas.json\n  json_path: \"$.areas[*]\"\n</code></pre> <p>Tip</p> <p>Generate work items in the setup phase and save them to a JSON file. This allows you to preview the items before processing and ensures consistent inputs if you need to resume the workflow.</p>"},{"location":"mapreduce/work-distribution/#jsonpath-extraction","title":"JSONPath Extraction","text":"<p>JSONPath expressions let you extract work items from complex nested JSON structures. Use the <code>json_path</code> field to specify an extraction pattern:</p> <pre><code># Source: workflows/mapreduce-example.yml\nmap:\n  input: data.json\n  json_path: \"$.items[*]\"\n</code></pre>"},{"location":"mapreduce/work-distribution/#common-jsonpath-patterns","title":"Common JSONPath Patterns","text":"<p>Common JSONPath Patterns</p> <p>Extract all array elements: <pre><code>json_path: \"$.items[*]\"\n</code></pre></p> <p>Extract from nested structure: <pre><code>json_path: \"$.data.results[*]\"\n</code></pre></p> <p>Extract specific field from each element: <pre><code>json_path: \"$.items[*].name\"\n</code></pre></p>"},{"location":"mapreduce/work-distribution/#how-jsonpath-works","title":"How JSONPath Works","text":"<p>The JSONPath expression is applied to the input data and returns an array of matching items. Each item becomes a work item processed by an agent.</p> <pre><code>{\n  \"items\": [\n    {\"id\": 1, \"priority\": 5},\n    {\"id\": 2, \"priority\": 3}\n  ]\n}\n</code></pre> <p>With <code>json_path: \"$.items[*]\"</code>, two work items are created, one for each array element.</p> <p>Note</p> <p>If no JSONPath is specified, the entire input is treated as either an array (if it's a JSON array) or a single work item (for other JSON types).</p>"},{"location":"mapreduce/work-distribution/#filtering","title":"Filtering","text":"<p>Filters let you selectively process work items based on boolean expressions. Use the <code>filter</code> field to specify selection criteria:</p> <pre><code># Source: workflows/mapreduce-example.yml\nmap:\n  filter: \"severity == 'high' || severity == 'critical'\"\n</code></pre>"},{"location":"mapreduce/work-distribution/#filter-syntax","title":"Filter Syntax","text":"<p>Filters support comparison operators, logical operators, and nested field access:</p> <p>Comparison operators: <pre><code># Equality\nfilter: \"status == 'active'\"\nfilter: \"status = 'active'\"  # (1)!\n\n# Inequality\nfilter: \"status != 'archived'\"\n\n# Numeric comparison\nfilter: \"priority &gt; 5\"\nfilter: \"priority &gt;= 5\"  # (2)!\nfilter: \"priority &lt; 10\"\nfilter: \"priority &lt;= 10\"\n\n1. Single `=` also works for equality checks\n2. Inclusive comparison - items with priority of 5 will be included\n</code></pre></p> <p>Logical operators: <pre><code># AND\nfilter: \"severity == 'high' &amp;&amp; priority &gt; 5\"\nfilter: \"severity == 'high' AND priority &gt; 5\"  # (1)!\n\n# OR\nfilter: \"severity == 'high' || severity == 'critical'\"\nfilter: \"severity == 'high' OR severity == 'critical'\"  # (2)!\n\n# NOT\nfilter: \"!(status == 'archived')\"\nfilter: \"!is_null(optional_field)\"\n\n1. Word-based `AND` operator is also supported\n2. Word-based `OR` operator is also supported\n</code></pre></p> <p>Nested field access: <pre><code># Source: src/cook/execution/data_pipeline/mod.rs:298-300\nfilter: \"unified_score.final_score &gt;= 5\"\nfilter: \"location.coordinates.lat &gt; 40.0\"\n</code></pre></p> <p>IN operator: <pre><code>filter: \"severity in ['high', 'critical']\"\nfilter: \"status in ['active', 'pending']\"\n</code></pre></p>"},{"location":"mapreduce/work-distribution/#filter-functions","title":"Filter Functions","text":"<p>Advanced filtering with built-in functions:</p> <pre><code># Null checks\nfilter: \"is_null(optional_field)\"\nfilter: \"is_not_null(required_field)\"\n\n# Type checks\nfilter: \"is_number(score)\"\nfilter: \"is_string(name)\"\nfilter: \"is_bool(active)\"\nfilter: \"is_array(tags)\"\nfilter: \"is_object(metadata)\"\n\n# String operations\nfilter: \"contains(name, 'test')\"\nfilter: \"starts_with(path, '/usr')\"\nfilter: \"ends_with(filename, '.rs')\"\n\n# Length checks\nfilter: \"length(tags) == 3\"\n\n# Regex matching\nfilter: \"matches(email, '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$')\"\n</code></pre>"},{"location":"mapreduce/work-distribution/#complex-filter-examples","title":"Complex Filter Examples","text":"<p>Combine multiple conditions: <pre><code># Source: src/cook/execution/data_pipeline/mod.rs:793-795\nfilter: \"unified_score.final_score &gt;= 5 &amp;&amp; debt_type.category == 'complexity'\"\n</code></pre></p> <p>Filter with type safety: <pre><code>filter: \"is_number(score) &amp;&amp; score &gt; 50\"\n</code></pre></p> <p>Pattern matching on file paths: <pre><code>filter: \"matches(path, '\\\\.rs$')\"  # Only Rust files\n</code></pre></p> <p>Warning</p> <p>When a field doesn't exist, most comparisons evaluate to <code>false</code>. Use <code>is_null()</code> or <code>is_not_null()</code> functions for explicit null checks if field presence is important.</p>"},{"location":"mapreduce/work-distribution/#sorting","title":"Sorting","text":"<p>Sort work items to control processing order. Use the <code>sort_by</code> field to specify one or more sort fields:</p> <pre><code># Source: workflows/mapreduce-example.yml\nmap:\n  sort_by: \"priority DESC\"\n</code></pre>"},{"location":"mapreduce/work-distribution/#sort-syntax","title":"Sort Syntax","text":"<p>Single field ascending: <pre><code>sort_by: \"created_at ASC\"\n</code></pre></p> <p>Single field descending: <pre><code>sort_by: \"priority DESC\"\n</code></pre></p> <p>Multiple fields: <pre><code># Source: src/cook/execution/data_pipeline/mod.rs:1498\nsort_by: \"severity DESC, priority ASC\"\n</code></pre></p> <p>Nested fields: <pre><code># Source: src/cook/execution/data_pipeline/mod.rs:1559\nsort_by: \"unified_score.final_score DESC\"\n</code></pre></p>"},{"location":"mapreduce/work-distribution/#null-handling","title":"Null Handling","text":"<p>By default, null values are sorted last regardless of sort direction (NULLS LAST). You can control this behavior explicitly:</p> <pre><code># Nulls come last (default)\nsort_by: \"score DESC NULLS LAST\"\n\n# Nulls come first\nsort_by: \"score DESC NULLS FIRST\"\n</code></pre> <p>Example with mixed types: <pre><code># Source: src/cook/execution/data_pipeline/mod.rs:1523-1526\nsort_by: \"File.score DESC NULLS LAST, Function.unified_score.final_score DESC NULLS LAST\"\n</code></pre></p> <p>This sorts items where <code>File.score</code> exists first (by score descending), then items where <code>Function.unified_score.final_score</code> exists (by score descending). Items missing both fields come last.</p> <p>Tip</p> <p>Use <code>NULLS LAST</code> (default) to prioritize items with values. Use <code>NULLS FIRST</code> when you want to handle missing data items first.</p>"},{"location":"mapreduce/work-distribution/#pagination","title":"Pagination","text":"<p>Control the number of items processed using offset and limit:</p>"},{"location":"mapreduce/work-distribution/#limit-max_items","title":"Limit (max_items)","text":"<p>Limit the total number of work items:</p> <pre><code># Source: src/cook/execution/data_pipeline/mod.rs:298-300\nmap:\n  json_path: \"$.items[*]\"\n  filter: \"unified_score.final_score &gt;= 5\"\n  sort_by: \"unified_score.final_score DESC\"\n  max_items: 3  # Process only top 3 items\n</code></pre>"},{"location":"mapreduce/work-distribution/#offset","title":"Offset","text":"<p>Skip the first N items:</p> <pre><code># Source: src/config/mapreduce.rs:84-91\nmap:\n  json_path: \"$.items[*]\"\n  offset: 10      # Skip first 10 items\n  max_items: 20   # Then take next 20\n</code></pre>"},{"location":"mapreduce/work-distribution/#use-cases","title":"Use Cases","text":"<p>Testing Workflows First</p> <p>Always test MapReduce workflows with a small subset before running on the full dataset: <pre><code>map:\n  max_items: 5  # Process only 5 items during development\n</code></pre></p> <p>Batched processing: <pre><code># Batch 1: items 0-99\nmap:\n  offset: 0\n  max_items: 100\n\n# Batch 2: items 100-199\nmap:\n  offset: 100\n  max_items: 100\n</code></pre></p> <p>Top-N processing: <pre><code># Process only the 10 highest priority items\nmap:\n  sort_by: \"priority DESC\"\n  max_items: 10\n</code></pre></p>"},{"location":"mapreduce/work-distribution/#deduplication","title":"Deduplication","text":"<p>Remove duplicate work items based on a field value using the <code>distinct</code> field:</p> <pre><code># Source: src/cook/execution/data_pipeline/mod.rs:361-363\nmap:\n  json_path: \"$.items[*]\"\n  distinct: \"id\"  # Keep only first occurrence of each unique ID\n</code></pre>"},{"location":"mapreduce/work-distribution/#how-deduplication-works","title":"How Deduplication Works","text":"<p>The deduplication process:</p> <ol> <li>Extracts the field value from each item</li> <li>Serializes the value to a string for comparison</li> <li>Keeps only the first item with each unique value</li> <li>Discards subsequent items with the same value</li> </ol> <p>Example: <pre><code>[\n  {\"id\": 1, \"value\": \"a\"},\n  {\"id\": 2, \"value\": \"b\"},\n  {\"id\": 1, \"value\": \"c\"},  // Duplicate id=1, discarded\n  {\"id\": 3, \"value\": \"d\"}\n]\n</code></pre></p> <p>With <code>distinct: \"id\"</code>, only 3 items remain (ids: 1, 2, 3).</p>"},{"location":"mapreduce/work-distribution/#nested-field-deduplication","title":"Nested Field Deduplication","text":"<p>You can deduplicate based on nested fields:</p> <pre><code>distinct: \"location.file\"  # Unique by file path\ndistinct: \"user.email\"     # Unique by email address\n</code></pre>"},{"location":"mapreduce/work-distribution/#null-value-handling","title":"Null Value Handling","text":"<p>Items with null values in the distinct field are treated as having the value <code>\"null\"</code>. This means: - Only one item with a null distinct value will be kept - Items with explicit <code>null</code> and missing fields are treated the same</p> <p>Note</p> <p>The correct field name is <code>distinct</code>, not <code>deduplicate_by</code>. The deduplication happens after filtering and sorting but before offset and limit.</p>"},{"location":"mapreduce/work-distribution/#processing-pipeline-order","title":"Processing Pipeline Order","text":"<p>Understanding the order of operations is important for building effective work distribution strategies:</p> <ol> <li>JSONPath Extraction - Extract items from input source</li> <li>Filtering - Apply filter expression to select items</li> <li>Sorting - Order items by sort fields</li> <li>Deduplication - Remove duplicates based on distinct field</li> <li>Offset - Skip first N items</li> <li>Limit (max_items) - Take only first N remaining items</li> </ol> <p>Optimization Tip</p> <p>Place expensive filtering early in the pipeline to reduce the number of items for subsequent operations. Sort only after filtering to minimize sort cost.</p> <pre><code># Source: src/cook/execution/data_pipeline/mod.rs:127-201\nmap:\n  input: data.json\n  json_path: \"$.items[*]\"           # (1)!\n  filter: \"score &gt;= 50\"             # (2)!\n  sort_by: \"score DESC\"             # (3)!\n  distinct: \"category\"              # (4)!\n  offset: 5                         # (5)!\n  max_items: 10                     # (6)!\n\n1. Extract all items from `$.items[*]` array\n2. Keep only items where `score &gt;= 50`\n3. Sort remaining items by score (highest first)\n4. Remove duplicates by category (keeps first of each)\n5. Skip the first 5 items\n6. Take the next 10 items for processing\n</code></pre> <p>This pipeline demonstrates the complete data transformation flow from extraction to final work item distribution.</p>"},{"location":"mapreduce/work-distribution/#complete-examples","title":"Complete Examples","text":""},{"location":"mapreduce/work-distribution/#high-priority-debt-items","title":"High-Priority Debt Items","text":"<p>Process technical debt items with high scores, sorted by priority:</p> <pre><code># Source: workflows/mapreduce-example.yml\nname: parallel-debt-elimination\nmode: mapreduce\n\nsetup:\n  - shell: \"debtmap analyze . --output debt_items.json\"  # (1)!\n\nmap:\n  input: debt_items.json  # (2)!\n  json_path: \"$.debt_items[*]\"  # (3)!\n  filter: \"severity == 'high' || severity == 'critical'\"  # (4)!\n  sort_by: \"priority DESC\"  # (5)!\n  max_parallel: 10  # (6)!\n\n  agent_template:\n    - claude: \"/fix-issue ${item.description}\"\n\n1. Generate work items in setup phase - ensures reproducible input\n2. Use JSON file output from setup phase\n3. Extract debt items from the array\n4. Process only high and critical severity items\n5. Process highest priority items first\n6. Run up to 10 agents concurrently\n</code></pre>"},{"location":"mapreduce/work-distribution/#top-scoring-items-with-deduplication","title":"Top Scoring Items with Deduplication","text":"<p>Process the top 3 unique high-scoring items:</p> <pre><code># Source: src/cook/execution/data_pipeline/mod.rs:294-355\nmap:\n  input: analysis.json\n  json_path: \"$.items[*]\"  # (1)!\n  filter: \"unified_score.final_score &gt;= 5\"  # (2)!\n  sort_by: \"unified_score.final_score DESC\"  # (3)!\n  distinct: \"location.file\"  # (4)!\n  max_items: 3  # (5)!\n\n1. Extract all items from analysis results\n2. Only process items with score &gt;= 5\n3. Sort by score, highest first\n4. Keep only one item per file (deduplication)\n5. Process only the top 3 unique items\n</code></pre>"},{"location":"mapreduce/work-distribution/#documentation-areas-by-priority","title":"Documentation Areas by Priority","text":"<p>Process high-priority documentation areas first:</p> <pre><code># Source: workflows/documentation-drift-mapreduce.yml\nsetup:\n  - shell: |\n      cat &gt; .prodigy/doc-areas.json &lt;&lt; 'EOF'\n      {\n        \"areas\": [\n          {\"name\": \"README\", \"priority\": 1},\n          {\"name\": \"API\", \"priority\": 2},\n          {\"name\": \"Examples\", \"priority\": 3}\n        ]\n      }\n      EOF\n\nmap:\n  input: .prodigy/doc-areas.json\n  json_path: \"$.areas[*]\"\n  sort_by: \"priority ASC\"  # Process priority 1 first\n  max_parallel: 4\n</code></pre>"},{"location":"mapreduce/work-distribution/#batched-processing-with-filters","title":"Batched Processing with Filters","text":"<p>Process work items in batches with filtering:</p> <pre><code>map:\n  input: large-dataset.json\n  json_path: \"$.tasks[*]\"\n  filter: \"status == 'pending' &amp;&amp; assigned_to == null\"\n  sort_by: \"created_at ASC\"\n  offset: 0       # Start from beginning\n  max_items: 50   # Process 50 at a time\n  max_parallel: 10\n</code></pre>"},{"location":"mapreduce/work-distribution/#integration-with-map-phase","title":"Integration with Map Phase","text":"<p>All work distribution fields are configured within the <code>map</code> phase configuration block:</p> <pre><code># Source: src/config/mapreduce.rs:49\nmap:\n  # Input source\n  input: &lt;path-to-json-file&gt;  # (1)!\n\n  # Work distribution pipeline\n  json_path: &lt;jsonpath-expression&gt;  # (2)!\n  filter: &lt;filter-expression&gt;  # (3)!\n  sort_by: &lt;sort-specification&gt;  # (4)!\n  distinct: &lt;field-name&gt;  # (5)!\n  offset: &lt;number&gt;  # (6)!\n  max_items: &lt;number&gt;  # (7)!\n\n  # Parallelization\n  max_parallel: &lt;number&gt;  # (8)!\n\n  # Agent template\n  agent_template:\n    - claude: \"/process-item ${item}\"\n\n1. Path to JSON file containing work items\n2. JSONPath expression to extract items (e.g., `$.items[*]`)\n3. Filter expression to select items (e.g., `score &gt;= 5`)\n4. Sort specification (e.g., `priority DESC`)\n5. Field name for deduplication (e.g., `id`)\n6. Number of items to skip from the start\n7. Maximum number of items to process\n8. Number of concurrent agents to run\n</code></pre> <p>These fields work together to control how work items are selected and distributed to parallel agents.</p>"},{"location":"mapreduce/work-distribution/#troubleshooting","title":"Troubleshooting","text":""},{"location":"mapreduce/work-distribution/#common-issues","title":"Common Issues","text":"<p>JSONPath returns no items: - Verify the input JSON structure matches your path - Test JSONPath expressions using online tools - Check for typos in field names</p> <p>Filter excludes all items: - Test filter expressions on sample data - Check for correct field names and types - Verify nested field paths are accurate</p> <p>Sorting doesn't work as expected: - Ensure sort field exists in all items - Use <code>NULLS LAST</code> to handle missing values - Check field types (strings sort alphabetically, numbers numerically)</p> <p>Deduplication removes too many items: - Verify the distinct field has the granularity you expect - Remember that null values are treated as identical - Check if nested field paths are correct</p>"},{"location":"mapreduce/work-distribution/#debugging-tips","title":"Debugging Tips","text":"<p>Preview filtered items: <pre><code>setup:\n  - shell: |\n      jq '.items[] | select(.score &gt;= 5)' data.json &gt; filtered-preview.json\n</code></pre></p> <p>Count items at each stage: <pre><code>setup:\n  - shell: |\n      echo \"Total items: $(jq '.items | length' data.json)\"\n      echo \"After filter: $(jq '[.items[] | select(.score &gt;= 5)] | length' data.json)\"\n</code></pre></p> <p>Validate JSONPath: <pre><code>setup:\n  - shell: |\n      jq '$.items[*]' data.json | jq 'length'\n</code></pre></p>"},{"location":"mapreduce/work-distribution/#see-also","title":"See Also","text":"<ul> <li>MapReduce Overview - Introduction to MapReduce workflows</li> <li>Setup Phase (Advanced) - Generating work items dynamically</li> <li>Checkpoint and Resume - Resuming interrupted workflows</li> <li>Dead Letter Queue (DLQ) - Handling failed work items</li> </ul>"},{"location":"mapreduce/workflow-format-comparison/","title":"Workflow Format Comparison","text":""},{"location":"mapreduce/workflow-format-comparison/#workflow-format-comparison","title":"Workflow Format Comparison","text":"<p>Prodigy supports multiple workflow format styles to balance simplicity for quick tasks with power for production workflows. This section explains the differences and helps you choose the right format.</p>"},{"location":"mapreduce/workflow-format-comparison/#standard-workflow-formats","title":"Standard Workflow Formats","text":"<p>Standard workflows (non-MapReduce) can be written in two formats:</p>"},{"location":"mapreduce/workflow-format-comparison/#simple-array-format","title":"Simple Array Format","text":"<p>For quick workflows, use a simple array of commands:</p> <pre><code># Simple array format - minimal syntax\n- claude: \"/prodigy-coverage\"\n  commit_required: true\n\n- shell: \"just test\"\n  on_failure:\n    claude: \"/prodigy-debug-test-failure\"\n</code></pre> <p>Use this format when: - Creating quick automation scripts - No environment variables or profiles needed - Workflow is self-contained and straightforward</p>"},{"location":"mapreduce/workflow-format-comparison/#full-configuration-format","title":"Full Configuration Format","text":"<p>For production workflows, use the full configuration format with metadata:</p> <pre><code># Full config format - includes metadata and environment\nname: mapreduce-env-example\nmode: mapreduce\n\nenv:\n  PROJECT_NAME: \"example-project\"\n  OUTPUT_DIR: \"output\"\n  DEBUG_MODE: \"false\"\n\n# Secrets are a separate top-level configuration\n# They support two formats: Simple and Provider-based\nsecrets:\n  # Simple format - directly references environment variable\n  SIMPLE_SECRET: \"ENV_VAR_NAME\"\n\n  # Provider format - explicit provider configuration\n  API_TOKEN:\n    provider: env\n    key: \"GITHUB_TOKEN\"\n\n  # File-based secret provider\n  FILE_SECRET:\n    provider: file\n    key: \"/path/to/secret/file\"\n\nprofiles:\n  development:\n    DEBUG_MODE: \"true\"\n  production:\n    DEBUG_MODE: \"false\"\n\n# Commands go here\nsetup:\n  - shell: \"echo Starting $PROJECT_NAME\"\n</code></pre> <p>Use this format when: - Deploying to multiple environments (dev, staging, prod) - Need environment variables or secrets - Workflow requires parameterization - Building reusable workflow templates</p> <p>Secret Format Details:</p> <p>Secrets support two formats (defined in <code>src/cook/environment/config.rs:86-95</code>):</p> <ol> <li> <p>Simple format - Direct environment variable reference:    <pre><code>secrets:\n  API_KEY: \"ENV_VAR_NAME\"  # Reads from $ENV_VAR_NAME\n</code></pre></p> </li> <li> <p>Provider format - Explicit provider configuration:    <pre><code>secrets:\n  API_KEY:\n    provider: env          # Providers: env, file, vault, aws\n    key: \"GITHUB_TOKEN\"    # Source key/path\n    version: \"v1\"          # Optional version\n</code></pre></p> </li> </ol> <p>Available Providers (defined in <code>src/cook/environment/config.rs:101-109</code>): - <code>env</code> - Environment variables (most common) - <code>file</code> - File-based secrets - <code>vault</code> - HashiCorp Vault integration - <code>aws</code> - AWS Secrets Manager integration</p> <p>Both formats mask secret values in logs. The Simple format is convenient for environment variables, while Provider format supports advanced secret management systems like Vault and AWS Secrets Manager.</p> <p>Note: For detailed Vault and AWS provider configuration, see Secrets Management and Environment Variables in Configuration.</p> <p>Source: Example workflow at <code>workflows/mapreduce-env-example.yml:23-26</code></p>"},{"location":"mapreduce/workflow-format-comparison/#mapreduce-syntax-evolution","title":"MapReduce Syntax Evolution","text":"<p>MapReduce workflows have evolved to use simpler, more concise syntax.</p>"},{"location":"mapreduce/workflow-format-comparison/#preferred-syntax-current","title":"Preferred Syntax (Current)","text":"<p>Commands are listed directly under <code>agent_template</code> and <code>reduce</code>:</p> <pre><code>name: parallel-debt-elimination\nmode: mapreduce\n\nsetup:\n  - shell: \"debtmap analyze . --output debt_items.json\"\n\nmap:\n  input: debt_items.json\n  json_path: \"$.debt_items[*]\"\n\n  # Direct array syntax - preferred\n  agent_template:\n    - claude: \"/fix-issue ${item.description}\"\n    - shell: \"cargo test\"\n      on_failure:\n        claude: \"/debug-test\"\n\n  max_parallel: 10\n\n# Direct array syntax for reduce - preferred\nreduce:\n  - claude: \"/summarize-fixes ${map.results}\"\n  - shell: \"echo Processed ${map.total} items\"\n</code></pre> <p>Benefits: - Less nesting, easier to read - Cleaner YAML structure - Follows YAML array conventions - Consistent with standard workflow format - Forward compatibility - the nested format may be removed in future versions</p>"},{"location":"mapreduce/workflow-format-comparison/#legacy-syntax-deprecated","title":"Legacy Syntax (Deprecated)","text":"<p>The old format nested commands under a <code>commands</code> field:</p> <pre><code># Old syntax - deprecated but still supported\nmap:\n  input: \"work-items.json\"\n  json_path: \"$.items[*]\"\n\n  # Nested under 'commands' - deprecated\n  agent_template:\n    commands:\n      - shell: echo \"Processing item ${item.id}\"\n      - shell: echo \"Completed ${item.task}\"\n\n  max_parallel: 3\n\n# Nested under 'commands' - deprecated\nreduce:\n  commands:\n    - shell: echo \"Processed ${map.total} items\"\n</code></pre> <p>Deprecation Notice: - This format is still supported for backward compatibility - New workflows should use the direct array syntax - Future versions may remove support for nested <code>commands</code> - When using the old format, Prodigy emits a warning: \"Using deprecated nested 'commands' syntax in agent_template. Consider using the simplified array format directly under 'agent_template'.\"</p> <p>Source: Deprecation warnings in <code>src/config/mapreduce.rs:310, 347</code></p>"},{"location":"mapreduce/workflow-format-comparison/#migration-guide","title":"Migration Guide","text":"<p>To migrate from old to new syntax:</p> <p>Before (Old): <pre><code>agent_template:\n  commands:\n    - claude: \"/process ${item}\"\n    - shell: \"test ${item.path}\"\n\nreduce:\n  commands:\n    - claude: \"/summarize ${map.results}\"\n</code></pre></p> <p>After (New): <pre><code>agent_template:\n  - claude: \"/process ${item}\"\n  - shell: \"test ${item.path}\"\n\nreduce:\n  - claude: \"/summarize ${map.results}\"\n</code></pre></p> <p>Migration Steps: 1. Remove the <code>commands:</code> line from <code>agent_template</code> 2. Remove the <code>commands:</code> line from <code>reduce</code> 3. Unindent the command list by one level 4. Test the workflow to ensure it works correctly</p> <p>Important Notes: - The workflow format is all-or-nothing - you cannot mix old and new formats within the same workflow - Both <code>agent_template</code> and <code>reduce</code> must use the same format (both direct array or both nested) - After migration, run <code>prodigy run workflow.yml --dry-run</code> to validate syntax before executing - If the workflow fails after migration, check for indentation errors - YAML is whitespace-sensitive</p>"},{"location":"mapreduce/workflow-format-comparison/#format-decision-tree","title":"Format Decision Tree","text":"<p>Choose your format based on these questions:</p> <ol> <li>Is this a MapReduce workflow?</li> <li>Yes \u2192 Use <code>mode: mapreduce</code> with direct array syntax</li> <li> <p>No \u2192 Continue to question 2</p> </li> <li> <p>Do you need environment variables or profiles?</p> </li> <li>Yes \u2192 Use full configuration format</li> <li> <p>No \u2192 Continue to question 3</p> </li> <li> <p>Is this a quick one-off workflow?</p> </li> <li>Yes \u2192 Use simple array format</li> <li>No \u2192 Use full configuration format for maintainability</li> </ol>"},{"location":"mapreduce/workflow-format-comparison/#cross-references","title":"Cross-References","text":"<ul> <li>Setup Phase Advanced - Detailed setup phase configuration and patterns</li> <li>MapReduce Overview - MapReduce workflow fundamentals and phase documentation</li> <li>Full Workflow Structure - Complete workflow configuration reference</li> <li>Environment Variables in Configuration - Using variables and secrets in workflows</li> </ul>"},{"location":"reference/troubleshooting/","title":"Troubleshooting Reference","text":"<p>Quick reference guide for diagnosing and resolving Prodigy issues. For detailed troubleshooting guidance, see the Troubleshooting chapter.</p>"},{"location":"reference/troubleshooting/#quick-diagnostics","title":"Quick Diagnostics","text":""},{"location":"reference/troubleshooting/#when-something-goes-wrong","title":"When Something Goes Wrong","text":"<pre><code>flowchart TD\n    Start[Issue Detected] --&gt; Verbose[Run with -v flag]\n    Verbose --&gt; Logs{Check Logs}\n\n    Logs --&gt;|Claude interaction| ClaudeLogs[prodigy logs --latest]\n    Logs --&gt;|Execution flow| Events[prodigy events ls]\n    Logs --&gt;|Failed items| DLQ[prodigy dlq show]\n\n    ClaudeLogs --&gt; State{Check State}\n    Events --&gt; State\n    DLQ --&gt; State\n\n    State --&gt;|Session info| Sessions[~/.prodigy/sessions/]\n    State --&gt;|Checkpoints| Checkpoints[~/.prodigy/state/]\n    State --&gt;|Worktrees| Worktrees[prodigy worktree ls]\n\n    Sessions --&gt; Resolve[Resolve Issue]\n    Checkpoints --&gt; Resolve\n    Worktrees --&gt; Resolve\n\n    style Start fill:#ffebee\n    style Verbose fill:#e1f5ff\n    style Logs fill:#fff3e0\n    style State fill:#f3e5f5\n    style Resolve fill:#e8f5e9</code></pre> <p>Figure: Diagnostic workflow for troubleshooting Prodigy issues.</p> <p>Quick Diagnostics Checklist</p> <p>Follow this sequence when troubleshooting:</p> <ol> <li>Check verbosity: Run with <code>-v</code> flag to see detailed output</li> <li>Inspect logs: Use <code>prodigy logs --latest --summary</code> for Claude interactions</li> <li>Review events: Use <code>prodigy events ls --job-id &lt;job_id&gt;</code> for execution timeline</li> <li>Check DLQ: Use <code>prodigy dlq show &lt;job_id&gt;</code> for failed items (MapReduce only)</li> <li>Verify state: Check <code>~/.prodigy/state/</code> for checkpoints and session state</li> </ol>"},{"location":"reference/troubleshooting/#common-error-patterns","title":"Common Error Patterns","text":"Symptom Likely Cause Quick Fix Variables show as <code>${var}</code> Wrong syntax or undefined Check spelling, use <code>${var}</code> syntax \"Session not found\" Wrong ID or expired Use <code>prodigy sessions list</code> to find correct ID \"Command not found: claude\" Claude not in PATH Install Claude Code or add to PATH \"No items to process\" Wrong JSONPath or missing file Verify input file exists, test JSONPath Cleanup fails Locked files or permissions Use <code>prodigy worktree clean-orphaned &lt;job_id&gt;</code> Resume starts over No checkpoint or wrong ID Check <code>~/.prodigy/state/</code> for checkpoint files High map phase failures Resource contention Reduce <code>max_parallel</code>, increase timeout"},{"location":"reference/troubleshooting/#issue-categories","title":"Issue Categories","text":""},{"location":"reference/troubleshooting/#mapreduce-issues","title":"MapReduce Issues","text":"<p>Agents failing silently: - Check: <code>prodigy dlq show &lt;job_id&gt;</code> - Inspect: <code>json_log_location</code> field in DLQ entries - See: Dead Letter Queue (DLQ)</p> <p>Checkpoint resume not working: - Check: <code>~/.prodigy/state/{repo}/mapreduce/jobs/{job_id}/</code> - Verify: Session/job ID with <code>prodigy sessions list</code> - See: Checkpoint and Resume</p> <p>Concurrent resume blocked: - Check: <code>~/.prodigy/resume_locks/{job_id}.lock</code> - Verify: Process still running with PID from error message - Clean: Remove lock file if process is dead - See: CLAUDE.md \"Concurrent Resume Protection (Spec 140)\"</p> <p>Cleanup failures: - Use: <code>prodigy worktree clean-orphaned &lt;job_id&gt;</code> - Check: Locked files with <code>lsof | grep worktree-path</code> - See: CLAUDE.md \"Cleanup Failure Handling (Spec 136)\"</p>"},{"location":"reference/troubleshooting/#session-and-resume-issues","title":"Session and Resume Issues","text":"<p>Resume fails with \"session not found\": - List sessions: <code>prodigy sessions list</code> - Try job ID: <code>prodigy resume-job &lt;job_id&gt;</code> or <code>prodigy resume &lt;job_id&gt;</code> - Check state: <code>~/.prodigy/sessions/{session-id}.json</code></p> <p>Session state corrupted: - Check: Session file in <code>~/.prodigy/sessions/</code> - Verify: Checkpoint files in <code>~/.prodigy/state/</code> - Last resort: Start new workflow run</p>"},{"location":"reference/troubleshooting/#variable-and-interpolation-issues","title":"Variable and Interpolation Issues","text":"<p>Variables not interpolating: - Check syntax: <code>${var}</code> not <code>$var</code> for complex expressions - Verify scope: Variable defined at workflow/step level - Check spelling: Variable names are case-sensitive - See: Environment Variables</p> <p>Environment variables empty: - Verify: Variable defined in <code>env</code> section - Check profile: Use <code>--profile &lt;name&gt;</code> if using profiles - Test: <code>echo \"$VAR\"</code> in shell command to verify</p>"},{"location":"reference/troubleshooting/#performance-issues","title":"Performance Issues","text":"<p>Resource Contention</p> <p>High <code>max_parallel</code> values can exhaust system resources. Start with 5-10 agents and monitor performance before increasing.</p> <p>Slow MapReduce execution: - Reduce: <code>max_parallel</code> to avoid resource exhaustion - Increase: <code>agent_timeout_secs</code> if agents timeout - Split: Use <code>max_items</code> and <code>offset</code> for chunking - Check: System resource usage with <code>top</code> or <code>htop</code></p> <p>High resource usage: - Lower parallelism in map phase - Reduce context size in Claude commands - Check for memory leaks in custom commands - Monitor: <code>prodigy events stats</code> for bottlenecks</p> <p>Performance Tuning</p> <p>For optimal MapReduce performance:</p> <ul> <li>10-1000 work items: Sweet spot for parallelism benefits</li> <li>10 sec - 5 min per item: Ideal task duration</li> <li>Start small: Test with <code>max_items: 10</code> before full run</li> </ul> <p>Timeout errors: - Increase: <code>timeout</code> field in command configuration - Split: Large operations into smaller steps - Check: For hung processes with <code>ps aux | grep prodigy</code></p>"},{"location":"reference/troubleshooting/#worktree-problems","title":"Worktree Problems","text":"<p>Cleanup Failures</p> <p>If cleanup fails during MapReduce execution, the agent is still marked as successful and results are preserved. Use <code>prodigy worktree clean-orphaned &lt;job_id&gt;</code> to clean up later.</p> <p>Orphaned worktrees: - List: <code>prodigy worktree ls</code> - Clean: <code>prodigy worktree clean-orphaned &lt;job_id&gt;</code> - Manual: <code>rm -rf ~/.prodigy/worktrees/{path}</code> (last resort)</p> <p>Merge conflicts: - Use: Custom merge workflow with conflict resolution - Review: Git status in worktree before merge - See: CLAUDE.md \"Custom Merge Workflows\"</p> <p>Worktree locked: - Check: Running processes with <code>lsof ~/.prodigy/worktrees/{path}</code> - Kill: Process if safe, or wait for completion - Clean: Use <code>prodigy worktree clean -f</code> if necessary</p>"},{"location":"reference/troubleshooting/#debugging-techniques","title":"Debugging Techniques","text":""},{"location":"reference/troubleshooting/#verbosity-levels","title":"Verbosity Levels","text":"<pre><code># Default: Clean output\nprodigy run workflow.yml              # (1)!\n\n# Verbose: Show Claude streaming output\nprodigy run workflow.yml -v           # (2)!\n\n# Very verbose: Add debug logs\nprodigy run workflow.yml -vv          # (3)!\n\n# Trace: Maximum detail\nprodigy run workflow.yml -vvv         # (4)!\n</code></pre> <ol> <li>Minimal output for production workflows - shows only progress and results</li> <li>Adds Claude JSON streaming output for debugging interactions</li> <li>Adds debug-level logs from Prodigy internals</li> <li>Maximum verbosity including trace-level execution details</li> </ol>"},{"location":"reference/troubleshooting/#log-inspection","title":"Log Inspection","text":"<p>Claude JSON Logs</p> <p>Every Claude command creates a streaming JSONL log with full conversation history:</p> <pre><code># View latest log with summary\nprodigy logs --latest --summary\n\n# Follow log in real-time\nprodigy logs --latest --tail\n\n# View specific log file\ncat ~/.claude/projects/{worktree-path}/{uuid}.jsonl | jq -c '.'\n</code></pre> <p>Log location is displayed after each Claude command execution.</p> <p>Event Logs</p> <p>Track workflow execution and identify bottlenecks:</p> <pre><code># List events for job\nprodigy events ls --job-id &lt;job_id&gt;\n\n# Follow events in real-time\nprodigy events follow --job-id &lt;job_id&gt;\n\n# Show statistics\nprodigy events stats\n</code></pre> <p>Source: src/cli/commands/events.rs:22-98</p>"},{"location":"reference/troubleshooting/#state-inspection","title":"State Inspection","text":"Session StateCheckpoint StateDLQ Contents <pre><code># List all sessions\nprodigy sessions list\n\n# View session details\ncat ~/.prodigy/sessions/{session-id}.json | jq '.'\n</code></pre> <pre><code># List checkpoints for job\nls ~/.prodigy/state/{repo}/mapreduce/jobs/{job_id}/\n\n# View checkpoint contents\ncat ~/.prodigy/state/{repo}/mapreduce/jobs/{job_id}/map-checkpoint-*.json | jq '.'\n</code></pre> <pre><code># Show failed items\nprodigy dlq show &lt;job_id&gt;\n\n# View DLQ file directly\ncat ~/.prodigy/dlq/{repo}/{job_id}.json | jq '.'\n</code></pre>"},{"location":"reference/troubleshooting/#git-context","title":"Git Context","text":"<p>Worktree inspection: <pre><code># Navigate to worktree\ncd ~/.prodigy/worktrees/{repo}/{session}/\n\n# View git log\ngit log --oneline -10\n\n# Check status\ngit status\n\n# View diff\ngit diff HEAD~1\n</code></pre></p>"},{"location":"reference/troubleshooting/#error-messages","title":"Error Messages","text":"<p>For detailed explanations of specific error messages, see:</p> <ul> <li>Common Error Messages</li> </ul> <p>Common error patterns:</p> <ul> <li>\"session not found\": Session ID incorrect or expired</li> <li>\"command not found: claude\": Claude Code not installed or not in PATH</li> <li>\"no items to process\": Input file missing or JSONPath incorrect</li> <li>\"cleanup failed\": Locked files or permission issues</li> <li>\"resume already in progress\": Concurrent resume protection active</li> <li>\"checkpoint not found\": Checkpoint files missing or wrong ID</li> </ul>"},{"location":"reference/troubleshooting/#best-practices-for-debugging","title":"Best Practices for Debugging","text":"<p>Debugging Workflow</p> <p>Follow this systematic approach to diagnose issues quickly:</p> <ol> <li>Start with verbosity: Always use <code>-v</code> flag when debugging</li> <li>Check logs first: Claude JSON logs contain full interaction details</li> <li>Review events: Event timeline shows execution flow and bottlenecks</li> <li>Inspect DLQ early: Failed items in DLQ indicate systematic issues</li> <li>Verify state: Check checkpoint and session files for corruption</li> <li>Test incrementally: Use <code>--dry-run</code> to preview execution</li> <li>Monitor resources: Watch CPU, memory, disk during execution</li> <li>Use specialized tools: <code>prodigy events</code>, <code>prodigy logs</code>, <code>prodigy dlq</code></li> </ol> <p>For comprehensive debugging strategies, see:</p> <ul> <li>Best Practices for Debugging</li> </ul>"},{"location":"reference/troubleshooting/#related-topics","title":"Related Topics","text":"<ul> <li>Troubleshooting Guide - Detailed troubleshooting for all issues</li> <li>FAQ - Frequently asked questions</li> <li>MapReduce Checkpoint and Resume - Resume functionality details</li> <li>Environment Variables - Variable configuration</li> <li>Observability Configuration - Logging and monitoring</li> </ul>"},{"location":"retry-configuration/","title":"Retry Configuration","text":"<p>Prodigy provides sophisticated retry mechanisms with multiple backoff strategies to handle transient failures gracefully. The retry system supports both command-level and workflow-level configurations with fine-grained control over retry behavior.</p>"},{"location":"retry-configuration/#overview","title":"Overview","text":"<p>Prodigy has two retry systems that work together:</p> <ol> <li>Enhanced Retry System - Rich, configurable retry with multiple backoff strategies, jitter, circuit breakers, and conditional retry (from <code>src/cook/retry_v2.rs</code>)</li> <li>Workflow-Level Retry - Simpler retry configuration for workflow-level error policies (from <code>src/cook/workflow/error_policy.rs</code>)</li> </ol> <p>This chapter focuses on the enhanced retry system which provides comprehensive retry capabilities. Circuit breakers prevent cascading failures by temporarily stopping retries when a threshold of consecutive failures is reached.</p>"},{"location":"retry-configuration/#when-to-use-each-retry-system","title":"When to Use Each Retry System","text":"<p>Use Enhanced Retry (retry_v2) for: - Individual command execution failures (API calls, shell commands, file operations) - Operations needing fine-grained control over backoff strategies - Situations requiring conditional retry based on error types - Commands where jitter is needed to prevent thundering herd - External API calls with rate limiting - Operations benefiting from circuit breakers</p> <p>Use Workflow-Level Retry (error_policy) for: - MapReduce work item failures - Workflow-wide error handling policies - Bulk operations requiring Dead Letter Queue (DLQ) integration - Scenarios needing failure thresholds and batch error collection - When you want to retry entire work items rather than individual commands</p> <p>For a detailed comparison with examples, see Workflow-Level vs Command-Level Retry.</p>"},{"location":"retry-configuration/#retryconfig-structure","title":"RetryConfig Structure","text":"<p>This table documents the enhanced retry system (<code>retry_v2::RetryConfig</code>). For workflow-level retry configuration, see the Workflow-Level vs Command-Level Retry subsection.</p> <p>The <code>RetryConfig</code> struct controls retry behavior with the following fields:</p> Field Type Default Description <code>attempts</code> <code>u32</code> <code>3</code> Maximum number of retry attempts <code>backoff</code> <code>BackoffStrategy</code> <code>Exponential (base: 2.0)</code> Strategy for calculating delays between retries <code>initial_delay</code> <code>Duration</code> <code>1s</code> Initial delay before first retry <code>max_delay</code> <code>Duration</code> <code>30s</code> Maximum delay between any two retries <code>jitter</code> <code>bool</code> <code>false</code> Whether to add randomness to delays <code>jitter_factor</code> <code>f64</code> <code>0.3</code> Amount of jitter (0.0 to 1.0) <code>retry_on</code> <code>Vec&lt;ErrorMatcher&gt;</code> <code>[]</code> Retry only on specific error types (empty = retry all) <code>retry_budget</code> <code>Option&lt;Duration&gt;</code> <code>None</code> Maximum total time for all retry attempts <code>on_failure</code> <code>FailureAction</code> <code>Stop</code> Action to take after all retries exhausted <p>Source: RetryConfig struct defined in <code>src/cook/retry_v2.rs:14-52</code></p>"},{"location":"retry-configuration/#yaml-configuration-syntax","title":"YAML Configuration Syntax","text":"<p>The RetryConfig fields map to YAML workflow syntax as follows:</p> <pre><code>commands:\n  - shell: \"your-command-here\"\n    retry_config:\n      attempts: 5                    # RetryConfig.attempts (u32)\n      backoff:\n        type: exponential            # BackoffStrategy::Exponential\n        base: 2.0                    # exponential base multiplier\n      initial_delay: \"1s\"            # RetryConfig.initial_delay (humantime format)\n      max_delay: \"30s\"               # RetryConfig.max_delay (humantime format)\n      jitter: true                   # RetryConfig.jitter (bool)\n      jitter_factor: 0.3             # RetryConfig.jitter_factor (0.0-1.0)\n      retry_on:                      # RetryConfig.retry_on (Vec&lt;ErrorMatcher&gt;)\n        - network\n        - timeout\n        - server_error\n      retry_budget: \"5m\"             # RetryConfig.retry_budget (Optional&lt;Duration&gt;)\n      on_failure: stop               # RetryConfig.on_failure (FailureAction)\n</code></pre> <p>Alternative Backoff Strategies:</p> <pre><code># Fixed delay\nbackoff: fixed\n\n# Linear backoff\nbackoff:\n  type: linear\n  increment: \"2s\"\n\n# Fibonacci backoff\nbackoff: fibonacci\n\n# Custom delay sequence\nbackoff:\n  type: custom\n  delays: [\"1s\", \"2s\", \"5s\", \"10s\"]\n</code></pre> <p>Note: Field names use snake_case in YAML but map to the exact struct fields in <code>src/cook/retry_v2.rs:14-52</code>. Duration values use humantime format (e.g., \"1s\", \"30s\", \"5m\").</p> <p>For complete working examples, see Complete Examples.</p>"},{"location":"retry-configuration/#circuit-breakers","title":"Circuit Breakers","text":"<p>Circuit breakers are configured separately via <code>RetryExecutor</code>, not as part of RetryConfig. Circuit breakers provide fail-fast behavior when downstream systems are consistently failing, preventing resource exhaustion from repeated failed retries.</p> <p>Configuration (programmatic): <pre><code>let executor = RetryExecutor::new(retry_config)\n    .with_circuit_breaker(\n        5,                          // failure_threshold: open after 5 consecutive failures\n        Duration::from_secs(30)     // recovery_timeout: attempt recovery after 30 seconds\n    );\n</code></pre></p> <p>Source: <code>src/cook/retry_v2.rs:184-188</code> (with_circuit_breaker method), <code>src/cook/retry_v2.rs:325-397</code> (CircuitBreaker implementation)</p> <p>Circuit States: - Closed: Normal operation, retries are attempted - Open: Circuit tripped, requests fail immediately without retry - HalfOpen: Testing recovery, limited requests allowed</p> <p>See Best Practices for guidance on combining retry with circuit breakers for high-reliability systems.</p>"},{"location":"retry-configuration/#additional-topics","title":"Additional Topics","text":"<p>See also: - Basic Retry Configuration - Backoff Strategies - Backoff Strategy Comparison - Jitter for Distributed Systems - Conditional Retry with Error Matchers - Retry Budget - Failure Actions - Complete Examples - Workflow-Level vs Command-Level Retry - Retry Metrics and Observability - Best Practices - Troubleshooting - Implementation References</p>"},{"location":"retry-configuration/#related-topics","title":"Related Topics","text":"<p>This section provides links to related documentation that complements retry configuration. Understanding these topics will help you build more resilient workflows.</p>"},{"location":"retry-configuration/#within-this-chapter","title":"Within This Chapter","text":"<p>The following subsections provide detailed information about specific aspects of retry configuration:</p> <ul> <li>Basic Retry Configuration - Start here to understand fundamental retry configuration syntax and options</li> <li>Backoff Strategies - Control the timing between retry attempts using exponential, linear, Fibonacci, or constant delays</li> <li>Backoff Strategy Comparison - Compare different backoff strategies with examples and use cases</li> <li>Failure Actions - Define custom actions to execute when commands fail or exhaust retries</li> <li>Conditional Retry with Error Matchers - Use error patterns to selectively retry only specific failures</li> <li>Jitter for Distributed Systems - Add randomization to retry delays to prevent thundering herd problems</li> <li>Retry Budget - Limit total retry attempts across your workflow to prevent infinite retry loops</li> <li>Retry Metrics and Observability - Monitor retry behavior through events and logging</li> <li>Workflow-Level vs Command-Level Retry - Understand the differences between retry scopes and when to use each</li> <li>Best Practices - Recommended patterns and anti-patterns for retry configuration</li> <li>Complete Examples - Real-world retry configuration examples demonstrating various strategies</li> <li>Troubleshooting - Debug common retry configuration issues</li> <li>Implementation References - Links to source code implementing retry logic</li> </ul>"},{"location":"retry-configuration/#related-chapters","title":"Related Chapters","text":"<p>These chapters cover topics that interact with or complement retry configuration:</p> <ul> <li>Error Handling - Overall error handling strategy and how Prodigy propagates errors through workflows. Retry configuration is one component of a comprehensive error handling approach.</li> <li>Workflow Configuration - Workflow-level settings including global retry defaults that apply to all commands unless overridden at the command level.</li> <li>MapReduce - Retry behavior in MapReduce workflows, where individual map agents can retry independently. MapReduce adds complexity to retry semantics due to parallel execution.</li> <li>Dead Letter Queue (DLQ) - Handling failed work items in MapReduce workflows. When map agents exhaust all retries, items move to the DLQ for manual inspection and retry.</li> <li>Environment Variables - Use environment variables in retry configuration to parameterize retry behavior across different deployment environments (dev, staging, production).</li> </ul>"},{"location":"retry-configuration/backoff-strategies/","title":"Backoff Strategies","text":""},{"location":"retry-configuration/backoff-strategies/#backoff-strategies","title":"Backoff Strategies","text":"<p>Note: This subsection documents the enhanced retry system (<code>retry_v2::RetryConfig</code>) used for command-level retry configuration. For workflow-level retry (MapReduce error policies), see Workflow-Level vs Command-Level Retry. The enhanced system provides more sophisticated backoff options and features.</p> <p>Prodigy supports five backoff strategies for controlling delay between retries. Backoff strategies determine how the delay between retry attempts increases over time, helping to avoid overwhelming systems while maximizing chances of success.</p> <p>All backoff strategies use <code>initial_delay</code> as the base delay and respect the <code>max_delay</code> cap. Delays are calculated per attempt and can be combined with jitter to avoid thundering herd problems.</p> <p>Source: BackoffStrategy enum defined in <code>src/cook/retry_v2.rs:70-98</code></p> <p>Default Strategy: If no backoff strategy is specified, Prodigy uses Exponential backoff with a base of 2.0 (src/cook/retry_v2.rs:92-98).</p>"},{"location":"retry-configuration/backoff-strategies/#fixed-backoff","title":"Fixed Backoff","text":"<p>Fixed backoff uses a constant delay between all retry attempts. This is the simplest strategy and works well when you want predictable, consistent retry timing.</p> <p>Delay Pattern: Same delay for every attempt - Attempt 1: <code>initial_delay</code> - Attempt 2: <code>initial_delay</code> - Attempt 3: <code>initial_delay</code></p> <p>YAML Configuration: <pre><code>map:\n  agent_template:\n    - shell: \"flaky-command\"\n      retry_config:\n        attempts: 5\n        initial_delay: \"2s\"\n        backoff: fixed\n</code></pre></p> <p>Note: <code>backoff: fixed</code> is the shorthand for the Fixed unit variant. Equivalent to <code>backoff: { fixed: null }</code> (src/cook/retry_v2.rs:75).</p> <p>When to Use: - Simple retry scenarios where timing is not critical - Testing and development environments - When you want predictable retry intervals</p>"},{"location":"retry-configuration/backoff-strategies/#linear-backoff","title":"Linear Backoff","text":"<p>Linear backoff increases the delay by a fixed increment for each retry attempt. This provides gradual backoff that's easy to reason about.</p> <p>Delay Pattern: Increases by constant increment - Attempt 1: <code>initial_delay</code> - Attempt 2: <code>initial_delay + increment</code> - Attempt 3: <code>initial_delay + 2 * increment</code></p> <p>YAML Configuration: <pre><code>map:\n  agent_template:\n    - shell: \"database-query\"\n      retry_config:\n        attempts: 5\n        initial_delay: \"1s\"\n        backoff:\n          linear:\n            increment: \"2s\"\n</code></pre></p> <p>Example Timeline (initial_delay=1s, increment=2s): - Attempt 1: 1s - Attempt 2: 3s (1s + 2s) - Attempt 3: 5s (1s + 4s) - Attempt 4: 7s (1s + 6s)</p> <p>When to Use: - Moderate load situations - When you need predictable but increasing delays - API rate limiting scenarios with linear cooldown</p>"},{"location":"retry-configuration/backoff-strategies/#exponential-backoff","title":"Exponential Backoff","text":"<p>Exponential backoff multiplies the delay by a base factor for each retry, causing delays to grow rapidly. This is the default strategy and is recommended for most retry scenarios.</p> <p>Delay Pattern: Multiplies by base^(attempt-1) - Attempt 1: <code>initial_delay * base^0</code> - Attempt 2: <code>initial_delay * base^1</code> - Attempt 3: <code>initial_delay * base^2</code></p> <p>YAML Configuration (default base=2.0): <pre><code>map:\n  agent_template:\n    - shell: \"network-call\"\n      retry_config:\n        attempts: 5\n        initial_delay: \"1s\"\n        max_delay: \"60s\"\n        # Uses exponential backoff with base 2.0 by default\n</code></pre></p> <p>YAML Configuration (custom base): <pre><code>map:\n  agent_template:\n    - shell: \"api-request\"\n      retry_config:\n        attempts: 5\n        initial_delay: \"1s\"\n        max_delay: \"60s\"\n        backoff:\n          exponential:\n            base: 3.0  # More aggressive backoff\n</code></pre></p> <p>Example Timeline (initial_delay=1s, base=2.0): - Attempt 1: 1s (1 * 2^0) - Attempt 2: 2s (1 * 2^1) - Attempt 3: 4s (1 * 2^2) - Attempt 4: 8s (1 * 2^3) - Attempt 5: 16s (1 * 2^4)</p> <p>When to Use: - Network requests and API calls (default choice) - Situations where quick retries might make things worse - Distributed systems with cascading failures - When you want rapid backoff to give systems time to recover</p>"},{"location":"retry-configuration/backoff-strategies/#fibonacci-backoff","title":"Fibonacci Backoff","text":"<p>Fibonacci backoff uses the Fibonacci sequence (1, 1, 2, 3, 5, 8, 13...) to calculate delays. This provides a middle ground between linear and exponential backoff, growing quickly but not as aggressively as exponential.</p> <p>Delay Pattern: Uses Fibonacci sequence multiplier - Attempt 1: <code>initial_delay * 1</code> - Attempt 2: <code>initial_delay * 1</code> - Attempt 3: <code>initial_delay * 2</code> - Attempt 4: <code>initial_delay * 3</code> - Attempt 5: <code>initial_delay * 5</code></p> <p>YAML Configuration: <pre><code>map:\n  agent_template:\n    - shell: \"distributed-operation\"\n      retry_config:\n        attempts: 6\n        initial_delay: \"1s\"\n        max_delay: \"30s\"\n        backoff: fibonacci\n</code></pre></p> <p>Note: <code>backoff: fibonacci</code> is the shorthand for the Fibonacci unit variant, similar to Fixed (src/cook/retry_v2.rs:87).</p> <p>Example Timeline (initial_delay=1s): - Attempt 1: 1s (fib(1) = 1) - Attempt 2: 1s (fib(2) = 1) - Attempt 3: 2s (fib(3) = 2) - Attempt 4: 3s (fib(4) = 3) - Attempt 5: 5s (fib(5) = 5) - Attempt 6: 8s (fib(6) = 8)</p> <p>When to Use: - Distributed systems where you want balanced backoff - Situations where exponential is too aggressive - When you want retry intervals that grow naturally but moderately</p>"},{"location":"retry-configuration/backoff-strategies/#custom-backoff","title":"Custom Backoff","text":"<p>Custom backoff allows you to specify an explicit sequence of delays for complete control over retry timing. If the retry attempt exceeds the number of delays specified, the <code>max_delay</code> is used.</p> <p>Delay Pattern: Uses explicit delay list - Delays are used in order from the array - If attempts exceed delays array length, uses <code>max_delay</code></p> <p>YAML Configuration: <pre><code>map:\n  agent_template:\n    - shell: \"custom-retry-operation\"\n      retry_config:\n        attempts: 5\n        max_delay: \"60s\"\n        backoff:\n          custom:\n            delays:\n              - secs: 1\n                nanos: 0\n              - secs: 3\n                nanos: 0\n              - secs: 7\n                nanos: 0\n              - secs: 15\n                nanos: 0\n              # Attempt 5 would use max_delay (60s)\n</code></pre></p> <p>Note: Custom backoff delays use Duration struct format (<code>{secs: N, nanos: 0}</code>) instead of humantime strings like \"1s\". This is because <code>Vec&lt;Duration&gt;</code> doesn't have the <code>humantime_serde</code> annotation (src/cook/retry_v2.rs:89).</p> <p>Example Timeline: - Attempt 1: 1s (delays[0]) - Attempt 2: 3s (delays[1]) - Attempt 3: 7s (delays[2]) - Attempt 4: 15s (delays[3]) - Attempt 5: 60s (max_delay, delays[4] doesn't exist)</p> <p>When to Use: - When you need precise control over retry timing - Integration with third-party APIs with specific retry requirements - Complex retry scenarios with non-standard delay patterns - Testing specific timing scenarios</p> <p>Edge Cases: - Empty delays array: Falls back to <code>max_delay</code> for all attempts - Fewer delays than attempts: Uses <code>max_delay</code> for remaining attempts</p>"},{"location":"retry-configuration/backoff-strategies/#integration-with-retryconfig","title":"Integration with RetryConfig","text":"<p>Backoff strategies work together with other retry configuration options:</p> <p>Complete Example: <pre><code>map:\n  input: \"work-items.json\"\n  json_path: \"$.items[*]\"\n\n  agent_template:\n    - shell: \"process-item ${item.id}\"\n      retry_config:\n        attempts: 5\n        initial_delay: \"2s\"      # Base delay for backoff calculation\n        max_delay: \"60s\"         # Cap on calculated delays\n        backoff:\n          exponential:\n            base: 2.0\n        jitter: true             # Add randomization (\u00b125% by default)\n        jitter_factor: 0.25\n        retry_on:\n          - timeout              # Built-in timeout matcher\n          - pattern: \"connection refused\"  # Custom pattern for specific errors\n</code></pre></p> <p>How It Works Together: 1. Backoff strategy calculates base delay using <code>initial_delay</code> 2. Calculated delay is capped at <code>max_delay</code> 3. If <code>jitter: true</code>, randomization is applied (\u00b1<code>jitter_factor</code> percentage) 4. Final delay is applied before next retry attempt 5. If <code>retry_on</code> is specified, error must match one of the matchers to trigger retry</p> <p>Error Matcher Syntax: The <code>retry_on</code> field uses <code>ErrorMatcher</code> enum variants. Built-in matchers (<code>timeout</code>, <code>network</code>, <code>server_error</code>, <code>rate_limit</code>) use lowercase names. Custom patterns use <code>pattern: \"regex\"</code> syntax. See Conditional Retry with Error Matchers for complete documentation.</p> <p>See also: - Basic Retry Configuration - Overall retry configuration options - Backoff Strategy Comparison - Visual comparison of strategies - Jitter for Distributed Systems - How jitter prevents thundering herd - Retry Metrics and Observability - Track retry performance - Best Practices - When to use which strategy</p>"},{"location":"retry-configuration/backoff-strategy-comparison/","title":"Backoff Strategy Comparison","text":""},{"location":"retry-configuration/backoff-strategy-comparison/#backoff-strategy-comparison","title":"Backoff Strategy Comparison","text":"<p>This comparison assumes <code>initial_delay = 1s</code> for all strategies. Actual delays depend on your retry configuration. All strategies are capped by <code>max_delay</code> (default 30s) as enforced in <code>src/cook/retry_v2.rs:304</code>.</p> <p>Source: Delay calculations from <code>src/cook/retry_v2.rs:284-305</code> (calculate_delay method)</p>"},{"location":"retry-configuration/backoff-strategy-comparison/#strategy-comparison-table","title":"Strategy Comparison Table","text":"Strategy Attempt 1 Attempt 2 Attempt 3 Attempt 4 Attempt 5 Best For Fixed (2s) 2s 2s 2s 2s 2s Predictable timing, simple errors without exponential backpressure Linear (+2s) 1s 3s 5s 7s 9s Moderate load reduction, gradual backoff without aggressive delays Exponential (base 2.0) 1s 2s 4s 8s 16s Recommended default - Aggressive backoff for most failures, best for temporary outages Fibonacci 1s 1s 2s 3s 5s Gentler than exponential, good for rate limits or distributed systems Custom user-defined user-defined user-defined user-defined user-defined Specific delay patterns, custom business logic <p>Note: Exponential is the default backoff strategy (see <code>src/cook/retry_v2.rs:92-97</code>)</p>"},{"location":"retry-configuration/backoff-strategy-comparison/#strategy-formulas","title":"Strategy Formulas","text":"<p>Each strategy calculates delay using a different formula:</p> <p>Fixed (src/cook/retry_v2.rs:286): <pre><code>delay = initial_delay\n</code></pre> Always returns the same delay, regardless of attempt number.</p> <p>Linear (src/cook/retry_v2.rs:287-289): <pre><code>delay = initial_delay + increment * (attempt - 1)\n</code></pre> Example: With <code>initial_delay = 1s</code> and <code>increment = 2s</code>: - Attempt 1: 1s + 2s \u00d7 (1-1) = 1s - Attempt 2: 1s + 2s \u00d7 (2-1) = 3s - Attempt 3: 1s + 2s \u00d7 (3-1) = 5s</p> <p>Exponential (src/cook/retry_v2.rs:290-292): <pre><code>delay = initial_delay * base^(attempt - 1)\n</code></pre> Example: With <code>initial_delay = 1s</code> and <code>base = 2.0</code>: - Attempt 1: 1s \u00d7 2^(1-1) = 1s \u00d7 1 = 1s - Attempt 2: 1s \u00d7 2^(2-1) = 1s \u00d7 2 = 2s - Attempt 3: 1s \u00d7 2^(3-1) = 1s \u00d7 4 = 4s</p> <p>The <code>base</code> parameter is configurable (default: 2.0). Common values: - <code>base = 2.0</code>: Aggressive backoff (doubles each retry) - <code>base = 1.5</code>: Gentler exponential growth - <code>base = 3.0</code>: Very aggressive backoff</p> <p>Fibonacci (src/cook/retry_v2.rs:294-297): <pre><code>delay = initial_delay * fibonacci(attempt)\n</code></pre> Example: With <code>initial_delay = 1s</code>: - Attempt 1: 1s \u00d7 fibonacci(1) = 1s \u00d7 1 = 1s - Attempt 2: 1s \u00d7 fibonacci(2) = 1s \u00d7 1 = 1s - Attempt 3: 1s \u00d7 fibonacci(3) = 1s \u00d7 2 = 2s - Attempt 4: 1s \u00d7 fibonacci(4) = 1s \u00d7 3 = 3s - Attempt 5: 1s \u00d7 fibonacci(5) = 1s \u00d7 5 = 5s</p> <p>Fibonacci sequence: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34...</p> <p>Custom (src/cook/retry_v2.rs:298-301): <pre><code>delay = delays[attempt - 1] or max_delay if out of bounds\n</code></pre> Allows you to specify exact delays for each attempt. If retry exceeds the array length, <code>max_delay</code> is used.</p>"},{"location":"retry-configuration/backoff-strategy-comparison/#max-delay-cap","title":"Max Delay Cap","text":"<p>All strategies are capped by <code>max_delay</code> (src/cook/retry_v2.rs:304): <pre><code>base_delay.min(self.config.max_delay)\n</code></pre></p> <p>This prevents unbounded delays in exponential/fibonacci strategies. Default <code>max_delay = 30s</code> (src/cook/retry_v2.rs:30).</p> <p>Example with <code>max_delay = 30s</code> and Exponential (base 2.0): - Attempt 6: Would be 32s \u2192 capped to 30s - Attempt 7+: All capped to 30s</p>"},{"location":"retry-configuration/backoff-strategy-comparison/#jitter-support","title":"Jitter Support","text":"<p>All strategies support optional jitter to prevent thundering herd problems (src/cook/retry_v2.rs:308-317).</p> <p>When <code>jitter = true</code> (default <code>jitter_factor = 0.3</code>), delays are randomized: <pre><code>jitter_range = delay * jitter_factor\nactual_delay = delay + random(-jitter_range/2, +jitter_range/2)\n</code></pre></p> <p>Example with 1s delay and 30% jitter (factor 0.3): - jitter_range = 1s \u00d7 0.3 = 0.3s - actual_delay = random(0.85s, 1.15s)</p> <p>See Jitter for Distributed Systems for when to use jitter.</p>"},{"location":"retry-configuration/backoff-strategy-comparison/#yaml-configuration-examples","title":"YAML Configuration Examples","text":"<p>Fixed Delay: <pre><code>retry_config:\n  backoff: fixed\n  initial_delay: \"2s\"  # Always wait 2s between retries\n</code></pre></p> <p>Linear Backoff: <pre><code>retry_config:\n  backoff:\n    type: linear\n    increment: \"2s\"\n  initial_delay: \"1s\"  # 1s, 3s, 5s, 7s, 9s...\n</code></pre></p> <p>Exponential Backoff (default): <pre><code>retry_config:\n  backoff:\n    type: exponential\n    base: 2.0  # Optional: default is 2.0\n  initial_delay: \"1s\"  # 1s, 2s, 4s, 8s, 16s...\n  max_delay: \"30s\"     # Cap at 30s\n</code></pre></p> <p>Fibonacci Backoff: <pre><code>retry_config:\n  backoff: fibonacci\n  initial_delay: \"1s\"  # 1s, 1s, 2s, 3s, 5s, 8s...\n</code></pre></p> <p>Custom Delays: <pre><code>retry_config:\n  backoff:\n    type: custom\n    delays: [\"1s\", \"2s\", \"5s\", \"10s\", \"30s\"]\n  max_delay: \"60s\"  # Fallback if attempts exceed array length\n</code></pre></p> <p>With Jitter (any strategy): <pre><code>retry_config:\n  backoff: exponential\n  initial_delay: \"1s\"\n  jitter: true         # Enable randomization\n  jitter_factor: 0.3   # 30% jitter (\u00b115%)\n</code></pre></p>"},{"location":"retry-configuration/backoff-strategy-comparison/#choosing-the-right-strategy","title":"Choosing the Right Strategy","text":"Use Case Recommended Strategy Reasoning Temporary service outage Exponential (base 2.0) Quickly backs off to avoid overwhelming recovering service Rate limiting (429 errors) Fibonacci or Linear Gentler backoff respects rate limits without excessive delays Network flakiness Exponential + Jitter Aggressive backoff with jitter prevents thundering herd Predictable timing needs Fixed Consistent delay for deterministic behavior Gradual load shedding Linear Steady increase allows system to recover gradually Custom business logic Custom Full control over delay pattern (e.g., comply with API retry-after headers) Distributed systems Fibonacci + Jitter Balances quick retries with avoiding cascade failures <p>Default recommendation: Exponential with <code>base = 2.0</code> is the default for good reason - it works well for most transient failures while avoiding excessive load on failing systems.</p>"},{"location":"retry-configuration/backoff-strategy-comparison/#performance-comparison","title":"Performance Comparison","text":"<p>Time to reach <code>max_delay = 30s</code> with <code>initial_delay = 1s</code>:</p> Strategy Attempts to Max Total Time (5 attempts) Fixed (2s) Never (always 2s) 10s Linear (+2s) 15 attempts 25s (1+3+5+7+9) Exponential (base 2.0) 5 attempts 31s (1+2+4+8+16) Fibonacci 9 attempts 12s (1+1+2+3+5) <p>Key insight: Exponential reaches max delay fastest, making it most aggressive. Fibonacci is gentler, making it better for gradual recovery scenarios.</p>"},{"location":"retry-configuration/backoff-strategy-comparison/#see-also","title":"See Also","text":"<ul> <li>Backoff Strategies - Detailed explanation of each strategy</li> <li>Basic Retry Configuration - How to configure retry in workflows</li> <li>Jitter for Distributed Systems - When and how to use jitter</li> <li>Complete Examples - Full workflow examples with retry configuration</li> <li>Best Practices - Guidelines for effective retry strategies</li> </ul> <p>Source References: - BackoffStrategy enum: <code>src/cook/retry_v2.rs:70-90</code> - Delay calculation: <code>src/cook/retry_v2.rs:284-305</code> - Jitter application: <code>src/cook/retry_v2.rs:308-317</code> - Default values: <code>src/cook/retry_v2.rs:26-31</code> (initial_delay, max_delay)</p>"},{"location":"retry-configuration/basic-retry-configuration/","title":"Basic Retry Configuration","text":""},{"location":"retry-configuration/basic-retry-configuration/#basic-retry-configuration","title":"Basic Retry Configuration","text":"<p>The enhanced retry system (<code>retry_v2</code>) provides command-level retry capabilities with sophisticated backoff strategies and error handling. This subsection covers the basic retry configuration options.</p> <p>Note: This documents the enhanced retry system (<code>retry_v2::RetryConfig</code>). For workflow-level retry configuration, see Workflow-Level vs Command-Level Retry.</p>"},{"location":"retry-configuration/basic-retry-configuration/#where-retry-config-is-used","title":"Where Retry Config is Used","text":"<p>Retry configuration can be applied at the command level in your workflow YAML files. The <code>retry_config</code> field is available on individual commands to control how that specific command is retried on failure.</p> <p>Source: <code>src/cook/retry_v2.rs:14-52</code> (RetryConfig struct definition)</p>"},{"location":"retry-configuration/basic-retry-configuration/#basic-configuration-example","title":"Basic Configuration Example","text":"<p>The simplest retry configuration uses just the attempts field:</p> <pre><code>commands:\n  - shell: \"curl https://api.example.com/data\"\n    retry_config:\n      attempts: 3\n</code></pre> <p>This configuration will: - Retry the command up to 3 times on failure - Use exponential backoff with base 2.0 (default) - Start with 1 second initial delay (default) - Cap delays at 30 seconds maximum (default) - Retry on all error types (default when <code>retry_on</code> is empty)</p>"},{"location":"retry-configuration/basic-retry-configuration/#complete-basic-configuration","title":"Complete Basic Configuration","text":"<p>For more control, you can specify all basic retry parameters:</p> <pre><code>commands:\n  - shell: \"make test\"\n    retry_config:\n      attempts: 5              # Maximum retry attempts (default: 3)\n      initial_delay: \"2s\"      # Initial delay between retries (default: 1s)\n      max_delay: \"60s\"         # Maximum delay cap (default: 30s)\n      backoff: exponential     # Backoff strategy (default: exponential)\n</code></pre> <p>Field Reference (from <code>src/cook/retry_v2.rs:54-68</code>): - <code>attempts: u32</code> - Maximum number of retry attempts (default: 3) - <code>initial_delay: Duration</code> - Starting delay between retries (default: 1 second) - <code>max_delay: Duration</code> - Maximum delay ceiling (default: 30 seconds) - <code>backoff: BackoffStrategy</code> - Strategy for calculating delays (default: Exponential { base: 2.0 })</p>"},{"location":"retry-configuration/basic-retry-configuration/#default-behavior","title":"Default Behavior","text":"<p>When <code>retry_config</code> is omitted entirely, commands run without retry. When <code>retry_config</code> is present but fields are omitted, defaults from <code>RetryConfig::default()</code> apply:</p> <pre><code>// Default values (src/cook/retry_v2.rs:54-68)\nRetryConfig {\n    attempts: 3,\n    backoff: BackoffStrategy::Exponential { base: 2.0 },\n    initial_delay: Duration::from_secs(1),\n    max_delay: Duration::from_secs(30),\n    jitter: false,\n    jitter_factor: 0.3,\n    retry_on: Vec::new(),  // Empty = retry all errors\n    retry_budget: None,\n    on_failure: FailureAction::Stop,\n}\n</code></pre>"},{"location":"retry-configuration/basic-retry-configuration/#relationship-to-command-execution","title":"Relationship to Command Execution","text":"<p>When a command with <code>retry_config</code> fails, the <code>RetryExecutor</code> orchestrates the retry logic:</p> <ol> <li>Execute Command: Run the shell/Claude command</li> <li>Check Result: If successful, return immediately</li> <li>Check Retry Budget: If set, ensure budget not exceeded</li> <li>Calculate Delay: Use backoff strategy to determine next delay</li> <li>Wait: Sleep for calculated delay (with optional jitter)</li> <li>Retry: Execute command again</li> <li>Repeat: Continue until success, max attempts reached, or budget exhausted</li> </ol> <p>Source: <code>src/cook/retry_v2.rs:191-262</code> (RetryExecutor::execute_with_retry)</p>"},{"location":"retry-configuration/basic-retry-configuration/#integration-with-retryconfig-struct","title":"Integration with RetryConfig Struct","text":"<p>All YAML retry configuration maps directly to the <code>RetryConfig</code> struct fields:</p> YAML Field Rust Field Type Default <code>attempts</code> <code>attempts</code> <code>u32</code> 3 <code>initial_delay</code> <code>initial_delay</code> <code>Duration</code> 1s <code>max_delay</code> <code>max_delay</code> <code>Duration</code> 30s <code>backoff</code> <code>backoff</code> <code>BackoffStrategy</code> Exponential <code>jitter</code> <code>jitter</code> <code>bool</code> false <code>jitter_factor</code> <code>jitter_factor</code> <code>f64</code> 0.3 <code>retry_on</code> <code>retry_on</code> <code>Vec&lt;ErrorMatcher&gt;</code> [] (all) <code>retry_budget</code> <code>retry_budget</code> <code>Option&lt;Duration&gt;</code> None <code>on_failure</code> <code>on_failure</code> <code>FailureAction</code> Stop <p>Source: <code>src/cook/retry_v2.rs:14-52</code></p>"},{"location":"retry-configuration/basic-retry-configuration/#minimal-vs-full-configuration","title":"Minimal vs Full Configuration","text":"<p>Minimal (use defaults for most fields): <pre><code>retry_config:\n  attempts: 5\n</code></pre></p> <p>Full (explicit control over all parameters): <pre><code>retry_config:\n  attempts: 10\n  backoff: fibonacci\n  initial_delay: \"500ms\"\n  max_delay: \"2m\"\n  jitter: true\n  jitter_factor: 0.5\n  retry_budget: \"10m\"\n  retry_on:\n    - network\n    - timeout\n  on_failure: continue\n</code></pre></p>"},{"location":"retry-configuration/basic-retry-configuration/#see-also","title":"See Also","text":"<ul> <li>Backoff Strategies - Detailed backoff strategy documentation</li> <li>Conditional Retry with Error Matchers - Selective retry with <code>retry_on</code></li> <li>Failure Actions - What happens after final failure</li> <li>Jitter for Distributed Systems - Preventing thundering herd</li> <li>Retry Budget - Time-based retry limits</li> <li>Complete Examples - Full workflow examples</li> </ul>"},{"location":"retry-configuration/best-practices/","title":"Best Practices","text":""},{"location":"retry-configuration/best-practices/#best-practices","title":"Best Practices","text":"<p>This guide covers best practices for configuring retry behavior in Prodigy workflows, based on the implementation patterns in <code>retry_v2</code>.</p> <p>Source: Best practices derived from retry_v2.rs implementation and test patterns (src/cook/retry_v2.rs:463-748)</p>"},{"location":"retry-configuration/best-practices/#choosing-the-right-backoff-strategy","title":"Choosing the Right Backoff Strategy","text":"<p>Different backoff strategies suit different failure modes:</p>"},{"location":"retry-configuration/best-practices/#exponential-backoff-default-best-for-most-cases","title":"Exponential Backoff (Default - Best for Most Cases)","text":"<p>When to use: - Network requests to external APIs - Transient failures that self-heal over time - Rate-limited services - Database connection retries</p> <p>Configuration: <pre><code>retry_config:\n  attempts: 5\n  backoff: exponential  # or: { base: 2.0 }\n  initial_delay: \"1s\"\n  max_delay: \"30s\"\n  jitter: true\n</code></pre></p> <p>Why: Exponential backoff quickly backs off from rapid retries, reducing load on failing systems and allowing recovery time.</p> <p>Source: <code>src/cook/retry_v2.rs:82-85</code> (Exponential variant), tests at lines 626-648</p>"},{"location":"retry-configuration/best-practices/#linear-backoff","title":"Linear Backoff","text":"<p>When to use: - Predictable delays needed - Testing and debugging (easier to reason about delays) - Systems with known recovery time patterns</p> <p>Configuration: <pre><code>retry_config:\n  attempts: 5\n  backoff:\n    linear:\n      increment: \"5s\"\n  initial_delay: \"1s\"\n</code></pre></p> <p>Delays: 1s, 6s, 11s, 16s, 21s (initial + n * increment)</p> <p>Source: <code>src/cook/retry_v2.rs:77-80</code> (Linear variant), tests at lines 604-625</p>"},{"location":"retry-configuration/best-practices/#fibonacci-backoff","title":"Fibonacci Backoff","text":"<p>When to use: - Gradual backoff with slower growth than exponential - Good balance between responsiveness and system protection - Distributed systems needing gentler backoff</p> <p>Configuration: <pre><code>retry_config:\n  attempts: 6\n  backoff: fibonacci\n  initial_delay: \"1s\"\n  max_delay: \"60s\"\n</code></pre></p> <p>Delays: 1s, 2s, 3s, 5s, 8s, 13s</p> <p>Source: <code>src/cook/retry_v2.rs:87</code> (Fibonacci variant), Fibonacci calculation at lines 424-440</p>"},{"location":"retry-configuration/best-practices/#fixed-delay","title":"Fixed Delay","text":"<p>When to use: - Polling operations - Systems with consistent retry requirements - Simplicity preferred over optimization</p> <p>Configuration: <pre><code>retry_config:\n  attempts: 10\n  backoff: fixed\n  initial_delay: \"5s\"\n</code></pre></p> <p>Source: <code>src/cook/retry_v2.rs:75</code> (Fixed variant), tests at lines 583-603</p>"},{"location":"retry-configuration/best-practices/#setting-appropriate-max_delay","title":"Setting Appropriate max_delay","text":"<p>Always set <code>max_delay</code> to prevent unbounded delays:</p> <p>\u2705 Good: <pre><code>retry_config:\n  backoff: exponential\n  max_delay: \"30s\"  # Caps exponential growth\n</code></pre></p> <p>\u274c Bad: <pre><code>retry_config:\n  backoff: exponential\n  # No max_delay - could delay minutes or hours!\n</code></pre></p> <p>Default: 30 seconds (src/cook/retry_v2.rs:64)</p> <p>Recommendation: - Interactive workflows: 10-30 seconds - Background jobs: 60-300 seconds - Critical paths: 5-15 seconds</p>"},{"location":"retry-configuration/best-practices/#using-jitter-in-distributed-systems","title":"Using Jitter in Distributed Systems","text":"<p>Always enable jitter for distributed systems to prevent thundering herd:</p> <pre><code>retry_config:\n  attempts: 5\n  backoff: exponential\n  initial_delay: \"1s\"\n  max_delay: \"30s\"\n  jitter: true           # Enable jitter\n  jitter_factor: 0.3     # 30% randomization\n</code></pre> <p>Source: <code>src/cook/retry_v2.rs:308-317</code> (apply_jitter method), tests at lines 650-673</p> <p>Why: Without jitter, multiple parallel agents/processes retry at the same time, overwhelming recovering systems.</p> <p>Jitter Formula (src/cook/retry_v2.rs:311-315): <pre><code>let jitter_range = delay_ms as f64 * jitter_factor;\nlet random_offset = thread_rng().gen_range(-jitter_range..=jitter_range);\nadjusted_delay = delay + Duration::from_millis(random_offset as u64);\n</code></pre></p> <p>When to use jitter: - MapReduce workflows with parallel agents - Multiple services hitting the same API - Distributed systems with shared resources - Any parallel retry scenario</p> <p>When to skip jitter: - Single-instance workflows - Deterministic testing - Debugging retry behavior</p>"},{"location":"retry-configuration/best-practices/#retry-budget-for-preventing-infinite-loops","title":"Retry Budget for Preventing Infinite Loops","text":"<p>Use <code>retry_budget</code> to cap total retry time:</p> <pre><code>retry_config:\n  attempts: 100          # High attempt count\n  backoff: exponential\n  retry_budget: \"5m\"     # But limit total time to 5 minutes\n</code></pre> <p>Source: <code>src/cook/retry_v2.rs:46-47</code> (retry_budget field)</p> <p>Why: Prevents workflows from retrying indefinitely when <code>attempts</code> is set high.</p> <p>Use Cases: - Long-running operations where total time matters - SLA-constrained workflows - Preventing resource exhaustion - Fail-fast on persistent failures</p> <p>Implementation Note: Retry budget is checked before each retry attempt. If budget is exhausted, retries stop immediately (verified in test at src/cook/retry_v2.rs:675-708).</p>"},{"location":"retry-configuration/best-practices/#selective-retry-with-error-matchers","title":"Selective Retry with Error Matchers","text":"<p>Don't retry everything - use <code>retry_on</code> for transient errors only:</p> <p>\u2705 Good (selective retry): <pre><code>retry_config:\n  attempts: 5\n  retry_on:\n    - network\n    - timeout\n    - server_error  # 5xx errors\n</code></pre></p> <p>\u274c Bad (retry all errors): <pre><code>retry_config:\n  attempts: 5\n  # Empty retry_on retries everything, including 404, 401, etc.\n</code></pre></p> <p>Why: Some errors are permanent and retrying wastes time: - 404 Not Found - resource doesn't exist - 401 Unauthorized - credentials are invalid - 400 Bad Request - request is malformed</p> <p>Transient errors worth retrying: - Network connectivity issues - Timeouts - 5xx server errors - Rate limits (with appropriate backoff) - Database locks (temporary)</p> <p>Source: <code>src/cook/retry_v2.rs:100-151</code> (ErrorMatcher enum), tests at lines 463-582</p>"},{"location":"retry-configuration/best-practices/#combining-retry-with-circuit-breakers","title":"Combining Retry with Circuit Breakers","text":"<p>For high-reliability systems, combine retry with circuit breakers:</p> <pre><code>// Circuit breaker configuration (applied via RetryExecutor)\nlet executor = RetryExecutor::new(retry_config)\n    .with_circuit_breaker(\n        5,                          // failure_threshold\n        Duration::from_secs(30)     // recovery_timeout\n    );\n</code></pre> <p>Source: <code>src/cook/retry_v2.rs:184-188</code> (with_circuit_breaker method), <code>src/cook/retry_v2.rs:325-397</code> (CircuitBreaker implementation)</p> <p>Circuit Breaker States: 1. Closed: Normal operation, requests flow through 2. Open: Circuit tripped after threshold failures, requests fail immediately 3. HalfOpen: Testing recovery, limited requests allowed</p> <p>Why: Circuit breakers prevent cascading failures by failing fast when downstream systems are down, rather than retrying indefinitely.</p> <p>Best Practice: Use circuit breakers for: - External API calls - Database connections - Microservice communication - Any dependency that might fail completely</p>"},{"location":"retry-configuration/best-practices/#failure-action-strategies","title":"Failure Action Strategies","text":"<p>Choose <code>on_failure</code> based on operation criticality:</p>"},{"location":"retry-configuration/best-practices/#critical-operations-use-stop","title":"Critical Operations (Use: Stop)","text":"<pre><code>retry_config:\n  attempts: 3\n  on_failure: stop\n</code></pre> <p>Examples: - Database migrations - Deployment prerequisites - Data integrity checks - Security validations</p>"},{"location":"retry-configuration/best-practices/#optional-operations-use-continue","title":"Optional Operations (Use: Continue)","text":"<pre><code>retry_config:\n  attempts: 2\n  on_failure: continue\n</code></pre> <p>Examples: - Cache warmup - Metrics collection - Notifications - Non-critical cleanup</p>"},{"location":"retry-configuration/best-practices/#fallback-operations-use-fallback","title":"Fallback Operations (Use: Fallback)","text":"<pre><code>retry_config:\n  attempts: 3\n  on_failure:\n    fallback:\n      command: \"cat cached-data.json\"\n</code></pre> <p>Examples: - API with cache fallback - Primary/secondary data sources - Graceful degradation scenarios</p> <p>Source: <code>src/cook/retry_v2.rs:153-165</code> (FailureAction enum)</p>"},{"location":"retry-configuration/best-practices/#retry-configuration-anti-patterns","title":"Retry Configuration Anti-Patterns","text":"<p>\u274c Don't: Set very high attempts without retry_budget <pre><code>retry_config:\n  attempts: 1000  # Could retry for hours!\n</code></pre></p> <p>\u2705 Do: Combine high attempts with retry_budget <pre><code>retry_config:\n  attempts: 100\n  retry_budget: \"5m\"  # Caps total time\n</code></pre></p> <p>\u274c Don't: Use exponential backoff without max_delay <pre><code>retry_config:\n  backoff: exponential\n  # Delay could grow to minutes!\n</code></pre></p> <p>\u2705 Do: Always set max_delay <pre><code>retry_config:\n  backoff: exponential\n  max_delay: \"30s\"\n</code></pre></p> <p>\u274c Don't: Retry non-idempotent operations <pre><code>retry_config:\n  attempts: 5\n  # If command creates resources, retries might duplicate!\n</code></pre></p> <p>\u2705 Do: Make operations idempotent or skip retry <pre><code># Use idempotency tokens, check-before-create, etc.\n</code></pre></p> <p>\u274c Don't: Skip jitter in parallel workflows <pre><code># MapReduce with 10 parallel agents\nretry_config:\n  jitter: false  # All agents retry simultaneously!\n</code></pre></p> <p>\u2705 Do: Enable jitter for parallel execution <pre><code>retry_config:\n  jitter: true\n  jitter_factor: 0.3\n</code></pre></p>"},{"location":"retry-configuration/best-practices/#testing-retry-configuration","title":"Testing Retry Configuration","text":"<p>Use tests to validate retry behavior (patterns from src/cook/retry_v2.rs:463-748):</p> <pre><code>#[tokio::test]\nasync fn test_retry_with_exponential_backoff() {\n    let config = RetryConfig {\n        attempts: 3,\n        backoff: BackoffStrategy::Exponential { base: 2.0 },\n        initial_delay: Duration::from_millis(100),\n        ..Default::default()\n    };\n\n    let executor = RetryExecutor::new(config);\n\n    // Test that retries happen with correct delays\n    // Verify exponential growth: 100ms, 200ms, 400ms\n}\n</code></pre> <p>Test Coverage: - Verify retry attempts match configuration - Check backoff delays are correct - Ensure error matchers work as expected - Validate circuit breaker state transitions - Test retry budget enforcement</p>"},{"location":"retry-configuration/best-practices/#production-monitoring","title":"Production Monitoring","text":"<p>Monitor retry metrics for operational insight:</p> <pre><code>// Access retry metrics\nlet metrics = executor.metrics();\nprintln!(\"Total attempts: {}\", metrics.total_attempts);\nprintln!(\"Successful: {}\", metrics.successful_attempts);\nprintln!(\"Failed: {}\", metrics.failed_attempts);\n</code></pre> <p>Source: <code>src/cook/retry_v2.rs:320-322</code> (metrics() method), <code>src/cook/retry_v2.rs:399-422</code> (RetryMetrics struct)</p> <p>Key Metrics: - Total attempts vs successful attempts (success rate) - Retry counts per operation (identify problematic operations) - Delay distributions (verify backoff is working) - Circuit breaker state changes (detect system issues)</p>"},{"location":"retry-configuration/best-practices/#summary","title":"Summary","text":"<ol> <li>Use exponential backoff for most retry scenarios (default)</li> <li>Always set max_delay to prevent unbounded delays</li> <li>Enable jitter for distributed/parallel systems</li> <li>Use retry_budget to cap total retry time</li> <li>Be selective with <code>retry_on</code> - don't retry permanent errors</li> <li>Combine with circuit breakers for high-reliability systems</li> <li>Choose appropriate failure actions based on operation criticality</li> <li>Test retry behavior to ensure it works as expected</li> <li>Monitor retry metrics in production</li> </ol>"},{"location":"retry-configuration/best-practices/#see-also","title":"See Also","text":"<ul> <li>Backoff Strategies - Detailed backoff documentation</li> <li>Jitter for Distributed Systems - Preventing thundering herd</li> <li>Retry Budget - Time-based retry limits</li> <li>Conditional Retry with Error Matchers - Selective retry</li> <li>Failure Actions - Handling final failures</li> <li>Complete Examples - Real-world retry configurations</li> </ul>"},{"location":"retry-configuration/complete-examples/","title":"Complete Examples","text":""},{"location":"retry-configuration/complete-examples/#complete-examples","title":"Complete Examples","text":"<p>This section provides complete, runnable YAML workflow examples demonstrating various retry configurations.</p> <p>Source: Examples based on test patterns from src/cook/retry_v2.rs:463-748</p>"},{"location":"retry-configuration/complete-examples/#example-1-basic-retry-with-exponential-backoff","title":"Example 1: Basic Retry with Exponential Backoff","text":"<p>Simple API call with standard exponential backoff:</p> <pre><code>name: fetch-api-data\nmode: standard\n\ncommands:\n  - shell: \"curl -f https://api.example.com/data\"\n    retry_config:\n      attempts: 5\n      backoff: exponential\n      initial_delay: \"1s\"\n      max_delay: \"30s\"\n</code></pre> <p>When it's useful: - External API calls - Network-dependent operations - Transient failure recovery</p> <p>Retry sequence: - Attempt 1: Immediate - Attempt 2: ~2s delay - Attempt 3: ~4s delay - Attempt 4: ~8s delay - Attempt 5: ~16s delay</p>"},{"location":"retry-configuration/complete-examples/#example-2-exponential-backoff-with-jitter-distributed-systems","title":"Example 2: Exponential Backoff with Jitter (Distributed Systems)","text":"<p>Multiple parallel agents with jitter to prevent thundering herd:</p> <pre><code>name: parallel-processing\nmode: mapreduce\n\nmap:\n  input: \"items.json\"\n  json_path: \"$.items[*]\"\n  max_parallel: 10\n\n  agent_template:\n    - shell: \"process-item ${item.id}\"\n      retry_config:\n        attempts: 5\n        backoff: exponential\n        initial_delay: \"1s\"\n        max_delay: \"30s\"\n        jitter: true          # Critical for parallel agents\n        jitter_factor: 0.3    # 30% randomization\n</code></pre> <p>Why jitter matters: Without jitter, all 10 parallel agents would retry at exactly the same time, overwhelming the recovering service.</p> <p>Source: Jitter implementation in src/cook/retry_v2.rs:308-317</p>"},{"location":"retry-configuration/complete-examples/#example-3-conditional-retry-with-error-matchers","title":"Example 3: Conditional Retry with Error Matchers","text":"<p>Only retry transient errors, fail fast on permanent errors:</p> <pre><code>name: selective-retry\nmode: standard\n\ncommands:\n  - shell: \"curl -f https://api.example.com/resource\"\n    retry_config:\n      attempts: 5\n      backoff: exponential\n      initial_delay: \"1s\"\n      max_delay: \"30s\"\n      retry_on:\n        - network        # Connection issues\n        - timeout        # Slow responses\n        - server_error   # 5xx errors\n</code></pre> <p>Behavior: - Retries: Network errors, timeouts, 500/502/503/504 - Fails immediately: 404, 401, 400 (permanent errors)</p> <p>Source: ErrorMatcher enum in src/cook/retry_v2.rs:100-151</p>"},{"location":"retry-configuration/complete-examples/#example-4-retry-budget-to-prevent-infinite-loops","title":"Example 4: Retry Budget to Prevent Infinite Loops","text":"<p>High retry attempts with time-based cap:</p> <pre><code>name: budget-limited-retry\nmode: standard\n\ncommands:\n  - shell: \"long-running-operation\"\n    retry_config:\n      attempts: 100          # High attempt count\n      backoff: fibonacci\n      initial_delay: \"1s\"\n      max_delay: \"60s\"\n      retry_budget: \"10m\"    # But never exceed 10 minutes total\n</code></pre> <p>Why: Prevents endless retries while allowing many attempts for operations that typically succeed eventually.</p> <p>Source: retry_budget field in src/cook/retry_v2.rs:46-47, tests at lines 675-708</p>"},{"location":"retry-configuration/complete-examples/#example-5-fallback-on-failure","title":"Example 5: Fallback on Failure","text":"<p>Use cached data when API fails:</p> <pre><code>name: fallback-example\nmode: standard\n\ncommands:\n  - shell: \"curl -f https://api.example.com/live-data\"\n    retry_config:\n      attempts: 3\n      backoff: exponential\n      initial_delay: \"2s\"\n      max_delay: \"10s\"\n      retry_on:\n        - network\n        - timeout\n      on_failure:\n        fallback:\n          command: \"cat /cache/data.json\"\n\n  # Continue processing with either live or cached data\n  - shell: \"process-data data.json\"\n</code></pre> <p>Execution flow: 1. Try to fetch live data (3 attempts with exponential backoff) 2. If all attempts fail \u2192 Use cached data 3. Continue with processing</p> <p>Source: FailureAction::Fallback in src/cook/retry_v2.rs:164</p>"},{"location":"retry-configuration/complete-examples/#example-6-continue-on-failure-non-critical-operations","title":"Example 6: Continue on Failure (Non-Critical Operations)","text":"<p>Allow workflow to continue even if optional operations fail:</p> <pre><code>name: mixed-criticality\nmode: standard\n\ncommands:\n  # Critical: must succeed\n  - shell: \"cargo build\"\n    retry_config:\n      attempts: 3\n      on_failure: stop\n\n  # Optional: nice to have but not critical\n  - shell: \"notify-slack 'Build started'\"\n    retry_config:\n      attempts: 2\n      initial_delay: \"5s\"\n      on_failure: continue    # Don't fail workflow if notification fails\n\n  # Critical: must succeed\n  - shell: \"cargo test\"\n    retry_config:\n      attempts: 3\n      on_failure: stop\n</code></pre> <p>Use case: Separating critical operations from best-effort operations.</p> <p>Source: FailureAction::Continue in src/cook/retry_v2.rs:162</p>"},{"location":"retry-configuration/complete-examples/#example-7-rate-limit-handling","title":"Example 7: Rate Limit Handling","text":"<p>Handle API rate limits with long delays:</p> <pre><code>name: rate-limit-aware\nmode: standard\n\ncommands:\n  - shell: \"api-call.sh\"\n    retry_config:\n      attempts: 10\n      backoff: exponential\n      initial_delay: \"60s\"    # Start with 1-minute delay\n      max_delay: \"10m\"        # Cap at 10 minutes\n      retry_on:\n        - rate_limit          # Only retry on 429 errors\n</code></pre> <p>Why: Rate limits often require longer delays than network errors.</p> <p>Source: ErrorMatcher::RateLimit in src/cook/retry_v2.rs:143-147</p>"},{"location":"retry-configuration/complete-examples/#example-8-custom-pattern-matching","title":"Example 8: Custom Pattern Matching","text":"<p>Retry database-specific errors:</p> <pre><code>name: database-retry\nmode: standard\n\ncommands:\n  - shell: \"sqlite3 db.sqlite 'INSERT INTO ...'\"\n    retry_config:\n      attempts: 5\n      backoff: linear\n      initial_delay: \"100ms\"\n      retry_on:\n        - pattern: \"database.*locked\"\n        - pattern: \"SQLITE_BUSY\"\n        - pattern: \"cannot commit.*in progress\"\n</code></pre> <p>Source: ErrorMatcher::Pattern in src/cook/retry_v2.rs:113</p>"},{"location":"retry-configuration/complete-examples/#example-9-fibonacci-backoff-for-gradual-recovery","title":"Example 9: Fibonacci Backoff for Gradual Recovery","text":"<p>Gentler backoff curve for services needing recovery time:</p> <pre><code>name: fibonacci-backoff-example\nmode: standard\n\ncommands:\n  - shell: \"connect-to-recovering-service.sh\"\n    retry_config:\n      attempts: 8\n      backoff: fibonacci\n      initial_delay: \"1s\"\n      max_delay: \"60s\"\n</code></pre> <p>Delay sequence: 1s, 2s, 3s, 5s, 8s, 13s, 21s, 34s</p> <p>Why Fibonacci: Grows slower than exponential, giving services more time to recover without aggressive backoff.</p> <p>Source: Fibonacci calculation in src/cook/retry_v2.rs:424-440</p>"},{"location":"retry-configuration/complete-examples/#example-10-linear-backoff-for-predictable-delays","title":"Example 10: Linear Backoff for Predictable Delays","text":"<p>Testing or debugging with consistent delays:</p> <pre><code>name: linear-backoff-example\nmode: standard\n\ncommands:\n  - shell: \"test-operation.sh\"\n    retry_config:\n      attempts: 5\n      backoff:\n        linear:\n          increment: \"3s\"\n      initial_delay: \"1s\"\n</code></pre> <p>Delay sequence: 1s, 4s, 7s, 10s, 13s (initial + n * increment)</p> <p>Source: BackoffStrategy::Linear in src/cook/retry_v2.rs:77-80</p>"},{"location":"retry-configuration/complete-examples/#example-11-fixed-delay-for-polling","title":"Example 11: Fixed Delay for Polling","text":"<p>Consistent polling interval:</p> <pre><code>name: polling-example\nmode: standard\n\ncommands:\n  - shell: \"check-job-status.sh\"\n    retry_config:\n      attempts: 20\n      backoff: fixed\n      initial_delay: \"5s\"\n</code></pre> <p>Delay sequence: 5s between every attempt</p> <p>Use case: Status polling, health checks</p> <p>Source: BackoffStrategy::Fixed in src/cook/retry_v2.rs:75</p>"},{"location":"retry-configuration/complete-examples/#example-12-complex-multi-command-workflow","title":"Example 12: Complex Multi-Command Workflow","text":"<p>Real-world example combining multiple retry strategies:</p> <pre><code>name: deployment-workflow\nmode: standard\n\ncommands:\n  # Step 1: Build (critical, retry network issues)\n  - shell: \"cargo build --release\"\n    retry_config:\n      attempts: 3\n      backoff: exponential\n      retry_on:\n        - network\n      on_failure: stop\n\n  # Step 2: Run tests (critical, no retry on real failures)\n  - shell: \"cargo test\"\n    retry_config:\n      attempts: 2\n      initial_delay: \"5s\"\n      retry_on:\n        - pattern: \"temporary.*failure\"\n      on_failure: stop\n\n  # Step 3: Upload artifacts (retry with backoff)\n  - shell: \"upload-to-s3.sh artifacts/\"\n    retry_config:\n      attempts: 5\n      backoff: exponential\n      initial_delay: \"2s\"\n      max_delay: \"60s\"\n      jitter: true\n      retry_on:\n        - network\n        - timeout\n        - server_error\n      on_failure:\n        fallback:\n          command: \"save-to-local-backup.sh artifacts/\"\n\n  # Step 4: Notify (optional, don't block on failure)\n  - shell: \"notify-deployment.sh\"\n    retry_config:\n      attempts: 2\n      initial_delay: \"5s\"\n      on_failure: continue\n\n  # Step 5: Health check (retry with fixed delay)\n  - shell: \"health-check.sh\"\n    retry_config:\n      attempts: 10\n      backoff: fixed\n      initial_delay: \"10s\"\n      on_failure: stop\n</code></pre>"},{"location":"retry-configuration/complete-examples/#example-13-mapreduce-with-dlq-and-retry","title":"Example 13: MapReduce with DLQ and Retry","text":"<p>MapReduce workflow with error handling:</p> <pre><code>name: mapreduce-with-retry\nmode: mapreduce\n\nerror_policy:\n  on_item_failure: dlq        # Send failures to Dead Letter Queue\n  continue_on_failure: true   # Keep processing other items\n  max_failures: 5             # Stop if more than 5 items fail\n\nmap:\n  input: \"work-items.json\"\n  json_path: \"$.items[*]\"\n  max_parallel: 10\n\n  agent_template:\n    - shell: \"process-item ${item.id}\"\n      retry_config:\n        attempts: 3\n        backoff: exponential\n        initial_delay: \"1s\"\n        max_delay: \"30s\"\n        jitter: true          # Important for parallel agents\n        retry_on:\n          - network\n          - timeout\n        on_failure: stop      # Let DLQ handle final failures\n\nreduce:\n  - shell: \"aggregate-results ${map.results}\"\n</code></pre> <p>Error handling flow: 1. Each work item is retried up to 3 times per agent 2. If all retries fail \u2192 Item goes to DLQ 3. Processing continues for other items 4. After map phase, retry DLQ items with: <code>prodigy dlq retry &lt;job_id&gt;</code></p> <p>Source: Workflow-level retry in src/cook/workflow/error_policy.rs:90-129</p>"},{"location":"retry-configuration/complete-examples/#testing-your-retry-configuration","title":"Testing Your Retry Configuration","text":"<p>Validate retry behavior with controlled failures:</p> <pre><code>name: test-retry-behavior\nmode: standard\n\ncommands:\n  # Use a script that fails N times then succeeds\n  - shell: \"./fail-then-succeed.sh 2\"  # Fails 2 times, succeeds on 3rd\n    retry_config:\n      attempts: 5\n      backoff: exponential\n      initial_delay: \"1s\"\n</code></pre> <p>fail-then-succeed.sh example: <pre><code>#!/bin/bash\nFAIL_COUNT=${1:-2}\nSTATE_FILE=\"/tmp/retry-test-$$\"\n\nif [ ! -f \"$STATE_FILE\" ]; then\n  echo \"0\" &gt; \"$STATE_FILE\"\nfi\n\nCURRENT=$(cat \"$STATE_FILE\")\nNEXT=$((CURRENT + 1))\necho \"$NEXT\" &gt; \"$STATE_FILE\"\n\nif [ \"$NEXT\" -le \"$FAIL_COUNT\" ]; then\n  echo \"Attempt $NEXT: Simulated failure\"\n  exit 1\nelse\n  echo \"Attempt $NEXT: Success!\"\n  rm \"$STATE_FILE\"\n  exit 0\nfi\n</code></pre></p>"},{"location":"retry-configuration/complete-examples/#see-also","title":"See Also","text":"<ul> <li>Basic Retry Configuration - Configuration options</li> <li>Backoff Strategies - Detailed backoff strategy documentation</li> <li>Conditional Retry with Error Matchers - Selective retry</li> <li>Failure Actions - Handling final failures</li> <li>Best Practices - When to use each pattern</li> </ul>"},{"location":"retry-configuration/conditional-retry-with-error-matchers/","title":"Conditional Retry with Error Matchers","text":""},{"location":"retry-configuration/conditional-retry-with-error-matchers/#conditional-retry-with-error-matchers","title":"Conditional Retry with Error Matchers","text":"<p>By default, Prodigy retries all errors when <code>retry_on</code> is empty. Use the <code>retry_on</code> field to retry only specific error types, allowing fine-grained control over which failures should trigger retries.</p> <p>Source: <code>src/cook/retry_v2.rs:100-151</code> (ErrorMatcher enum definition)</p> <p>Case Sensitivity Behavior: - Built-in matchers (Network, Timeout, ServerError, RateLimit): Case-insensitive - error messages are normalized to lowercase before matching - Pattern matcher: Case-sensitive by default - matches against original error message case (src/cook/retry_v2.rs:142-148)   - Use regex flag <code>(?i)</code> for case-insensitive pattern matching   - Example: <code>pattern: '(?i)database locked'</code> matches \"Database Locked\", \"DATABASE LOCKED\", etc.</p>"},{"location":"retry-configuration/conditional-retry-with-error-matchers/#available-error-matchers","title":"Available Error Matchers","text":"<p>The <code>ErrorMatcher</code> enum provides five built-in matchers for common error categories:</p>"},{"location":"retry-configuration/conditional-retry-with-error-matchers/#1-network-errors","title":"1. Network Errors","text":"<p>Matches network connectivity issues:</p> <pre><code>retry_config:\n  attempts: 5\n  retry_on:\n    - network\n</code></pre> <p>Matches (case-insensitive): - \"network\" - \"connection\" - \"refused\" - \"unreachable\"</p> <p>Source: <code>src/cook/retry_v2.rs:128-132</code></p> <p>Use Case: Retrying HTTP requests, database connections, or API calls that fail due to network issues.</p>"},{"location":"retry-configuration/conditional-retry-with-error-matchers/#2-timeout-errors","title":"2. Timeout Errors","text":"<p>Matches timeout-related failures:</p> <pre><code>retry_config:\n  attempts: 3\n  retry_on:\n    - timeout\n</code></pre> <p>Matches (case-insensitive): - \"timeout\" - \"timed out\"</p> <p>Source: <code>src/cook/retry_v2.rs:133-137</code></p> <p>Use Case: Retrying slow external services or operations with strict time limits.</p>"},{"location":"retry-configuration/conditional-retry-with-error-matchers/#3-server-errors","title":"3. Server Errors","text":"<p>Matches HTTP 5xx server errors:</p> <pre><code>retry_config:\n  attempts: 4\n  retry_on:\n    - server_error\n</code></pre> <p>Matches (case-insensitive): - \"500\" - \"502\" - \"503\" - \"504\" - \"server error\"</p> <p>Source: <code>src/cook/retry_v2.rs:138-142</code></p> <p>Use Case: Retrying API requests during transient server failures or deployments.</p>"},{"location":"retry-configuration/conditional-retry-with-error-matchers/#4-rate-limit-errors","title":"4. Rate Limit Errors","text":"<p>Matches rate limiting responses:</p> <pre><code>retry_config:\n  attempts: 10\n  initial_delay: \"60s\"\n  retry_on:\n    - rate_limit\n</code></pre> <p>Matches (case-insensitive): - \"rate limit\" - \"429\" - \"too many requests\"</p> <p>Source: <code>src/cook/retry_v2.rs:143-147</code></p> <p>Use Case: Retrying API calls with exponential backoff when hitting rate limits.</p>"},{"location":"retry-configuration/conditional-retry-with-error-matchers/#5-custom-pattern-matching","title":"5. Custom Pattern Matching","text":"<p>Match specific error messages using regex patterns:</p> <pre><code>retry_config:\n  attempts: 3\n  retry_on:\n    - pattern: \"(?i)database locked\"    # Case-insensitive: matches any case\n    - pattern: \"SQLITE_BUSY\"            # Case-sensitive: exact match only\n    - pattern: \"(?i)temporary failure\"  # Case-insensitive\n</code></pre> <p>Pattern Syntax: - Regex patterns matched against original error message (case-sensitive by default) - Use <code>(?i)</code> flag at start of pattern for case-insensitive matching - Invalid regex patterns return false (no match)</p> <p>Source: <code>src/cook/retry_v2.rs:142-148</code> (Pattern variant implementation)</p> <p>Use Case: Matching application-specific error messages or database-specific errors.</p> <p>Case Sensitivity Examples: - <code>pattern: \"SQLITE_BUSY\"</code> - Only matches \"SQLITE_BUSY\" (not \"sqlite_busy\") - <code>pattern: \"(?i)SQLITE_BUSY\"</code> - Matches \"SQLITE_BUSY\", \"sqlite_busy\", \"Sqlite_Busy\", etc. - <code>pattern: \"(?i)database.*locked\"</code> - Case-insensitive regex with wildcards</p>"},{"location":"retry-configuration/conditional-retry-with-error-matchers/#combining-multiple-matchers","title":"Combining Multiple Matchers","text":"<p>You can specify multiple error matchers to retry on any of them:</p> <pre><code>retry_config:\n  attempts: 5\n  backoff: exponential\n  initial_delay: \"2s\"\n  retry_on:\n    - network\n    - timeout\n    - server_error\n</code></pre> <p>This configuration retries if the error matches any of: - Network errors - Timeout errors - Server errors (5xx)</p> <p>Behavior: Matchers are evaluated with OR logic - if any matcher matches, the error is retryable.</p>"},{"location":"retry-configuration/conditional-retry-with-error-matchers/#empty-retry_on-retry-all-errors","title":"Empty retry_on (Retry All Errors)","text":"<p>When <code>retry_on</code> is empty or omitted, all errors trigger retry:</p> <pre><code>retry_config:\n  attempts: 3\n  # retry_on is empty - retries all errors\n</code></pre> <p>Source: <code>src/cook/retry_v2.rs:42-43</code> (retry_on field with default <code>Vec::new()</code>)</p> <p>This is equivalent to having no error filtering - every failure triggers the retry logic.</p>"},{"location":"retry-configuration/conditional-retry-with-error-matchers/#selective-retry-example","title":"Selective Retry Example","text":"<p>Only retry transient network and timeout issues, but fail immediately on other errors:</p> <pre><code>commands:\n  - shell: \"curl -f https://api.example.com/data\"\n    retry_config:\n      attempts: 5\n      backoff: exponential\n      initial_delay: \"1s\"\n      max_delay: \"30s\"\n      retry_on:\n        - network\n        - timeout\n</code></pre> <p>Behavior: - If curl fails with \"connection refused\" \u2192 Retry - If curl fails with \"timeout\" \u2192 Retry - If curl fails with \"404 Not Found\" \u2192 Fail immediately (no retry) - If curl fails with \"401 Unauthorized\" \u2192 Fail immediately (no retry)</p>"},{"location":"retry-configuration/conditional-retry-with-error-matchers/#regex-pattern-syntax-and-case-sensitivity","title":"Regex Pattern Syntax and Case Sensitivity","text":"<p>Understanding case sensitivity is critical when using Pattern matchers:</p> Matcher Type Case Sensitivity Normalization Network Case-insensitive Error converted to lowercase Timeout Case-insensitive Error converted to lowercase ServerError Case-insensitive Error converted to lowercase RateLimit Case-insensitive Error converted to lowercase Pattern Case-sensitive by default No normalization (original case) <p>Making Pattern Matching Case-Insensitive:</p> <p>Use the <code>(?i)</code> flag at the start of your regex pattern:</p> <pre><code>retry_on:\n  # Case-insensitive patterns (recommended)\n  - pattern: \"(?i)connection refused\"  # Matches any case variation\n  - pattern: \"(?i)database.*locked\"    # Case-insensitive with wildcards\n\n  # Case-sensitive patterns (use with caution)\n  - pattern: \"SQLITE_BUSY\"             # Only matches exact case\n  - pattern: \"ERROR: Authentication\"   # Must match exact case\n</code></pre> <p>Invalid Regex Handling:</p> <p>If a pattern contains invalid regex syntax, it returns <code>false</code> (no match):</p> <pre><code>retry_on:\n  - pattern: \"[invalid(regex\"  # Invalid syntax \u2192 returns false \u2192 no retry\n</code></pre> <p>Source: src/cook/retry_v2.rs:142-148</p>"},{"location":"retry-configuration/conditional-retry-with-error-matchers/#advanced-pattern-matching","title":"Advanced Pattern Matching","text":"<p>Use regex patterns for precise error matching:</p> <pre><code>retry_config:\n  attempts: 3\n  retry_on:\n    # Case-insensitive patterns (recommended for flexible matching)\n    - pattern: \"(?i)SQLite.*database is locked\"\n    - pattern: \"(?i)deadlock detected\"\n\n    # Case-sensitive pattern (exact match required)\n    - pattern: \"SQLITE_BUSY\"\n</code></pre> <p>Pattern Matching Logic (src/cook/retry_v2.rs:116-150): 1. Each matcher's <code>matches()</code> method is called with the error message 2. Built-in matchers normalize error to lowercase before checking 3. Pattern matcher applies regex to original case of error message 4. Invalid regex patterns return false (no match, no retry) 5. If any matcher returns true, error is retryable</p>"},{"location":"retry-configuration/conditional-retry-with-error-matchers/#implementation-details","title":"Implementation Details","text":"<p>The matching logic is implemented in <code>ErrorMatcher::matches()</code>:</p> <pre><code>// Simplified implementation (src/cook/retry_v2.rs:116-150)\nimpl ErrorMatcher {\n    pub fn matches(&amp;self, error_msg: &amp;str) -&gt; bool {\n        let error_lower = error_msg.to_lowercase();\n        match self {\n            Self::Network =&gt; {\n                // Case-insensitive matching via lowercase normalization\n                error_lower.contains(\"network\")\n                    || error_lower.contains(\"connection\")\n                    || error_lower.contains(\"refused\")\n                    || error_lower.contains(\"unreachable\")\n            }\n            Self::Timeout =&gt; {\n                error_lower.contains(\"timeout\") || error_lower.contains(\"timed out\")\n            }\n            Self::ServerError =&gt; {\n                error_lower.contains(\"500\")\n                    || error_lower.contains(\"502\")\n                    || error_lower.contains(\"503\")\n                    || error_lower.contains(\"504\")\n                    || error_lower.contains(\"server error\")\n            }\n            Self::RateLimit =&gt; {\n                error_lower.contains(\"rate limit\")\n                    || error_lower.contains(\"429\")\n                    || error_lower.contains(\"too many requests\")\n            }\n            Self::Pattern(pattern) =&gt; {\n                // Case-sensitive by default - matches against original error_msg\n                // Use (?i) flag in pattern for case-insensitive matching\n                if let Ok(re) = regex::Regex::new(pattern) {\n                    re.is_match(error_msg)  // Uses original case, not error_lower\n                } else {\n                    false  // Invalid regex = no match\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"retry-configuration/conditional-retry-with-error-matchers/#testing-error-matchers","title":"Testing Error Matchers","text":"<p>The retry_v2 module includes tests for built-in matchers (src/cook/retry_v2.rs:463-502):</p> <pre><code>#[test]\nfn test_error_matcher_network() {\n    let matcher = ErrorMatcher::Network;\n    assert!(matcher.matches(\"Connection refused\"));\n    assert!(matcher.matches(\"Network unreachable\"));\n    assert!(matcher.matches(\"connection timeout\"));  // Case-insensitive\n    assert!(!matcher.matches(\"Syntax error\"));\n}\n\n#[test]\nfn test_error_matcher_timeout() {\n    let matcher = ErrorMatcher::Timeout;\n    assert!(matcher.matches(\"Operation timeout\"));\n    assert!(matcher.matches(\"Request timed out\"));\n    assert!(!matcher.matches(\"Network error\"));\n}\n\n#[test]\nfn test_error_matcher_rate_limit() {\n    let matcher = ErrorMatcher::RateLimit;\n    assert!(matcher.matches(\"Rate limit exceeded\"));\n    assert!(matcher.matches(\"Error 429\"));\n    assert!(matcher.matches(\"Too many requests\"));\n    assert!(!matcher.matches(\"Server error\"));\n}\n</code></pre> <p>Note: Pattern matcher tests are not yet implemented in the test suite. The above tests cover Network, Timeout, and RateLimit matchers only.</p>"},{"location":"retry-configuration/conditional-retry-with-error-matchers/#see-also","title":"See Also","text":"<ul> <li>Basic Retry Configuration - Core retry configuration</li> <li>Failure Actions - What happens when all retries are exhausted</li> <li>Best Practices - When to use selective retry</li> <li>Complete Examples - Full workflow examples with error matchers</li> </ul>"},{"location":"retry-configuration/failure-actions/","title":"Failure Actions","text":""},{"location":"retry-configuration/failure-actions/#failure-actions","title":"Failure Actions","text":"<p>IMPORTANT: This subsection documents the <code>on_failure</code> mechanism for workflow shell commands, which uses <code>TestDebugConfig</code> for automatic debugging and retry. This is distinct from the theoretical <code>retry_config.on_failure</code> field shown in the parent chapter overview, which is defined in code but not yet integrated into command execution.</p> <p>Configure automatic debugging and retry behavior when shell commands fail using the <code>on_failure</code> field. This mechanism allows Claude to automatically diagnose and fix test failures, build errors, and other command failures.</p> <p>Source: <code>src/config/command.rs:168-183</code> (TestDebugConfig struct), <code>src/config/command.rs:372</code> (WorkflowStepCommand.on_failure field)</p>"},{"location":"retry-configuration/failure-actions/#what-is-testdebugconfig","title":"What is TestDebugConfig?","text":"<p>The <code>on_failure</code> field in workflow commands accepts a <code>TestDebugConfig</code> object that specifies: - A Claude command to run when the shell command fails - How many times to retry the debug-fix cycle - Whether to fail the entire workflow if debugging doesn't fix the issue - Whether the debug command should create git commits</p> <p>This enables self-healing workflows where Claude automatically attempts to fix failures before escalating them.</p> <p>Source: <code>src/config/command.rs:168-183</code></p> <pre><code>pub struct TestDebugConfig {\n    /// Claude command to run on test failure\n    pub claude: String,\n\n    /// Maximum number of retry attempts (default: 3)\n    pub max_attempts: u32,\n\n    /// Whether to fail the workflow if max attempts reached (default: false)\n    pub fail_workflow: bool,\n\n    /// Whether the debug command should create commits (default: true)\n    pub commit_required: bool,\n}\n</code></pre>"},{"location":"retry-configuration/failure-actions/#basic-syntax","title":"Basic Syntax","text":"<p>The simplest form specifies just the Claude command to run on failure:</p> <pre><code>- shell: \"cargo test\"\n  on_failure:\n    claude: \"/prodigy-debug-test-failure --output ${shell.output}\"\n</code></pre> <p>Execution Flow: 1. Run <code>cargo test</code> 2. If it fails \u2192 Capture output in <code>${shell.output}</code> 3. Run <code>/prodigy-debug-test-failure --output ${shell.output}</code> 4. Claude analyzes failure and attempts fix 5. If Claude creates commits \u2192 Re-run <code>cargo test</code> 6. Repeat up to <code>max_attempts</code> times (default: 3)</p> <p>Source: Example from <code>workflows/documentation-drift.yml:47-53</code></p>"},{"location":"retry-configuration/failure-actions/#configuration-options","title":"Configuration Options","text":""},{"location":"retry-configuration/failure-actions/#full-configuration","title":"Full Configuration","text":"<p>All <code>on_failure</code> options:</p> <pre><code>- shell: \"cargo test --doc\"\n  on_failure:\n    claude: \"/prodigy-debug-test-failure --output ${shell.output}\"\n    max_attempts: 3           # Maximum debug-fix-retest cycles (default: 3)\n    fail_workflow: false      # Continue workflow even if can't fix (default: false)\n    commit_required: true     # Debug command should create commits (default: true)\n</code></pre> <p>Source: Example from <code>workflows/documentation-drift.yml:47-53</code></p>"},{"location":"retry-configuration/failure-actions/#max_attempts","title":"max_attempts","text":"<p>Controls how many debug-fix-retest cycles to attempt:</p> <pre><code>- shell: \"just fmt-check &amp;&amp; just lint\"\n  on_failure:\n    claude: \"/prodigy-lint ${shell.output}\"\n    max_attempts: 5           # Try up to 5 times to fix linting issues\n</code></pre> <p>Default: 3 attempts (defined in <code>src/config/command.rs:173</code>)</p> <p>Use Cases: - 1 attempt: Quick fixes only, don't waste time on hard problems - 3 attempts (default): Reasonable for most test failures - 5+ attempts: Complex issues that might need multiple iterations</p> <p>Source: <code>workflows/implement.yml:27-30</code></p>"},{"location":"retry-configuration/failure-actions/#fail_workflow","title":"fail_workflow","text":"<p>Controls whether to stop the entire workflow if debugging can't fix the issue:</p> <pre><code># CRITICAL: Must succeed - fail workflow if can't fix\n- shell: \"cargo test --lib\"\n  on_failure:\n    claude: \"/prodigy-debug-test-failure --output ${shell.output}\"\n    max_attempts: 3\n    fail_workflow: true       # Stop workflow if tests still fail after 3 attempts\n</code></pre> <pre><code># NON-CRITICAL: Best effort - continue even if can't fix\n- shell: \"cargo test --doc\"\n  on_failure:\n    claude: \"/prodigy-fix-doc-tests --output ${shell.output}\"\n    max_attempts: 2\n    fail_workflow: false      # Continue workflow even if doc tests fail\n</code></pre> <p>Default: <code>false</code> - workflow continues even if debugging doesn't fix the issue (defined in <code>src/config/command.rs:177</code>)</p> <p>Source: Examples from <code>workflows/coverage-with-test-debug.yml:14-23</code></p>"},{"location":"retry-configuration/failure-actions/#commit_required","title":"commit_required","text":"<p>Controls whether the debug command must create git commits:</p> <pre><code>- shell: \"cargo clippy -- -D warnings\"\n  on_failure:\n    claude: \"/prodigy-lint ${shell.output}\"\n    commit_required: true     # Must commit fixes (default)\n</code></pre> <p>Default: <code>true</code> - debug command must create commits (defined in <code>src/config/command.rs:181</code>)</p> <p>When to set to false: Rare - only if the debug command doesn't modify code (e.g., just logs diagnostic info)</p> <p>Source: Field definition <code>src/config/command.rs:180-182</code></p>"},{"location":"retry-configuration/failure-actions/#real-world-examples","title":"Real-World Examples","text":""},{"location":"retry-configuration/failure-actions/#example-1-test-debugging-with-multiple-attempts","title":"Example 1: Test Debugging with Multiple Attempts","text":"<p>From <code>workflows/implement.yml</code>:</p> <pre><code>- shell: \"cargo test\"\n  timeout: 600\n  on_failure:\n    claude: \"/prodigy-debug-test-failure --spec $ARG --output ${shell.output}\"\n    max_attempts: 5\n    fail_workflow: false      # Continue even if tests can't be fixed\n</code></pre> <p>Behavior: - Run tests with 10-minute timeout - On failure, Claude analyzes output and fixes issues - Try up to 5 debug-fix-test cycles - If still failing after 5 attempts, continue workflow anyway - Each fix creates a commit</p> <p>Source: <code>workflows/implement.yml:19-24</code></p>"},{"location":"retry-configuration/failure-actions/#example-2-critical-vs-non-critical-commands","title":"Example 2: Critical vs Non-Critical Commands","text":"<p>From <code>workflows/coverage-with-test-debug.yml</code>:</p> <pre><code># CRITICAL: Library tests must pass\n- shell: \"cargo test --lib\"\n  on_failure:\n    claude: \"/prodigy-debug-test-failure --spec ${coverage.spec} --output ${shell.output}\"\n    max_attempts: 3\n    fail_workflow: true       # STOP if can't fix\n\n# NON-CRITICAL: Doc tests are best-effort\n- shell: \"cargo test --doc\"\n  on_failure:\n    claude: \"/prodigy-fix-doc-tests --output ${shell.output}\"\n    max_attempts: 2\n    fail_workflow: false      # CONTINUE if can't fix\n</code></pre> <p>Source: <code>workflows/coverage-with-test-debug.yml:13-24</code></p>"},{"location":"retry-configuration/failure-actions/#example-3-chaining-with-on_success","title":"Example 3: Chaining with on_success","text":"<p>From <code>workflows/implement-with-tests.yml</code>:</p> <pre><code>- shell: \"cargo test\"\n  on_failure:\n    # If tests fail, debug and fix them\n    claude: \"/prodigy-debug-test-failures '${test_output}'\"\n    commit_required: true\n    on_success:\n      # After fixing, verify tests now pass\n      - shell: \"cargo test\"\n        on_failure:\n          # If STILL failing, try deeper analysis\n          claude: \"/prodigy-fix-test-failures '${shell.output}' --deep-analysis\"\n          commit_required: true\n</code></pre> <p>Nested Retry Logic: 1. Run tests 2. If fail \u2192 Debug and fix 3. If fix succeeded (commit created) \u2192 Re-run tests 4. If tests STILL fail \u2192 Try deeper analysis 5. Verify second fix works</p> <p>Source: <code>workflows/implement-with-tests.yml:27-39</code></p>"},{"location":"retry-configuration/failure-actions/#example-4-linting-after-implementation","title":"Example 4: Linting After Implementation","text":"<p>From <code>workflows/documentation-drift.yml</code>:</p> <pre><code>- shell: \"just fmt-check &amp;&amp; just lint\"\n  on_failure:\n    claude: \"/prodigy-lint ${shell.output}\"\n    commit_required: true\n    max_attempts: 3\n    fail_workflow: false      # Non-blocking - formatting issues shouldn't stop workflow\n</code></pre> <p>Use Case: After implementing features, ensure code is properly formatted and linted, but don't block if there are style issues.</p> <p>Source: <code>workflows/documentation-drift.yml:56-61</code></p>"},{"location":"retry-configuration/failure-actions/#how-it-works-execution-flow","title":"How It Works: Execution Flow","text":""},{"location":"retry-configuration/failure-actions/#retry-loop","title":"Retry Loop","text":"<p>When a shell command with <code>on_failure</code> fails:</p> <ol> <li>Initial Failure: Shell command exits with non-zero code</li> <li>Capture Output: Stderr/stdout saved to <code>${shell.output}</code></li> <li>First Debug Attempt:</li> <li>Run Claude command with failure output</li> <li>Claude analyzes error and makes fixes</li> <li>If <code>commit_required=true</code>, check for git commits</li> <li>Retry Original Command:</li> <li>If commits were created \u2192 Re-run original shell command</li> <li>If no commits \u2192 Debug didn't fix anything, stop retrying</li> <li>Repeat: Continue debug-fix-retry cycle up to <code>max_attempts</code></li> <li>Final Result:</li> <li>If any attempt succeeded \u2192 Command passes</li> <li>If all attempts failed and <code>fail_workflow=true</code> \u2192 Workflow stops</li> <li>If all attempts failed and <code>fail_workflow=false</code> \u2192 Workflow continues</li> </ol>"},{"location":"retry-configuration/failure-actions/#commit-requirement-logic","title":"Commit Requirement Logic","text":"<p>The <code>commit_required</code> field interacts with the retry loop:</p> <ul> <li><code>commit_required=true</code> (default): Only retry if Claude created commits</li> <li>Rationale: If Claude didn't commit, it didn't find a fix</li> <li> <p>Prevents infinite loops where Claude can't solve the problem</p> </li> <li> <p><code>commit_required=false</code>: Retry even if no commits</p> </li> <li>Rare use case: Debug command doesn't modify code (logging, diagnostics)</li> </ul> <p>Source: Conceptual flow based on <code>src/config/command.rs:168-183</code> definition and actual usage in workflows</p>"},{"location":"retry-configuration/failure-actions/#limitations-and-gotchas","title":"Limitations and Gotchas","text":""},{"location":"retry-configuration/failure-actions/#1-only-works-for-shell-commands","title":"1. Only Works for Shell Commands","text":"<p>The <code>on_failure</code> field is only available for <code>shell:</code> commands, NOT for <code>claude:</code> commands:</p> <pre><code># \u2713 WORKS - shell command with on_failure\n- shell: \"cargo test\"\n  on_failure:\n    claude: \"/debug-test\"\n\n# \u2717 DOES NOT WORK - claude command doesn't support on_failure\n- claude: \"/implement-feature\"\n  on_failure:              # This field is ignored!\n    claude: \"/fix-issue\"\n</code></pre> <p>Source: <code>src/config/command.rs:320-400</code> - WorkflowStepCommand has separate <code>shell</code> and <code>claude</code> fields, <code>on_failure</code> applies only to shell commands</p>"},{"location":"retry-configuration/failure-actions/#2-not-the-same-as-retry_config","title":"2. Not the Same as retry_config","text":"<p>IMPORTANT: The <code>on_failure: TestDebugConfig</code> documented here is NOT the same as the theoretical <code>retry_config.on_failure: FailureAction</code> mentioned in the parent chapter.</p> <pre><code># \u2713 ACTUAL SYNTAX (what this subsection documents)\n- shell: \"cargo test\"\n  on_failure:\n    claude: \"/debug\"\n    max_attempts: 3\n\n# \u2717 DOES NOT WORK (theoretical syntax, not implemented)\n- shell: \"cargo test\"\n  retry_config:\n    on_failure: stop        # This field exists in code but is NOT wired into execution\n</code></pre> <p>The <code>retry_config</code> field is not defined in WorkflowStepCommand (see <code>src/config/command.rs:320-400</code>). The <code>retry_v2::FailureAction</code> enum exists in <code>src/cook/retry_v2.rs:157-165</code> but is not integrated into command execution.</p>"},{"location":"retry-configuration/failure-actions/#3-testdebugconfig-is-not-retry-configuration","title":"3. TestDebugConfig is Not Retry Configuration","text":"<p>Despite the name and retry-like behavior, <code>TestDebugConfig</code> is a debugging mechanism, not a retry mechanism:</p> <ul> <li>Retry: Re-run the same command without changes (for transient failures)</li> <li>Debug: Run a DIFFERENT command (Claude) to diagnose and fix the issue, THEN re-run</li> </ul> <p>The <code>TestDebugConfig</code> mechanism assumes failures are due to code issues that need fixing, not transient errors.</p>"},{"location":"retry-configuration/failure-actions/#4-max-attempts-includes-initial-failure","title":"4. Max Attempts Includes Initial Failure","text":"<p>If <code>max_attempts: 3</code>, the execution pattern is: 1. Initial run (fails) 2. Debug attempt #1 \u2192 Retry 3. Debug attempt #2 \u2192 Retry 4. Debug attempt #3 \u2192 Retry</p> <p>So the original command runs up to 4 times total (1 initial + 3 debug-retry cycles).</p>"},{"location":"retry-configuration/failure-actions/#comparison-with-other-retry-mechanisms","title":"Comparison with Other Retry Mechanisms","text":"<p>Prodigy has multiple mechanisms that might be confused:</p> Mechanism Purpose Scope When to Use <code>on_failure: TestDebugConfig</code> Auto-debug and fix code issues Shell commands Test failures, build errors that need code fixes <code>retry_config</code> (theoretical) Retry with backoff for transient errors Commands (not implemented) Network errors, timeouts (when implemented) Workflow-level retry Retry entire work items MapReduce jobs Bulk operation failures, DLQ retry <code>on_success</code> chaining Sequential command execution Any command Multi-step validation, verification after fixes <p>This subsection documents only: <code>on_failure: TestDebugConfig</code></p> <p>For workflow-level retry, see Workflow-Level vs Command-Level Retry.</p>"},{"location":"retry-configuration/failure-actions/#migration-from-deprecated-test-syntax","title":"Migration from Deprecated test: Syntax","text":"<p>The <code>test:</code> command type is deprecated. Migrate to <code>shell:</code> with <code>on_failure:</code>:</p> <p>Old (Deprecated): <pre><code>- test:\n    command: \"cargo test\"\n    on_failure:\n      claude: \"/debug\"\n</code></pre></p> <p>New (Current): <pre><code>- shell: \"cargo test\"\n  on_failure:\n    claude: \"/debug\"\n</code></pre></p> <p>Source: Deprecation warning in <code>src/config/command.rs:446-455</code></p>"},{"location":"retry-configuration/failure-actions/#see-also","title":"See Also","text":"<ul> <li>Basic Retry Configuration - Retry configuration basics</li> <li>Conditional Retry with Error Matchers - Selective retry by error type</li> <li>Best Practices - When to use each failure handling mechanism</li> <li>Complete Examples - Full workflow examples</li> <li>Workflow-Level vs Command-Level Retry - Comparison of retry systems</li> </ul>"},{"location":"retry-configuration/implementation-references/","title":"Implementation References","text":""},{"location":"retry-configuration/implementation-references/#implementation-references","title":"Implementation References","text":"<p>This section provides pointers to the source code implementing the retry system. These references are useful for developers who want to understand implementation details, extend the retry functionality, or troubleshoot issues.</p>"},{"location":"retry-configuration/implementation-references/#core-retry-system","title":"Core Retry System","text":""},{"location":"retry-configuration/implementation-references/#enhanced-retry-configuration-srccookretry_v2rs14-461","title":"Enhanced Retry Configuration (<code>src/cook/retry_v2.rs:14-461</code>)","text":"<p>The primary implementation of Prodigy's retry system with comprehensive features:</p> <ul> <li>RetryConfig struct (lines 14-52): Main configuration type with fields for:</li> <li>Maximum retry attempts</li> <li>Backoff strategies (Fixed, Linear, Exponential, Fibonacci, Custom)</li> <li>Initial and maximum delays</li> <li>Jitter support for preventing thundering herd</li> <li>Error matching for conditional retries</li> <li>Retry budget (maximum total time)</li> <li> <p>Failure actions</p> </li> <li> <p>BackoffStrategy enum (lines 70-90): Implements multiple delay calculation strategies:</p> </li> <li><code>Fixed</code>: Constant delay between retries</li> <li><code>Linear</code>: Incrementing delay (initial + n * increment)</li> <li><code>Exponential</code>: Doubling delay (initial * base^n)</li> <li><code>Fibonacci</code>: Fibonacci sequence delays</li> <li> <p><code>Custom</code>: User-defined delay sequence</p> </li> <li> <p>Retry execution logic: Core retry loop with backoff calculation, jitter application, and timeout enforcement</p> </li> </ul> <p>Example usage (from src/cook/retry_v2.rs): <pre><code>let config = RetryConfig {\n    attempts: 5,\n    backoff: BackoffStrategy::Exponential { base: 2.0 },\n    initial_delay: Duration::from_millis(100),\n    max_delay: Duration::from_secs(60),\n    jitter: true,\n    jitter_factor: 0.1,\n    retry_on: vec![ErrorMatcher::Network, ErrorMatcher::Timeout],\n    retry_budget: Some(Duration::from_secs(300)),\n    on_failure: FailureAction::Fail,\n};\n</code></pre></p>"},{"location":"retry-configuration/implementation-references/#workflow-error-policy-srccookworkflowerror_policyrs91-129","title":"Workflow Error Policy (<code>src/cook/workflow/error_policy.rs:91-129</code>)","text":"<p>Simpler retry configuration used at the workflow level:</p> <ul> <li>RetryConfig struct (lines 91-99): Workflow-scoped retry settings with:</li> <li><code>max_attempts</code>: Maximum number of retry attempts (default: 3)</li> <li> <p><code>backoff</code>: Backoff strategy for delay calculation</p> </li> <li> <p>BackoffStrategy enum (lines 105-120): Workflow backoff strategies with duration parameters:</p> </li> <li><code>Fixed { delay }</code>: Fixed delay between retries</li> <li><code>Linear { initial, increment }</code>: Linear backoff with configurable increment</li> <li><code>Exponential { initial, multiplier }</code>: Exponential backoff with configurable base</li> <li> <p><code>Fibonacci { initial }</code>: Fibonacci sequence starting from initial delay</p> </li> <li> <p>WorkflowErrorPolicy struct (lines 131-140+): Workflow-level error handling with:</p> </li> <li><code>on_item_failure</code>: Action to take when items fail</li> <li><code>continue_on_failure</code>: Whether to continue processing after failures</li> </ul> <p>Example usage (from workflow YAML): <pre><code>retry_config:\n  max_attempts: 5\n  backoff:\n    exponential:\n      initial: 1s\n      multiplier: 2.0\n</code></pre></p>"},{"location":"retry-configuration/implementation-references/#command-metadata-srcconfigcommandrs135","title":"Command Metadata (<code>src/config/command.rs:135</code>)","text":"<p>Command-level retry override configuration:</p> <ul> <li>CommandMetadata struct (lines 133-149): Per-command settings including:</li> <li><code>retries: Option&lt;u32&gt;</code>: Override global retry attempts for specific commands</li> <li><code>timeout: Option&lt;u64&gt;</code>: Command-specific timeout in seconds</li> <li><code>continue_on_error: Option&lt;bool&gt;</code>: Whether workflow continues if command fails</li> <li><code>env</code>: Environment variables for the command</li> <li><code>commit_required</code>: Whether command must create git commits</li> </ul> <p>This allows fine-grained control over retry behavior at the command level, overriding workflow-level settings when needed.</p> <p>Example usage (from workflow YAML): <pre><code>commands:\n  - shell: \"flaky-network-operation.sh\"\n    retries: 10\n    timeout: 300\n    continue_on_error: false\n</code></pre></p>"},{"location":"retry-configuration/implementation-references/#error-handling-components","title":"Error Handling Components","text":""},{"location":"retry-configuration/implementation-references/#error-matchers-srccookretry_v2rs100-151","title":"Error Matchers (<code>src/cook/retry_v2.rs:100-151</code>)","text":"<p>Conditional retry based on error patterns:</p> <ul> <li>ErrorMatcher enum (lines 101-114): Pre-defined error categories:</li> <li><code>Network</code>: Connection, refused, unreachable errors</li> <li><code>Timeout</code>: Timeout and \"timed out\" errors</li> <li><code>ServerError</code>: HTTP 5xx status codes (500, 502, 503, 504)</li> <li><code>RateLimit</code>: HTTP 429 and \"rate limit\" errors</li> <li> <p><code>Pattern(String)</code>: Custom regex pattern matching</p> </li> <li> <p>Matching logic (lines 116-151): Case-insensitive error message matching with regex support</p> </li> </ul> <p>This enables intelligent retry behavior that only retries on transient errors (network issues, timeouts, rate limits) while failing fast on permanent errors (validation failures, authentication errors).</p> <p>Example usage (from src/cook/retry_v2.rs): <pre><code>retry_on: vec![\n    ErrorMatcher::Network,\n    ErrorMatcher::Timeout,\n    ErrorMatcher::ServerError,\n    ErrorMatcher::Pattern(\"temporary.*unavailable\".to_string()),\n]\n</code></pre></p>"},{"location":"retry-configuration/implementation-references/#resilience-features","title":"Resilience Features","text":""},{"location":"retry-configuration/implementation-references/#circuit-breaker-srccookretry_v2rs325-397","title":"Circuit Breaker (<code>src/cook/retry_v2.rs:325-397</code>)","text":"<p>Protection against cascading failures:</p> <ul> <li>CircuitBreaker struct (lines 325-331): Stateful circuit breaker with:</li> <li><code>failure_threshold</code>: Number of consecutive failures before opening</li> <li><code>recovery_timeout</code>: Time to wait before attempting recovery</li> <li><code>state</code>: Current circuit state (Closed, Open, HalfOpen)</li> <li> <p><code>consecutive_failures</code>: Counter for failure tracking</p> </li> <li> <p>State machine (lines 333-338): Three-state circuit breaker:</p> </li> <li><code>Closed</code>: Normal operation, requests allowed</li> <li><code>Open { until }</code>: Failing, requests blocked until timeout</li> <li> <p><code>HalfOpen</code>: Testing recovery, limited requests allowed</p> </li> <li> <p>State transitions:</p> </li> <li><code>is_open()</code> (lines 351-367): Check state and transition from Open to HalfOpen after timeout</li> <li><code>record_success()</code> (lines 369-379): Reset failures, close circuit if in HalfOpen</li> <li><code>record_failure()</code> (lines 381-397): Increment failures, open circuit if threshold exceeded</li> </ul> <p>Example usage: <pre><code>let breaker = CircuitBreaker::new(5, Duration::from_secs(30));\n\n// Check before operation\nif breaker.is_open().await {\n    return Err(anyhow!(\"Circuit breaker open\"));\n}\n\n// Record result\nmatch operation().await {\n    Ok(result) =&gt; {\n        breaker.record_success().await;\n        Ok(result)\n    }\n    Err(e) =&gt; {\n        breaker.record_failure().await;\n        Err(e)\n    }\n}\n</code></pre></p>"},{"location":"retry-configuration/implementation-references/#observability","title":"Observability","text":""},{"location":"retry-configuration/implementation-references/#retry-metrics-srccookretry_v2rs399-422","title":"Retry Metrics (<code>src/cook/retry_v2.rs:399-422</code>)","text":"<p>Tracking retry behavior for monitoring:</p> <ul> <li>RetryMetrics struct (lines 399-422): Statistics collection including:</li> <li><code>total_attempts</code>: Total retry attempts made</li> <li><code>successful_attempts</code>: Number of successful retries</li> <li><code>failed_attempts</code>: Number of failed retries</li> <li><code>total_delay</code>: Cumulative delay time across all retries</li> <li>Additional timing and success rate metrics</li> </ul> <p>These metrics enable monitoring of retry effectiveness, identifying problematic operations, and tuning retry configurations for optimal performance.</p> <p>Example usage: <pre><code>let metrics = RetryMetrics::default();\n// Metrics are updated during retry execution\nprintln!(\"Success rate: {}/{}\",\n    metrics.successful_attempts,\n    metrics.total_attempts);\n</code></pre></p>"},{"location":"retry-configuration/implementation-references/#organization-by-component","title":"Organization by Component","text":"<p>The retry system is organized into logical components:</p> <p>Core Retry System: - Main retry configuration and execution (src/cook/retry_v2.rs) - Workflow-level configuration (src/cook/workflow/error_policy.rs) - Command-level overrides (src/config/command.rs)</p> <p>Error Handling: - Conditional retry with error matchers (src/cook/retry_v2.rs:100-151) - Error pattern matching and classification</p> <p>Resilience: - Circuit breaker implementation (src/cook/retry_v2.rs:325-397) - State management and failure protection</p> <p>Observability: - Retry metrics and monitoring (src/cook/retry_v2.rs:399-422) - Performance tracking and debugging</p>"},{"location":"retry-configuration/implementation-references/#next-steps","title":"Next Steps","text":"<ul> <li>For practical examples, see the Complete Examples subsection</li> <li>For configuration details, see the Basic Retry Configuration subsection</li> <li>For backoff strategies, see the Backoff Strategies subsection</li> <li>For best practices, see the Best Practices subsection</li> <li>For troubleshooting, see the Troubleshooting subsection</li> </ul>"},{"location":"retry-configuration/jitter-for-distributed-systems/","title":"Jitter for Distributed Systems","text":""},{"location":"retry-configuration/jitter-for-distributed-systems/#jitter-for-distributed-systems","title":"Jitter for Distributed Systems","text":"<p>Jitter adds randomness to retry delays to prevent the \"thundering herd\" problem where many clients retry at the same time. By introducing controlled randomness, jitter helps distribute retry attempts over time rather than having all clients retry simultaneously after a failure.</p>"},{"location":"retry-configuration/jitter-for-distributed-systems/#configuration","title":"Configuration","text":"<pre><code>retry:\n  attempts: 5\n  backoff:\n    exponential:\n      base: 2.0\n  initial_delay: 10s\n  jitter: true           # Enable jitter (default: false)\n  jitter_factor: 0.3     # Jitter factor (default: 0.3 when jitter is enabled)\n</code></pre> <p>Configuration options: - <code>jitter</code>: Boolean flag to enable/disable jitter (default: <code>false</code>) - <code>jitter_factor</code>: Controls the randomness range as a fraction of the base delay (default: <code>0.3</code>)</p> <p>Source: Configuration structure defined in <code>src/cook/retry_v2.rs:455-457</code> (default_jitter_factor)</p>"},{"location":"retry-configuration/jitter-for-distributed-systems/#how-jitter-works","title":"How Jitter Works","text":"<p>The jitter calculation follows this formula:</p> <pre><code>jitter_range = base_delay * jitter_factor\njitter = random(-jitter_range / 2, +jitter_range / 2)\nfinal_delay = base_delay + jitter\n</code></pre> <p>Source: Implementation in <code>src/cook/retry_v2.rs:308-317</code> (apply_jitter method)</p> <p>The implementation uses Rust's <code>random_range</code> function to generate a uniformly distributed random value within the inclusive range. The jitter can be: - Negative: Reduces the delay below the base value - Positive: Increases the delay above the base value</p> <p>This bidirectional randomness ensures retry attempts are spread across time.</p>"},{"location":"retry-configuration/jitter-for-distributed-systems/#example-calculations","title":"Example Calculations","text":"<p>With default <code>jitter_factor: 0.3</code>:</p> <p>For a 10 second base delay: 1. <code>jitter_range = 10s * 0.3 = 3s</code> 2. <code>jitter = random(-1.5s, +1.5s)</code> 3. <code>final_delay = 10s + jitter</code> \u2192 Range: 8.5s to 11.5s</p> <p>With <code>jitter_factor: 0.5</code>:</p> <p>For a 10 second base delay: 1. <code>jitter_range = 10s * 0.5 = 5s</code> 2. <code>jitter = random(-2.5s, +2.5s)</code> 3. <code>final_delay = 10s + jitter</code> \u2192 Range: 7.5s to 12.5s</p> <p>For a 20 second base delay: 1. <code>jitter_range = 20s * 0.5 = 10s</code> 2. <code>jitter = random(-5s, +5s)</code> 3. <code>final_delay = 20s + jitter</code> \u2192 Range: 15s to 25s</p> <p>Source: Test validation in <code>src/cook/retry_v2.rs:654-658</code></p>"},{"location":"retry-configuration/jitter-for-distributed-systems/#interaction-with-max-delay","title":"Interaction with Max Delay","text":"<p>Jitter is applied after backoff calculation and max_delay capping. The execution order is:</p> <ol> <li>Calculate base delay using backoff strategy</li> <li>Apply <code>max_delay</code> cap if configured</li> <li>Apply jitter to the capped delay</li> </ol> <p>This means jittered delays can still exceed <code>max_delay</code> temporarily, but only by the jitter amount. If you need strict delay caps, consider using a lower <code>jitter_factor</code>.</p> <p>Source: Execution order in <code>src/cook/retry_v2.rs:234-235</code> (calculate_delay followed by apply_jitter)</p>"},{"location":"retry-configuration/jitter-for-distributed-systems/#when-to-use-jitter","title":"When to Use Jitter","text":"<p>Recommended use cases: - Multiple clients accessing the same service: Prevents all clients from retrying simultaneously after a service outage - Distributed systems with many workers: Spreads retry load across worker nodes - Rate-limited APIs: Reduces the chance of hitting rate limits from synchronized retries - Preventing synchronized retry storms: Breaks up \"thundering herd\" patterns that can overwhelm recovering services</p> <p>Adjusting jitter_factor: - Low values (0.1 - 0.2): Tight clustering with minor randomization, suitable for stable systems - Medium values (0.3 - 0.5): Balanced spread, recommended for most distributed systems (default: 0.3) - High values (0.6 - 1.0): Wide distribution, useful for highly contended resources or aggressive load spreading</p>"},{"location":"retry-configuration/jitter-for-distributed-systems/#performance-impact","title":"Performance Impact","text":"<p>Jitter adds minimal computational overhead (a single random number generation per retry). The randomization happens in-memory and does not require external resources.</p>"},{"location":"retry-configuration/jitter-for-distributed-systems/#complete-example-with-multiple-backoff-strategies","title":"Complete Example with Multiple Backoff Strategies","text":"<p>Exponential backoff with jitter: <pre><code>retry:\n  attempts: 5\n  backoff:\n    exponential:\n      base: 2.0\n  initial_delay: 1s\n  max_delay: 30s\n  jitter: true\n  jitter_factor: 0.3\n</code></pre> Delay sequence (without jitter): 1s, 2s, 4s, 8s, 16s Delay sequence (with jitter): ~0.85s-1.15s, ~1.7s-2.3s, ~3.4s-4.6s, ~6.8s-9.2s, ~13.6s-18.4s</p> <p>Linear backoff with jitter: <pre><code>retry:\n  attempts: 5\n  backoff:\n    linear:\n      increment: 2s\n  initial_delay: 5s\n  jitter: true\n  jitter_factor: 0.4\n</code></pre> Delay sequence (without jitter): 5s, 7s, 9s, 11s, 13s Delay sequence (with jitter): ~4s-6s, ~5.6s-8.4s, ~7.2s-10.8s, ~8.8s-13.2s, ~10.4s-15.6s</p>"},{"location":"retry-configuration/jitter-for-distributed-systems/#see-also","title":"See Also","text":"<ul> <li>Backoff Strategies - Overview of different backoff strategies</li> <li>Basic Retry Configuration - Core retry configuration options</li> <li>Best Practices - Recommendations for retry configuration in production systems</li> </ul>"},{"location":"retry-configuration/retry-budget/","title":"Retry Budget","text":""},{"location":"retry-configuration/retry-budget/#retry-budget","title":"Retry Budget","text":"<p>A retry budget provides a time-based upper bound on retry operations, preventing workflows from hanging indefinitely even when attempt counts are high. The retry budget limits the cumulative delay time (not total execution time) spent on retries.</p> <p>Source: <code>src/cook/retry_v2.rs:47</code> (retry_budget field in RetryConfig struct)</p>"},{"location":"retry-configuration/retry-budget/#configuration","title":"Configuration","text":"<p>The <code>retry_budget</code> field accepts human-readable duration formats using the <code>humantime_serde</code> parser:</p> <pre><code>retry_config:\n  attempts: 10\n  retry_budget: \"5m\"        # 5 minutes\n  backoff:\n    exponential:\n      base: 2.0\n  initial_delay: \"1s\"\n</code></pre> <p>Supported Duration Formats: - Seconds: <code>\"30s\"</code>, <code>\"300s\"</code> - Minutes: <code>\"5m\"</code>, <code>\"10m\"</code> - Hours: <code>\"1h\"</code>, <code>\"2h\"</code> - Combined: <code>\"1h30m\"</code>, <code>\"2m30s\"</code></p> <p>Source: Duration field with <code>humantime_serde</code> annotation at <code>src/cook/retry_v2.rs:47</code></p>"},{"location":"retry-configuration/retry-budget/#how-it-works","title":"How It Works","text":"<p>Prodigy uses two complementary mechanisms to enforce retry budgets:</p>"},{"location":"retry-configuration/retry-budget/#1-duration-based-enforcement-active-execution","title":"1. Duration-Based Enforcement (Active Execution)","text":"<p>During active command execution, the <code>RetryExecutor</code> tracks cumulative delay time and checks the budget before each retry:</p> <pre><code>// From src/cook/retry_v2.rs:236-244\nif let Some(budget) = self.config.retry_budget {\n    if total_delay + jittered_delay &gt; budget {\n        warn!(\"Retry budget exhausted for {}\", context);\n        return Err(anyhow!(\"Retry budget exhausted\"));\n    }\n}\n</code></pre> <p>Behavior: - Tracks <code>total_delay</code>: cumulative duration spent waiting between retries - Before each retry, checks: <code>total_delay + next_delay &gt; budget</code> - If the next retry would exceed the budget, stops immediately - Returns error: <code>\"Retry budget exhausted\"</code></p> <p>Source: <code>src/cook/retry_v2.rs:236-244</code></p>"},{"location":"retry-configuration/retry-budget/#2-timestamp-based-enforcement-stateful-tracking","title":"2. Timestamp-Based Enforcement (Stateful Tracking)","text":"<p>For checkpoint/resume scenarios, the <code>RetryStateManager</code> uses expiration timestamps:</p> <pre><code>// From src/cook/retry_state.rs:299-301\nretry_budget_expires_at: config\n    .retry_budget\n    .map(|budget| Utc::now() + ChronoDuration::from_std(budget).unwrap())\n</code></pre> <p>Behavior: - Calculates expiration: <code>retry_budget_expires_at = now + budget_duration</code> - Set at first failure (not at command start) - Before each retry attempt, checks: <code>Utc::now() &gt;= retry_budget_expires_at</code> - Prevents retries even after workflow interruption and resume</p> <p>Source: <code>src/cook/retry_state.rs:299-301, 342-346</code></p>"},{"location":"retry-configuration/retry-budget/#interaction-with-attempts-and-backoff","title":"Interaction with Attempts and Backoff","text":"<p>Critical behavior: Retries stop when EITHER the attempts limit OR the retry budget is exceeded, whichever comes first.</p> <p>Example 1: Budget Limits Before Attempts <pre><code>retry_config:\n  attempts: 100           # High attempt limit\n  retry_budget: \"2m\"      # Budget will be hit first\n  backoff:\n    exponential:\n      base: 2.0\n  initial_delay: \"1s\"\n</code></pre></p> <p>With exponential backoff (1s, 2s, 4s, 8s, 16s, 32s, 64s...), delays sum to ~2 minutes after 7-8 attempts. The budget stops retries at ~8 attempts even though 100 are allowed.</p> <p>Example 2: Attempts Limit Before Budget <pre><code>retry_config:\n  attempts: 3             # Attempts will be hit first\n  retry_budget: \"10m\"     # Budget won't be reached\n  backoff: fixed\n  initial_delay: \"5s\"\n</code></pre></p> <p>Total delay: 5s + 5s + 5s = 15 seconds, well under the 10-minute budget. Stops after 3 attempts.</p> <p>Source: Dual checking logic in <code>src/cook/retry_v2.rs:236-244</code> and <code>src/cook/retry_state.rs:337-347</code></p>"},{"location":"retry-configuration/retry-budget/#what-time-is-counted","title":"What Time is Counted?","text":"<p>Included in Budget: - \u2705 Backoff delay time (waiting between retries) - \u2705 Jitter-adjusted delays (if jitter is enabled)</p> <p>NOT Included in Budget: - \u274c Command execution time - \u274c Time for successful operations - \u274c Time before first failure</p> <p>Example Timeline: <pre><code>Command Start \u2192 Execute (30s) \u2192 Fail\n                \u2193 retry_budget timer starts\n                Wait 1s (counted) \u2192 Execute (30s, NOT counted) \u2192 Fail\n                Wait 2s (counted) \u2192 Execute (30s, NOT counted) \u2192 Fail\n                Wait 4s (counted) \u2192 Execute (30s, NOT counted) \u2192 Success\nTotal Time: 30+1+30+2+30+4+30 = 127 seconds\nBudget Used: 1+2+4 = 7 seconds\n</code></pre></p> <p>If <code>retry_budget: \"5s\"</code>, the workflow would fail at the third retry (1+2+4 = 7s &gt; 5s budget).</p>"},{"location":"retry-configuration/retry-metrics-and-observability/","title":"Retry Metrics and Observability","text":""},{"location":"retry-configuration/retry-metrics-and-observability/#retry-metrics-and-observability","title":"Retry Metrics and Observability","text":"<p>Prodigy provides comprehensive observability into retry behavior through logging and event tracking, rather than programmatic metrics access. This enables debugging retry issues and monitoring workflow resilience without requiring code changes.</p>"},{"location":"retry-configuration/retry-metrics-and-observability/#how-retry-observability-works","title":"How Retry Observability Works","text":"<p>Retry operations are observable through three primary mechanisms:</p> <ol> <li>Console Logs - Visible during workflow execution</li> <li>Tracing Infrastructure - Controlled by verbosity flags</li> <li>Event Tracking - Recorded for MapReduce workflows</li> </ol>"},{"location":"retry-configuration/retry-metrics-and-observability/#internal-metrics-structure","title":"Internal Metrics Structure","text":"<p>While retry metrics are tracked internally via the <code>RetryMetrics</code> structure (src/cook/retry_v2.rs:399-422), this data is not exposed as a queryable API for workflow users. Instead, metrics are surfaced through logging at key decision points.</p> <p>The internal structure tracks: - <code>total_attempts</code> - Total number of attempts made - <code>successful_attempts</code> - Number of successful operations - <code>failed_attempts</code> - Number of failed operations - <code>retries</code> - History of retry attempts with delays</p>"},{"location":"retry-configuration/retry-metrics-and-observability/#observing-retry-behavior-in-workflows","title":"Observing Retry Behavior in Workflows","text":""},{"location":"retry-configuration/retry-metrics-and-observability/#console-output-during-retries","title":"Console Output During Retries","text":"<p>When a command fails and triggers a retry, you'll see log messages showing:</p> <pre><code>Retrying &lt;operation&gt; (attempt 2/5) after 200ms\nRetrying &lt;operation&gt; (attempt 3/5) after 400ms\n</code></pre> <p>Source: These messages come from the retry executor's logging (src/cook/retry_v2.rs:247-250).</p>"},{"location":"retry-configuration/retry-metrics-and-observability/#verbosity-flags-for-detailed-logs","title":"Verbosity Flags for Detailed Logs","text":"<p>Use Prodigy's verbosity flags to see more retry details:</p> <pre><code># Default - shows retry attempts\nprodigy run workflow.yml\n\n# Verbose - shows retry decision-making\nprodigy run workflow.yml -v\n\n# Very verbose - shows all retry internals\nprodigy run workflow.yml -vv\n</code></pre> <p>With <code>-v</code> or higher verbosity, you'll see: - Retry attempt counts and delays - Circuit breaker state transitions - Retry budget consumption - Error pattern matching results</p>"},{"location":"retry-configuration/retry-metrics-and-observability/#retry-budget-exhaustion","title":"Retry Budget Exhaustion","text":"<p>When retry budget is exhausted, a warning is logged:</p> <pre><code>Retry budget exhausted for &lt;operation&gt;\n</code></pre> <p>Source: src/cook/retry_v2.rs:238-243</p> <p>This indicates the total retry delay exceeded the configured <code>retry_budget</code>, preventing further attempts even if <code>max_attempts</code> hasn't been reached.</p>"},{"location":"retry-configuration/retry-metrics-and-observability/#circuit-breaker-observability","title":"Circuit Breaker Observability","text":"<p>Circuit breaker state changes are logged automatically:</p>"},{"location":"retry-configuration/retry-metrics-and-observability/#circuit-breaker-opens","title":"Circuit Breaker Opens","text":"<pre><code>Circuit breaker opened after 5 consecutive failures\n</code></pre> <p>Source: src/cook/retry_v2.rs:391</p> <p>When the failure threshold is exceeded, the circuit breaker opens and blocks further retry attempts until the recovery timeout expires.</p>"},{"location":"retry-configuration/retry-metrics-and-observability/#circuit-breaker-transitions","title":"Circuit Breaker Transitions","text":"<pre><code>Circuit breaker transitioning to half-open\nCircuit breaker closed after successful operation\n</code></pre> <p>Source: src/cook/retry_v2.rs:359, 377</p> <p>These debug-level messages show circuit breaker recovery: - Half-open: Testing if failures have cleared (one attempt allowed) - Closed: Normal operation resumed after successful attempt</p>"},{"location":"retry-configuration/retry-metrics-and-observability/#workflow-level-observability","title":"Workflow-Level Observability","text":""},{"location":"retry-configuration/retry-metrics-and-observability/#on-failure-handler-execution","title":"On-Failure Handler Execution","text":"<p>When retries are configured via <code>on_failure</code> handlers, you'll see:</p> <pre><code>- shell: \"flaky-command\"\n  on_failure:\n    claude: \"/fix-issue\"\n    max_attempts: 5\n</code></pre> <p>Workflow output shows: 1. Initial command failure 2. Retry attempt messages with delays 3. Handler command execution 4. Final success or failure after exhausting attempts</p>"},{"location":"retry-configuration/retry-metrics-and-observability/#example-log-progression","title":"Example Log Progression","text":"<pre><code>\u274c Command failed: exit code 1\nRetrying (attempt 2/5) after 100ms\n\u274c Command failed: exit code 1\nRetrying (attempt 3/5) after 200ms\n\u2705 Command succeeded\n</code></pre>"},{"location":"retry-configuration/retry-metrics-and-observability/#mapreduce-event-tracking","title":"MapReduce Event Tracking","text":"<p>MapReduce workflows provide additional observability through the event tracking system. Retry attempts are recorded as events:</p> <pre><code># View events for a MapReduce job\nprodigy events show &lt;job_id&gt;\n</code></pre> <p>Events include: - Retry attempt counts per work item - Individual agent retry behavior - Correlation between retries and DLQ items</p> <p>See Event Tracking for comprehensive event details.</p>"},{"location":"retry-configuration/retry-metrics-and-observability/#troubleshooting-with-retry-logs","title":"Troubleshooting with Retry Logs","text":""},{"location":"retry-configuration/retry-metrics-and-observability/#identifying-retry-configuration-issues","title":"Identifying Retry Configuration Issues","text":"<p>Problem: Commands retry unnecessarily on non-transient errors</p> <p>Check logs for: <pre><code>Retrying (attempt 2/5) after 100ms\nRetrying (attempt 3/5) after 200ms\n...\n</code></pre></p> <p>Solution: Configure <code>retry_on</code> error matchers to only retry specific errors:</p> <pre><code>retry_config:\n  max_attempts: 5\n  retry_on:\n    - pattern: \"connection refused\"\n    - pattern: \"timeout\"\n</code></pre> <p>See Conditional Retry with Error Matchers.</p>"},{"location":"retry-configuration/retry-metrics-and-observability/#debugging-circuit-breaker-behavior","title":"Debugging Circuit Breaker Behavior","text":"<p>Problem: Circuit breaker opening too aggressively</p> <p>Check logs for: <pre><code>Circuit breaker opened after 3 consecutive failures\n</code></pre></p> <p>Solution: Adjust <code>failure_threshold</code> and <code>recovery_timeout</code>:</p> <pre><code>circuit_breaker:\n  failure_threshold: 5      # Increase tolerance\n  recovery_timeout_ms: 30000  # Longer recovery window\n</code></pre>"},{"location":"retry-configuration/retry-metrics-and-observability/#analyzing-retry-budget-exhaustion","title":"Analyzing Retry Budget Exhaustion","text":"<p>Problem: Retries stop before <code>max_attempts</code> reached</p> <p>Check logs for: <pre><code>Retry budget exhausted for &lt;operation&gt;\n</code></pre></p> <p>Solution: Increase <code>retry_budget</code> or reduce <code>max_delay_ms</code>:</p> <pre><code>retry_config:\n  retry_budget_ms: 120000  # Allow up to 2 minutes of retries\n  max_delay_ms: 10000       # Cap individual delays at 10s\n</code></pre> <p>See Retry Budget.</p>"},{"location":"retry-configuration/retry-metrics-and-observability/#integration-with-error-handling","title":"Integration with Error Handling","text":"<p>Retry observability integrates with Prodigy's error handling pipeline:</p> <ol> <li>Retry - First line of defense for transient failures</li> <li>On-Failure Handlers - Second line when retries exhausted</li> <li>Dead Letter Queue - Final capture for MapReduce failures</li> </ol> <p>Example workflow progression: <pre><code>Command fails \u2192 Retry with backoff \u2192 Still failing \u2192 Execute on_failure handler \u2192\nHandler succeeds \u2192 Continue OR Handler fails \u2192 DLQ (MapReduce only)\n</code></pre></p> <p>Logs show the complete progression: <pre><code>\u274c shell: flaky-script failed\nRetrying (attempt 2/3) after 100ms\n\u274c shell: flaky-script failed\nRetrying (attempt 3/3) after 200ms\n\u274c shell: flaky-script failed\nExecuting on_failure handler: claude /debug-failure\n\u2705 on_failure handler succeeded\n</code></pre></p>"},{"location":"retry-configuration/retry-metrics-and-observability/#mapreduce-dlq-relationship","title":"MapReduce DLQ Relationship","text":"<p>For MapReduce workflows, retry exhaustion leads to DLQ:</p> <pre><code>Retrying work item 'item-123' (attempt 3/3)\n\u274c Failed after 3 attempts\n\u2192 Item moved to Dead Letter Queue\n</code></pre> <p>View DLQ items with retry history:</p> <pre><code>prodigy dlq show &lt;job_id&gt;\n</code></pre> <p>Each DLQ item preserves: - Number of retry attempts made - Final error message - Retry delays used - Circuit breaker state at failure</p> <p>See Dead Letter Queue (DLQ).</p>"},{"location":"retry-configuration/retry-metrics-and-observability/#monitoring-retry-patterns","title":"Monitoring Retry Patterns","text":""},{"location":"retry-configuration/retry-metrics-and-observability/#identifying-flaky-operations","title":"Identifying Flaky Operations","text":"<p>Watch for patterns in logs indicating flaky behavior:</p> <pre><code># Count retry attempts across workflow execution\nprodigy run workflow.yml -v 2&gt;&amp;1 | grep \"Retrying\" | wc -l\n\n# Find operations that retry most frequently\nprodigy run workflow.yml -v 2&gt;&amp;1 | grep \"Retrying\" | \\\n  grep -oE 'Retrying [^ ]+' | sort | uniq -c | sort -rn\n</code></pre>"},{"location":"retry-configuration/retry-metrics-and-observability/#measuring-retry-effectiveness","title":"Measuring Retry Effectiveness","text":"<p>Track retry success vs. failure rates:</p> <pre><code># Find successful retries\ngrep \"Retrying\" workflow.log | grep -A 5 \"succeeded\"\n\n# Find exhausted retries\ngrep \"Retry budget exhausted\" workflow.log\ngrep \"Failed after .* attempts\" workflow.log\n</code></pre>"},{"location":"retry-configuration/retry-metrics-and-observability/#related-topics","title":"Related Topics","text":"<ul> <li>Basic Retry Configuration - Configure retry behavior</li> <li>Backoff Strategies - Choose appropriate backoff patterns</li> <li>Event Tracking - MapReduce observability</li> <li>Troubleshooting - Debug common retry issues</li> </ul>"},{"location":"retry-configuration/troubleshooting/","title":"Troubleshooting","text":""},{"location":"retry-configuration/troubleshooting/#troubleshooting","title":"Troubleshooting","text":"<p>Common retry configuration issues and their solutions.</p>"},{"location":"retry-configuration/troubleshooting/#issue-retries-not-triggering","title":"Issue: Retries Not Triggering","text":"<p>Symptoms: - Command fails immediately without retry - Only see one attempt in logs - Expected retries don't happen</p> <p>Possible Causes and Solutions:</p>"},{"location":"retry-configuration/troubleshooting/#1-retry_on-matchers-too-specific","title":"1. retry_on Matchers Too Specific","text":"<pre><code># Problem: Error doesn't match any configured matcher\nretry_config:\n  attempts: 5\n  retry_on:\n    - network\n    # Error is \"connection timeout\" but matcher only catches \"network\"\n</code></pre> <p>Solution: Use broader matchers or add custom patterns</p> <pre><code>retry_config:\n  attempts: 5\n  retry_on:\n    - network\n    - timeout      # Add timeout matcher\n    - pattern: \"connection.*timeout\"  # Or custom pattern\n</code></pre> <p>Debug: Check actual error message in logs and verify it matches one of your configured matchers (remember: matching is case-insensitive).</p> <p>Source: ErrorMatcher matching logic in src/cook/retry_v2.rs:128-149</p>"},{"location":"retry-configuration/troubleshooting/#2-empty-retry_on-with-unexpected-error-filtering","title":"2. Empty retry_on with Unexpected Error Filtering","text":"<pre><code># Misconception: Empty retry_on means \"don't retry\"\n# Reality: Empty retry_on means \"retry ALL errors\"\nretry_config:\n  attempts: 3\n  # retry_on is empty - retries everything!\n</code></pre> <p>Solution: If you see retries happening when you don't expect them, check if <code>retry_on</code> is empty.</p> <p>Source: Default behavior in src/cook/retry_v2.rs:42-43</p>"},{"location":"retry-configuration/troubleshooting/#3-retry_config-missing-entirely","title":"3. retry_config Missing Entirely","text":"<pre><code>commands:\n  - shell: \"curl https://api.example.com\"\n    # No retry_config - no retry happens\n</code></pre> <p>Solution: Add <code>retry_config</code> block to enable retry.</p>"},{"location":"retry-configuration/troubleshooting/#issue-retries-happening-but-taking-too-long","title":"Issue: Retries Happening But Taking Too Long","text":"<p>Symptoms: - Workflow hangs during retries - Total retry time exceeds expectations - Delays grow too large</p> <p>Possible Causes and Solutions:</p>"},{"location":"retry-configuration/troubleshooting/#1-exponential-backoff-without-max_delay","title":"1. Exponential Backoff Without max_delay","text":"<pre><code># Problem: Exponential growth uncapped\nretry_config:\n  attempts: 10\n  backoff: exponential\n  initial_delay: \"1s\"\n  # No max_delay - delay can grow to minutes!\n</code></pre> <p>Solution: Always set <code>max_delay</code></p> <pre><code>retry_config:\n  attempts: 10\n  backoff: exponential\n  initial_delay: \"1s\"\n  max_delay: \"30s\"  # Cap delays at 30 seconds\n</code></pre> <p>Default: max_delay defaults to 30 seconds if not specified (src/cook/retry_v2.rs:64)</p>"},{"location":"retry-configuration/troubleshooting/#2-too-many-attempts-without-retry_budget","title":"2. Too Many Attempts Without retry_budget","text":"<pre><code># Problem: High attempts can retry for hours\nretry_config:\n  attempts: 100\n  backoff: exponential\n</code></pre> <p>Solution: Use <code>retry_budget</code> to cap total time</p> <pre><code>retry_config:\n  attempts: 100\n  backoff: exponential\n  retry_budget: \"10m\"  # Never exceed 10 minutes total\n</code></pre> <p>Source: retry_budget enforcement in tests at src/cook/retry_v2.rs:675-708</p>"},{"location":"retry-configuration/troubleshooting/#3-initial-delay-too-high","title":"3. Initial Delay Too High","text":"<pre><code># Problem: First retry waits too long\nretry_config:\n  attempts: 3\n  initial_delay: \"60s\"  # 1-minute delay before first retry!\n</code></pre> <p>Solution: Use shorter initial delay, let backoff grow</p> <pre><code>retry_config:\n  attempts: 3\n  initial_delay: \"1s\"   # Start small\n  backoff: exponential  # Let it grow\n  max_delay: \"30s\"\n</code></pre>"},{"location":"retry-configuration/troubleshooting/#issue-circuit-breaker-always-open","title":"Issue: Circuit Breaker Always Open","text":"<p>Symptoms: - Circuit breaker trips and stays open - Requests fail immediately after threshold - Recovery doesn't happen</p> <p>Possible Causes and Solutions:</p>"},{"location":"retry-configuration/troubleshooting/#1-failure-threshold-too-low","title":"1. Failure Threshold Too Low","text":"<pre><code>// Problem: Circuit opens too easily\nlet executor = RetryExecutor::new(config)\n    .with_circuit_breaker(\n        1,                          // Opens after single failure!\n        Duration::from_secs(30)\n    );\n</code></pre> <p>Solution: Use appropriate threshold for failure rate</p> <pre><code>let executor = RetryExecutor::new(config)\n    .with_circuit_breaker(\n        5,                          // Open after 5 consecutive failures\n        Duration::from_secs(30)\n    );\n</code></pre> <p>Source: CircuitBreaker implementation in src/cook/retry_v2.rs:325-397</p>"},{"location":"retry-configuration/troubleshooting/#2-recovery-timeout-too-long","title":"2. Recovery Timeout Too Long","text":"<pre><code>// Problem: Circuit stays open too long\nlet executor = RetryExecutor::new(config)\n    .with_circuit_breaker(\n        5,\n        Duration::from_secs(600)    // 10-minute recovery time!\n    );\n</code></pre> <p>Solution: Use shorter recovery timeout for faster testing</p> <pre><code>let executor = RetryExecutor::new(config)\n    .with_circuit_breaker(\n        5,\n        Duration::from_secs(30)     // 30-second recovery attempts\n    );\n</code></pre>"},{"location":"retry-configuration/troubleshooting/#3-no-success-to-close-circuit","title":"3. No Success to Close Circuit","text":"<p>Problem: Circuit opens but downstream never recovers, so circuit never closes</p> <p>Solution: - Check if downstream service is actually recovering - Verify circuit enters HalfOpen state and test requests succeed - Monitor circuit state transitions with logging</p>"},{"location":"retry-configuration/troubleshooting/#issue-thundering-herd-multiple-parallel-agents-retrying-simultaneously","title":"Issue: Thundering Herd (Multiple Parallel Agents Retrying Simultaneously)","text":"<p>Symptoms: - Service overwhelmed during recovery - All parallel agents retry at same time - Cascading failures</p> <p>Problem:</p> <pre><code># MapReduce with 10 parallel agents, no jitter\nmap:\n  max_parallel: 10\n  agent_template:\n    - shell: \"api-call.sh\"\n      retry_config:\n        attempts: 5\n        backoff: exponential\n        jitter: false  # All agents retry at same time!\n</code></pre> <p>Solution: Enable jitter</p> <pre><code>map:\n  max_parallel: 10\n  agent_template:\n    - shell: \"api-call.sh\"\n      retry_config:\n        attempts: 5\n        backoff: exponential\n        jitter: true          # Randomize retry timing\n        jitter_factor: 0.3    # 30% randomization\n</code></pre> <p>Why: Without jitter, all 10 agents calculate the same exponential delay and retry simultaneously.</p> <p>Source: Jitter application in src/cook/retry_v2.rs:308-317</p>"},{"location":"retry-configuration/troubleshooting/#issue-retrying-non-transient-errors","title":"Issue: Retrying Non-Transient Errors","text":"<p>Symptoms: - Retrying 404 Not Found errors - Retrying authentication failures (401) - Wasting time on permanent failures</p> <p>Problem:</p> <pre><code># Empty retry_on retries everything\nretry_config:\n  attempts: 5\n  # Retries 404, 401, 400, etc. - permanent errors!\n</code></pre> <p>Solution: Use selective retry with error matchers</p> <pre><code>retry_config:\n  attempts: 5\n  retry_on:\n    - network        # Transient\n    - timeout        # Transient\n    - server_error   # Transient (5xx)\n    # Don't retry 404, 401, 400, etc.\n</code></pre> <p>Best Practice: Only retry errors that might succeed on next attempt.</p>"},{"location":"retry-configuration/troubleshooting/#issue-retry-budget-not-working-as-expected","title":"Issue: Retry Budget Not Working as Expected","text":"<p>Symptoms: - Retries exceed configured budget - Budget seems ignored</p> <p>Possible Cause: Misunderstanding retry_budget behavior</p> <p>How retry_budget Works: - Budget is checked before each retry - If budget would be exceeded, retry stops - Budget includes backoff delay time - Budget does NOT include time for command execution itself</p> <p>Example:</p> <pre><code>retry_config:\n  attempts: 10\n  backoff: exponential\n  initial_delay: \"1s\"\n  max_delay: \"60s\"\n  retry_budget: \"2m\"  # 2-minute budget\n</code></pre> <p>Behavior: - If accumulated delays + next delay &gt; 2 minutes \u2192 Stop - Command execution time is NOT counted in budget - If command takes 1 minute to execute each time, total time could be: 2m (budget) + (attempts * 1m execution) = ~12 minutes</p> <p>Source: retry_budget field in src/cook/retry_v2.rs:46-47, tests at lines 675-708</p>"},{"location":"retry-configuration/troubleshooting/#issue-fallback-command-also-failing","title":"Issue: Fallback Command Also Failing","text":"<p>Symptoms: - Primary command fails after retries - Fallback command executes but also fails - Workflow stops</p> <p>Problem: Fallback command isn't reliable</p> <pre><code>retry_config:\n  attempts: 3\n  on_failure:\n    fallback:\n      command: \"curl https://backup-api.com/data\"  # This can also fail!\n</code></pre> <p>Solution: Make fallback truly reliable</p> <pre><code>retry_config:\n  attempts: 3\n  on_failure:\n    fallback:\n      command: \"cat /cache/data.json\"  # Local cache, very reliable\n</code></pre> <p>Best Practice: Fallback commands should be: - Local operations (file reads, not network calls) - Idempotent - Very unlikely to fail - Fast</p>"},{"location":"retry-configuration/troubleshooting/#issue-retry-metrics-not-matching-expectations","title":"Issue: Retry Metrics Not Matching Expectations","text":"<p>Symptoms: - Metrics show different attempt count than configured - Unexpected success/failure counts</p> <p>Debugging:</p> <pre><code>// Access retry metrics for debugging\nlet metrics = executor.metrics();\nprintln!(\"Total attempts: {}\", metrics.total_attempts);\nprintln!(\"Successful: {}\", metrics.successful_attempts);\nprintln!(\"Failed: {}\", metrics.failed_attempts);\nprintln!(\"Retries: {:?}\", metrics.retries);  // Vec&lt;(attempt, delay)&gt;\n</code></pre> <p>Source: RetryMetrics struct in src/cook/retry_v2.rs:399-422</p> <p>Check: - <code>total_attempts</code> = successful + failed - <code>retries</code> vector shows actual delays used - Compare with configured backoff strategy</p>"},{"location":"retry-configuration/troubleshooting/#debugging-workflow","title":"Debugging Workflow","text":"<p>When retry behavior is unexpected:</p> <ol> <li>Check retry_on matchers:</li> <li>Verify error message matches configured matchers</li> <li>Remember matching is case-insensitive</li> <li> <p>Empty <code>retry_on</code> = retry all errors</p> </li> <li> <p>Check backoff configuration:</p> </li> <li>Verify <code>max_delay</code> is set</li> <li>Check <code>initial_delay</code> isn't too high</li> <li> <p>Ensure <code>backoff</code> strategy matches intent</p> </li> <li> <p>Check retry_budget:</p> </li> <li>Remember budget is delay time, not total time</li> <li>Budget checked before each retry</li> <li> <p>Command execution time NOT included</p> </li> <li> <p>Enable jitter for parallel workflows:</p> </li> <li>Always use jitter in MapReduce</li> <li> <p>Set <code>jitter_factor</code> between 0.1 and 0.5</p> </li> <li> <p>Use selective retry:</p> </li> <li>Don't retry permanent errors (404, 401, 400)</li> <li> <p>Use <code>retry_on</code> to specify transient errors only</p> </li> <li> <p>Monitor metrics:</p> </li> <li>Access <code>RetryMetrics</code> for actual attempt counts</li> <li>Verify delays match expectations</li> <li>Check circuit breaker state transitions</li> </ol>"},{"location":"retry-configuration/troubleshooting/#getting-help","title":"Getting Help","text":"<p>If retry behavior is still unclear:</p> <ol> <li>Check retry_v2.rs implementation: src/cook/retry_v2.rs</li> <li>Review tests: src/cook/retry_v2.rs:463-748 (comprehensive test coverage)</li> <li>Enable verbose logging: Add logging around retry logic to see what's happening</li> <li>Test with simple cases: Start with fixed backoff and 2-3 attempts to isolate issue</li> </ol>"},{"location":"retry-configuration/troubleshooting/#see-also","title":"See Also","text":"<ul> <li>Basic Retry Configuration - Configuration fundamentals</li> <li>Backoff Strategies - Understanding backoff behavior</li> <li>Conditional Retry with Error Matchers - Error matching details</li> <li>Best Practices - Avoiding common pitfalls</li> <li>Complete Examples - Working configurations</li> </ul>"},{"location":"retry-configuration/workflow-level-vs-command-level-retry/","title":"Workflow-Level vs Command-Level Retry","text":""},{"location":"retry-configuration/workflow-level-vs-command-level-retry/#workflow-level-vs-command-level-retry","title":"Workflow-Level vs Command-Level Retry","text":"<p>Prodigy has two distinct retry systems that serve different purposes. Understanding when to use each is critical for effective error handling.</p>"},{"location":"retry-configuration/workflow-level-vs-command-level-retry/#overview","title":"Overview","text":"Feature Command-Level (retry_v2) Workflow-Level (error_policy) Scope Individual command execution Work item failure in MapReduce Location <code>retry_config</code> on commands <code>error_policy</code> + <code>retry_config</code> in workflow Implementation <code>src/cook/retry_v2.rs</code> <code>src/cook/workflow/error_policy.rs</code> Use Case Retry transient command failures Retry failed MapReduce work items Features Backoff, jitter, error matchers, circuit breakers DLQ integration, failure thresholds, batch collection"},{"location":"retry-configuration/workflow-level-vs-command-level-retry/#command-level-retry-retry_v2","title":"Command-Level Retry (retry_v2)","text":"<p>The enhanced retry system (<code>retry_v2</code>) provides sophisticated retry capabilities for individual command execution.</p> <p>Source: <code>src/cook/retry_v2.rs</code></p>"},{"location":"retry-configuration/workflow-level-vs-command-level-retry/#key-features","title":"Key Features","text":"<ol> <li>Multiple Backoff Strategies:</li> <li>Exponential (default)</li> <li>Linear</li> <li>Fibonacci</li> <li>Fixed</li> <li> <p>Custom</p> </li> <li> <p>Error Matchers:</p> </li> <li>Selective retry based on error type</li> <li>Built-in matchers: Network, Timeout, ServerError, RateLimit</li> <li> <p>Custom regex patterns</p> </li> <li> <p>Jitter Support:</p> </li> <li>Prevents thundering herd in distributed systems</li> <li> <p>Configurable jitter factor (default: 0.3)</p> </li> <li> <p>Retry Budget:</p> </li> <li>Time-based caps on total retry time</li> <li> <p>Prevents infinite retry loops</p> </li> <li> <p>Failure Actions:</p> </li> <li>Stop (default) - halt workflow</li> <li>Continue - proceed despite failure</li> <li> <p>Fallback - execute alternative command</p> </li> <li> <p>Circuit Breakers:</p> </li> <li>Fail-fast when downstream is down</li> <li>Automatic recovery testing (HalfOpen state)</li> </ol>"},{"location":"retry-configuration/workflow-level-vs-command-level-retry/#configuration","title":"Configuration","text":"<pre><code>commands:\n  - shell: \"curl https://api.example.com/data\"\n    retry_config:\n      attempts: 5\n      backoff: exponential\n      initial_delay: \"1s\"\n      max_delay: \"30s\"\n      jitter: true\n      jitter_factor: 0.3\n      retry_on:\n        - network\n        - timeout\n      retry_budget: \"5m\"\n      on_failure: stop\n</code></pre> <p>RetryConfig Fields (src/cook/retry_v2.rs:14-52): - <code>attempts: u32</code> - Maximum retry attempts - <code>backoff: BackoffStrategy</code> - Delay calculation strategy - <code>initial_delay: Duration</code> - Starting delay - <code>max_delay: Duration</code> - Delay cap - <code>jitter: bool</code> - Enable jitter - <code>jitter_factor: f64</code> - Jitter randomization (0.0-1.0) - <code>retry_on: Vec&lt;ErrorMatcher&gt;</code> - Selective retry matchers - <code>retry_budget: Option&lt;Duration&gt;</code> - Total retry time limit - <code>on_failure: FailureAction</code> - Final failure handling</p>"},{"location":"retry-configuration/workflow-level-vs-command-level-retry/#when-to-use-command-level-retry","title":"When to Use Command-Level Retry","text":"<p>Use <code>retry_config</code> on individual commands for: - External API calls with transient failures - Network operations that might timeout - Database operations with lock conflicts - Resource initialization that needs retry - Any single command that benefits from retry</p> <p>Example: <pre><code>commands:\n  - shell: \"make build\"\n    retry_config:\n      attempts: 3\n      retry_on:\n        - network  # Only retry network errors during dependency fetch\n</code></pre></p>"},{"location":"retry-configuration/workflow-level-vs-command-level-retry/#workflow-level-retry-error_policy","title":"Workflow-Level Retry (error_policy)","text":"<p>The workflow-level retry system handles failures in MapReduce work items, integrating with the Dead Letter Queue (DLQ).</p> <p>Source: <code>src/cook/workflow/error_policy.rs</code></p>"},{"location":"retry-configuration/workflow-level-vs-command-level-retry/#key-features_1","title":"Key Features","text":"<ol> <li>Work Item Failure Handling:</li> <li>DLQ (Dead Letter Queue) - Save failed items for retry</li> <li>Retry - Immediate retry with backoff</li> <li>Skip - Skip failed item, continue</li> <li>Stop - Stop entire workflow</li> <li> <p>Custom - Custom failure handler</p> </li> <li> <p>Failure Thresholds:</p> </li> <li><code>max_failures</code> - Stop after N failures</li> <li> <p><code>failure_threshold</code> - Stop at failure rate (0.0-1.0)</p> </li> <li> <p>Error Collection Strategies:</p> </li> <li>Aggregate - Collect all errors before reporting</li> <li>Immediate - Report errors as they occur</li> <li> <p>Batched - Report in batches of N errors</p> </li> <li> <p>Circuit Breaker Integration:</p> </li> <li>Workflow-level circuit breaker</li> <li>Failure/success thresholds</li> <li> <p>Half-open request limits</p> </li> <li> <p>DLQ Retry:</p> </li> <li>Failed items stored in DLQ</li> <li>Retry with <code>prodigy dlq retry &lt;job_id&gt;</code></li> <li>Preserves correlation IDs</li> </ol>"},{"location":"retry-configuration/workflow-level-vs-command-level-retry/#configuration_1","title":"Configuration","text":"<pre><code>name: mapreduce-workflow\nmode: mapreduce\n\nerror_policy:\n  on_item_failure: dlq          # Send failures to DLQ\n  continue_on_failure: true     # Keep processing other items\n  max_failures: 10              # Stop after 10 failures\n  failure_threshold: 0.25       # Or stop at 25% failure rate\n  error_collection: aggregate   # Collect errors before reporting\n  circuit_breaker:\n    failure_threshold: 5        # Open after 5 failures\n    success_threshold: 3        # Close after 3 successes\n    timeout: \"30s\"              # Recovery timeout\n    half_open_requests: 3       # Test requests in half-open\n  retry_config:                 # Workflow-level retry (simpler)\n    max_attempts: 3\n    backoff: exponential\n\nmap:\n  input: \"items.json\"\n  json_path: \"$.items[*]\"\n  agent_template:\n    - shell: \"process ${item.id}\"\n</code></pre> <p>WorkflowErrorPolicy Fields (src/cook/workflow/error_policy.rs:131-179): - <code>on_item_failure: ItemFailureAction</code> - What to do when item fails - <code>continue_on_failure: bool</code> - Continue processing other items - <code>max_failures: Option&lt;usize&gt;</code> - Maximum failures before stopping - <code>failure_threshold: Option&lt;f64&gt;</code> - Failure rate threshold (0.0-1.0) - <code>error_collection: ErrorCollectionStrategy</code> - Error reporting strategy - <code>circuit_breaker: Option&lt;CircuitBreakerConfig&gt;</code> - Circuit breaker config - <code>retry_config: Option&lt;RetryConfig&gt;</code> - Simplified retry config</p> <p>Workflow-Level RetryConfig (src/cook/workflow/error_policy.rs:90-129): - <code>max_attempts: u32</code> - Maximum retry attempts - <code>backoff: BackoffStrategy</code> - Simpler backoff variants</p>"},{"location":"retry-configuration/workflow-level-vs-command-level-retry/#when-to-use-workflow-level-retry","title":"When to Use Workflow-Level Retry","text":"<p>Use <code>error_policy</code> in MapReduce workflows for: - Work item failure handling - DLQ, thresholds, collection - MapReduce workflows - Parallel work item processing - Failure rate monitoring - Stop at threshold percentage - Batch error handling - Collect and report errors in batches</p> <p>Example: <pre><code>error_policy:\n  on_item_failure: dlq        # Failed items go to DLQ\n  continue_on_failure: true   # Process other items\n  max_failures: 5             # But stop if more than 5 fail\n</code></pre></p>"},{"location":"retry-configuration/workflow-level-vs-command-level-retry/#using-both-systems-together","title":"Using Both Systems Together","text":"<p>You can combine both retry systems for comprehensive error handling:</p> <pre><code>name: robust-mapreduce\nmode: mapreduce\n\n# Workflow-level error policy\nerror_policy:\n  on_item_failure: dlq        # Failed items to DLQ\n  continue_on_failure: true   # Keep processing\n  max_failures: 10            # Stop after 10 failures\n  circuit_breaker:\n    failure_threshold: 5\n    timeout: \"30s\"\n\nmap:\n  input: \"items.json\"\n  json_path: \"$.items[*]\"\n  max_parallel: 10\n\n  agent_template:\n    # Command-level retry for transient failures\n    - shell: \"process ${item.id}\"\n      retry_config:\n        attempts: 3              # Try 3 times per agent\n        backoff: exponential\n        initial_delay: \"1s\"\n        max_delay: \"30s\"\n        jitter: true             # Prevent thundering herd\n        retry_on:\n          - network\n          - timeout\n        on_failure: stop         # Let workflow error_policy handle final failure\n</code></pre> <p>How they work together:</p> <ol> <li>Agent attempts to process work item</li> <li>Command fails \u2192 <code>retry_config</code> retries with exponential backoff (up to 3 attempts)</li> <li>All command retries fail \u2192 <code>on_failure: stop</code> ends agent execution</li> <li>Agent reports failure to workflow \u2192 <code>error_policy</code> sends item to DLQ</li> <li>Workflow continues processing other items \u2192 <code>continue_on_failure: true</code></li> <li>After map phase \u2192 Retry DLQ items with <code>prodigy dlq retry &lt;job_id&gt;</code></li> </ol> <p>Benefits: - Fast recovery from transient errors (command-level retry) - Isolation of persistent failures (DLQ) - Continued processing of other items (workflow-level policy) - Manual review of failed items before retry</p>"},{"location":"retry-configuration/workflow-level-vs-command-level-retry/#key-differences","title":"Key Differences","text":""},{"location":"retry-configuration/workflow-level-vs-command-level-retry/#scope-of-retry","title":"Scope of Retry","text":"<p>Command-Level: Single command execution <pre><code>commands:\n  - shell: \"api-call.sh\"\n    retry_config:\n      attempts: 5\n</code></pre> \u2192 Retries the <code>api-call.sh</code> command 5 times</p> <p>Workflow-Level: Entire work item (may contain multiple commands) <pre><code>map:\n  agent_template:\n    - shell: \"step1.sh ${item.id}\"\n    - shell: \"step2.sh ${item.id}\"\n    - shell: \"step3.sh ${item.id}\"\n</code></pre> \u2192 If agent fails, entire work item (all 3 steps) sent to DLQ for retry</p>"},{"location":"retry-configuration/workflow-level-vs-command-level-retry/#retry-granularity","title":"Retry Granularity","text":"<p>Command-Level: Immediate retry after command failure - Retry happens in same agent execution - Backoff delays between retries - Same environment/context</p> <p>Workflow-Level: Work item retry after agent failure - Retry happens in new agent execution (via DLQ) - Fresh environment/context - Manual triggering with <code>prodigy dlq retry</code></p>"},{"location":"retry-configuration/workflow-level-vs-command-level-retry/#feature-comparison","title":"Feature Comparison","text":"Feature Command-Level Workflow-Level Backoff strategies 5 strategies 4 strategies Jitter support \u2705 Yes \u274c No Error matchers \u2705 Yes \u274c No Retry budget \u2705 Yes \u274c No Failure actions \u2705 Yes (Stop/Continue/Fallback) \u274c No (uses error_policy) Circuit breakers \u2705 Yes (per RetryExecutor) \u2705 Yes (per workflow) DLQ integration \u274c No \u2705 Yes Failure thresholds \u274c No \u2705 Yes Error collection \u274c No \u2705 Yes (Aggregate/Immediate/Batched)"},{"location":"retry-configuration/workflow-level-vs-command-level-retry/#decision-tree","title":"Decision Tree","text":"<p>Use command-level retry when: - You need fine-grained control over which errors to retry - You want jitter to prevent thundering herd - You need retry budget to cap total time - You want fallback commands on failure - You're configuring a single command, not a work item</p> <p>Use workflow-level retry when: - You're running a MapReduce workflow - You want DLQ integration for failed work items - You need failure rate thresholds (stop at 25% failure) - You want batch error collection - You want to retry entire work items later</p> <p>Use both when: - You want fast recovery from transient errors (command-level) - AND you want to isolate persistent failures (DLQ) - AND you want to continue processing other items</p>"},{"location":"retry-configuration/workflow-level-vs-command-level-retry/#examples","title":"Examples","text":""},{"location":"retry-configuration/workflow-level-vs-command-level-retry/#example-1-command-level-only-standard-workflow","title":"Example 1: Command-Level Only (Standard Workflow)","text":"<pre><code>name: standard-workflow\nmode: standard\n\ncommands:\n  - shell: \"fetch-data.sh\"\n    retry_config:\n      attempts: 5\n      backoff: exponential\n      retry_on:\n        - network\n        - timeout\n\n  - shell: \"process-data.sh\"\n    retry_config:\n      attempts: 3\n      on_failure: continue  # Non-critical, can fail\n\n  - shell: \"upload-results.sh\"\n    retry_config:\n      attempts: 5\n      backoff: exponential\n      retry_on:\n        - network\n        - server_error\n      on_failure:\n        fallback:\n          command: \"save-to-local.sh\"\n</code></pre>"},{"location":"retry-configuration/workflow-level-vs-command-level-retry/#example-2-workflow-level-only-simple-mapreduce","title":"Example 2: Workflow-Level Only (Simple MapReduce)","text":"<pre><code>name: simple-mapreduce\nmode: mapreduce\n\nerror_policy:\n  on_item_failure: dlq\n  continue_on_failure: true\n  max_failures: 10\n\nmap:\n  input: \"items.json\"\n  json_path: \"$.items[*]\"\n  agent_template:\n    - shell: \"process ${item.id}\"\n    # No retry_config - relies on DLQ for failed items\n</code></pre>"},{"location":"retry-configuration/workflow-level-vs-command-level-retry/#example-3-both-systems-robust-mapreduce","title":"Example 3: Both Systems (Robust MapReduce)","text":"<pre><code>name: robust-mapreduce\nmode: mapreduce\n\nerror_policy:\n  on_item_failure: dlq\n  continue_on_failure: true\n  max_failures: 10\n  circuit_breaker:\n    failure_threshold: 5\n    timeout: \"30s\"\n\nmap:\n  input: \"items.json\"\n  json_path: \"$.items[*]\"\n  max_parallel: 10\n\n  agent_template:\n    - shell: \"process ${item.id}\"\n      retry_config:\n        attempts: 3           # Try 3 times quickly\n        backoff: exponential\n        jitter: true          # Prevent simultaneous retries\n        retry_on:\n          - network\n          - timeout\n        on_failure: stop      # Let DLQ handle final failure\n\nreduce:\n  - shell: \"aggregate ${map.results}\"\n</code></pre> <p>Retry flow: 1. Command fails with network error 2. Command retries (attempt 2, 3) with exponential backoff 3. All command retries fail \u2192 Agent fails 4. Work item sent to DLQ 5. Other work items continue processing 6. After workflow completes \u2192 <code>prodigy dlq retry &lt;job_id&gt;</code> to retry failed items</p>"},{"location":"retry-configuration/workflow-level-vs-command-level-retry/#see-also","title":"See Also","text":"<ul> <li>Basic Retry Configuration - Command-level retry config</li> <li>Backoff Strategies - Backoff strategy details</li> <li>Best Practices - When to use each system</li> <li>Complete Examples - Full workflow examples</li> </ul>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>This chapter provides comprehensive guidance for diagnosing and resolving common issues with Prodigy workflows. Whether you're experiencing MapReduce failures, checkpoint issues, or variable interpolation problems, you'll find practical solutions here.</p>"},{"location":"troubleshooting/#common-issues","title":"Common Issues","text":""},{"location":"troubleshooting/#variables-not-interpolating","title":"Variables not interpolating","text":"<p>Symptoms: Literal <code>${var}</code> appears in output instead of value</p> <p>Causes: - Variable name typo or case mismatch - Variable not in scope - Incorrect syntax - Variable not captured</p> <p>Solutions: - Check variable name spelling and case sensitivity - Verify variable is available in current scope (step vs workflow) - Ensure proper syntax: <code>${var}</code> not <code>$var</code> for complex expressions - Verify capture_output command succeeded - Check variable was set before use (e.g., in previous step)</p>"},{"location":"troubleshooting/#mapreduce-items-not-found","title":"MapReduce items not found","text":"<p>Symptoms: No items to process, empty JSONPath result, or \"items.json not found\"</p> <p>Causes: - Input file doesn't exist - Incorrect JSONPath - Setup phase failed - Wrong file format</p> <p>Solutions: - Verify input file exists with correct path - Test JSONPath expression with jsonpath-cli or jq - Check json_path field syntax (default: <code>$[*]</code>) - Ensure setup phase generated the input file successfully - Validate JSON format with jq or json validator</p>"},{"location":"troubleshooting/#timeout-errors","title":"Timeout errors","text":"<p>Symptoms: Commands or phases timing out before completion</p> <p>Causes: - Operation too slow - Insufficient timeout - Hung processes - Deadlock</p> <p>Solutions: - Increase timeout value for long operations - Optimize command execution for better performance - Split work into smaller chunks (use max_items, offset) - Check for hung processes with ps or top - Look for deadlocks in concurrent operations - Use agent_timeout_secs for MapReduce agents</p>"},{"location":"troubleshooting/#checkpoint-resume-not-working","title":"Checkpoint resume not working","text":"<p>Symptoms: Resume starts from beginning, fails to load state, or \"checkpoint not found\"</p> <p>Causes: - Checkpoint files missing - Wrong session/job ID - Workflow changed - Concurrent resume</p> <p>Solutions: - Verify checkpoint files exist in <code>~/.prodigy/state/{repo}/mapreduce/jobs/{job_id}/</code> - Check session/job ID is correct with <code>prodigy sessions list</code> - Ensure workflow file hasn't changed significantly - Check for concurrent resume lock in <code>~/.prodigy/resume_locks/</code> - Review checkpoint file contents for corruption</p> <p>See MapReduce Checkpoint and Resume for detailed information.</p>"},{"location":"troubleshooting/#dlq-items-not-retrying-or-re-failing","title":"DLQ items not retrying or re-failing","text":"<p>Symptoms: Retry command fails, items immediately fail again, or no progress</p> <p>Causes: - Systematic error not transient - DLQ file corrupted - Underlying issue not fixed</p> <p>Solutions: - Check DLQ file format and contents with <code>prodigy dlq show &lt;job_id&gt;</code> - Verify error was transient not systematic (e.g., rate limit vs bug) - Fix underlying issue before retry (e.g., API credentials, file permissions) - Increase max-parallel for retry if parallelism helps - Check json_log_location in DLQ for detailed error info</p> <p>See Dead Letter Queue (DLQ) for complete DLQ management details.</p>"},{"location":"troubleshooting/#worktree-cleanup-failures","title":"Worktree cleanup failures","text":"<p>Symptoms: Orphaned worktrees after failures, \"permission denied\" on cleanup</p> <p>Causes: - Locked files - Running processes - Permission issues - Disk full</p> <p>Solutions: - Use <code>prodigy worktree clean-orphaned &lt;job_id&gt;</code> for automatic cleanup - Check for locked files with lsof or similar tools - Verify no running processes using worktree with ps - Check disk space with <code>df -h</code> - Verify file permissions on worktree directory - Manual cleanup if necessary: <code>rm -rf ~/.prodigy/worktrees/&lt;path&gt;</code></p> <p>For more on cleanup failures, see \"Cleanup Failure Handling (Spec 136)\" in the CLAUDE.md file.</p>"},{"location":"troubleshooting/#environment-variables-not-resolved","title":"Environment variables not resolved","text":"<p>Symptoms: Literal <code>${VAR}</code> or <code>$VAR</code> appears in commands instead of value</p> <p>Causes: - Variable not defined - Wrong profile - Scope issue - Syntax error</p> <p>Solutions: - Check variable defined in env, secrets, or profiles section - Verify correct profile activated with --profile flag - Use proper syntax: <code>${VAR}</code> for workflow vars, <code>$VAR</code> may work for shell - Check variable scope (global vs step-level) - Ensure env_files loaded correctly</p> <p>See Environment Variables for variable configuration details.</p>"},{"location":"troubleshooting/#git-context-variables-empty","title":"Git context variables empty","text":"<p>Symptoms: <code>${step.files_added}</code> returns empty string or undefined</p> <p>Causes: - No commits created - Git repo not initialized - Step not completed - Wrong format</p> <p>Solutions: - Ensure commands created commits (use <code>commit_required: true</code>) - Check git repository is initialized in working directory - Verify step completed before accessing variables - Use appropriate format modifier (e.g., :json, :newline) - Check git status to verify changes exist</p> <p>See Advanced Git Context for available git variables.</p>"},{"location":"troubleshooting/#foreach-iteration-failures","title":"Foreach iteration failures","text":"<p>Symptoms: Foreach command fails partway through, items skipped, or parallel execution errors</p> <p>Causes: - Command failure with continue_on_error disabled - Parallel execution resource exhaustion - Variable interpolation errors in item context - Max items limit reached unexpectedly</p> <p>Solutions: - Enable continue_on_error to process remaining items on failure - Reduce parallelism: <code>parallel: 5</code> instead of <code>parallel: true</code> - Verify ${item}, ${index}, ${total} variable interpolation - Check max_items setting matches expectations - Review progress bar output for failure patterns - Use shell command for debugging: <code>shell: \"echo Processing ${item}\"</code></p> <p>Example foreach with error handling: <pre><code>foreach:\n  input:\n    list: [\"file1.py\", \"file2.py\", \"file3.py\"]\n  parallel: 5\n  max_items: 10\n  continue_on_error: true\n  commands:\n    - shell: \"echo Processing ${item} (${index}/${total})\"\n    - claude: \"/refactor ${item}\"\n</code></pre></p> <p>Source: src/cook/execution/foreach.rs:44-515</p>"},{"location":"troubleshooting/#workflow-composition-errors","title":"Workflow composition errors","text":"<p>Symptoms: \"Template not found\", \"Circular dependency detected\", \"Required parameter not provided\"</p> <p>Causes: - Missing or unregistered templates - Circular extends/imports chains - Required parameters not provided - Path resolution issues</p> <p>Solutions: - Verify template exists and is registered: <code>prodigy template list</code> - Register template if needed: <code>prodigy template register &lt;path&gt;</code> - Check for circular dependencies in extends/imports chains - Provide required parameters via <code>--param NAME=value</code> or <code>--param-file</code> - Review template parameter definitions for requirements - Check template paths are correct (relative to registry or filesystem)</p> <p>See Common Error Messages for specific composition error details.</p>"},{"location":"troubleshooting/#goal-seek-not-converging","title":"Goal seek not converging","text":"<p>Symptoms: \"Max attempts reached\", \"Timeout\", \"Converged\" with low score, validate command fails</p> <p>Causes: - Validate command not returning valid score (0-100) - Threshold too high for achievable goal - Max attempts too low - Claude/shell command not making effective progress - Score calculation logic incorrect</p> <p>Solutions: - Test validate command independently: <code>bash -c \"your-validate-command\"</code> - Ensure validate returns numeric score 0-100 on stdout - Lower threshold if goal is unrealistic: <code>threshold: 80</code> instead of <code>threshold: 95</code> - Increase max_attempts for complex refinements: <code>max_attempts: 10</code> - Review attempt history to see if scores are improving - Check timeout_seconds isn't too restrictive - Verify goal description is clear and actionable for Claude - Add verbose logging to validate command for debugging</p> <p>Example goal_seek configuration: <pre><code># Source: workflows/implement-goal.yml\ngoal_seek:\n  goal: \"Improve test coverage to 80%\"\n  claude: \"/improve-coverage\"\n  validate: \"cargo tarpaulin --output-format json | jq '.coverage'\"\n  threshold: 80\n  max_attempts: 5\n  timeout_seconds: 600\n</code></pre></p> <p>Source: src/cook/goal_seek/mod.rs:13-76, src/cook/goal_seek/engine.rs</p>"},{"location":"troubleshooting/#validate-command-failures","title":"Validate command failures","text":"<p>Symptoms: \"Schema validation failed\", \"Threshold not met\", \"Gap-filling failed\", validate command error</p> <p>Causes: - Validation output doesn't match expected_schema - Completeness percentage below threshold - Invalid JSON output from validation command - Timeout during validation - Gap-filling commands fail</p> <p>Solutions: - Test validation command independently and check JSON output - Verify expected_schema matches actual validation output structure - Check completeness threshold is realistic: <code>threshold: 80.0</code> - Increase validation timeout if needed: <code>timeout: 300</code> - Ensure validation command writes proper JSON to stdout or result_file - Review gap-filling commands in on_incomplete section - Use verbose mode to see validation output: <code>prodigy run workflow.yml -v</code></p> <p>Example validate configuration: <pre><code># Source: src/cook/workflow/validation.rs:10-49\nvalidate:\n  shell: \"cargo test --no-fail-fast -- --format json\"\n  expected_schema:\n    type: \"object\"\n    required: [\"passed\", \"total\"]\n  threshold: 90.0\n  on_incomplete:\n    commands:\n      - claude: \"/fix-failing-tests\"\n</code></pre></p> <p>Source: src/cook/workflow/validation.rs:10-49</p>"},{"location":"troubleshooting/#write-file-failures","title":"Write file failures","text":"<p>Symptoms: \"Permission denied\", \"Directory not found\", \"Invalid format\", file not created or corrupted</p> <p>Causes: - Parent directory doesn't exist and create_dirs not enabled - Insufficient permissions to write to path - Invalid JSON/YAML content when using format validation - Variable interpolation error in path or content - Invalid file mode permissions</p> <p>Solutions: - Enable create_dirs to auto-create parent directories: <code>create_dirs: true</code> - Check directory permissions: <code>ls -ld $(dirname path/to/file)</code> - Verify format validation for JSON/YAML: test content with <code>jq</code> or <code>yq</code> - Test variable interpolation independently: <code>echo \"${var}\"</code> - Ensure file mode is valid octal: <code>mode: \"0644\"</code> not <code>mode: \"644\"</code> - Use absolute paths or verify working directory context - Check disk space: <code>df -h</code></p> <p>Example write_file configuration: <pre><code># Source: src/config/command.rs:278-298\n- write_file:\n    path: \"output/results-${item.id}.json\"\n    content: |\n      {\n        \"item_id\": \"${item.id}\",\n        \"status\": \"completed\"\n      }\n    format: json\n    create_dirs: true\n    mode: \"0644\"\n</code></pre></p> <p>Source: src/config/command.rs:278-313, tests/write_file_integration_test.rs</p>"},{"location":"troubleshooting/#claude-command-fails-with-command-not-found","title":"Claude command fails with \"command not found\"","text":"<p>Symptoms: Shell error about claude command not existing</p> <p>Causes: - Claude Code not installed - Not in PATH - Wrong executable name</p> <p>Solutions: - Install Claude Code CLI if not present - Verify claude is in PATH with <code>which claude</code> - Check command name matches Claude Code CLI (not \"claude-code\") - Use full path if necessary: <code>/path/to/claude</code></p>"},{"location":"troubleshooting/#debug-tips","title":"Debug Tips","text":""},{"location":"troubleshooting/#use-dry-run-mode-to-preview-execution","title":"Use dry-run mode to preview execution","text":"<pre><code>prodigy run workflow.yml --dry-run\n</code></pre> <p>Shows: Preview of commands that would be executed without actually running them</p> <p>Use when: Verifying workflow steps before execution, testing variable interpolation, checking command syntax</p> <p>Source: src/cli/args.rs:64</p>"},{"location":"troubleshooting/#use-verbose-mode-for-execution-details","title":"Use verbose mode for execution details","text":"<pre><code>prodigy run workflow.yml -v\n</code></pre> <p>Shows: Claude streaming output, tool invocations, and execution timeline</p> <p>Use when: Understanding what Claude is doing, debugging tool calls</p>"},{"location":"troubleshooting/#check-claude-json-logs-for-full-interaction","title":"Check Claude JSON logs for full interaction","text":"<pre><code>prodigy logs --latest --summary\n</code></pre> <p>Shows: Full Claude interaction including messages, tools, token usage, errors</p> <p>Use when: Claude command failed, understanding why Claude made certain decisions</p> <p>For more on Claude JSON logs, see the \"Viewing Claude Execution Logs (Spec 126)\" section in the project CLAUDE.md file.</p>"},{"location":"troubleshooting/#inspect-event-logs-for-execution-timeline","title":"Inspect event logs for execution timeline","text":"<pre><code># List events for a job\nprodigy events ls --job-id &lt;job_id&gt;\n\n# Follow events in real-time\nprodigy events follow --job-id &lt;job_id&gt;\n\n# Show event statistics\nprodigy events stats\n</code></pre> <p>Shows: Detailed execution timeline, agent starts/completions, durations, real-time event stream</p> <p>Use when: Understanding workflow execution flow, finding bottlenecks, monitoring active jobs</p> <p>Source: src/cli/commands/events.rs:22-98</p>"},{"location":"troubleshooting/#review-dlq-for-failed-item-details","title":"Review DLQ for failed item details","text":"<pre><code>prodigy dlq show &lt;job_id&gt;\n</code></pre> <p>Shows: Failed items with full error details, retry history, json_log_location</p> <p>Use when: MapReduce items failing, understanding failure patterns</p>"},{"location":"troubleshooting/#check-checkpoint-state-for-resume-issues","title":"Check checkpoint state for resume issues","text":"<p>Location: <code>~/.prodigy/state/{repo}/mapreduce/jobs/{job_id}/</code></p> <p>Shows: Saved execution state, completed items, variables, phase progress</p> <p>Use when: Resume not working, understanding saved state</p>"},{"location":"troubleshooting/#examine-worktree-git-log-for-commits","title":"Examine worktree git log for commits","text":"<pre><code>cd ~/.prodigy/worktrees/{repo}/{session}/ &amp;&amp; git log\n</code></pre> <p>Shows: All commits created during workflow execution with full details</p> <p>Use when: Understanding what changed, verifying commits created</p>"},{"location":"troubleshooting/#tail-claude-json-log-in-real-time","title":"Tail Claude JSON log in real-time","text":"<pre><code>prodigy logs --latest --tail\n</code></pre> <p>Shows: Live streaming of Claude JSON log as it's being written</p> <p>Use when: Watching long-running Claude command, debugging in real-time</p>"},{"location":"troubleshooting/#additional-topics","title":"Additional Topics","text":"<p>For more specific troubleshooting guidance, see: - FAQ - Frequently asked questions - Common Error Messages - Specific error messages explained - Best Practices for Debugging - Proven debugging strategies</p>"},{"location":"troubleshooting/best-practices-for-debugging/","title":"Best Practices for Debugging","text":""},{"location":"troubleshooting/best-practices-for-debugging/#best-practices-for-debugging","title":"Best Practices for Debugging","text":"<p>This guide provides practical techniques for diagnosing and resolving issues in Prodigy workflows.</p>"},{"location":"troubleshooting/best-practices-for-debugging/#quick-debugging-checklist","title":"Quick Debugging Checklist","text":"<ol> <li>Start simple: Test commands individually before adding to workflow</li> <li>Use verbosity flags: <code>-v</code> for Claude interactions, <code>-vv</code> for debug logs, <code>-vvv</code> for trace</li> <li>Check recent logs: <code>prodigy logs --latest</code> for the last Claude execution</li> <li>Enable environment variables for detailed output (see below)</li> <li>Validate input data before running MapReduce workflows</li> <li>Test incrementally: Add commands one at a time and verify after each</li> <li>Version control: Commit working workflows before making changes</li> </ol>"},{"location":"troubleshooting/best-practices-for-debugging/#debugging-environment-variables","title":"Debugging Environment Variables","text":"<p>Control debugging output and logging behavior with environment variables (Source: <code>src/config/mod.rs:116</code>, <code>src/cook/execution/claude.rs:429-435</code>):</p>"},{"location":"troubleshooting/best-practices-for-debugging/#prodigy_log_level","title":"<code>PRODIGY_LOG_LEVEL</code>","text":"<p>Purpose: Control log verbosity Values: <code>trace</code>, <code>debug</code>, <code>info</code>, <code>warn</code>, <code>error</code> Default: <code>info</code></p> <pre><code>export PRODIGY_LOG_LEVEL=debug\nprodigy run workflow.yml\n</code></pre> <p>Use <code>debug</code> or <code>trace</code> to see internal state transitions, command execution details, and variable interpolation.</p>"},{"location":"troubleshooting/best-practices-for-debugging/#prodigy_claude_console_output","title":"<code>PRODIGY_CLAUDE_CONSOLE_OUTPUT</code>","text":"<p>Purpose: Force streaming output regardless of verbosity Values: <code>true</code>, <code>false</code> Default: Not set (respects verbosity level)</p> <pre><code>export PRODIGY_CLAUDE_CONSOLE_OUTPUT=true\nprodigy run workflow.yml  # Shows Claude output even without -v flag\n</code></pre> <p>Useful for debugging specific runs without changing command flags.</p>"},{"location":"troubleshooting/best-practices-for-debugging/#prodigy_claude_streaming","title":"<code>PRODIGY_CLAUDE_STREAMING</code>","text":"<p>Purpose: Control Claude JSON streaming mode Values: <code>true</code>, <code>false</code> Default: <code>true</code></p> <pre><code>export PRODIGY_CLAUDE_STREAMING=false  # Disable JSON streaming\nprodigy run workflow.yml\n</code></pre> <p>Streaming is enabled by default for auditability. Only disable in CI/CD environments with storage constraints.</p> <p>See also: Environment Variables for complete reference.</p>"},{"location":"troubleshooting/best-practices-for-debugging/#variable-interpolation-debugging","title":"Variable Interpolation Debugging","text":"<p>Techniques for diagnosing variable issues:</p> <ol> <li> <p>Use echo statements to verify variable values:    <pre><code>commands:\n  - shell: \"echo 'Debug: item=${item.name}, path=${item.path}'\"\n  - claude: \"/process ${item.name}\"\n</code></pre></p> </li> <li> <p>Capture command outputs with <code>capture_output</code> for later use:    <pre><code>commands:\n  - shell: \"git rev-parse HEAD\"\n    capture_output: current_commit\n  - shell: \"echo 'Current commit: ${current_commit}'\"\n</code></pre></p> </li> <li> <p>Enable verbose mode (<code>-v</code>) to see variable interpolation in real-time</p> </li> <li>Check variable scope: Distinguish step-level vs workflow-level variables</li> </ol> <p>See also: Common Issues for variable troubleshooting patterns.</p>"},{"location":"troubleshooting/best-practices-for-debugging/#profile-debugging","title":"Profile Debugging","text":"<p>Profiles allow different configuration values for different environments (dev, staging, prod). When profile-specific values aren't being used, debug the profile selection and fallback behavior.</p> <p>Verify Active Profile: <pre><code># Check which profile is active (from command line)\nprodigy run workflow.yml --profile prod -v\n\n# Profile is shown in verbose output during variable interpolation\n</code></pre></p> <p>Profile Configuration Structure (Source: CLAUDE.md \"Environment Variables (Spec 120)\"): <pre><code>env:\n  API_URL:\n    default: \"http://localhost:3000\"\n    staging: \"https://staging.api.com\"\n    prod: \"https://api.com\"\n\n  DEBUG_MODE:\n    default: \"true\"\n    prod: \"false\"\n</code></pre></p> <p>Common Profile Issues:</p> <ol> <li>Profile not applied: Using default value instead of profile-specific</li> <li>Debug: Check <code>--profile</code> flag is passed correctly</li> <li>Verify: Profile name matches exactly (case-sensitive)</li> <li> <p>Test: Echo variable to see which value is used: <code>shell: \"echo API_URL=$API_URL\"</code></p> </li> <li> <p>Profile doesn't exist: \"Invalid profile\" error</p> </li> <li>Debug: List defined profiles in <code>env:</code> section of workflow</li> <li> <p>Fix: Add profile definition or use correct profile name</p> </li> <li> <p>Variable undefined in profile: Falls back to default</p> </li> <li>Debug: Check if variable has profile-specific value defined</li> <li>Expected: Fallback to <code>default</code> is normal behavior if profile value missing</li> </ol> <p>Debugging Profile Selection: <pre><code># Add debug output to workflow\n- shell: \"echo Active environment: $ENVIRONMENT\"\n- shell: \"echo API URL: $API_URL\"\n- shell: \"echo Debug mode: $DEBUG_MODE\"\n</code></pre></p> <p>Profile Fallback Behavior: - If profile value exists: Use profile-specific value - If profile value missing: Fall back to <code>default</code> - If no <code>default</code>: Variable is undefined</p>"},{"location":"troubleshooting/best-practices-for-debugging/#sub-workflow-debugging","title":"Sub-Workflow Debugging","text":"<p>Sub-workflows allow composing complex workflows from smaller, reusable pieces. Debugging sub-workflows requires understanding result passing, context isolation, and failure propagation.</p> <p>Sub-Workflow Execution Flow (Source: features.json:workflow_composition.sub_workflows): <pre><code># Parent workflow\nsteps:\n  - sub_workflow: \"./workflows/build.yml\"\n    capture_result: build_output\n\n  - sub_workflow: \"./workflows/test.yml\"\n    params:\n      artifact_path: \"${build_output.artifact_path}\"\n</code></pre></p> <p>Common Sub-Workflow Issues:</p> <ol> <li>Result not passed correctly between sub-workflows</li> <li>Debug: Check <code>capture_result</code> is used to capture sub-workflow output</li> <li>Verify: Variable interpolation uses correct captured variable name</li> <li> <p>Test: Echo captured result to verify structure</p> </li> <li> <p>Sub-workflow failure doesn't fail parent</p> </li> <li>Debug: Check failure propagation settings</li> <li> <p>Expected: By default, sub-workflow failures should fail parent workflow</p> </li> <li> <p>Context isolation issues: Variables not available in sub-workflow</p> </li> <li>Debug: Use <code>params:</code> to explicitly pass variables to sub-workflow</li> <li>Understanding: Sub-workflows have isolated context by default</li> <li>Solution: Pass required variables via <code>params</code></li> </ol> <p>Debugging Sub-Workflow Execution: <pre><code># Check sub-workflow was invoked\ncat ~/.prodigy/sessions/{session-id}.json | jq '.workflow_data.completed_steps'\n\n# View sub-workflow results\ncat ~/.prodigy/sessions/{session-id}.json | jq '.workflow_data.variables |\n  to_entries[] | select(.key | contains(\"result\"))'\n\n# Check sub-workflow commits in worktree\ncd ~/.prodigy/worktrees/{repo}/{session}/\ngit log --grep=\"sub_workflow\" --oneline\n</code></pre></p> <p>Sub-Workflow Parameter Passing: <pre><code># Explicit parameter passing\n- sub_workflow: \"./workflows/deploy.yml\"\n  params:\n    environment: \"staging\"\n    version: \"${build_version}\"\n    artifact: \"${build_artifact}\"\n</code></pre></p> <p>Debugging Parameter Interpolation: - Use verbose mode: <code>prodigy run workflow.yml -v</code> - Add echo statements in sub-workflow to verify parameters received - Check sub-workflow definition expects parameters with correct names</p>"},{"location":"troubleshooting/best-practices-for-debugging/#log-and-state-inspection","title":"Log and State Inspection","text":"<p>Event Logs (MapReduce execution tracking): <pre><code># View event log for a specific job\nls ~/.prodigy/events/{repo_name}/{job_id}/\n\n# Follow events in real-time\ntail -f ~/.prodigy/events/{repo_name}/{job_id}/events-*.jsonl\n</code></pre></p> <p>Unified Sessions (workflow state): <pre><code># Check current session state\ncat ~/.prodigy/sessions/{session-id}.json | jq .\n\n# View session status\njq '.status, .metadata' ~/.prodigy/sessions/{session-id}.json\n</code></pre></p> <p>Checkpoints (MapReduce resume state): <pre><code># List available checkpoints\nls ~/.prodigy/state/{repo_name}/mapreduce/jobs/{job_id}/\n</code></pre></p> <p>See also: Event Tracking for event log structure.</p>"},{"location":"troubleshooting/best-practices-for-debugging/#advanced-claude-log-analysis","title":"Advanced Claude Log Analysis","text":"<p>Prodigy creates detailed JSON logs for every Claude command execution. Use <code>jq</code> to analyze these logs:</p> <p>View most recent log: <pre><code>prodigy logs --latest\n</code></pre></p> <p>Watch live execution: <pre><code>prodigy logs --latest --tail\n</code></pre></p> <p>Extract specific information (Source: CLAUDE.md \"Viewing Claude Execution Logs\"):</p> <pre><code># Extract error messages\ncat ~/.local/state/claude/logs/session-abc123.json | jq 'select(.type == \"error\")'\n\n# View all tool invocations\ncat ~/.local/state/claude/logs/session-abc123.json | jq '.messages[].content[] | select(.type == \"tool_use\")'\n\n# Analyze token usage\ncat ~/.local/state/claude/logs/session-abc123.json | jq '.usage'\n\n# Filter by specific tool\ncat ~/.local/state/claude/logs/session-abc123.json | jq '.messages[].content[] | select(.type == \"tool_use\" and .name == \"Bash\")'\n\n# View assistant responses\ncat ~/.local/state/claude/logs/session-abc123.json | jq '.messages[] | select(.role == \"assistant\")'\n</code></pre> <p>JSONL format (newer logs use line-delimited JSON): <pre><code># Count messages by type\ncat ~/.claude/projects/.../6ded63ac.jsonl | jq -r '.type' | sort | uniq -c\n\n# Extract tool uses\ncat ~/.claude/projects/.../6ded63ac.jsonl | jq -c 'select(.type == \"assistant\") | .content[]? | select(.type == \"tool_use\")'\n\n# View token usage\ncat ~/.claude/projects/.../6ded63ac.jsonl | jq -c 'select(.usage)'\n</code></pre></p>"},{"location":"troubleshooting/best-practices-for-debugging/#mapreduce-debugging","title":"MapReduce Debugging","text":"<p>Dead Letter Queue (DLQ) for failed work items:</p> <pre><code># View failed items with error details\nprodigy dlq show &lt;job_id&gt;\n\n# Check Claude execution logs for failed items\nprodigy dlq show &lt;job_id&gt; | jq '.items[].failure_history[].json_log_location'\n\n# Retry failed items (with dry run first)\nprodigy dlq retry &lt;job_id&gt; --dry-run\nprodigy dlq retry &lt;job_id&gt;\n</code></pre> <p>See also: Dead Letter Queue for DLQ management.</p>"},{"location":"troubleshooting/best-practices-for-debugging/#checkpoint-inspection","title":"Checkpoint Inspection","text":"<p>When resume fails, inspect checkpoint state to identify issues:</p> <p>Checkpoint file types (Source: CLAUDE.md \"MapReduce Checkpoint and Resume\"): - <code>setup-checkpoint.json</code>: Setup phase results and artifacts - <code>map-checkpoint-{timestamp}.json</code>: Map phase progress and work item state - <code>reduce-checkpoint-v1-{timestamp}.json</code>: Reduce phase step results and variables - <code>job-state.json</code>: Overall job state and metadata</p> <p>What to check:</p> <pre><code># Inspect map checkpoint\ncat ~/.prodigy/state/{repo}/mapreduce/jobs/{job_id}/map-checkpoint-*.json | jq '\n  {\n    total: .work_items | length,\n    completed: [.work_items[] | select(.state == \"Completed\")] | length,\n    pending: [.work_items[] | select(.state == \"Pending\")] | length,\n    in_progress: [.work_items[] | select(.state == \"InProgress\")] | length\n  }\n'\n\n# Check for failed items with error details\ncat ~/.prodigy/state/{repo}/mapreduce/jobs/{job_id}/map-checkpoint-*.json | \\\n  jq '.work_items[] | select(.state == \"Failed\") | {item_id, error}'\n\n# Verify reduce checkpoint variables\ncat ~/.prodigy/state/{repo}/mapreduce/jobs/{job_id}/reduce-checkpoint-*.json | jq '.variables'\n</code></pre> <p>Signs of corruption or inconsistencies: - Work items with <code>InProgress</code> state but no active agent - Mismatch between <code>completed_count</code> and actual completed items - Missing or null <code>variables</code> in reduce checkpoint - Duplicate <code>item_id</code> values in work items array</p> <p>Recovery strategies: - Minor corruption: Manually edit checkpoint to fix state (move <code>InProgress</code> items back to <code>Pending</code>) - Major corruption: Delete checkpoint and restart phase from beginning - Variable issues: Restore missing variables from previous checkpoint or setup output</p> <p>See also: Checkpoint and Resume for checkpoint structure.</p>"},{"location":"troubleshooting/best-practices-for-debugging/#workflow-specific-debugging","title":"Workflow-Specific Debugging","text":"<p>Different workflow types have different debugging considerations:</p> <p>Standard Workflows: - Verify variable interpolation with echo commands - Test steps individually before chaining - Check captured outputs are in correct scope</p> <p>MapReduce Workflows: - Verify agent isolation (check individual agent worktrees) - Debug parallel execution issues with event logs - Investigate merge conflicts in parent worktree after agent completion - Check work item discovery with <code>--dry-run</code> on map phase</p> <p>Goal Seek Workflows: - Validate score calculation logic - Check convergence criteria (threshold, max iterations) - Debug score trends with iteration history - Verify goal direction (maximize vs minimize)</p> <p>Foreach Workflows: - Test with small item lists first - Check parallel execution limits (<code>max_parallel</code>) - Verify item processing order if order matters - Debug failed items with individual command execution</p>"},{"location":"troubleshooting/best-practices-for-debugging/#circuit-breaker-debugging","title":"Circuit Breaker Debugging","text":"<p>When commands fail repeatedly, Prodigy's circuit breaker prevents further attempts until recovery timeout expires. Understanding circuit breaker states is essential for troubleshooting persistent failures.</p> <p>Circuit Breaker States (Source: src/cook/retry_state.rs:126-149): - Closed: Normal operation, retries allowed - Open: Circuit tripped due to failures, all attempts immediately fail - Half-Open: Testing recovery, limited attempts allowed</p> <p>Diagnosing Circuit Breaker Issues:</p> <pre><code># Check checkpoint for circuit breaker state\ncat ~/.prodigy/state/{repo}/mapreduce/jobs/{job_id}/*checkpoint*.json | \\\n  jq '.circuit_breaker_states'\n\n# Look for open circuits\ncat ~/.prodigy/state/{repo}/mapreduce/jobs/{job_id}/*checkpoint*.json | \\\n  jq '.circuit_breaker_states | to_entries[] | select(.value.state == \"Open\")'\n</code></pre> <p>Circuit Breaker Configuration (from retry config): <pre><code># Source: src/cook/retry_state.rs:126-144\nretry:\n  circuit_breaker:\n    failure_threshold: 5        # Failures before opening\n    recovery_timeout: 30s       # Wait before half-open\n    half_open_max_calls: 3      # Test attempts in half-open\n</code></pre></p> <p>Common Circuit Breaker Problems:</p> <ol> <li>Circuit stuck open: Too many failures, waiting for recovery_timeout</li> <li>Solution: Wait for timeout to expire, or fix underlying issue and restart</li> <li> <p>Debug: Check <code>last_failure_at</code> timestamp and <code>recovery_timeout</code></p> </li> <li> <p>Circuit repeatedly opening: Underlying issue not resolved</p> </li> <li>Solution: Fix root cause (permissions, credentials, resource availability)</li> <li> <p>Debug: Review <code>failure_count</code> and error messages in retry history</p> </li> <li> <p>Half-open failures: Recovery attempts still failing</p> </li> <li>Solution: Increase <code>recovery_timeout</code> to allow more time for system recovery</li> <li>Debug: Check <code>half_open_success_count</code> vs <code>half_open_max_calls</code></li> </ol> <p>Adjusting Circuit Breaker Thresholds: - Increase <code>failure_threshold</code> for transient errors: <code>failure_threshold: 10</code> - Increase <code>recovery_timeout</code> for slow-recovering services: <code>recovery_timeout: 60s</code> - Increase <code>half_open_max_calls</code> for more recovery attempts: <code>half_open_max_calls: 5</code></p> <p>Monitoring Circuit Breaker State: <pre><code># View circuit state transitions over time\ncat ~/.prodigy/events/{repo}/{job_id}/events-*.jsonl | \\\n  jq 'select(.circuit_breaker_state) | {timestamp, command_id, state: .circuit_breaker_state}'\n</code></pre></p>"},{"location":"troubleshooting/best-practices-for-debugging/#performance-debugging","title":"Performance Debugging","text":"<p>Identify and resolve performance bottlenecks:</p> <p>Timing Analysis: <pre><code># Check session timings\ncat ~/.prodigy/sessions/{session-id}.json | jq '.timings'\n\n# View step durations\ncat ~/.prodigy/sessions/{session-id}.json | jq '.timings | to_entries | sort_by(.value.secs) | reverse'\n</code></pre></p> <p>MapReduce Agent Duration: <pre><code># Track agent execution times from event logs\ncat ~/.prodigy/events/{repo}/{job_id}/events-*.jsonl | \\\n  jq 'select(.event_type == \"AgentCompleted\") | {agent_id, duration_secs: .duration.secs}'\n</code></pre></p> <p>Resource Monitoring: - Check disk space: <code>df -h ~/.prodigy/</code> - Monitor parallel execution: <code>ps aux | grep prodigy</code> - Track memory usage: <code>top</code> or <code>htop</code> during execution</p> <p>Timeout Issues (Source: <code>src/config/mapreduce.rs</code>, see Timeout Configuration): - Adjust <code>timeout_config</code> for map/reduce phases - Set <code>agent_timeout_secs</code> for long-running agents - Use <code>PRODIGY_TIMEOUT</code> environment variable for global timeout override</p>"},{"location":"troubleshooting/best-practices-for-debugging/#worktree-debugging","title":"Worktree Debugging","text":"<p>Examine worktree execution history:</p> <pre><code># Navigate to worktree\ncd ~/.prodigy/worktrees/{repo}/{session}/\n\n# View all commits (execution trace)\ngit log --oneline\n\n# Check current branch state\ngit status\n\n# View specific commit details\ngit show &lt;commit-hash&gt;\n\n# Compare with parent branch\ngit diff origin/master\n</code></pre> <p>Verify worktree isolation: <pre><code># Check main repo is clean\ngit status\n\n# Verify worktree has changes\ncd ~/.prodigy/worktrees/{repo}/{session}/\ngit status\n</code></pre></p> <p>See also: MapReduce Worktree Architecture for worktree isolation guarantees.</p>"},{"location":"troubleshooting/best-practices-for-debugging/#interactive-debugging-techniques","title":"Interactive Debugging Techniques","text":"<p>Dry-Run Mode: <pre><code># Preview DLQ retry actions without executing\nprodigy dlq retry &lt;job_id&gt; --dry-run\n\n# Test workflow validation without execution\nprodigy validate workflow.yml\n</code></pre></p> <p>Manual Checkpoint Inspection and Modification: 1. Pause workflow execution (Ctrl+C) 2. Inspect checkpoint files (see Checkpoint Inspection above) 3. Edit checkpoint JSON to fix state if needed 4. Resume with <code>prodigy resume &lt;session-id&gt;</code></p> <p>Git History Tracing: <pre><code># View execution sequence from commits\ncd ~/.prodigy/worktrees/{repo}/{session}/\ngit log --oneline --decorate --graph\n\n# Find when specific file was modified\ngit log --follow -- path/to/file\n\n# See what changed in specific step\ngit show &lt;commit-hash&gt;\n</code></pre></p>"},{"location":"troubleshooting/best-practices-for-debugging/#error-message-analysis","title":"Error Message Analysis","text":"<p>Read error messages carefully - MapReduce error types indicate specific failure modes:</p> <ul> <li><code>WorkItemProcessingError</code>: Individual agent execution failure (check DLQ)</li> <li><code>CheckpointLoadError</code>: Checkpoint corruption or missing file (inspect checkpoint directory)</li> <li><code>TimeoutError</code>: Execution exceeded configured timeout (adjust timeout settings)</li> <li><code>ValidationError</code>: Workflow configuration issue (check YAML syntax and validation output)</li> <li><code>WorktreeError</code>: Git worktree operation failure (check disk space, permissions)</li> </ul> <p>Common error patterns and fixes in Common Error Messages.</p>"},{"location":"troubleshooting/best-practices-for-debugging/#getting-help","title":"Getting Help","text":"<p>When seeking support, include:</p> <ol> <li>Full error messages (not just excerpts)</li> <li>Workflow configuration (YAML file)</li> <li>Verbosity output (<code>-vv</code> or <code>-vvv</code>)</li> <li>Recent Claude log: <code>prodigy logs --latest</code></li> <li>Session state: <code>cat ~/.prodigy/sessions/{session-id}.json</code></li> <li>Environment details: OS, Prodigy version, Claude Code version</li> <li>Reproduction steps: Minimal example that demonstrates the issue</li> </ol> <p>Where to get help: - GitHub Issues: https://github.com/prodigy-ai/prodigy - Documentation: https://prodigy.dev/docs - Community Discord: (link in README)</p>"},{"location":"troubleshooting/common-error-messages/","title":"Common Error Messages","text":""},{"location":"troubleshooting/common-error-messages/#common-error-messages","title":"Common Error Messages","text":"<p>This section documents specific error messages you may encounter while using Prodigy, along with their meanings and solutions.</p>"},{"location":"troubleshooting/common-error-messages/#quick-reference","title":"Quick Reference","text":"Error Message Category Common Cause checkpoint not found Resume Missing checkpoint files items.json not found MapReduce Setup phase failed command not found: claude Environment Claude Code not in PATH permission denied File System Insufficient permissions timeout exceeded Execution Slow operation Resume already in progress Concurrency Lock conflict JSONPath returned no results MapReduce Invalid JSONPath No commits found Git No file changes Variable not found Variables Undefined variable Invalid profile Configuration Profile not defined Disk quota exceeded File System Out of space Job already completed Resume Job finished Template not found Composition Template missing Required parameter not provided Composition Missing parameter Circular dependency detected Composition Circular imports Concurrent modification detected Concurrency Race condition"},{"location":"troubleshooting/common-error-messages/#checkpoint-not-found","title":"\"checkpoint not found\"","text":"<p>Full message: <code>Error: Checkpoint not found for session/job {id}</code></p> <p>What it means: Prodigy cannot locate checkpoint files needed to resume execution.</p> <p>Causes: - Session or job ID is incorrect - Checkpoint files were deleted or moved - Wrong repository context - Checkpoint never created (workflow didn't reach checkpoint phase)</p> <p>Solutions: 1. Verify the correct ID: <code>prodigy sessions list</code> or <code>prodigy resume-job list</code> 2. Check checkpoint directory: <code>~/.prodigy/state/{repo_name}/mapreduce/jobs/{job_id}/</code> 3. Ensure you're in the correct git repository 4. Start fresh if checkpoint is unrecoverable</p>"},{"location":"troubleshooting/common-error-messages/#itemsjson-not-found","title":"\"items.json not found\"","text":"<p>Full message: <code>Error: Input file not found: items.json</code></p> <p>What it means: The MapReduce input file specified in the workflow doesn't exist.</p> <p>Causes: - Setup phase failed to create the file - Wrong file path in workflow configuration - File created in wrong directory - File path is relative but CWD is incorrect</p> <p>Solutions: 1. Check setup phase output for errors 2. Verify <code>input:</code> path in workflow YAML 3. Ensure file path is correct (relative to workflow directory) 4. Run setup phase manually to debug file creation</p>"},{"location":"troubleshooting/common-error-messages/#command-not-found-claude","title":"\"command not found: claude\"","text":"<p>Full message: <code>bash: claude: command not found</code></p> <p>What it means: The Claude Code CLI executable is not found in the system PATH.</p> <p>Causes: - Claude Code not installed - Installation directory not in PATH - Wrong executable name in workflow - Shell environment not configured</p> <p>Solutions: 1. Install Claude Code if not present 2. Verify installation: <code>which claude</code> 3. Add Claude Code to PATH if needed 4. Use full path in workflow: <code>/usr/local/bin/claude</code></p>"},{"location":"troubleshooting/common-error-messages/#permission-denied","title":"\"permission denied\"","text":"<p>Full message: <code>Error: Permission denied: {path}</code> or <code>rm: cannot remove '{path}': Permission denied</code></p> <p>What it means: Insufficient permissions to access or modify a file/directory.</p> <p>Causes: - File/directory owned by different user - Read-only filesystem - Locked files or directories - Insufficient user permissions</p> <p>Solutions: 1. Check file ownership: <code>ls -l {path}</code> 2. Verify permissions: <code>ls -ld {directory}</code> 3. Check for locked files: <code>lsof {path}</code> 4. Run with appropriate permissions or fix ownership 5. For worktree cleanup: Use <code>prodigy worktree clean-orphaned</code></p>"},{"location":"troubleshooting/common-error-messages/#timeout-exceeded","title":"\"timeout exceeded\"","text":"<p>Full message: <code>Error: Operation timed out after {n} seconds</code></p> <p>What it means: A command or phase took longer than the configured timeout.</p> <p>Causes: - Operation genuinely slow - Hung process or deadlock - Insufficient timeout value - Resource exhaustion (CPU, memory, disk I/O)</p> <p>Solutions: 1. Increase timeout in workflow configuration 2. Check for hung processes: <code>ps aux | grep prodigy</code> 3. Optimize command performance 4. Split work into smaller chunks (use <code>max_items</code>) 5. Check system resources: <code>top</code>, <code>df -h</code></p>"},{"location":"troubleshooting/common-error-messages/#resume-already-in-progress","title":"\"Resume already in progress\"","text":"<p>Full message: <code>Error: Resume already in progress for job {job_id}. Lock held by: PID {pid} on {hostname}</code></p> <p>What it means: Another process is currently resuming this job.</p> <p>Causes: - Concurrent resume attempt - Stale lock from crashed process - Multiple terminals running resume</p> <p>Solutions: 1. Wait for other process to complete 2. Check if process is running: <code>ps aux | grep {pid}</code> 3. Remove stale lock if process is dead: <code>rm ~/.prodigy/resume_locks/{job_id}.lock</code> 4. Retry - stale locks are auto-detected and cleaned</p> <p>For details on concurrent resume protection, see \"Concurrent Resume Protection (Spec 140)\" in the CLAUDE.md file.</p>"},{"location":"troubleshooting/common-error-messages/#jsonpath-returned-no-results","title":"\"JSONPath returned no results\"","text":"<p>Full message: <code>Error: JSONPath expression '{path}' returned no results</code></p> <p>What it means: The JSONPath query didn't match any items in the input file.</p> <p>Causes: - Incorrect JSONPath syntax - Wrong data structure in input file - Empty input file - Case-sensitive key mismatch</p> <p>Solutions: 1. Test JSONPath with jq: <code>cat items.json | jq '{your_path}'</code> 2. Verify input file structure: <code>cat items.json | jq .</code> 3. Check for typos in key names 4. Ensure array brackets are correct: <code>$[*]</code> vs <code>$.items[*]</code> 5. Validate JSON format: <code>jq . items.json</code></p>"},{"location":"troubleshooting/common-error-messages/#no-commits-found","title":"\"No commits found\"","text":"<p>Full message: <code>Error: No commits found in worktree</code> or <code>${step.files_added} returned empty</code></p> <p>What it means: Git context variables are empty because no commits were created.</p> <p>Causes: - Commands didn't modify any files - Changes not committed - Wrong git repository context - Worktree not initialized properly</p> <p>Solutions: 1. Verify commands created changes: <code>git status</code> 2. Use <code>commit_required: true</code> to enforce commits 3. Check git log: <code>git log -1</code> 4. Ensure working in correct repository 5. Check if files were actually modified</p>"},{"location":"troubleshooting/common-error-messages/#variable-not-found-var","title":"\"Variable not found: {var}\"","text":"<p>Full message: <code>Error: Variable not found: {var}</code> or literal <code>${var}</code> in output</p> <p>What it means: A workflow variable reference couldn't be resolved.</p> <p>Causes: - Variable name typo or case mismatch - Variable not defined in workflow - Variable out of scope - Syntax error in interpolation</p> <p>Solutions: 1. Check variable spelling and case 2. Verify variable defined in <code>env:</code> or previous step 3. Use <code>capture_output</code> to capture command results 4. Check scope (step vs workflow level) 5. Verify syntax: <code>${var}</code> not <code>$var</code></p>"},{"location":"troubleshooting/common-error-messages/#invalid-profile-name","title":"\"Invalid profile: {name}\"","text":"<p>Full message: <code>Error: Invalid profile: {name}</code></p> <p>What it means: The specified profile doesn't exist in the workflow configuration.</p> <p>Causes: - Profile name typo - Profile not defined in workflow - Wrong flag syntax</p> <p>Solutions: 1. Check profile name spelling 2. Verify profile exists in workflow <code>env:</code> section 3. Use correct flag: <code>--profile prod</code> 4. List available profiles in workflow YAML</p>"},{"location":"troubleshooting/common-error-messages/#disk-quota-exceeded","title":"\"Disk quota exceeded\"","text":"<p>Full message: <code>Error: No space left on device</code> or <code>write: disk quota exceeded</code></p> <p>What it means: Insufficient disk space to complete operation.</p> <p>Causes: - Disk full - Quota limit reached - Large log files accumulating - Orphaned worktrees consuming space</p> <p>Solutions: 1. Check disk space: <code>df -h</code> 2. Clean orphaned worktrees: <code>prodigy worktree clean-orphaned</code> 3. Remove old logs: <code>rm ~/.prodigy/events/old-job-*/</code> 4. Clean Claude logs: <code>rm ~/.local/state/claude/logs/old-*.json</code> 5. Increase disk space or quota</p>"},{"location":"troubleshooting/common-error-messages/#job-already-completed","title":"\"Job already completed\"","text":"<p>Full message: <code>Error: Cannot resume job {job_id}: already completed</code></p> <p>What it means: Attempting to resume a job that finished successfully.</p> <p>Causes: - Job actually completed - Wrong job ID - Attempting re-run instead of resume</p> <p>Solutions: 1. Verify job status: <code>prodigy sessions list</code> 2. Check for correct job ID 3. Start new run instead of resume: <code>prodigy run workflow.yml</code> 4. Review job results if completion was successful</p>"},{"location":"troubleshooting/common-error-messages/#template-not-found","title":"\"Template not found\"","text":"<p>Full message: <code>Error: Template '{name}' not found in registry or file system</code></p> <p>What it means: A workflow template referenced in composition cannot be located.</p> <p>Causes: - Template name typo - Template not registered - Template file doesn't exist - Wrong template path</p> <p>Solutions: 1. Check template spelling and case 2. List available templates: <code>prodigy template list</code> 3. Register template if needed: <code>prodigy template register &lt;path&gt;</code> 4. Verify template file exists at specified path 5. Check template registry: <code>~/.prodigy/templates/</code></p> <p>Source: src/cook/workflow/composer_integration.rs:20, src/cook/workflow/composition/registry.rs:119</p>"},{"location":"troubleshooting/common-error-messages/#required-parameter-not-provided","title":"\"Required parameter not provided\"","text":"<p>Full message: <code>Error: Required parameter '{name}' not provided</code></p> <p>What it means: A template requires a parameter that wasn't provided during workflow composition.</p> <p>Causes: - Missing --param flag - Parameter not in param file - Template definition requires parameter - Parameter name mismatch</p> <p>Solutions: 1. Provide parameter: <code>prodigy run workflow.yml --param NAME=value</code> 2. Use parameter file: <code>--param-file params.json</code> 3. Check template parameter definitions 4. Verify parameter names match exactly (case-sensitive)</p> <p>Example parameter file: <pre><code>{\n  \"project_name\": \"my-project\",\n  \"version\": \"1.0.0\"\n}\n</code></pre></p> <p>Source: src/cook/workflow/composer_integration.rs:23</p>"},{"location":"troubleshooting/common-error-messages/#circular-dependency-detected","title":"\"Circular dependency detected\"","text":"<p>Full message: <code>Error: Circular dependency detected: {description}</code> or <code>Error: Circular dependency detected in workflow composition</code></p> <p>What it means: Workflow composition has a circular dependency where templates reference each other in a loop.</p> <p>Causes: - Template A extends/imports Template B, which extends/imports Template A - Chain of dependencies forms a cycle - Sub-workflow references create circular imports</p> <p>Solutions: 1. Review template dependencies and break the cycle 2. Restructure templates to have clear dependency hierarchy 3. Extract common functionality into a shared template 4. Check extends and imports chains</p> <p>Example circular dependency: <pre><code># template-a.yml\nextends: template-b\n\n# template-b.yml\nextends: template-a  # Circular!\n</code></pre></p> <p>Source: src/cook/workflow/composition/composer.rs:265, 727, src/error/codes.rs:78</p>"},{"location":"troubleshooting/common-error-messages/#concurrent-modification-detected","title":"\"Concurrent modification detected\"","text":"<p>Full message: <code>Error: Concurrent modification detected in checkpoint file</code></p> <p>What it means: Multiple processes tried to modify the same checkpoint simultaneously.</p> <p>Causes: - Parallel resume attempts - File system race condition - Stale file handle</p> <p>Solutions: 1. Ensure only one resume process runs at a time 2. Check for concurrent resume lock 3. Wait and retry 4. Use resume lock mechanism (automatic in newer versions)</p>"},{"location":"troubleshooting/faq/","title":"FAQ","text":""},{"location":"troubleshooting/faq/#faq","title":"FAQ","text":""},{"location":"troubleshooting/faq/#how-do-i-debug-variable-interpolation-issues","title":"How do I debug variable interpolation issues?","text":"<p>When <code>${var}</code> appears literally in output instead of being replaced:</p> <ol> <li>Check spelling and case: Variable names are case-sensitive</li> <li>Verify scope: Ensure the variable is available (step vs workflow level)</li> <li>Use verbose mode: Run with <code>-v</code> to see variable interpolation in real-time</li> <li>Verify capture: If using <code>capture_output</code>, ensure the command succeeded</li> <li>Check syntax: Use <code>${var}</code> for workflow variables, not just <code>$var</code></li> </ol> <p>Example debugging: <pre><code># Add echo to verify variable value\n- shell: \"echo Variable value: ${my_var}\"\n</code></pre></p>"},{"location":"troubleshooting/faq/#what-should-i-do-when-checkpoint-resume-fails","title":"What should I do when checkpoint resume fails?","text":"<p>If resume starts from the beginning or shows \"checkpoint not found\":</p> <ol> <li>Verify checkpoint exists: Check <code>~/.prodigy/state/{repo}/mapreduce/jobs/{job_id}/</code></li> <li>Confirm job ID: Use <code>prodigy sessions list</code> to find the correct ID</li> <li>Check for concurrent resume: Look for lock files in <code>~/.prodigy/resume_locks/</code></li> <li>Review checkpoint integrity: Read the checkpoint JSON to ensure it's valid</li> <li>Ensure workflow unchanged: Significant workflow changes may prevent resume</li> </ol> <p>See MapReduce Checkpoint and Resume for complete details.</p>"},{"location":"troubleshooting/faq/#how-do-i-retry-failed-dlq-items","title":"How do I retry failed DLQ items?","text":"<p>To retry items that failed during MapReduce execution:</p> <pre><code># View failed items\nprodigy dlq list --job-id &lt;job_id&gt;\n\n# Retry all failed items\nprodigy dlq retry &lt;job_id&gt;\n\n# Retry with custom parallelism\nprodigy dlq retry &lt;job_id&gt; --parallel 10\n\n# Dry run to preview retry\nprodigy dlq retry &lt;job_id&gt; --dry-run\n</code></pre> <p>Important: Before retrying, ensure the underlying issue is fixed. If the error is systematic (not transient), items will fail again.</p> <p>Check <code>json_log_location</code> in DLQ entries to debug the original failure.</p>"},{"location":"troubleshooting/faq/#why-are-my-mapreduce-items-not-being-found","title":"Why are my MapReduce items not being found?","text":"<p>If you see \"No items to process\" or \"items.json not found\":</p> <ol> <li>Verify input file exists: Check the path specified in <code>input:</code></li> <li>Confirm setup succeeded: Ensure setup phase created the input file</li> <li>Test JSONPath: Use <code>jq</code> to test your <code>json_path</code> expression</li> <li>Validate JSON format: Ensure the file is valid JSON with <code>jq .</code></li> <li>Check file location: Input file path is relative to workflow directory</li> </ol> <p>Example JSONPath test: <pre><code>cat items.json | jq '$.items[*]'\n</code></pre></p>"},{"location":"troubleshooting/faq/#how-do-i-view-claude-execution-logs","title":"How do I view Claude execution logs?","text":"<p>To see detailed logs of what Claude did during a command:</p> <pre><code># View most recent log\nprodigy logs --latest\n\n# View with summary\nprodigy logs --latest --summary\n\n# Tail log in real-time\nprodigy logs --latest --tail\n\n# Direct access to logs\ncat ~/.local/state/claude/logs/session-*.json\n</code></pre> <p>Logs contain: - Complete message history - All tool invocations with parameters - Token usage statistics - Error details and stack traces</p> <p>For detailed log analysis techniques, see \"Viewing Claude Execution Logs (Spec 126)\" in the project CLAUDE.md file.</p>"},{"location":"troubleshooting/faq/#what-does-command-not-found-claude-mean","title":"What does \"command not found: claude\" mean?","text":"<p>This error indicates Claude Code CLI is not installed or not in your PATH:</p> <ol> <li>Verify installation: Check if Claude Code is installed</li> <li>Check PATH: Run <code>which claude</code> to see if it's accessible</li> <li>Use full path: Specify <code>/path/to/claude</code> in workflow if needed</li> <li>Verify executable name: Should be <code>claude</code>, not <code>claude-code</code></li> </ol> <p>Installation varies by platform - refer to Claude Code documentation.</p>"},{"location":"troubleshooting/faq/#how-do-i-clean-up-orphaned-worktrees","title":"How do I clean up orphaned worktrees?","text":"<p>When worktree cleanup fails during MapReduce execution:</p> <pre><code># Clean orphaned worktrees for a job\nprodigy worktree clean-orphaned &lt;job_id&gt;\n\n# Dry run to preview cleanup\nprodigy worktree clean-orphaned &lt;job_id&gt; --dry-run\n\n# Force cleanup without confirmation\nprodigy worktree clean-orphaned &lt;job_id&gt; --force\n</code></pre> <p>Common causes of cleanup failures: - Locked files (check with <code>lsof</code>) - Running processes (check with <code>ps</code>) - Permission issues (verify with <code>ls -ld</code>) - Insufficient disk space (check with <code>df -h</code>)</p> <p>For details on cleanup failures, see \"Cleanup Failure Handling (Spec 136)\" in the CLAUDE.md file.</p>"},{"location":"troubleshooting/faq/#why-are-environment-variables-not-being-resolved","title":"Why are environment variables not being resolved?","text":"<p>If <code>${VAR}</code> or <code>$VAR</code> appears literally in commands:</p> <ol> <li>Check definition: Ensure variable is defined in <code>env:</code> section</li> <li>Verify profile: Use <code>--profile</code> flag if using profile-specific values</li> <li>Check scope: Confirm variable is global or in correct scope</li> <li>Use correct syntax: <code>${VAR}</code> for workflow vars, <code>$VAR</code> for shell vars</li> <li>Validate env_files: Ensure external env files are loaded correctly</li> </ol> <p>Example: <pre><code>env:\n  API_KEY: \"my-key\"\n  DATABASE_URL:\n    default: \"localhost\"\n    prod: \"prod-server\"\n</code></pre></p> <p>See Environment Variables for configuration details.</p>"},{"location":"troubleshooting/faq/#how-do-i-debug-timeout-errors","title":"How do I debug timeout errors?","text":"<p>When commands or phases time out:</p> <ol> <li>Increase timeout: Adjust timeout values for long operations</li> <li>Check for hung processes: Use <code>ps</code> or <code>top</code> to find stuck processes</li> <li>Optimize performance: Split work into smaller chunks</li> <li>Use agent_timeout_secs: Set per-agent timeout for MapReduce</li> <li>Look for deadlocks: Check for concurrent operations blocking each other</li> </ol> <p>MapReduce-specific timeout configuration: <pre><code>map:\n  agent_timeout_secs: 600  # 10 minutes per agent\n  max_items: 10  # Process fewer items per run\n</code></pre></p>"},{"location":"troubleshooting/faq/#where-are-event-logs-stored","title":"Where are event logs stored?","text":"<p>Event logs use a global storage architecture:</p> <p>Location: <code>~/.prodigy/events/{repo_name}/{job_id}/</code></p> <p>What's stored: - Agent lifecycle events (started, completed, failed) - Work item processing status - Checkpoint saves - Error details with correlation IDs</p> <p>How to view: <pre><code># List events for a job\nprodigy events ls --job-id &lt;job_id&gt;\n\n# Show event statistics\nprodigy events stats\n\n# View detailed event timeline\ncat ~/.prodigy/events/{repo_name}/{job_id}/events-*.jsonl\n\n# For real-time monitoring, tail the event file:\ntail -f ~/.prodigy/events/{repo_name}/{job_id}/events-*.jsonl\n</code></pre></p> <p>Source: src/cli/commands/events.rs:22-98</p> <p>Events are shared across worktrees, enabling centralized monitoring of parallel jobs.</p>"},{"location":"variables/","title":"Variable Interpolation","text":""},{"location":"variables/#overview","title":"Overview","text":"<p>Prodigy provides two complementary variable systems:</p> <ol> <li>Built-in Variables: Automatically available based on workflow context (workflow state, step info, work items, etc.)</li> <li>Custom Captured Variables: User-defined variables created via the <code>capture:</code> field in commands</li> </ol> <p>Both systems use the same <code>${variable.name}</code> interpolation syntax and can be freely mixed in your workflows.</p>"},{"location":"variables/#variable-availability-by-phase","title":"Variable Availability by Phase","text":"Variable Category Setup Map Reduce Merge Standard Variables \u2713 \u2713 \u2713 \u2713 Output Variables \u2713 \u2713 \u2713 \u2713 Item Variables (<code>${item.*}</code>) \u2717 \u2713 \u2717 \u2717 Map Aggregation (<code>${map.total}</code>, etc.) \u2717 \u2717 \u2713 \u2717 Merge Variables \u2717 \u2717 \u2717 \u2713 Custom Captured Variables \u2713 \u2713 \u2713 \u2713 <p>Note: Using phase-specific variables outside their designated phase (e.g., <code>${item}</code> in reduce phase, <code>${map.results}</code> in map phase) will result in interpolation errors or empty values. Always verify variable availability matches your workflow phase.</p> <p>Reduce Phase Access to Item Data: In reduce phase, individual item variables (<code>${item.*}</code>) are not directly available, but you can access all item data through <code>${map.results}</code> which contains the aggregated results from all map agents. This allows you to process item-level information during aggregation.</p>"},{"location":"variables/#additional-topics","title":"Additional Topics","text":"<p>See also: - Available Variables - Custom Variable Capture - Troubleshooting Variable Interpolation - See Also</p>"},{"location":"variables/available-variables/","title":"Available Variables","text":""},{"location":"variables/available-variables/#available-variables","title":"Available Variables","text":"<p>Prodigy provides a comprehensive set of built-in variables that are automatically available based on your workflow context. All variables use the <code>${variable.name}</code> interpolation syntax.</p>"},{"location":"variables/available-variables/#standard-variables","title":"Standard Variables","text":"<p>These variables capture output from the most recently executed command:</p> Variable Description Example <code>${last.output}</code> Output from the last command of any type (shell, claude, handler) <code>echo ${last.output}</code> <code>${last.exit_code}</code> Exit code from the last command <code>if [ ${last.exit_code} -eq 0 ]</code> <code>${shell.output}</code> Output from the last shell command specifically <code>echo ${shell.output}</code> <code>${claude.output}</code> Output from the last Claude command specifically <code>echo ${claude.output}</code> <p>Note: Use <code>${last.output}</code> when you need output from any command type. Use <code>${shell.output}</code> or <code>${claude.output}</code> when you specifically want output from that command type.</p> <p>Example: <pre><code>- shell: \"cargo test --lib\"\n- shell: \"echo 'Test output: ${shell.output}'\"\n\n# last.output works with any command type\n- claude: \"/analyze-code\"\n- shell: \"echo 'Claude analysis: ${last.output}'\"\n</code></pre></p>"},{"location":"variables/available-variables/#computed-variables","title":"Computed Variables","text":"<p>Computed variables are dynamically evaluated at runtime, providing access to external data sources and generated values. These variables are prefixed with specific identifiers that trigger their evaluation.</p> <p>Source: src/cook/execution/variables.rs:100-305</p> Variable Type Syntax Description Cached Example Environment <code>${env.VAR_NAME}</code> Read environment variable Yes <code>${env.HOME}</code>, <code>${env.PATH}</code> File Content <code>${file:path/to/file}</code> Read file contents Yes <code>${file:config.txt}</code>, <code>${file:data.json}</code> Command Output <code>${cmd:shell-command}</code> Execute command and capture output Yes <code>${cmd:git rev-parse HEAD}</code>, <code>${cmd:date +%Y}</code> JSON Path <code>${json:path:from:source_var}</code> Extract from JSON using JSONPath No <code>${json:$.items[0].name:from:data}</code> Date Format <code>${date:format}</code> Current date/time with format No <code>${date:%Y-%m-%d}</code>, <code>${date:%H:%M:%S}</code> UUID <code>${uuid}</code> Generate random UUID v4 No <code>${uuid}</code> (always unique) <p>Available in: All phases</p>"},{"location":"variables/available-variables/#environment-variables-env","title":"Environment Variables (<code>env.*</code>)","text":"<p>Access environment variables at runtime. Useful for reading system configuration or secrets.</p> <p>Source: src/cook/execution/variables.rs:160-187</p> <p>Examples: <pre><code># Read user's home directory\n- shell: \"echo 'Home: ${env.HOME}'\"\n\n# Use CI environment variables\n- shell: \"echo 'Running in ${env.CI_PROVIDER:-local}'\"\n\n# Access secrets from environment\n- shell: \"curl -H 'Authorization: Bearer ${env.API_TOKEN}' https://api.example.com\"\n</code></pre></p> <p>Caching: Environment variable reads are cached for performance (LRU cache, 100 entries).</p>"},{"location":"variables/available-variables/#file-content-file","title":"File Content (<code>file:</code>)","text":"<p>Read file contents directly into variables. Useful for configuration, templates, or data files.</p> <p>Source: src/cook/execution/variables.rs:189-216</p> <p>Examples: <pre><code># Read version from file\n- shell: \"echo 'Version: ${file:VERSION}'\"\n\n# Use file content in command\n- shell: \"git commit -m '${file:.commit-message.txt}'\"\n\n# Read JSON configuration\n- shell: \"echo '${file:config.json}' | jq '.database.host'\"\n</code></pre></p> <p>Caching: File reads are cached (file content is expensive to read repeatedly).</p> <p>Note: File paths are relative to workflow execution directory.</p>"},{"location":"variables/available-variables/#command-output-cmd","title":"Command Output (<code>cmd:</code>)","text":"<p>Execute shell commands and capture their output as variable values. Powerful for dynamic configuration.</p> <p>Source: src/cook/execution/variables.rs:218-256</p> <p>Examples: <pre><code># Get current git commit\n- shell: \"echo 'Building from ${cmd:git rev-parse --short HEAD}'\"\n\n# Use command output in logic\n- shell: \"if [ '${cmd:uname}' = 'Darwin' ]; then echo 'macOS'; fi\"\n\n# Capture timestamp\n- shell: \"echo 'Build started at ${cmd:date +%Y-%m-%d_%H-%M-%S}'\"\n\n# Dynamic configuration\n- shell: \"cargo build --jobs ${cmd:nproc}\"\n</code></pre></p> <p>Caching: Command execution results are cached (commands are expensive to execute repeatedly).</p> <p>Security Warning: Be cautious with <code>cmd:</code> variables in untrusted workflows - they execute arbitrary shell commands.</p>"},{"location":"variables/available-variables/#json-path-extraction-json","title":"JSON Path Extraction (<code>json:</code>)","text":"<p>Extract values from JSON data using JSONPath syntax. Useful for processing complex JSON structures.</p> <p>Source: src/cook/execution/variables.rs:350-379</p> <p>Syntax: <code>${json:path:from:source_variable}</code></p> <p>Examples: <pre><code># Extract from captured variable\n- shell: \"curl https://api.example.com/data\"\n  capture_output: \"api_response\"\n- shell: \"echo 'ID: ${json:$.id:from:api_response}'\"\n\n# Extract array element\n- shell: \"echo 'First item: ${json:$.items[0].name:from:api_response}'\"\n\n# Extract nested field\n- shell: \"echo 'Author: ${json:$.metadata.author:from:config}'\"\n</code></pre></p> <p>Not Cached: JSON path extraction is fast and not cached.</p> <p>Requires: Source variable must contain valid JSON.</p>"},{"location":"variables/available-variables/#date-formatting-date","title":"Date Formatting (<code>date:</code>)","text":"<p>Generate current date/time with custom formatting using chrono format specifiers.</p> <p>Source: src/cook/execution/variables.rs:278-305</p> <p>Syntax: <code>${date:format}</code> (uses chrono format specifiers)</p> <p>Examples: <pre><code># ISO 8601 date\n- shell: \"echo 'Report generated: ${date:%Y-%m-%d}'\"\n\n# Full timestamp\n- shell: \"echo 'Build time: ${date:%Y-%m-%d %H:%M:%S}'\"\n\n# Custom format\n- shell: \"mkdir backup-${date:%Y%m%d-%H%M%S}\"\n\n# Use in filenames\n- shell: \"cp logs.txt logs-${date:%Y-%m-%d}.txt\"\n</code></pre></p> <p>Common Format Specifiers: - <code>%Y</code> - 4-digit year (2025) - <code>%m</code> - Month (01-12) - <code>%d</code> - Day (01-31) - <code>%H</code> - Hour 24h (00-23) - <code>%M</code> - Minute (00-59) - <code>%S</code> - Second (00-59) - <code>%F</code> - ISO 8601 date (2025-01-15) - <code>%T</code> - ISO 8601 time (14:30:45)</p> <p>Not Cached: Date values change over time and are not cached.</p>"},{"location":"variables/available-variables/#uuid-generation-uuid","title":"UUID Generation (<code>uuid</code>)","text":"<p>Generate a random UUID version 4. Useful for unique identifiers, temporary filenames, or correlation IDs.</p> <p>Source: src/cook/execution/variables.rs:258-276</p> <p>Examples: <pre><code># Generate unique identifier\n- shell: \"echo 'Request ID: ${uuid}'\"\n\n# Create unique temporary file\n- shell: \"mkdir /tmp/build-${uuid}\"\n\n# Correlation ID for tracking\n- shell: \"curl -H 'X-Correlation-ID: ${uuid}' https://api.example.com\"\n\n# Unique test run ID\n- shell: \"cargo test -- --test-id ${uuid}\"\n</code></pre></p> <p>Not Cached: Each <code>${uuid}</code> reference generates a NEW unique UUID. If you need the same UUID multiple times, capture it first:</p> <pre><code>- shell: \"echo '${uuid}'\"\n  capture_output: \"run_id\"\n- shell: \"echo 'Run ID: ${run_id}'\"  # Same UUID\n- shell: \"echo 'Same ID: ${run_id}'\" # Still same UUID\n</code></pre>"},{"location":"variables/available-variables/#computed-variable-caching","title":"Computed Variable Caching","text":"<p>Computed variables have different caching behaviors based on their expense and volatility:</p> <p>Cached (Expensive Operations): - <code>env.*</code> - Environment variable reads - <code>file:*</code> - File system operations - <code>cmd:*</code> - Shell command execution</p> <p>Not Cached (Fast or Volatile): - <code>json:*</code> - JSON parsing is fast - <code>date:*</code> - Values change over time - <code>uuid</code> - Must be unique each time</p> <p>Cache Details: - Type: LRU (Least Recently Used) cache - Size: 100 entries maximum - Scope: Per workflow execution - Thread Safety: Async RwLock protection</p> <p>Source: src/cook/execution/variables.rs:218-256 (caching implementation)</p>"},{"location":"variables/available-variables/#workflow-context-variables","title":"Workflow Context Variables","text":"<p>Variables that provide information about the current workflow execution:</p> Variable Description Example <code>${workflow.name}</code> Workflow name from YAML config <code>echo \"Running ${workflow.name}\"</code> <code>${workflow.id}</code> Unique workflow identifier <code>log-${workflow.id}.txt</code> <code>${workflow.iteration}</code> Current iteration number (for loops) <code>Iteration ${workflow.iteration}</code> <p>Available in: All phases (setup, map, reduce, merge)</p>"},{"location":"variables/available-variables/#step-context-variables","title":"Step Context Variables","text":"<p>Variables providing information about the current execution step:</p> Variable Description Example <code>${step.name}</code> Step name or identifier <code>echo \"Step: ${step.name}\"</code> <code>${step.index}</code> Zero-based step index <code>Step ${step.index} of ${total_steps}</code> <p>Available in: All phases</p>"},{"location":"variables/available-variables/#item-variables-map-phase-only","title":"Item Variables (Map Phase Only)","text":"<p>Variables for accessing work item data during parallel processing. The <code>${item.*}</code> syntax supports arbitrary field access - you can access any field present in your JSON work items, not just the predefined ones shown below.</p> <p>Source: src/cook/workflow/variables.rs:16-23 (item variable resolution)</p> Variable Description Example <code>${item}</code> Full item object (as string) <code>echo ${item}</code> <code>${item.value}</code> Item value for simple types <code>process ${item.value}</code> <code>${item.path}</code> File path (for file inputs) <code>cat ${item.path}</code> <code>${item.name}</code> Item display name <code>echo \"Processing ${item.name}\"</code> <code>${item_index}</code> Zero-based item index <code>Item ${item_index}</code> <code>${item_total}</code> Total number of items <code>of ${item_total}</code> <code>${item.*}</code> Any JSON field - Access arbitrary fields from your work items <code>${item.priority}</code>, <code>${item.custom_field}</code> <p>Available in: Map phase only</p>"},{"location":"variables/available-variables/#arbitrary-field-access","title":"Arbitrary Field Access","text":"<p>The <code>${item.*}</code> syntax provides full access to any field in your JSON work items. This includes:</p> <ul> <li>Top-level fields: <code>${item.priority}</code>, <code>${item.status}</code>, <code>${item.category}</code></li> <li>Nested fields: <code>${item.metadata.author}</code>, <code>${item.config.database.host}</code></li> <li>Array indices: <code>${item.tags[0]}</code>, <code>${item.dependencies[2].version}</code></li> <li>Mixed access: <code>${item.data.results[0].score}</code></li> </ul> <p>Example with custom JSON structure: <pre><code># Input: items.json\n# [\n#   {\n#     \"file\": \"src/main.rs\",\n#     \"priority\": 10,\n#     \"owner\": \"backend-team\",\n#     \"metadata\": {\n#       \"last_modified\": \"2025-01-10\",\n#       \"reviewer\": \"alice\"\n#     },\n#     \"tags\": [\"critical\", \"security\"]\n#   }\n# ]\n\nmap:\n  input: \"items.json\"\n  json_path: \"$[*]\"\n  agent_template:\n    # Access any field from your JSON structure\n    - shell: \"echo 'Processing ${item.file}'\"\n    - shell: \"echo 'Priority: ${item.priority}'\"\n    - shell: \"echo 'Owner: ${item.owner}'\"\n    - shell: \"echo 'Reviewer: ${item.metadata.reviewer}'\"\n    - shell: \"echo 'First tag: ${item.tags[0]}'\"\n    - claude: \"/analyze '${item.file}' --priority ${item.priority} --owner ${item.owner}\"\n</code></pre></p> <p>Best Practice: Use descriptive field names in your JSON work items - they become your variable names.</p>"},{"location":"variables/available-variables/#mapreduce-variables-reduce-phase-only","title":"MapReduce Variables (Reduce Phase Only)","text":"<p>Variables for accessing aggregated results from map phase. Map results support indexed access for retrieving individual agent results and nested field access for extracting specific properties.</p> <p>Source: src/cook/execution/mapreduce/utils.rs:119-121, src/cook/execution/mapreduce/reduce_phase.rs:146</p> Variable Description Example <code>${map.total}</code> Total items in map phase <code>echo \"Processed ${map.total} items\"</code> <code>${map.successful}</code> Successfully processed items <code>echo \"${map.successful} succeeded\"</code> <code>${map.failed}</code> Failed items count <code>echo \"${map.failed} failed\"</code> <code>${map.results}</code> All map results as JSON array <code>echo '${map.results}' \\| jq</code> <code>${map.results_json}</code> Alias for <code>map.results</code> (same value) <code>echo '${map.results_json}' \\| jq</code> <code>${map.results[index]}</code> Individual result by index (0-based) <code>${map.results[0]}</code>, <code>${map.results[5]}</code> <code>${map.results[index].field}</code> Nested field access <code>${map.results[0].output}</code>, <code>${map.results[2].item_id}</code> <code>${map.key}</code> Key for map output (optional) <code>${map.key}</code> <code>${worker.id}</code> Worker ID for tracking <code>Worker ${worker.id}</code> <p>Available in: Reduce phase only</p>"},{"location":"variables/available-variables/#indexed-access-to-map-results","title":"Indexed Access to Map Results","text":"<p>You can access individual agent results using bracket notation <code>[index]</code> and drill into nested fields with dot notation.</p> <p>Syntax patterns: - <code>${map.results[0]}</code> - First agent result (full object) - <code>${map.results[0].output}</code> - Output from first agent - <code>${map.results[0].item_id}</code> - Item ID processed by first agent - <code>${map.results[0].success}</code> - Success status (\"true\" or \"false\")</p> <p>Example: <pre><code>reduce:\n  # Access specific agent results\n  - shell: \"echo 'First result: ${map.results[0]}'\"\n  - shell: \"echo 'First output: ${map.results[0].output}'\"\n  - shell: \"echo 'Second agent processed: ${map.results[1].item_id}'\"\n\n  # Combine with shell commands\n  - shell: |\n      if [ \"${map.results[0].success}\" = \"true\" ]; then\n        echo \"First agent succeeded\"\n      fi\n\n  # Process multiple results\n  - shell: |\n      echo \"Results 0-2:\"\n      echo \"${map.results[0].item_id}\"\n      echo \"${map.results[1].item_id}\"\n      echo \"${map.results[2].item_id}\"\n</code></pre></p>"},{"location":"variables/available-variables/#full-array-processing","title":"Full Array Processing","text":"<p>For processing all results, use <code>${map.results}</code> with JSON tools like <code>jq</code>:</p> <pre><code>reduce:\n  # Count errors using jq\n  - shell: |\n      echo '${map.results}' | jq '[.[] | select(.status == \"error\")] | length'\n    capture_output: \"error_count\"\n\n  # Extract all item IDs\n  - shell: |\n      echo '${map.results}' | jq -r '.[].item_id'\n    capture_output: \"processed_items\"\n\n  # Calculate average score\n  - shell: |\n      echo '${map.results}' | jq '[.[].score] | add / length'\n    capture_output: \"avg_score\"\n\n  # Filter successful results\n  - shell: |\n      echo '${map.results}' | jq '[.[] | select(.success == true)]'\n    capture_output: \"successful_results\"\n\n  # Generate summary\n  - claude: \"/summarize ${map.results} --total ${map.total} --failed ${map.failed}\"\n</code></pre> <p>Note: <code>${map.results}</code> and <code>${map.results_json}</code> are equivalent - use whichever is clearer in your context.</p>"},{"location":"variables/available-variables/#git-context-variables","title":"Git Context Variables","text":"<p>Variables tracking git changes throughout workflow execution:</p> Variable Description Example <code>${step.files_added}</code> Files added in current step <code>echo ${step.files_added}</code> <code>${step.files_modified}</code> Files modified in current step <code>echo ${step.files_modified}</code> <code>${step.files_deleted}</code> Files deleted in current step <code>echo ${step.files_deleted}</code> <code>${step.files_changed}</code> All changed files (added + modified + deleted) <code>echo ${step.files_changed}</code> <code>${step.commits}</code> Commits in current step <code>echo ${step.commits}</code> <code>${step.commit_count}</code> Number of commits in step <code>echo \"${step.commit_count} commits\"</code> <code>${step.insertions}</code> Lines inserted in step <code>echo \"+${step.insertions}\"</code> <code>${step.deletions}</code> Lines deleted in step <code>echo \"-${step.deletions}\"</code> <code>${workflow.commits}</code> All commits in workflow <code>git show ${workflow.commits}</code> <code>${workflow.commit_count}</code> Total number of commits <code>echo \"${workflow.commit_count} commits\"</code> <p>Available in: All phases (requires git repository)</p>"},{"location":"variables/available-variables/#format-modifiers","title":"Format Modifiers","text":"<p>Important: These format modifiers work with all git context variables that return file or commit lists, not just the examples shown. Apply them to any of: <code>step.files_added</code>, <code>step.files_modified</code>, <code>step.files_deleted</code>, <code>step.files_changed</code>, <code>step.commits</code>, <code>workflow.commits</code>, and merge phase git variables.</p> <p>Git context variables support multiple output formats:</p> Modifier Description Example (default) Space-separated list <code>${step.files_added}</code> \u2192 <code>file1.rs file2.rs</code> <code>:json</code> JSON array format <code>${step.files_added:json}</code> \u2192 <code>[\"file1.rs\", \"file2.rs\"]</code> <code>:lines</code> Newline-separated list <code>${step.files_added:lines}</code> \u2192 <code>file1.rs\\nfile2.rs</code> <code>:csv</code> Comma-separated list <code>${step.files_added:csv}</code> \u2192 <code>file1.rs,file2.rs</code> <code>:*.ext</code> Glob pattern filter <code>${step.files_added:*.rs}</code> \u2192 only Rust files <code>:path/**/*.ext</code> Path with glob <code>${step.files_added:src/**/*.rs}</code> \u2192 Rust files in src/ <p>Format Examples: <pre><code># JSON format for jq processing\n- shell: \"echo '${step.files_added:json}' | jq -r '.[]'\"\n\n# Newline format for iteration\n- shell: |\n    echo '${step.files_modified:lines}' | while read file; do\n      cargo fmt \"$file\"\n    done\n\n# Glob filtering for language-specific operations\n- shell: \"cargo clippy ${step.files_modified:*.rs}\"\n\n# Multiple glob patterns\n- shell: \"git diff ${step.files_modified:*.rs,*.toml}\"\n</code></pre></p>"},{"location":"variables/available-variables/#merge-variables-merge-phase-only","title":"Merge Variables (Merge Phase Only)","text":"<p>Variables available during the merge phase when integrating worktree changes. Merge variables include both basic context and comprehensive git tracking information.</p> <p>Source: src/worktree/merge_orchestrator.rs:340-423</p>"},{"location":"variables/available-variables/#basic-merge-context","title":"Basic Merge Context","text":"Variable Description Example <code>${merge.worktree}</code> Worktree name being merged <code>echo ${merge.worktree}</code> <code>${merge.source_branch}</code> Source branch from worktree <code>git log ${merge.source_branch}</code> <code>${merge.target_branch}</code> Target branch (where you started) <code>git merge ${merge.source_branch}</code> <code>${merge.session_id}</code> Session ID for correlation <code>echo ${merge.session_id}</code>"},{"location":"variables/available-variables/#merge-git-context-variables","title":"Merge Git Context Variables","text":"<p>Additional variables tracking git changes during the merge operation:</p> Variable Description Format Example <code>${merge.commits}</code> All commits from worktree JSON array <code>echo '${merge.commits}' \\| jq</code> <code>${merge.commit_count}</code> Number of commits Integer <code>echo \"${merge.commit_count} commits\"</code> <code>${merge.commit_ids}</code> Short commit IDs Comma-separated <code>git show ${merge.commit_ids}</code> <code>${merge.modified_files}</code> Modified files with metadata JSON array <code>echo '${merge.modified_files}' \\| jq</code> <code>${merge.file_count}</code> Number of modified files Integer <code>echo \"${merge.file_count} files\"</code> <code>${merge.file_list}</code> File paths Comma-separated <code>echo ${merge.file_list}</code> <p>Available in: Merge phase only</p> <p>Limits: Capped at 100 commits and 500 files to prevent overwhelming workflows (configurable in GitOperationsConfig).</p>"},{"location":"variables/available-variables/#merge-context-examples","title":"Merge Context Examples","text":"<p>Basic merge workflow: <pre><code>merge:\n  commands:\n    - shell: \"git fetch origin\"\n    - shell: \"git merge origin/${merge.target_branch}\"\n    - claude: \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"\n</code></pre></p> <p>Using git context variables: <pre><code>merge:\n  commands:\n    # Show merge summary\n    - shell: |\n        echo \"Merging worktree: ${merge.worktree}\"\n        echo \"Commits: ${merge.commit_count}\"\n        echo \"Files modified: ${merge.file_count}\"\n\n    # List all commits being merged\n    - shell: \"echo 'Commit IDs: ${merge.commit_ids}'\"\n\n    # Process commits as JSON\n    - shell: |\n        echo '${merge.commits}' | jq -r '.[] | \"\\(.short_id): \\(.message)\"'\n\n    # Check specific files\n    - shell: |\n        echo '${merge.modified_files}' | jq -r '.[].path'\n\n    # Conditional merge based on file count\n    - shell: |\n        if [ ${merge.file_count} -gt 50 ]; then\n          echo \"Large merge detected, requesting review\"\n        fi\n\n    # Perform merge\n    - claude: \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"\n</code></pre></p>"},{"location":"variables/available-variables/#commit-object-structure","title":"Commit Object Structure","text":"<p>The <code>${merge.commits}</code> variable contains an array of commit objects with this structure:</p> <pre><code>[\n  {\n    \"id\": \"full-sha-hash\",\n    \"short_id\": \"abc1234\",\n    \"author\": {\n      \"name\": \"Author Name\",\n      \"email\": \"author@example.com\"\n    },\n    \"message\": \"Commit message\",\n    \"timestamp\": \"2025-01-10T12:00:00Z\",\n    \"files_changed\": [\"file1.rs\", \"file2.rs\"]\n  }\n]\n</code></pre> <p>Source: src/cook/execution/mapreduce/resources/git_operations.rs:280-293</p>"},{"location":"variables/available-variables/#file-object-structure","title":"File Object Structure","text":"<p>The <code>${merge.modified_files}</code> variable contains an array of file modification objects:</p> <pre><code>[\n  {\n    \"path\": \"src/main.rs\",\n    \"modification_type\": \"Modified\",\n    \"size_before\": 1024,\n    \"size_after\": 1156,\n    \"last_modified\": \"2025-01-10T12:00:00Z\",\n    \"commit_id\": \"abc1234\"\n  }\n]\n</code></pre> <p>Source: src/cook/execution/mapreduce/resources/git_operations.rs:311-322</p>"},{"location":"variables/available-variables/#validation-variables","title":"Validation Variables","text":"<p>Variables for workflow validation and completion tracking:</p> Variable Description Example <code>${validation.completion}</code> Completion percentage (0-100) <code>echo \"${validation.completion}%\"</code> <code>${validation.gaps}</code> Array of missing requirements <code>echo '${validation.gaps}'</code> <code>${validation.status}</code> Status: complete/incomplete/failed <code>if [ \"${validation.status}\" = \"complete\" ]</code> <p>Available in: Goal seek and validation phases</p>"},{"location":"variables/available-variables/#variable-interpolation-syntax","title":"Variable Interpolation Syntax","text":"<p>Prodigy supports two interpolation syntaxes:</p> <ul> <li><code>${VAR}</code> - Preferred syntax, works in all contexts (recommended)</li> <li><code>$VAR</code> - Shell-style syntax, simpler but may have limitations</li> </ul> <p>When to use <code>${VAR}</code>: - In YAML values with special characters - For nested field access: <code>${item.nested.field}</code> - When combining with text: <code>prefix_${var}_suffix</code> - For format modifiers: <code>${step.files:json}</code></p> <p>When <code>$VAR</code> works: - Simple variable names in shell commands - Environment variables in shell context - Quick substitutions without special characters</p> <p>Best Practice: Always use <code>${VAR}</code> syntax for consistency and reliability.</p>"},{"location":"variables/available-variables/#legacy-variable-aliases","title":"Legacy Variable Aliases","text":"<p>For backward compatibility, Prodigy supports legacy variable aliases from earlier versions. These are still functional but deprecated - prefer the current variable names in new workflows.</p> <p>Source: src/cook/workflow/variables.rs (legacy alias definitions)</p> Legacy Alias Current Variable Context Status <code>${ARG}</code> <code>${item.value}</code> Map phase Deprecated <code>${ARGUMENT}</code> <code>${item.value}</code> Map phase Deprecated <code>${FILE}</code> <code>${item.path}</code> Map phase Deprecated <code>${FILE_PATH}</code> <code>${item.path}</code> Map phase Deprecated <p>Example: <pre><code># Old style (still works but discouraged)\nmap:\n  agent_template:\n    - shell: \"process ${ARG}\"\n    - shell: \"cat ${FILE}\"\n\n# New style (recommended)\nmap:\n  agent_template:\n    - shell: \"process ${item.value}\"\n    - shell: \"cat ${item.path}\"\n</code></pre></p> <p>Migration Recommendation: Update legacy aliases to current variable names when maintaining older workflows. The current names are more explicit and work better with arbitrary JSON field access.</p>"},{"location":"variables/available-variables/#default-values","title":"Default Values","text":"<p>Provide fallback values for undefined or missing variables using the <code>:-</code> syntax (bash/shell convention):</p> <p>Syntax: <code>${variable:-default_value}</code></p> <p>Source: src/cook/execution/interpolation.rs:277</p> <p>Examples: <pre><code># Use default if variable is undefined\n- shell: \"echo 'Timeout: ${timeout:-600}'\"\n  # Output: \"Timeout: 600\" if timeout is not defined\n\n# Fallback for optional configuration\n- shell: \"cargo build --profile ${build_profile:-dev}\"\n  # Uses \"dev\" profile if build_profile not set\n\n# Default for MapReduce variables\n- shell: \"echo 'Processed ${map.successful:-0} items'\"\n  # Shows \"0\" if map.successful is not available\n</code></pre></p> <p>Behavior with Interpolation Modes: - Non-strict mode (default): Uses default value if variable is undefined - Strict mode: Default value syntax prevents errors for optional variables</p>"},{"location":"variables/available-variables/#interpolation-modes","title":"Interpolation Modes","text":"<p>Prodigy supports two modes for handling undefined variables:</p> <p>Non-strict Mode (Default): - Leaves placeholders unresolved when variable is undefined - Example: <code>${undefined}</code> remains as <code>${undefined}</code> in output - With default: <code>${undefined:-fallback}</code> becomes <code>fallback</code> - Use case: Workflows that can handle partial variable resolution</p> <p>Strict Mode: - Fails immediately on undefined variables - Example: <code>${undefined}</code> causes workflow to fail with comprehensive error - Error message lists all available variables for debugging - Use case: Production workflows requiring all variables to be properly defined</p> <p>Source: src/cook/execution/interpolation.rs:16-17, 104-137</p> <p>Configuration: Strict mode is configured per InterpolationEngine instance and controlled at the workflow execution level.</p> <p>Best Practice: Use strict mode during development to catch variable name typos and scope issues early. Use default values (<code>${var:-default}</code>) for truly optional configuration.</p> <p>Examples: <pre><code># Non-strict mode (graceful degradation)\n- shell: \"echo 'Config: ${optional_config:-none}'\"\n  # Works even if optional_config is undefined\n\n# Strict mode (fail fast)\n# If required_var is undefined, workflow stops with error:\n# \"Variable interpolation failed: required_var not found.\n#  Available variables: workflow.name, workflow.id, step.index, ...\"\n- shell: \"echo 'Required: ${required_var}'\"\n</code></pre></p>"},{"location":"variables/available-variables/#variable-scoping-and-precedence","title":"Variable Scoping and Precedence","text":""},{"location":"variables/available-variables/#scope-by-phase","title":"Scope by Phase","text":"Phase Variables Available Setup Standard, workflow context, step context, git context, custom captured Map Standard, workflow context, step context, git context, item variables, custom captured Reduce Standard, workflow context, step context, git context, MapReduce variables, custom captured Merge Standard, workflow context, step context, merge variables, custom captured <p>Important: Setup phase captures are available in map and reduce phases. Map phase captures are only available within that specific agent. Reduce phase captures are available to subsequent reduce steps.</p>"},{"location":"variables/available-variables/#variable-precedence-highest-to-lowest","title":"Variable Precedence (highest to lowest)","text":"<ol> <li>Custom captured variables (<code>capture_output</code>)</li> <li>Phase-specific built-in variables (<code>item.*</code>, <code>map.*</code>, <code>merge.*</code>)</li> <li>Step context variables (<code>step.*</code>)</li> <li>Workflow context variables (<code>workflow.*</code>)</li> <li>Standard output variables (<code>last.output</code>, <code>shell.output</code>)</li> <li>Environment variables (static workflow-level <code>env</code> block)</li> <li>Computed variables (<code>env.*</code>, <code>file:*</code>, <code>cmd:*</code>, <code>json:*</code>, <code>date:*</code>, <code>uuid</code>)</li> </ol> <p>Note: Computed variables have lowest precedence because they're evaluated on-demand. If a custom variable has the same name as a computed variable, the custom variable wins.</p> <p>Shadowing Warning: Custom captures can shadow built-in variable names. Avoid using names like <code>item</code>, <code>map</code>, <code>workflow</code>, etc. as custom variable names.</p> <p>Example: <pre><code># Bad: shadows built-in ${item}\n- shell: \"custom command\"\n  capture_output: \"item\"  # Don't do this!\n\n# Good: descriptive custom name\n- shell: \"custom command\"\n  capture_output: \"custom_result\"\n</code></pre></p>"},{"location":"variables/available-variables/#parent-context-resolution","title":"Parent Context Resolution","text":"<p>Variable resolution walks up a parent context chain when variables are not found in the current context. This enables variable inheritance across workflow phases and nested contexts.</p> <p>Source: src/cook/execution/interpolation.rs:200-226, InterpolationContext struct at :376-381</p> <p>Resolution Order: 1. Check current context 2. If not found, check parent context 3. If not found in parent, check parent's parent 4. Continue until variable is found or no parent exists 5. If not found and has default value, use default 6. If not found in strict mode, fail with error listing available variables</p> <p>Benefits: - Nested workflow contexts inherit variables from parent workflows - Foreach loops access both loop-level and workflow-level variables - Map agents access setup phase variables - Reduce phase accesses both map results and setup variables</p> <p>Example: <pre><code>setup:\n  - shell: \"pwd\"\n    capture_output: \"workspace_root\"  # Available to all agents via parent context\n  - shell: \"git rev-parse HEAD\"\n    capture_output: \"base_commit\"     # Also inherited by map agents\n\nmap:\n  input: \"items.json\"\n  json_path: \"$.items[*]\"\n  agent_template:\n    - shell: \"echo 'processing ${item.name}'\"\n      capture_output: \"item_status\"  # Only in this agent's context\n    - shell: \"cd ${workspace_root}\"  # Resolved from parent (setup) context\n    - shell: \"git diff ${base_commit}\" # Also from parent context\n    - shell: \"echo 'Status: ${item_status}'\" # From current agent context\n\nreduce:\n  # Can access setup variables but NOT individual agent's item_status\n  - shell: \"cd ${workspace_root}\"  # From setup phase parent context\n  - shell: \"echo 'Base: ${base_commit}'\"  # Also from setup phase\n</code></pre></p> <p>Context Hierarchy: <pre><code>Setup Context (workspace_root, base_commit)\n    \u2193 parent\nMap Agent Context (item, item_status, workspace_root*, base_commit*)\n    \u2193 parent\nReduce Context (map.results, workspace_root*, base_commit*)\n</code></pre></p> <p>*Inherited from parent context</p>"},{"location":"variables/available-variables/#reduce-phase-access-to-item-data","title":"Reduce Phase Access to Item Data","text":"<p>In the reduce phase, individual item variables (<code>${item.*}</code>) are not directly available, but you can access all item data through <code>${map.results}</code>, which contains the aggregated results from all map agents.</p> <p>Examples: <pre><code>reduce:\n  # Count items with specific property\n  - shell: |\n      echo '${map.results}' | jq '[.[] | select(.type == \"error\")] | length'\n    capture_output: \"error_count\"\n\n  # Extract all file paths processed\n  - shell: |\n      echo '${map.results}' | jq -r '.[].item.path'\n    capture_output: \"all_paths\"\n\n  # Aggregate numeric field\n  - shell: |\n      echo '${map.results}' | jq '[.[].coverage] | add / length'\n    capture_output: \"avg_coverage\"\n\n  # Filter and transform results\n  - shell: |\n      echo '${map.results}' | jq '[.[] | select(.item.priority &gt; 5) | .item.name]'\n    capture_output: \"high_priority_items\"\n</code></pre></p>"},{"location":"variables/available-variables/#performance-template-caching","title":"Performance: Template Caching","text":"<p>Prodigy implements dual caching for optimal performance: template parsing cache and operation result cache.</p> <p>Source: src/cook/execution/interpolation.rs:18-19, 68-75; src/cook/execution/variables.rs:218-256</p>"},{"location":"variables/available-variables/#template-parse-caching","title":"Template Parse Caching","text":"<p>When the same variable template is used multiple times, the template is parsed once and reused:</p> <p>How It Works: - First use: Template is parsed and cached - Subsequent uses: Cached template is reused (no re-parsing) - Cache key: Exact template string - Automatic: No configuration needed</p> <p>Example: <pre><code># Template \"${item.path} --priority ${item.metadata.priority:-5}\"\n# is parsed once, then reused for all 1000 items\nmap:\n  input: \"items.json\"  # 1000 items\n  json_path: \"$.items[*]\"\n  agent_template:\n    - shell: \"process ${item.path} --priority ${item.metadata.priority:-5}\"\n</code></pre></p>"},{"location":"variables/available-variables/#computed-variable-caching_1","title":"Computed Variable Caching","text":"<p>Expensive computed operations (file reads, command execution) have separate result caching:</p> <p>Cached Operations: - <code>${env.VAR}</code> - Environment variable lookups - <code>${file:path}</code> - File system reads - <code>${cmd:command}</code> - Shell command execution</p> <p>Not Cached: - <code>${json:path:from:var}</code> - JSON parsing is fast - <code>${date:format}</code> - Values change over time - <code>${uuid}</code> - Must be unique</p> <p>Cache Details: - Type: LRU (Least Recently Used) cache - Size: 100 entries maximum - Scope: Per workflow execution - Thread Safety: Async RwLock protection</p> <p>Performance Impact: <pre><code># First shell command: Reads .commit-message.txt from disk\n- shell: \"git commit -m '${file:.commit-message.txt}'\"\n\n# Second shell command: Uses cached file content (no disk read)\n- shell: \"echo 'Message: ${file:.commit-message.txt}'\"\n\n# Third shell command: Still uses cache\n- shell: \"test -n '${file:.commit-message.txt}'\"\n</code></pre></p> <p>Benefits: - Faster interpolation for repeated templates (template cache) - Reduced I/O for repeated file reads (operation cache) - Lower CPU for repeated command execution (operation cache) - Reduced latency in MapReduce workflows</p> <p>When It Matters Most: - MapReduce workflows with many work items (&gt;100) - Workflows using the same computed variables repeatedly - High-frequency variable interpolation in loops - Templates with multiple variables and nested field access</p> <p>Note: All caching is transparent and automatic. You don't need any configuration to benefit from it. Both caches persist for the lifetime of the workflow execution.</p>"},{"location":"variables/custom-variable-capture/","title":"Custom Variable Capture","text":""},{"location":"variables/custom-variable-capture/#custom-variable-capture","title":"Custom Variable Capture","text":"<p>Custom capture variables allow you to save command output with explicit names for later use. This provides fine-grained control over what data is captured, how it's formatted, and when it's available to subsequent steps.</p>"},{"location":"variables/custom-variable-capture/#basic-syntax","title":"Basic Syntax","text":"<p>The <code>capture_output</code> field can be used with any command type (shell, claude, handler):</p> <pre><code># Option 1: No capture (default)\n- shell: \"cargo test\"\n  capture_output: false\n\n# Option 2: Boolean - use default variable name\n- shell: \"cargo test\"\n  capture_output: true  # Creates ${shell.output}\n\n# Option 3: Custom variable name (recommended)\n- shell: \"ls -la | wc -l\"\n  capture_output: \"file_count\"  # Creates ${file_count}\n</code></pre>"},{"location":"variables/custom-variable-capture/#default-variable-names","title":"Default Variable Names","text":"<p>When using <code>capture_output: true</code>, the variable name depends on the command type:</p> Command Type Default Variable Example <code>shell:</code> <code>${shell.output}</code> <code>echo ${shell.output}</code> <code>claude:</code> <code>${claude.output}</code> <code>echo ${claude.output}</code> <code>handler:</code> <code>${handler.output}</code> <code>echo ${handler.output}</code> <p>Recommendation: Use custom variable names for clarity and to avoid overwriting previous captures.</p>"},{"location":"variables/custom-variable-capture/#capture-formats","title":"Capture Formats","text":"<p>Control how output is parsed and structured using <code>capture_format</code>:</p> <pre><code># String format (default) - raw output\n- shell: \"cat readme.txt\"\n  capture_output: \"readme\"\n  capture_format: \"string\"  # default, can be omitted\n\n# JSON format - parse as JSON, access nested fields\n- shell: \"cargo metadata --format-version 1\"\n  capture_output: \"metadata\"\n  capture_format: \"json\"\n- shell: \"echo 'Package: ${metadata.packages[0].name}'\"\n\n# Lines format - split into array\n- shell: \"git log --oneline -5\"\n  capture_output: \"recent_commits\"\n  capture_format: \"lines\"\n\n# Number format - parse as numeric\n- shell: \"cargo test 2&gt;&amp;1 | grep -c 'test result: ok'\"\n  capture_output: \"passed_tests\"\n  capture_format: \"number\"\n\n# Boolean format - parse as boolean or use exit status\n- shell: \"cargo clippy -- -D warnings\"\n  capture_output: \"clippy_clean\"\n  capture_format: \"boolean\"\n</code></pre>"},{"location":"variables/custom-variable-capture/#metadata-fields","title":"Metadata Fields","text":"<p>Captured variables automatically include metadata fields:</p> Field Description Example <code>${var}</code> Primary output value <code>echo ${file_count}</code> <code>${var.stderr}</code> Standard error output <code>echo ${build.stderr}</code> <code>${var.exit_code}</code> Command exit code <code>if [ ${build.exit_code} -eq 0 ]</code> <code>${var.success}</code> Boolean success status <code>if ${build.success}</code> <code>${var.duration}</code> Execution time in seconds <code>echo \"Took ${build.duration}s\"</code> <p>Example: <pre><code>- shell: \"cargo build --release\"\n  capture_output: \"build\"\n- shell: |\n    if ${build.success}; then\n      echo \"Build succeeded in ${build.duration}s\"\n    else\n      echo \"Build failed: ${build.stderr}\"\n      exit ${build.exit_code}\n    fi\n</code></pre></p>"},{"location":"variables/custom-variable-capture/#capture-streams","title":"Capture Streams","text":"<p>Customize which streams and metadata are captured using <code>capture_streams</code>:</p> <pre><code>- shell: \"cargo test 2&gt;&amp;1\"\n  capture_output: \"test_output\"\n  capture_streams:\n    stdout: true      # default: true\n    stderr: true      # default: false (include stderr in output)\n    exit_code: true   # default: true\n    success: true     # default: true\n    duration: true    # default: true\n</code></pre> <p>Default Behavior Explained:</p> <p>Source: src/cook/workflow/variables.rs:269-291</p> <ul> <li><code>stdout: true</code> - Command output is captured (default)</li> <li><code>stderr: false</code> - Error output is NOT captured by default (prevents noise from warnings)</li> <li><code>exit_code: true</code> - Exit code is always captured</li> <li><code>success: true</code> - Boolean success status (exit_code == 0) is always captured</li> <li><code>duration: true</code> - Execution duration is always captured</li> </ul> <p>When stderr is false (default): - Error output is discarded - Only stdout is captured in the variable - Use this when stderr contains progress bars, warnings, or noise</p> <p>When stderr is true: - Both stdout and stderr are combined in the captured output - Useful for debugging failures or when errors contain useful information</p> <p>Common Patterns: <pre><code># Capture both stdout and stderr for debugging\n- shell: \"cargo build 2&gt;&amp;1\"\n  capture_output: \"build_output\"\n  capture_streams:\n    stdout: true\n    stderr: true  # Override default to capture errors\n\n# Only capture exit code for validation (no output)\n- shell: \"test -f config.toml\"\n  capture_output: \"config_exists\"\n  capture_streams:\n    stdout: false  # Don't capture output\n    stderr: false\n    exit_code: true  # Just check if it succeeded\n    success: true    # Get boolean result\n\n# Capture output but measure performance\n- shell: \"cargo test --lib\"\n  capture_output: \"test_results\"\n  capture_streams:\n    stdout: true\n    stderr: false\n    duration: true  # Track how long tests took\n</code></pre></p>"},{"location":"variables/custom-variable-capture/#variable-scoping","title":"Variable Scoping","text":"<p>Captured variables have different scopes depending on the workflow phase:</p> Phase Scope Available To Setup Workflow-wide All map agents and reduce steps Map Agent-local Only that specific agent's steps Reduce Step-forward Current and subsequent reduce steps Merge Merge-local Only merge phase commands <p>Important: Setup phase captures are the only way to share data across all map agents.</p> <p>Example: <pre><code>setup:\n  - shell: \"cargo metadata --format-version 1\"\n    capture_output: \"cargo_metadata\"\n    capture_format: \"json\"\n  - shell: \"echo '${cargo_metadata.workspace_root}'\"\n    capture_output: \"workspace_root\"\n\nmap:\n  input: \"packages.json\"\n  agent_template:\n    # ${workspace_root} and ${cargo_metadata} available here\n    - shell: \"cd ${workspace_root} &amp;&amp; cargo test -p ${item.name}\"\n      capture_output: \"test_result\"  # only available in this agent\n</code></pre></p>"},{"location":"variables/custom-variable-capture/#json-field-access","title":"JSON Field Access","text":"<p>When using <code>capture_format: \"json\"</code>, access nested fields with dot notation:</p> <pre><code>- shell: \"cargo metadata --format-version 1\"\n  capture_output: \"metadata\"\n  capture_format: \"json\"\n\n# Access nested fields\n- shell: \"echo 'Workspace: ${metadata.workspace_root}'\"\n- shell: \"echo 'Target: ${metadata.target_directory}'\"\n- shell: \"echo 'First package: ${metadata.packages[0].name}'\"\n- shell: \"echo 'Version: ${metadata.packages[0].version}'\"\n</code></pre> <p>Tip: Use <code>jq</code> to explore JSON structure first: <pre><code>- shell: \"cargo metadata --format-version 1 | jq -r '.packages[0] | keys'\"\n</code></pre></p>"},{"location":"variables/custom-variable-capture/#complete-examples","title":"Complete Examples","text":""},{"location":"variables/custom-variable-capture/#example-1-setup-phase-variables","title":"Example 1: Setup Phase Variables","text":"<pre><code>name: analyze-codebase\nmode: mapreduce\n\nsetup:\n  # Capture workspace info for all agents\n  - shell: \"cargo metadata --format-version 1\"\n    capture_output: \"metadata\"\n    capture_format: \"json\"\n\n  - shell: \"find . -name '*.rs' | wc -l\"\n    capture_output: \"total_rust_files\"\n    capture_format: \"number\"\n\n  - shell: \"git rev-parse --abbrev-ref HEAD\"\n    capture_output: \"current_branch\"\n\nmap:\n  input: \"packages.json\"\n  agent_template:\n    # All setup variables available here\n    - shell: \"echo 'Processing ${item.name} on ${current_branch}'\"\n    - shell: \"echo 'Workspace has ${total_rust_files} Rust files'\"\n</code></pre>"},{"location":"variables/custom-variable-capture/#example-2-map-phase-local-variables","title":"Example 2: Map Phase Local Variables","text":"<pre><code>map:\n  input: \"tests.json\"\n  agent_template:\n    - shell: \"cargo test ${item.test_name}\"\n      capture_output: \"test_result\"\n      capture_streams:\n        stdout: true\n        stderr: true\n\n    - shell: |\n        if ${test_result.success}; then\n          echo \"PASS: ${item.test_name}\"\n        else\n          echo \"FAIL: ${item.test_name}\"\n          echo \"${test_result.stderr}\"\n        fi\n</code></pre>"},{"location":"variables/custom-variable-capture/#example-3-reduce-phase-aggregation","title":"Example 3: Reduce Phase Aggregation","text":"<pre><code>reduce:\n  # Extract error count\n  - shell: \"echo '${map.results}' | jq '[.[] | select(.status == \\\"failed\\\")] | length'\"\n    capture_output: \"failure_count\"\n    capture_format: \"number\"\n\n  # Extract failed test names\n  - shell: \"echo '${map.results}' | jq -r '[.[] | select(.status == \\\"failed\\\") | .item.test_name] | join(\\\", \\\")'\"\n    capture_output: \"failed_tests\"\n\n  # Generate summary\n  - shell: |\n      echo \"Total: ${map.total}\"\n      echo \"Passed: ${map.successful}\"\n      echo \"Failed: ${failure_count}\"\n      if [ ${failure_count} -gt 0 ]; then\n        echo \"Failed tests: ${failed_tests}\"\n      fi\n</code></pre>"},{"location":"variables/custom-variable-capture/#example-4-conditional-execution-based-on-captures","title":"Example 4: Conditional Execution Based on Captures","text":"<pre><code>- shell: \"cargo clippy -- -D warnings 2&gt;&amp;1\"\n  capture_output: \"clippy\"\n  capture_format: \"boolean\"\n\n- shell: \"cargo fmt --check\"\n  capture_output: \"fmt\"\n  capture_format: \"boolean\"\n\n- shell: |\n    if ${clippy.success} &amp;&amp; ${fmt.success}; then\n      echo \"All checks passed!\"\n      exit 0\n    else\n      echo \"Checks failed:\"\n      [ ! ${clippy.success} ] &amp;&amp; echo \"  - clippy: ${clippy.stderr}\"\n      [ ! ${fmt.success} ] &amp;&amp; echo \"  - fmt: ${fmt.stderr}\"\n      exit 1\n    fi\n</code></pre>"},{"location":"variables/custom-variable-capture/#access-fields-directly","title":"Access fields directly","text":"<ul> <li>shell: \"cd ${metadata.workspace_root}\"</li> <li>shell: \"echo 'Package: ${metadata.packages[0].name}'\" <pre><code>**Use manual jq parsing when:**\n- You need complex filtering or transformations\n- You want to extract only specific fields (reduce memory)\n- The JSON structure varies or is deeply nested\n- You need to combine or reshape data\n- You want error handling for invalid JSON\n\n**Example:**\n```yaml\n# Complex transformation with jq\n- shell: |\n    cargo test --format json 2&gt;&amp;1 | \\\n      jq -s '[.[] | select(.type == \"test\")] |\n             group_by(.event) |\n             map({event: .[0].event, count: length})'\n  capture_output: \"test_summary\"\n  capture_format: \"json\"\n\n# Filter to specific data only\n- shell: |\n    cargo metadata --format-version 1 | \\\n      jq '.packages[] | select(.name == \"prodigy\") | {name, version, edition}'\n  capture_output: \"prodigy_info\"\n  capture_format: \"json\"\n</code></pre></li> </ul> <p>Combined Approach: <pre><code># Capture full JSON for direct field access\n- shell: \"cargo metadata --format-version 1\"\n  capture_output: \"metadata\"\n  capture_format: \"json\"\n\n# Access simple fields directly (fast, no jq needed)\n- shell: \"echo 'Workspace: ${metadata.workspace_root}'\"\n\n# Use jq for complex operations on the same data\n- shell: |\n    echo '${metadata}' | jq '.packages[] | select(.name == \"prodigy\")'\n  capture_output: \"prodigy_package\"\n  capture_format: \"json\"\n</code></pre></p> <p>Performance Considerations: - capture_format: json parses once, enables direct field access - Manual jq can filter early to reduce memory for large JSON - Combined approach gives both direct access and complex queries</p> <p>Source: Examples from book/src/variables/custom-variable-capture.md:143-161, 289-297</p>"},{"location":"variables/custom-variable-capture/#common-patterns","title":"Common Patterns","text":"<p>Pattern 1: Conditional Claude Command <pre><code>- shell: \"cargo test 2&gt;&amp;1\"\n  capture_output: \"tests\"\n\n- claude: \"/fix-failing-tests '${tests}'\"\n  condition: \"! ${tests.success}\"\n</code></pre></p> <p>Pattern 2: Counting and Statistics <pre><code>- shell: \"find . -name '*.rs' | wc -l\"\n  capture_output: \"rust_files\"\n  capture_format: \"number\"\n\n- shell: \"echo 'Found ${rust_files} Rust files'\"\n</code></pre></p> <p>Pattern 3: Passing Structured Data <pre><code>- shell: \"cargo metadata --format-version 1\"\n  capture_output: \"metadata\"\n  capture_format: \"json\"\n\n- claude: \"/analyze-dependencies '${metadata.packages:json}'\"\n</code></pre></p>"},{"location":"variables/see-also/","title":"See Also","text":""},{"location":"variables/see-also/#see-also","title":"See Also","text":""},{"location":"variables/see-also/#related-documentation","title":"Related Documentation","text":"<ul> <li>Environment Variables - Configure environment variables, secrets, and profiles for workflow parameterization</li> <li>Git Context Advanced - Deep dive into git variable formats, filtering, and advanced use cases</li> <li>MapReduce Workflows - Using item.* and map.* variables in distributed parallel processing</li> <li>Commands - Command-specific capture_output configuration and command types</li> <li>Examples - Real-world variable usage patterns and complete workflows</li> <li>Troubleshooting - Debugging workflows with variable-related issues</li> </ul>"},{"location":"variables/see-also/#key-concepts","title":"Key Concepts","text":"<ul> <li>Built-in vs Custom Variables: Built-in variables are automatic (workflow., item., etc.), while custom variables require explicit capture_output configuration</li> <li>Phase Availability: Variables have different availability depending on workflow phase (setup, map, reduce, merge)</li> <li>Variable Scoping: Setup captures are workflow-wide, map captures are agent-local, reduce captures are step-forward</li> <li>Format Modifiers: Git context variables support format modifiers (:json, :lines, :csv, :*.ext), while custom captures use capture_format</li> <li>Metadata Fields: All captured variables include .success, .exit_code, .stderr, and .duration fields. Goal-seeking commands also provide validation.completion, validation.gaps, and validation.status metadata</li> </ul>"},{"location":"variables/see-also/#external-resources","title":"External Resources","text":"<ul> <li>JSONPath Syntax - For json_path in MapReduce input extraction</li> <li>Glob Pattern Syntax - For git context filtering with <code>:*.ext</code> modifiers</li> <li>jq Manual - Essential tool for working with JSON variables in workflows</li> </ul>"},{"location":"variables/see-also/#quick-reference","title":"Quick Reference","text":"<p>Phase-Specific Variables: - Setup: All standard variables, custom captures (workflow-wide scope) - Map: item.*, item_index, item_total, worker.id + inherited setup captures - Reduce: map.total, map.successful, map.failed, map.results - Merge: merge.worktree, merge.source_branch, merge.target_branch</p> <p>Git Context Variables: - step.files_added, step.files_modified, step.files_deleted, step.files_changed - step.commits, step.commit_count, step.insertions, step.deletions - workflow.commits, workflow.commit_count - Format modifiers: :json, :lines (or :newline), :csv (or :comma), :*.ext</p> <p>Custom Capture: - capture_output: \"var_name\" - Creates ${var_name} - capture_format: \"json|string|lines|number|boolean\" - Metadata: ${var.success}, ${var.exit_code}, ${var.stderr}, ${var.duration}</p>"},{"location":"variables/troubleshooting-variable-interpolation/","title":"Troubleshooting Variable Interpolation","text":""},{"location":"variables/troubleshooting-variable-interpolation/#troubleshooting-variable-interpolation","title":"Troubleshooting Variable Interpolation","text":"<p>This guide helps you diagnose and fix common variable interpolation issues in Prodigy workflows.</p>"},{"location":"variables/troubleshooting-variable-interpolation/#issue-variables-not-interpolating","title":"Issue: Variables Not Interpolating","text":"<p>Symptom: Literal <code>${var}</code> or <code>$VAR</code> appears in output instead of the value.</p> <p>Common Causes:</p> <ol> <li> <p>Variable name typo or case mismatch <pre><code># Wrong\n- shell: \"echo ${Item.path}\"  # Should be lowercase: ${item.path}\n\n# Correct\n- shell: \"echo ${item.path}\"\n</code></pre></p> </li> <li> <p>Variable doesn't exist in current scope <pre><code># Wrong - using item variable in reduce phase\nreduce:\n  - shell: \"echo ${item.name}\"  # ${item.*} not available here!\n\n# Correct - access via map.results\nreduce:\n  - shell: \"echo '${map.results}' | jq -r '.[].item.name'\"\n</code></pre></p> </li> <li> <p>Command failed before setting variable <pre><code>- shell: \"cargo test\"  # If this fails...\n  capture_output: \"tests\"\n- shell: \"echo ${tests}\"  # ...this might be empty\n\n# Better: Check success first\n- shell: |\n    if ${tests.success}; then\n      echo \"Tests: ${tests}\"\n    else\n      echo \"Tests failed: ${tests.stderr}\"\n    fi\n</code></pre></p> </li> </ol> <p>Solutions: - Check spelling and case sensitivity - Verify variable exists in current phase (see phase availability table) - Use verbose mode (<code>-v</code>) to see variable values during execution - Echo variables to debug: <code>shell: \"echo 'DEBUG: var=${my_var}'\"</code></p>"},{"location":"variables/troubleshooting-variable-interpolation/#issue-variable-empty-or-undefined","title":"Issue: Variable Empty or Undefined","text":"<p>Symptom: Variable interpolates but contains empty string or null.</p> <p>Common Causes:</p> <ol> <li> <p>Variable used before being set <pre><code># Wrong - using before capture\n- shell: \"echo ${count}\"\n- shell: \"wc -l file.txt\"\n  capture_output: \"count\"\n\n# Correct - capture first, use second\n- shell: \"wc -l file.txt\"\n  capture_output: \"count\"\n- shell: \"echo ${count}\"\n</code></pre></p> </li> <li> <p>Command produced no output <pre><code>- shell: \"find . -name 'nonexistent.txt'\"\n  capture_output: \"result\"  # Will be empty if no matches\n- shell: \"echo ${result}\"  # Empty string\n</code></pre></p> </li> <li> <p>Capture not configured <pre><code># Wrong - forgot capture_output\n- shell: \"cargo --version\"\n- shell: \"echo ${shell.output}\"  # Empty unless capture_output: true\n\n# Correct\n- shell: \"cargo --version\"\n  capture_output: true  # or capture_output: \"cargo_version\"\n</code></pre></p> </li> </ol> <p>Solutions: - Ensure capture_output is set when you need to save output - Check command actually produces output - Use verbose mode to see when variables are set - Provide defaults: <code>${var:-default_value}</code> (shell syntax)</p>"},{"location":"variables/troubleshooting-variable-interpolation/#issue-phase-specific-variable-not-available","title":"Issue: Phase-Specific Variable Not Available","text":"<p>Symptom: Error about undefined variable or empty value when using phase-specific variables.</p> <p>Common Causes:</p> Variable Wrong Phase Correct Phase Fix <code>${item.*}</code> Reduce, Setup, Merge Map only Use <code>${map.results}</code> in reduce <code>${map.*}</code> Setup, Map, Merge Reduce only Move logic to reduce phase <code>${merge.*}</code> Setup, Map, Reduce Merge only Only use in merge commands <p>Example Problem: <pre><code># Wrong - can't use ${item} in reduce\nreduce:\n  - shell: \"process ${item.name}\"  # ERROR!\n\n# Correct - iterate through map.results\nreduce:\n  - shell: \"echo '${map.results}' | jq -r '.[] | .item.name' | while read name; do process \\\"$name\\\"; done\"\n</code></pre></p> <p>Solutions: - Review phase availability table in main Variables documentation - Move variable usage to appropriate phase - In reduce, access item data through <code>${map.results}</code> - Restructure workflow if necessary</p>"},{"location":"variables/troubleshooting-variable-interpolation/#issue-nested-field-access-fails","title":"Issue: Nested Field Access Fails","text":"<p>Symptom: Can't access nested JSON fields like <code>${var.field.nested}</code>.</p> <p>Common Causes:</p> <ol> <li> <p>Format not specified as JSON <pre><code># Wrong - no format specification\n- shell: \"cargo metadata --format-version 1\"\n  capture_output: \"metadata\"\n- shell: \"echo ${metadata.workspace_root}\"  # Won't work!\n\n# Correct - specify JSON format\n- shell: \"cargo metadata --format-version 1\"\n  capture_output: \"metadata\"\n  capture_format: \"json\"\n- shell: \"echo ${metadata.workspace_root}\"  # Works!\n</code></pre></p> </li> <li> <p>Field doesn't exist in JSON <pre><code>- shell: \"echo ${item.nonexistent_field}\"  # Empty if field missing\n</code></pre></p> </li> <li> <p>JSON is invalid <pre><code># Command produces malformed JSON\n- shell: \"echo '{incomplete json'\"\n  capture_output: \"data\"\n  capture_format: \"json\"  # Will fail to parse\n</code></pre></p> </li> </ol> <p>Solutions: - Always use <code>capture_format: \"json\"</code> for JSON output - Verify JSON structure with <code>jq</code>: <code>echo '${var}' | jq .</code> - Check field exists: <code>echo '${var}' | jq -r '.field // \"default\"'</code> - Validate JSON before capture</p>"},{"location":"variables/troubleshooting-variable-interpolation/#issue-git-context-variables-empty","title":"Issue: Git Context Variables Empty","text":"<p>Symptom: Git variables like <code>${step.files_added}</code> are empty.</p> <p>Common Causes:</p> <ol> <li> <p>No commits created <pre><code>- shell: \"echo 'hello' &gt; file.txt\"\n- shell: \"echo ${step.files_added}\"  # Empty - no commit yet!\n\n# Correct - ensure command creates commit\n- shell: \"echo 'hello' &gt; file.txt\"\n  commit_required: true  # Forces commit\n- shell: \"echo ${step.files_added}\"  # Now has value\n</code></pre></p> </li> <li> <p>Not in a git repository <pre><code># Git variables require git repo\n- shell: \"echo ${step.files_added}\"  # Empty if not in git repo\n</code></pre></p> </li> <li> <p>No files changed in step <pre><code>- shell: \"cargo check\"  # Doesn't modify files\n- shell: \"echo ${step.files_added}\"  # Empty - no files added\n</code></pre></p> </li> </ol> <p>Solutions: - Ensure commands that modify files use <code>commit_required: true</code> - Verify you're in a git repository - Check that commands actually modify files - Use <code>git status</code> to verify changes exist</p>"},{"location":"variables/troubleshooting-variable-interpolation/#issue-format-modifiers-not-working","title":"Issue: Format Modifiers Not Working","text":"<p>Symptom: Format modifiers like <code>:json</code> or <code>:*.rs</code> don't apply.</p> <p>Common Causes:</p> <ol> <li> <p>Wrong variable type <pre><code># Wrong - not a git context variable\n- shell: \"ls\"\n  capture_output: \"files\"\n- shell: \"echo ${files:json}\"  # Format modifiers only work on git vars!\n\n# Correct - use capture_format instead\n- shell: \"ls\"\n  capture_output: \"files\"\n  capture_format: \"json\"\n</code></pre></p> </li> <li> <p>Syntax error <pre><code># Wrong syntax\n- shell: \"echo ${step.files_added:.rs}\"  # Missing * in glob\n\n# Correct\n- shell: \"echo ${step.files_added:*.rs}\"\n</code></pre></p> </li> </ol> <p>Solutions: - Format modifiers (<code>:json</code>, <code>:lines</code>, <code>:csv</code>, <code>:*.ext</code>) only work on git context variables - For custom captures, use <code>capture_format</code> instead - Check glob pattern syntax</p>"},{"location":"variables/troubleshooting-variable-interpolation/#debugging-techniques","title":"Debugging Techniques","text":""},{"location":"variables/troubleshooting-variable-interpolation/#1-use-verbose-mode","title":"1. Use Verbose Mode","text":"<pre><code># Run with verbose flag to see variable resolution\nprodigy run workflow.yml -v\n</code></pre> <p>Verbose mode shows: - Variable values at each step - When variables are captured - Interpolated command strings before execution</p>"},{"location":"variables/troubleshooting-variable-interpolation/#2-echo-variables-for-debugging","title":"2. Echo Variables for Debugging","text":"<pre><code>- shell: \"echo 'DEBUG: item=${item}'\"\n- shell: \"echo 'DEBUG: item.path=${item.path}'\"\n- shell: \"echo 'DEBUG: item_index=${item_index}'\"\n</code></pre>"},{"location":"variables/troubleshooting-variable-interpolation/#3-check-claude-json-logs","title":"3. Check Claude JSON Logs","text":"<p>Claude command logs contain variable interpolation details:</p> <pre><code># View most recent Claude command log\ncat ~/.claude/projects/*/latest.jsonl | jq -c 'select(.type == \"assistant\")'\n</code></pre>"},{"location":"variables/troubleshooting-variable-interpolation/#4-verify-variables-in-checkpoint-files","title":"4. Verify Variables in Checkpoint Files","text":"<p>For resume issues, check checkpoint files:</p> <pre><code># View checkpoint variables\ncat ~/.prodigy/state/*/checkpoints/latest.json | jq .variables\n</code></pre>"},{"location":"variables/troubleshooting-variable-interpolation/#5-use-jq-to-explore-json-variables","title":"5. Use jq to Explore JSON Variables","text":"<pre><code># Explore structure\n- shell: \"echo '${map.results}' | jq .\"\n\n# List available keys\n- shell: \"echo '${metadata}' | jq 'keys'\"\n\n# Pretty print\n- shell: \"echo '${item}' | jq -C .\"\n</code></pre>"},{"location":"variables/troubleshooting-variable-interpolation/#common-syntax-issues","title":"Common Syntax Issues","text":""},{"location":"variables/troubleshooting-variable-interpolation/#issue-special-characters-in-variables","title":"Issue: Special Characters in Variables","text":"<p>Problem: <pre><code># Variable contains spaces or special chars\n- shell: echo ${item.name}  # Breaks if name has spaces!\n</code></pre></p> <p>Solution: <pre><code># Always quote variables in shell commands\n- shell: \"echo \\\"${item.name}\\\"\"\n</code></pre></p>"},{"location":"variables/troubleshooting-variable-interpolation/#issue-yaml-string-escaping","title":"Issue: YAML String Escaping","text":"<p>Problem: <pre><code># Single quotes prevent interpolation\n- shell: 'echo ${item.name}'  # Literal ${item.name} printed!\n</code></pre></p> <p>Solution: <pre><code># Use double quotes for interpolation\n- shell: \"echo ${item.name}\"\n</code></pre></p>"},{"location":"variables/troubleshooting-variable-interpolation/#issue-combining-variables-with-text","title":"Issue: Combining Variables with Text","text":"<p>Problem: <pre><code># Ambiguous variable name\n- shell: \"echo $item_path\"  # Is it ${item_path} or ${item}_path?\n</code></pre></p> <p>Solution: <pre><code># Use ${} syntax to clarify boundaries\n- shell: \"echo ${item}_path\"\n- shell: \"echo prefix_${item}_suffix\"\n</code></pre></p>"},{"location":"variables/troubleshooting-variable-interpolation/#best-practices-for-avoiding-issues","title":"Best Practices for Avoiding Issues","text":"<ol> <li>Always use <code>${VAR}</code> syntax - More reliable than <code>$VAR</code></li> <li>Check phase availability - Review phase table before using variables</li> <li>Quote shell variables - Use <code>\"${var}\"</code> in shell commands</li> <li>Capture before use - Set <code>capture_output</code> before referencing</li> <li>Specify JSON format - Use <code>capture_format: \"json\"</code> for structured data</li> <li>Use verbose mode - Debug with <code>-v</code> flag</li> <li>Validate JSON - Test with <code>jq</code> before using in workflow</li> <li>Document assumptions - Comment expected variable structure</li> <li>Provide fallbacks - Handle empty variables gracefully</li> <li>Test incrementally - Add variables one at a time</li> </ol>"},{"location":"variables/troubleshooting-variable-interpolation/#getting-help","title":"Getting Help","text":"<p>If you're still stuck after trying these debugging techniques:</p> <ol> <li>Check logs: Review Claude JSON logs for variable resolution</li> <li>Inspect checkpoints: Look at stored variable values</li> <li>Simplify workflow: Remove complexity to isolate issue</li> <li>Review examples: Check working examples in documentation</li> <li>Verify phase: Double-check variable is available in current phase</li> </ol> <p>See Also: - Available Variables - Full variable reference with phase availability - Custom Variable Capture - Capture configuration and formats - Examples - Working examples of variable usage</p>"},{"location":"workflow-basics/","title":"Workflow Basics","text":"<p>This chapter covers the fundamentals of creating Prodigy workflows. You'll learn about workflow structure, basic commands, and configuration options.</p>"},{"location":"workflow-basics/#overview","title":"Overview","text":"<p>Prodigy workflows are YAML files that define a sequence of commands to execute. They can be as simple as a list of shell commands or as complex as parallel MapReduce jobs.</p> <p>Two Main Workflow Types: - Standard Workflows: Sequential command execution (covered here) - MapReduce Workflows: Parallel processing with map/reduce phases (see MapReduce chapter)</p>"},{"location":"workflow-basics/#simple-workflows","title":"Simple Workflows","text":"<p>The simplest workflow is just an array of commands:</p> <pre><code># Simple array format - just list your commands\n- shell: \"echo 'Starting workflow...'\"\n- claude: \"/prodigy-analyze\"\n- shell: \"cargo test\"\n</code></pre> <p>This executes each command sequentially. No additional configuration needed.</p>"},{"location":"workflow-basics/#additional-topics","title":"Additional Topics","text":"<p>See also: - Full Workflow Structure - Available Fields - Command Types - Command-Level Options - Environment Configuration - Merge Workflows - Complete Example - Next Steps</p>"},{"location":"workflow-basics/available-fields/","title":"Available Fields","text":""},{"location":"workflow-basics/available-fields/#available-fields","title":"Available Fields","text":"<p>Standard workflows support these top-level fields:</p> Field Type Required Description <code>name</code> String No Workflow name for identification (defaults to \"default\") <code>commands</code> Array Yes* List of commands to execute sequentially <code>env</code> Map No Global environment variables <code>secrets</code> Map No Secret environment variables (masked in logs) <code>env_files</code> Array No Paths to .env files to load <code>profiles</code> Map No Named environment profiles for different contexts <code>merge</code> MergeWorkflow No Custom merge workflow for worktree integration <p>Source: Type definitions from <code>src/config/workflow.rs:11-38</code></p> <p>Note: <code>commands</code> is only required in the full format. Use the simple array format for quick workflows without environment configuration. Use the full format when you need environment variables, profiles, or custom merge workflows.</p>"},{"location":"workflow-basics/available-fields/#field-relationships-and-precedence","title":"Field Relationships and Precedence","text":"<p>Understanding how fields interact is important for effective workflow configuration:</p> <ol> <li>Environment Variable Resolution Order:</li> <li>Command-level <code>env</code> overrides all other sources</li> <li>Profile-specific variables (when a profile is active) override global <code>env</code></li> <li>Global <code>env</code> variables provide base configuration</li> <li> <p>Variables from <code>env_files</code> are loaded first and can be overridden</p> </li> <li> <p>Secrets vs. Environment Variables:</p> </li> <li><code>secrets</code> are a special type of environment variable that are masked in logs and output</li> <li>Both <code>env</code> and <code>secrets</code> are available to all commands</li> <li> <p>Secrets take precedence over regular environment variables with the same name</p> </li> <li> <p>Profile Activation:</p> </li> <li>Profiles are activated via <code>--profile &lt;name&gt;</code> CLI flag</li> <li>Profile variables merge with global <code>env</code>, with profile values taking precedence</li> <li>Common use case: different configurations for dev, staging, and production</li> </ol>"},{"location":"workflow-basics/available-fields/#format-examples","title":"Format Examples","text":"<p>Simple Array Format (from <code>examples/standard-workflow.yml</code>):</p> <pre><code>- shell: echo \"Starting code analysis...\"\n- shell: cargo check --quiet\n- shell: echo \"Workflow complete\"\n</code></pre> <p>Use this format when: - You don't need environment variables - You have a quick, straightforward sequence of commands - You want minimal YAML verbosity</p> <p>Full Format with Environment (from <code>workflows/environment-example.yml</code>):</p> <pre><code>name: production-deploy\n\nenv:\n  NODE_ENV: production\n  API_URL: https://api.example.com\n\nsecrets:\n  API_KEY: \"${env:SECRET_API_KEY}\"\n\nenv_files:\n  - .env.production\n\nprofiles:\n  development:\n    NODE_ENV: development\n    API_URL: http://localhost:3000\n  staging:\n    NODE_ENV: staging\n    API_URL: https://staging.api.example.com\n\ncommands:\n  - shell: echo \"Deploying with NODE_ENV=$NODE_ENV\"\n  - shell: npm run build\n  - shell: npm run deploy\n</code></pre> <p>Use this format when: - You need environment variables - You have different configurations for different environments (profiles) - You need to mask sensitive values (secrets) - You want to load variables from <code>.env</code> files</p>"},{"location":"workflow-basics/available-fields/#see-also","title":"See Also","text":"<ul> <li>Environment Configuration - Detailed environment variable documentation</li> <li>Merge Workflows - Custom merge workflow configuration</li> <li>Command Types - Available command types and their options</li> <li>Full Workflow Structure - Complete workflow structure reference</li> </ul>"},{"location":"workflow-basics/command-level-options/","title":"Command-Level Options","text":""},{"location":"workflow-basics/command-level-options/#command-level-options","title":"Command-Level Options","text":"<p>All command types (<code>claude:</code>, <code>shell:</code>, <code>goal_seek:</code>, <code>foreach:</code>) support additional fields for advanced control and orchestration. These options enable timeout management, output capture, error handling, conditional execution, and more.</p>"},{"location":"workflow-basics/command-level-options/#core-options","title":"Core Options","text":""},{"location":"workflow-basics/command-level-options/#timeout","title":"timeout","text":"<p>Sets a maximum execution time for the command (in seconds). If the command exceeds this duration, it will be terminated.</p> <p>Type: <code>Option&lt;u64&gt;</code> (optional, no default timeout)</p> <p>Source: <code>src/config/command.rs:383</code></p> <pre><code>commands:\n  # 5 minute timeout for test suite\n  - shell: \"npm test\"\n    timeout: 300\n\n  # 10 minute timeout for Claude implementation\n  - claude: \"/implement feature\"\n    timeout: 600\n\n  # No timeout (runs until completion)\n  - shell: \"cargo build --release\"\n</code></pre> <p>Real-world examples: - <code>workflows/debtmap-reduce.yml:6</code> - 15 minute timeout for coverage generation - <code>workflows/complex-build-pipeline.yml:23</code> - 10 minute timeout for benchmarks - <code>workflows/documentation-drift.yml:48</code> - 5 minute timeout for doc tests</p>"},{"location":"workflow-basics/command-level-options/#id","title":"id","text":"<p>Assigns an identifier to the command for referencing its outputs in subsequent commands via workflow variables.</p> <p>Type: <code>Option&lt;String&gt;</code> (optional)</p> <p>Source: <code>src/config/command.rs:351-352</code></p> <pre><code>commands:\n  - shell: \"git rev-parse --short HEAD\"\n    id: \"get_commit\"\n    capture_output: \"commit_hash\"\n\n  # Use the captured output\n  - shell: \"echo 'Building commit ${commit_hash}'\"\n</code></pre>"},{"location":"workflow-basics/command-level-options/#commit_required","title":"commit_required","text":"<p>Specifies whether the command is expected to create git commits. Prodigy tracks commits for workflow provenance and rollback.</p> <p>Type: <code>bool</code> (default: <code>false</code>)</p> <p>Source: <code>src/config/command.rs:354-356</code></p> <pre><code>commands:\n  # Claude commands that modify code should commit\n  - claude: \"/prodigy-coverage\"\n    commit_required: true\n\n  # Test commands typically don't commit\n  - shell: \"cargo test\"\n    commit_required: false\n\n  # Linting fixes may commit changes\n  - claude: \"/prodigy-lint\"\n    commit_required: true\n</code></pre> <p>Real-world examples: - <code>workflows/coverage.yml:5,11</code> - Coverage and implementation commands - <code>workflows/documentation-drift.yml:19,23,27</code> - Documentation update commands - <code>workflows/implement-with-tests.yml:27,31</code> - Test vs implementation distinction</p>"},{"location":"workflow-basics/command-level-options/#output-capture-options","title":"Output Capture Options","text":""},{"location":"workflow-basics/command-level-options/#capture_output","title":"capture_output","text":"<p>Captures command output and stores it in a workflow variable. Supports both boolean mode (captures to default variable) and variable name mode (captures to named variable).</p> <p>Type: <code>Option&lt;CaptureOutputConfig&gt;</code> where <code>CaptureOutputConfig</code> is: <pre><code>enum CaptureOutputConfig {\n    Boolean(bool),      // Simple capture (true/false)\n    Variable(String),   // Capture to named variable\n}\n</code></pre></p> <p>Source: <code>src/config/command.rs:367-368, 403-411</code></p> <pre><code>commands:\n  # Capture to default variable name (shell.output)\n  - shell: \"echo 'Starting analysis...'\"\n    capture_output: true\n\n  # Capture to custom variable name\n  - shell: \"ls -la | wc -l\"\n    capture_output: \"file_count\"\n\n  # Use the captured variable\n  - shell: \"echo 'Found ${file_count} files'\"\n\n  # Disable capture explicitly\n  - shell: \"cargo build\"\n    capture_output: false\n</code></pre> <p>Real-world examples: - <code>examples/capture-output-custom-vars.yml:10-48</code> - Custom variable names - <code>workflows/implement-with-tests.yml:26,55</code> - Test output capture - <code>workflows/complex-build-pipeline.yml:17,24</code> - Build diagnostics</p>"},{"location":"workflow-basics/command-level-options/#capture_format","title":"capture_format","text":"<p>Specifies how to parse captured output. Determines the data type and structure of the captured variable.</p> <p>Type: <code>Option&lt;String&gt;</code> with values: <code>string</code> (default), <code>json</code>, <code>lines</code>, <code>number</code>, <code>boolean</code></p> <p>Source: <code>src/config/command.rs:391-392</code>, <code>src/cook/workflow/variables.rs:250-265</code></p> <p>Supported formats: - <code>string</code> - Raw string output (default) - <code>json</code> - Parse as JSON object/array - <code>lines</code> - Split into array of lines - <code>number</code> - Parse as numeric value - <code>boolean</code> - Parse as boolean (<code>true</code>/<code>false</code>)</p> <pre><code>commands:\n  # Capture as JSON object\n  - shell: \"cat package.json\"\n    capture_output: \"package_info\"\n    capture_format: \"json\"\n\n  # Access JSON fields\n  - shell: \"echo 'Package: ${package_info.name} v${package_info.version}'\"\n\n  # Capture as boolean\n  - shell: \"cargo test --quiet &amp;&amp; echo true || echo false\"\n    capture_output: \"tests_passed\"\n    capture_format: \"boolean\"\n\n  # Capture as number\n  - shell: \"find src -name '*.rs' | wc -l\"\n    capture_output: \"file_count\"\n    capture_format: \"number\"\n\n  # Capture as array of lines\n  - shell: \"git diff --name-only\"\n    capture_output: \"changed_files\"\n    capture_format: \"lines\"\n</code></pre> <p>Real-world examples: - <code>examples/capture-json-processing.yml:9-54</code> - JSON metadata extraction - <code>examples/capture-conditional-flow.yml:9-37</code> - Boolean and number formats - <code>examples/capture-parallel-analysis.yml:9-101</code> - Multi-format capture pipeline</p>"},{"location":"workflow-basics/command-level-options/#capture_streams","title":"capture_streams","text":"<p>Controls which output streams to capture from command execution. By default, only stdout is captured.</p> <p>Type: <code>Option&lt;String&gt;</code> or structured configuration</p> <p>Source: <code>src/config/command.rs:394-396</code>, <code>src/cook/workflow/variables.rs:267-292</code></p> <p>Captured fields: - <code>stdout</code> - Standard output (default: <code>true</code>) - <code>stderr</code> - Standard error (default: <code>false</code>) - <code>exit_code</code> - Process exit code (default: <code>true</code>) - <code>success</code> - Whether command succeeded (default: <code>true</code>) - <code>duration</code> - Execution duration (default: <code>true</code>)</p> <pre><code>commands:\n  # Capture all streams and metadata\n  - shell: \"cargo build --release\"\n    capture_output: \"build_result\"\n    capture_streams:\n      stdout: true\n      stderr: true\n      exit_code: true\n      success: true\n      duration: true\n\n  # Access captured fields\n  - shell: |\n      echo \"Build Success: ${build_result.success}\"\n      echo \"Exit Code: ${build_result.exit_code}\"\n      echo \"Duration: ${build_result.duration}s\"\n</code></pre> <p>Real-world examples: - <code>examples/capture-conditional-flow.yml:44-51</code> - Multi-stream capture with conditionals</p>"},{"location":"workflow-basics/command-level-options/#output_file","title":"output_file","text":"<p>Redirects command output to a file. This option is defined in the type definition but not yet widely used in the codebase.</p> <p>Type: <code>Option&lt;String&gt;</code> (file path)</p> <p>Source: <code>src/config/command.rs:398-400</code></p> <pre><code>commands:\n  - shell: \"cargo test --verbose\"\n    output_file: \"test-results.txt\"\n\n  - shell: \"cargo doc --no-deps\"\n    output_file: \"docs/api-output.log\"\n</code></pre> <p>Note: This feature is defined but no production examples currently exist. Consider contributing examples if you use this option.</p>"},{"location":"workflow-basics/command-level-options/#error-handling","title":"Error Handling","text":""},{"location":"workflow-basics/command-level-options/#on_failure","title":"on_failure","text":"<p>Specifies commands to execute when the main command fails. Supports automatic retry with configurable attempts and workflow failure control.</p> <p>Type: <code>Option&lt;TestDebugConfig&gt;</code> with fields: - <code>claude: String</code> - Claude command to run on failure - <code>max_attempts: u32</code> - Maximum retry attempts (default: 3) - <code>fail_workflow: bool</code> - Whether to fail workflow after max attempts (default: <code>false</code>) - <code>commit_required: bool</code> - Whether debug command should commit (default: <code>true</code>)</p> <p>Source: <code>src/config/command.rs:370-372, 166-183</code></p> <pre><code>commands:\n  # Retry tests with automated fixing\n  - shell: \"cargo test\"\n    on_failure:\n      claude: \"/prodigy-debug-test-failure --output ${shell.output}\"\n      max_attempts: 3\n      fail_workflow: false\n\n  # Critical check that must pass\n  - shell: \"just fmt-check &amp;&amp; just lint\"\n    on_failure:\n      claude: \"/prodigy-lint ${shell.output}\"\n      max_attempts: 5\n      fail_workflow: true\n\n  # Doc test failures with commit requirement\n  - shell: \"cargo test --doc\"\n    on_failure:\n      claude: \"/prodigy-fix-doc-tests --output ${shell.output}\"\n      max_attempts: 2\n      fail_workflow: false\n      commit_required: true\n</code></pre> <p>Real-world examples: - <code>workflows/coverage-with-test-debug.yml:13-23</code> - Test debugging with retries - <code>workflows/debtmap-reduce.yml:58-70</code> - Critical quality gates - <code>workflows/documentation-drift.yml:47-53</code> - Doc test recovery</p>"},{"location":"workflow-basics/command-level-options/#on_success","title":"on_success","text":"<p>Executes another command when the main command succeeds. Enables chaining of dependent operations.</p> <p>Type: <code>Option&lt;Box&lt;WorkflowStepCommand&gt;&gt;</code> (nested command)</p> <p>Source: <code>src/config/command.rs:374-376</code></p> <pre><code>commands:\n  # Chain successful operations\n  - shell: \"cargo check\"\n    on_success:\n      shell: \"cargo build --release\"\n      on_success:\n        shell: \"cargo test --release\"\n\n  # Nested success/failure handlers\n  - shell: \"cargo test\"\n    capture_output: \"test_output\"\n    on_failure:\n      claude: \"/prodigy-debug-test-failures '${test_output}'\"\n      commit_required: true\n      on_success:\n        # After fixing, verify tests pass\n        shell: \"cargo test\"\n        on_failure:\n          claude: \"/prodigy-fix-test-failures '${shell.output}' --deep-analysis\"\n\n  # Success notification\n  - shell: \"cargo test --release\"\n    capture_output: \"final_results\"\n    on_success:\n      shell: \"echo '\u2705 All tests passing!'\"\n</code></pre> <p>Real-world examples: - <code>workflows/implement-with-tests.yml:28-40,61-63</code> - Nested test-fix-verify loops - <code>workflows/complex-build-pipeline.yml:7-13</code> - Build pipeline chaining</p>"},{"location":"workflow-basics/command-level-options/#conditional-execution","title":"Conditional Execution","text":""},{"location":"workflow-basics/command-level-options/#when","title":"when","text":"<p>Evaluates a boolean expression to determine whether to execute the command. Supports variable interpolation and boolean logic.</p> <p>Type: <code>Option&lt;String&gt;</code> (boolean expression)</p> <p>Source: <code>src/config/command.rs:387-388</code></p> <pre><code>commands:\n  # Capture test status\n  - shell: \"cargo test --quiet &amp;&amp; echo true || echo false\"\n    capture_output: \"tests_passed\"\n    capture_format: \"boolean\"\n\n  # Conditional execution based on test results\n  - shell: \"echo 'Running coverage analysis...'\"\n    when: \"${tests_passed}\"\n\n  # Multiple conditions\n  - shell: \"cargo build --release\"\n    when: \"${tests_passed} &amp;&amp; ${lint_passed}\"\n    capture_output: \"build_output\"\n\n  # Conditional deployment\n  - shell: |\n      if [ \"${tests_passed}\" = \"true\" ] &amp;&amp; [ \"${build_output.success}\" = \"true\" ]; then\n        echo \"\u2705 Deployment ready!\"\n      fi\n    when: \"${tests_passed}\"\n</code></pre> <p>Real-world examples: - <code>examples/capture-conditional-flow.yml:20-51</code> - Multi-stage conditional pipeline</p>"},{"location":"workflow-basics/command-level-options/#advanced-options","title":"Advanced Options","text":""},{"location":"workflow-basics/command-level-options/#validate","title":"validate","text":"<p>Configures implementation completeness validation with automatic gap detection and filling.</p> <p>Type: <code>Option&lt;ValidationConfig&gt;</code> (validation configuration)</p> <p>Source: <code>src/config/command.rs:378-380</code></p> <pre><code>commands:\n  - claude: \"/implement-feature\"\n    validate:\n      force_refresh: false\n      max_cache_age: 300\n</code></pre> <p>Note: See validation documentation for comprehensive coverage of this feature.</p>"},{"location":"workflow-basics/command-level-options/#analysis","title":"analysis","text":"<p>Specifies per-step analysis requirements for code quality and coverage tracking.</p> <p>Type: <code>Option&lt;AnalysisConfig&gt;</code> with fields: - <code>force_refresh: bool</code> - Force fresh analysis even if cached (default: <code>false</code>) - <code>max_cache_age: u64</code> - Maximum cache age in seconds (default: 300)</p> <p>Source: <code>src/config/command.rs:116-126</code></p> <pre><code>commands:\n  - claude: \"/implement-feature\"\n    analysis:\n      force_refresh: true\n      max_cache_age: 600\n</code></pre>"},{"location":"workflow-basics/command-level-options/#env","title":"env","text":"<p>Sets command-specific environment variables that override workflow-level and system environment variables.</p> <p>Type: <code>HashMap&lt;String, String&gt;</code> (key-value pairs)</p> <p>Source: <code>src/config/command.rs:143-145</code></p> <pre><code>commands:\n  - shell: \"cargo build\"\n    env:\n      RUST_BACKTRACE: \"1\"\n      CARGO_INCREMENTAL: \"0\"\n\n  - claude: \"/analyze-code\"\n    env:\n      DEBUG_MODE: \"true\"\n      LOG_LEVEL: \"trace\"\n</code></pre>"},{"location":"workflow-basics/command-level-options/#option-combinations","title":"Option Combinations","text":""},{"location":"workflow-basics/command-level-options/#test-fix-verify-pattern","title":"Test-Fix-Verify Pattern","text":"<p>Combines output capture, error handling, and conditionals for robust test workflows:</p> <pre><code>commands:\n  # Initial test run\n  - shell: \"cargo test\"\n    capture_output: \"test_output\"\n    commit_required: false\n    on_failure:\n      # Automated fix on failure\n      claude: \"/prodigy-debug-test-failures '${test_output}'\"\n      commit_required: true\n      max_attempts: 3\n      on_success:\n        # Verify fix worked\n        shell: \"cargo test\"\n        commit_required: false\n</code></pre>"},{"location":"workflow-basics/command-level-options/#conditional-pipeline-pattern","title":"Conditional Pipeline Pattern","text":"<p>Uses capture formats and conditionals for decision-making workflows:</p> <pre><code>commands:\n  # Capture test status as boolean\n  - shell: \"cargo test --quiet &amp;&amp; echo true || echo false\"\n    capture_output: \"tests_passed\"\n    capture_format: \"boolean\"\n\n  # Capture coverage as number\n  - shell: \"cargo tarpaulin --output-dir coverage | grep -oP '\\\\d+\\\\.\\\\d+(?=%)' | head -1\"\n    capture_output: \"coverage_percent\"\n    capture_format: \"number\"\n    when: \"${tests_passed}\"\n\n  # Deploy only if quality gates pass\n  - shell: \"echo 'Deploying to production...'\"\n    when: \"${tests_passed} &amp;&amp; ${coverage_percent} &gt;= 80\"\n</code></pre>"},{"location":"workflow-basics/command-level-options/#parallel-capture-pattern","title":"Parallel Capture Pattern","text":"<p>Captures multiple metrics in parallel for aggregation:</p> <pre><code>commands:\n  # Capture metadata\n  - shell: \"find src -name '*.rs' | wc -l\"\n    capture_output: \"total_files\"\n    capture_format: \"number\"\n\n  # Build summary JSON\n  - shell: |\n      echo '{\n        \"repository\": \"'$(basename $(pwd))'\",\n        \"total_files\": ${total_files},\n        \"timestamp\": \"'$(date -u +%Y-%m-%dT%H:%M:%SZ)'\"\n      }'\n    capture_output: \"metadata\"\n    capture_format: \"json\"\n</code></pre>"},{"location":"workflow-basics/command-level-options/#troubleshooting","title":"Troubleshooting","text":""},{"location":"workflow-basics/command-level-options/#command-times-out","title":"Command Times Out","text":"<p>Problem: Command exceeds timeout and is terminated</p> <p>Solutions: - Increase <code>timeout</code> value - Optimize command performance - Split into multiple smaller commands - Remove timeout for commands with unpredictable duration</p>"},{"location":"workflow-basics/command-level-options/#capture-format-mismatch","title":"Capture Format Mismatch","text":"<p>Problem: Error parsing captured output with specified format</p> <p>Solutions: - Verify command output matches expected format (use <code>echo</code> to inspect) - Add format validation to command output - Use <code>capture_format: \"string\"</code> as fallback - Check for extra whitespace or unexpected characters</p>"},{"location":"workflow-basics/command-level-options/#on_failure-not-triggering","title":"on_failure Not Triggering","text":"<p>Problem: Failure handler doesn't execute when command fails</p> <p>Solutions: - Verify command actually returns non-zero exit code - Check <code>max_attempts</code> hasn't been exceeded - Ensure <code>on_failure</code> syntax is correct (nested under command) - Review workflow logs for execution details</p>"},{"location":"workflow-basics/command-level-options/#variable-not-available","title":"Variable Not Available","text":"<p>Problem: <code>${variable}</code> not found or empty in subsequent command</p> <p>Solutions: - Verify <code>capture_output</code> is set with correct variable name - Check command actually produces output (not empty) - Ensure <code>capture_format</code> matches output type - Use default values: <code>${var|default:fallback}</code></p>"},{"location":"workflow-basics/command-level-options/#see-also","title":"See Also","text":"<ul> <li>Environment Configuration - Workflow-level environment configuration</li> <li>Timeout Configuration - Advanced timeout strategies</li> <li>Parallel Iteration with Foreach - Foreach command with parallel options</li> <li>Examples - Complete workflow examples</li> </ul>"},{"location":"workflow-basics/command-types/","title":"Command Types","text":""},{"location":"workflow-basics/command-types/#command-types","title":"Command Types","text":"<p>Prodigy supports several types of commands in workflows. Each command step must specify exactly one command type - they are mutually exclusive.</p>"},{"location":"workflow-basics/command-types/#quick-reference","title":"Quick Reference","text":"Command Type Primary Use Case Key Features <code>shell:</code> Execute shell commands Output capture, conditional execution, timeouts <code>claude:</code> Run Claude AI commands Variable interpolation, commit tracking, output declarations <code>analyze:</code> Codebase analysis with caching Cache control, force refresh, format options <code>goal_seek:</code> Iterative refinement Score-based validation, automatic retry, convergence detection <code>foreach:</code> Parallel iteration Process lists in parallel, item limits, error handling <code>write_file:</code> Create files Format validation (JSON/YAML), directory creation, permissions <code>validate:</code> Implementation validation Threshold checking, gap detection, multi-step validation <p>Command Exclusivity</p> <p>Each workflow step must use exactly one command type. You cannot combine <code>shell:</code> and <code>claude:</code> in the same step. Use <code>on_success:</code> or <code>on_failure:</code> to chain commands together.</p>"},{"location":"workflow-basics/command-types/#choosing-the-right-command-type","title":"Choosing the Right Command Type","text":"<pre><code>flowchart TD\n    Start{What are you&lt;br/&gt;trying to do?}\n\n    Start --&gt;|Run shell commands| Shell[shell:]\n    Start --&gt;|Execute Claude AI| Claude[claude:]\n    Start --&gt;|Analyze codebase| Analyze{Need fresh&lt;br/&gt;metrics?}\n    Start --&gt;|Improve until goal met| GoalSeek[goal_seek:]\n    Start --&gt;|Process multiple items| Foreach[foreach:]\n    Start --&gt;|Save data to file| WriteFile[write_file:]\n    Start --&gt;|Check completeness| Validate[validate:]\n\n    Analyze --&gt;|Yes, after changes| AnalyzeForce[analyze:&lt;br/&gt;force_refresh: true]\n    Analyze --&gt;|No, use cache| AnalyzeCache[analyze:&lt;br/&gt;max_cache_age: 300]\n\n    style Shell fill:#e1f5ff\n    style Claude fill:#f3e5f5\n    style AnalyzeForce fill:#fff3e0\n    style AnalyzeCache fill:#fff3e0\n    style GoalSeek fill:#e8f5e9\n    style Foreach fill:#fce4ec\n    style WriteFile fill:#f1f8e9\n    style Validate fill:#ede7f6</code></pre> <p>Figure: Decision tree for selecting the appropriate command type based on your workflow needs.</p>"},{"location":"workflow-basics/command-types/#shell-commands","title":"Shell Commands","text":"<p>Execute shell commands during workflow execution.</p> <p>Source: src/config/command.rs:328</p> <p>Syntax: <pre><code>- shell: \"command to execute\"\n  timeout: 300                    # (1)!\n  capture_output: true            # (2)!\n  capture_format: \"json\"          # (3)!\n  capture_streams: \"both\"         # (4)!\n  output_file: \"results.txt\"      # (5)!\n  on_failure:                     # (6)!\n    claude: \"/debug-failure\"\n  on_success:                     # (7)!\n    shell: \"echo 'Success!'\"\n  when: \"variable == 'value'\"     # (8)!\n</code></pre></p> <ol> <li>Maximum execution time in seconds before the command is terminated</li> <li>Capture command output to a variable (boolean or variable name)</li> <li>Format for captured output: <code>json</code>, <code>text</code>, or <code>lines</code></li> <li>Which streams to capture: <code>stdout</code>, <code>stderr</code>, or <code>both</code></li> <li>File path to redirect command output to</li> <li>Nested command to execute if the shell command fails (non-zero exit)</li> <li>Nested command to execute if the shell command succeeds (zero exit)</li> <li>Conditional expression - command only runs if the condition evaluates to true</li> </ol> <p>Fields: - <code>shell</code> (required): The shell command to execute - <code>timeout</code> (optional): Maximum execution time in seconds - <code>capture_output</code> (optional): Boolean or variable name to capture output to - <code>capture_format</code> (optional): Format for captured output (json, text, lines) - <code>capture_streams</code> (optional): Which streams to capture (stdout, stderr, both) - <code>output_file</code> (optional): File path to redirect output to - <code>on_failure</code> (optional): Nested command to execute if shell command fails - <code>on_success</code> (optional): Nested command to execute if shell command succeeds - <code>when</code> (optional): Conditional expression - command only runs if true</p> <p>Example (from workflows/fix-files-mapreduce.yml:36): <pre><code>- shell: \"cargo check --lib 2&gt;&amp;1 | grep -E '(error|warning)' | head -10 || echo 'No errors'\"\n  capture_output: true\n</code></pre></p> <p>Example with on_failure (from workflows/coverage.yml:13): <pre><code>- shell: \"just test\"\n  on_failure:\n    claude: \"/prodigy-debug-test-failure --spec ${coverage.spec} --output ${shell.output}\"\n</code></pre></p> <p>Common Use Cases</p> <ul> <li>Running build commands (<code>cargo build</code>, <code>npm install</code>)</li> <li>Executing tests (<code>cargo test</code>, <code>pytest</code>)</li> <li>File operations (<code>mkdir</code>, <code>cp</code>, <code>rm</code>)</li> <li>Running analysis tools (<code>cargo clippy</code>, <code>eslint</code>)</li> <li>Generating data files</li> </ul> <p>Error Handling Flow</p> <p>Shell commands support error handling via <code>on_failure</code> and <code>on_success</code> hooks. The flow looks like:</p> <pre><code>flowchart LR\n    Exec[Execute shell command] --&gt; Check{Exit code&lt;br/&gt;zero?}\n    Check --&gt;|Yes| Success[Run on_success&lt;br/&gt;if defined]\n    Check --&gt;|No| Failure[Run on_failure&lt;br/&gt;if defined]\n    Success --&gt; Next[Continue workflow]\n    Failure --&gt; Next\n\n    style Check fill:#fff3e0\n    style Success fill:#e8f5e9\n    style Failure fill:#ffebee</code></pre>"},{"location":"workflow-basics/command-types/#claude-commands","title":"Claude Commands","text":"<p>Execute Claude CLI commands via Claude Code.</p> <p>Source: src/config/command.rs:324</p> <p>Syntax: <pre><code>- claude: \"/command-name args\"\n  id: \"command_id\"                # (1)!\n  commit_required: true           # (2)!\n  timeout: 600                    # (3)!\n  outputs:                        # (4)!\n    spec:\n      file_pattern: \"*.md\"\n  when: \"condition\"               # (5)!\n  on_failure:                     # (6)!\n    claude: \"/fix-issue\"\n  on_success:                     # (7)!\n    shell: \"echo 'Done'\"\n</code></pre></p> <ol> <li>Unique identifier for this command - used to reference outputs with <code>${id.output_name}</code></li> <li>Whether the command is expected to create git commits (validates commit after execution)</li> <li>Maximum execution time in seconds before the command is terminated</li> <li>Declare outputs that downstream commands can reference (e.g., files created)</li> <li>Conditional expression - command only runs if the condition evaluates to true</li> <li>Nested command to execute if Claude command fails or returns error</li> <li>Nested command to execute if Claude command succeeds</li> </ol> <p>Fields: - <code>claude</code> (required): The Claude command string with arguments - <code>id</code> (optional): Unique identifier for this command in the workflow - <code>commit_required</code> (optional): Whether command is expected to create git commits (default: false) - <code>timeout</code> (optional): Maximum execution time in seconds - <code>outputs</code> (optional): Declare outputs that can be referenced by other commands - <code>when</code> (optional): Conditional expression for execution - <code>on_failure</code> (optional): Nested command to execute on failure - <code>on_success</code> (optional): Nested command to execute on success</p> <p>Example with outputs (from workflows/coverage.yml:3): <pre><code>- claude: \"/prodigy-coverage\"\n  id: coverage\n  commit_required: true\n  outputs:\n    spec:\n      file_pattern: \"*-coverage-improvements.md\"\n\n- claude: \"/prodigy-implement-spec ${coverage.spec}\"\n  commit_required: true\n</code></pre></p> <p>Example in MapReduce (from workflows/fix-files-mapreduce.yml:31): <pre><code>- claude: \"/analyze-and-fix-file ${item.path} --complexity ${item.complexity}\"\n  capture_output: true\n  timeout: 300\n</code></pre></p> <p>Common Use Cases</p> <ul> <li>Running custom Claude commands (slash commands)</li> <li>Code analysis and generation</li> <li>Implementing specifications from markdown</li> <li>Debugging test failures with AI assistance</li> <li>Automated code review and linting</li> </ul>"},{"location":"workflow-basics/command-types/#analyze-commands","title":"Analyze Commands","text":"<p>Perform codebase analysis with intelligent caching to avoid redundant computation. Analysis results include both metrics and context data that can be used by subsequent Claude commands.</p> <p>Source: src/config/command.rs:332</p> <p>Syntax: <pre><code># Source: workflows/analysis-workflow.yml:19-23\n- analyze:\n    max_cache_age: 300        # Optional: cache validity in seconds\n    force_refresh: false      # Optional: bypass cache (default: false)\n    save: true                # Optional: save analysis results (default: true)\n    format: \"summary\"         # Optional: output format (summary, detailed)\n</code></pre></p> <p>Fields: - <code>max_cache_age</code> (optional): Maximum age of cached analysis in seconds before refresh - <code>force_refresh</code> (optional): Force fresh analysis even if cache is valid (default: false) - <code>save</code> (optional): Save analysis results to context directory (default: true) - <code>format</code> (optional): Output format for analysis results (summary, detailed)</p> <p>Caching Behavior: - If cache exists and is younger than <code>max_cache_age</code>, use cached results - If <code>force_refresh: true</code>, always perform fresh analysis regardless of cache - Analysis runs automatically at workflow start unless <code>--skip-analysis</code> flag is used - Cached results are stored per-repository for reuse across workflows</p> <p>Example with cached analysis (from workflows/analysis-workflow.yml:19-26): <pre><code># Source: workflows/analysis-workflow.yml:19-26\n- analyze:\n    max_cache_age: 300        # Use cache if less than 5 minutes old\n    force_refresh: false      # Don't force if cache is fresh\n    save: true\n    format: \"summary\"\n\n- claude: \"/prodigy-code-review\"\n</code></pre></p> <p>Example with forced refresh (from workflows/analysis-workflow.yml:29-34): <pre><code># Source: workflows/analysis-workflow.yml:29-34\n- analyze:\n    force_refresh: true       # Always get fresh analysis for accuracy\n    save: true\n    format: \"summary\"\n\n- claude: \"/prodigy-cleanup-tech-debt\"\n</code></pre></p> <p>Example with short cache (from workflows/tech-debt.yml:3-5): <pre><code># Source: workflows/tech-debt.yml:3-5\n- analyze:\n    max_cache_age: 300\n    save: true\n\n- claude: \"/prodigy-cleanup-tech-debt\"\n  id: cleanup\n</code></pre></p> <p>Common Use Cases</p> <ul> <li>Refreshing analysis after significant code changes</li> <li>Providing fresh metrics for tech debt cleanup</li> <li>Updating context before documentation generation</li> <li>Caching analysis for multiple commands to save time</li> <li>Force refresh for critical operations requiring accuracy</li> </ul> <p>When to Use Analyze</p> <p>Use <code>analyze:</code> when you need fresh codebase metrics or context for Claude commands. The first analyze in a workflow can use cache, but subsequent analyses after modifications should use <code>force_refresh: true</code> for accuracy.</p> <p>Cache Strategy</p> <ul> <li>Development workflows: Use <code>max_cache_age: 300</code> (5 minutes) for fast iteration</li> <li>After code changes: Use <code>force_refresh: true</code> for accurate metrics</li> <li>Multiple commands: Share one analysis with appropriate <code>max_cache_age</code></li> <li>CI/CD pipelines: Use <code>force_refresh: true</code> for consistency</li> </ul>"},{"location":"workflow-basics/command-types/#goal-seek-commands","title":"Goal Seek Commands","text":"<p>Iteratively refine implementation until validation threshold is met.</p> <p>Source: src/cook/goal_seek/mod.rs:15-41</p> <p>Syntax: <pre><code>- goal_seek:\n    goal: \"Human-readable goal description\"\n    claude: \"/command-for-refinement\"  # Either claude or shell required\n    shell: \"refinement-command\"        # Either claude or shell required\n    validate: \"validation-command\"     # Required: returns score 0-100\n    threshold: 90                      # Required: success threshold (0-100)\n    max_attempts: 5                    # Required: maximum refinement attempts\n    timeout_seconds: 600               # Optional: timeout for entire operation\n    fail_on_incomplete: true           # Optional: fail workflow if goal not met\n  commit_required: true\n</code></pre></p> <p>Fields: - <code>goal</code> (required): Human-readable description of what to achieve - <code>claude</code> (optional): Claude command for refinement attempts - <code>shell</code> (optional): Shell command for refinement attempts (use claude OR shell) - <code>validate</code> (required): Command that returns validation score (0-100) - <code>threshold</code> (required): Minimum score required for success (0-100) - <code>max_attempts</code> (required): Maximum number of refinement attempts - <code>timeout_seconds</code> (optional): Timeout for the entire goal-seeking operation - <code>fail_on_incomplete</code> (optional): Whether to fail workflow if goal is not met</p> <p>Validation Output Format: The validate command must output a line containing <code>score: &lt;number&gt;</code> where number is 0-100: <pre><code>score: 85\n</code></pre></p> <p>Result Types: - <code>Complete</code>: Goal achieved (score &gt;= threshold) - <code>Incomplete</code>: Maximum attempts reached without achieving threshold - <code>Failed</code>: Error during execution</p> <p>Execution Flow:</p> <pre><code>flowchart TD\n    Start[Start goal_seek] --&gt; Run[Execute refinement&lt;br/&gt;claude or shell]\n    Run --&gt; Validate[Run validate command]\n    Validate --&gt; Parse[Parse score from output]\n    Parse --&gt; Check{Score &gt;=&lt;br/&gt;threshold?}\n\n    Check --&gt;|Yes| Complete[Complete&lt;br/&gt;Goal achieved]\n    Check --&gt;|No| Attempts{Attempts &lt;&lt;br/&gt;max_attempts?}\n\n    Attempts --&gt;|Yes| Run\n    Attempts --&gt;|No| Incomplete[Incomplete&lt;br/&gt;Max attempts reached]\n\n    Complete --&gt; End[End]\n    Incomplete --&gt; FailCheck{fail_on_incomplete?}\n    FailCheck --&gt;|true| Fail[Fail workflow]\n    FailCheck --&gt;|false| End\n\n    style Complete fill:#e8f5e9\n    style Incomplete fill:#fff3e0\n    style Fail fill:#ffebee\n    style Check fill:#e1f5ff</code></pre> <p>Figure: Goal-seeking iteration flow showing refinement loop and termination conditions.</p> <p>Example (from workflows/implement-goal.yml:8): <pre><code>- goal_seek:\n    goal: \"Implement specification: $ARG\"\n    claude: \"/prodigy-implement-spec $ARG\"\n    validate: \"git diff --stat | grep -q '.*\\\\.rs' &amp;&amp; echo 'score: 100' || echo 'score: 0'\"\n    threshold: 90\n    max_attempts: 3\n    timeout_seconds: 600\n  commit_required: true\n</code></pre></p> <p>Example with shell refinement (from workflows/goal-seeking-examples.yml:7): <pre><code>- goal_seek:\n    goal: \"Achieve 90% test coverage\"\n    shell: \"cargo tarpaulin --out Lcov\"\n    validate: \"cargo tarpaulin --print-summary 2&gt;/dev/null | grep 'Coverage' | sed 's/.*Coverage=\\\\([0-9]*\\\\).*/score: \\\\1/'\"\n    threshold: 90\n    max_attempts: 5\n    timeout_seconds: 300\n    fail_on_incomplete: true\n  commit_required: true\n</code></pre></p> <p>Common Use Cases</p> <ul> <li>Iteratively improving code quality until metrics are met</li> <li>Achieving test coverage thresholds (e.g., 90% coverage)</li> <li>Performance optimization to target benchmarks</li> <li>Fixing linting issues until clean</li> <li>Implementing specifications completely with validation</li> </ul> <p>Validation Score Format</p> <p>The validate command must output <code>score: &lt;number&gt;</code> (0-100) for goal-seeking to work. Example: <code>echo \"score: 85\"</code></p>"},{"location":"workflow-basics/command-types/#foreach-commands","title":"Foreach Commands","text":"<p>Iterate over a list of items, executing commands for each item in parallel or sequentially.</p> <p>Source: src/config/command.rs:344, src/config/command.rs:188-211</p> <p>Syntax: <pre><code>- foreach:\n    items: [\"item1\", \"item2\"]       # Static list\n    # OR\n    items: \"shell-command\"          # Command that produces items\n    do:                             # Commands to execute per item\n      - claude: \"/process ${item}\"\n      - shell: \"test ${item}\"\n    parallel: 5                     # Optional: number of parallel workers\n    continue_on_error: true         # Optional: continue if item fails\n    max_items: 100                  # Optional: limit number of items\n</code></pre></p> <p>Input Types: 1. Static List: Array of strings directly in YAML 2. Command Output: Shell command whose output produces item list</p> <p>Fields: - <code>items</code> (required): Either a static list or a command to generate items - <code>do</code> (required): List of commands to execute for each item - <code>parallel</code> (optional): Number of parallel workers (boolean or count)   - <code>true</code>: Default parallel count   - <code>false</code>: Sequential execution   - Number: Specific parallel count - <code>continue_on_error</code> (optional): Continue processing other items if one fails (default: false) - <code>max_items</code> (optional): Maximum number of items to process</p> <p>Variable Access: Inside <code>do</code> commands, use <code>${item}</code> to reference the current item.</p> <p>Parallel Execution Strategy:</p> <pre><code>graph TD\n    Items[Work Items:&lt;br/&gt;item1, item2, ..., itemN] --&gt; Distribute{parallel setting}\n\n    Distribute --&gt;|false or 1| Sequential[Sequential Execution&lt;br/&gt;One at a time]\n    Distribute --&gt;|Number N| Parallel[Parallel Execution&lt;br/&gt;N workers]\n\n    Sequential --&gt; W1[Worker: item1]\n    W1 --&gt; W2[Worker: item2]\n    W2 --&gt; W3[Worker: item3]\n\n    Parallel --&gt; P1[Worker 1: item1]\n    Parallel --&gt; P2[Worker 2: item2]\n    Parallel --&gt; P3[Worker N: itemN]\n\n    W3 --&gt; Continue{continue_on_error}\n    P1 --&gt; Continue\n    P2 --&gt; Continue\n    P3 --&gt; Continue\n\n    Continue --&gt;|false| StopOnError[Stop on first error]\n    Continue --&gt;|true| ProcessAll[Process all items]\n\n    style Sequential fill:#e1f5ff\n    style Parallel fill:#e8f5e9\n    style StopOnError fill:#ffebee\n    style ProcessAll fill:#fff3e0</code></pre> <p>Figure: Foreach execution showing sequential vs parallel processing and error handling.</p> <p>Example (from workflows/fix-files-mapreduce.yml - conceptual): <pre><code>- foreach:\n    items: \"find src -name '*.rs' -type f\"\n    do:\n      - claude: \"/analyze-file ${item}\"\n      - shell: \"rustfmt ${item}\"\n    parallel: 4\n    continue_on_error: true\n</code></pre></p> <p>Common Use Cases</p> <ul> <li>Processing multiple files (e.g., formatting all <code>.rs</code> files)</li> <li>Running operations on a list of identifiers</li> <li>Batch operations with controlled parallelism</li> <li>Conditional processing based on item properties</li> </ul> <p>Parallel Execution</p> <p>Set <code>parallel: 5</code> to process 5 items concurrently. Use <code>parallel: false</code> for sequential processing.</p>"},{"location":"workflow-basics/command-types/#write-file-commands","title":"Write File Commands","text":"<p>Write content to files with format validation and directory creation.</p> <p>Source: src/config/command.rs:348, src/config/command.rs:279-317</p> <p>Syntax: <pre><code>- write_file:\n    path: \"output/file.json\"        # Required: file path (supports variables)\n    content: \"${map.results}\"       # Required: content to write (supports variables)\n    format: json                    # Optional: text, json, yaml (default: text)\n    mode: \"0644\"                    # Optional: file permissions (default: 0644)\n    create_dirs: true               # Optional: create parent directories (default: false)\n</code></pre></p> <p>Fields: - <code>path</code> (required): File path to write to (supports variable interpolation) - <code>content</code> (required): Content to write (supports variable interpolation) - <code>format</code> (optional): Output format - text, json, or yaml (default: text)   - <code>text</code>: Write content as-is   - <code>json</code>: Validate JSON and pretty-print   - <code>yaml</code>: Validate YAML and format - <code>mode</code> (optional): File permissions in octal format (default: \"0644\") - <code>create_dirs</code> (optional): Create parent directories if they don't exist (default: false)</p> <p>Format Types (src/config/command.rs:301-313): - <code>Text</code>: No processing, write content directly - <code>Json</code>: Validate JSON syntax and pretty-print - <code>Yaml</code>: Validate YAML syntax and format</p> <p>Example (from workflows/debtmap-reduce.yml:105): <pre><code>- write_file:\n    path: \".prodigy/map-results.json\"\n    content: \"${map.results}\"\n    format: json\n    create_dirs: true\n</code></pre></p> <p>Common Use Cases</p> <ul> <li>Saving workflow results to files</li> <li>Generating configuration files from templates</li> <li>Writing JSON/YAML data with automatic validation</li> <li>Creating reports and logs</li> <li>Persisting intermediate data between workflow steps</li> </ul> <p>Format Validation</p> <p>Use <code>format: json</code> or <code>format: yaml</code> to automatically validate and pretty-print output. Invalid content will fail the step.</p>"},{"location":"workflow-basics/command-types/#validation-commands","title":"Validation Commands","text":"<p>Validate implementation completeness using shell or Claude commands with threshold-based checking.</p> <p>Source: src/cook/workflow/validation.rs:12-50</p> <p>Syntax: <pre><code>- validate:\n    shell: \"validation-command\"         # Either shell or claude or commands\n    claude: \"/validate-command\"         # Either shell or claude or commands\n    commands:                           # Or array of commands for multi-step validation\n      - shell: \"step1\"\n      - claude: \"/step2\"\n    expected_schema: {...}              # Optional: JSON schema for validation output\n    threshold: 100                      # Optional: completion threshold % (default: 100)\n    timeout: 300                        # Optional: timeout in seconds\n    result_file: \"validation.json\"      # Optional: where validation results are written\n    on_incomplete:                      # Optional: commands to run if validation fails\n      claude: \"/fix-gaps ${validation.gaps}\"\n      max_attempts: 3\n</code></pre></p> <p>Fields: - <code>shell</code> (optional): Shell command for validation (deprecated, use <code>commands</code>) - <code>claude</code> (optional): Claude command for validation - <code>commands</code> (optional): Array of commands for multi-step validation - <code>expected_schema</code> (optional): JSON schema that validation output must match - <code>threshold</code> (optional): Minimum completion percentage required (default: 100) - <code>timeout</code> (optional): Timeout for validation command in seconds - <code>result_file</code> (optional): File where validation results are written - <code>on_incomplete</code> (optional): Configuration for handling incomplete implementations   - Nested command to execute if validation fails   - <code>max_attempts</code>: Maximum retry attempts</p> <p>Validation Output Format: Validation commands should output JSON matching this structure: <pre><code>{\n  \"complete\": true,\n  \"score\": 100,\n  \"gaps\": [],\n  \"incomplete_specs\": []\n}\n</code></pre></p> <p>Example with result_file (from workflows/implement.yml:8): <pre><code>validate:\n  claude: \"/prodigy-validate-spec $ARG --output .prodigy/validation-result.json\"\n  result_file: \".prodigy/validation-result.json\"\n  threshold: 100\n  on_incomplete:\n    claude: \"/prodigy-complete-spec $ARG --gaps ${validation.gaps}\"\n</code></pre></p> <p>Example with commands array (from workflows/debtmap.yml:13): <pre><code>validate:\n  commands:\n    - claude: \"/prodigy-validate-debtmap-plan --before .prodigy/debtmap-before.json --plan .prodigy/IMPLEMENTATION_PLAN.md --output .prodigy/plan-validation.json\"\n  result_file: \".prodigy/plan-validation.json\"\n  threshold: 75\n  on_incomplete:\n    claude: \"/fix-plan-gaps --gaps ${validation.gaps}\"\n    max_attempts: 3\n</code></pre></p> <p>Common Use Cases</p> <ul> <li>Verifying specification completeness</li> <li>Checking implementation against requirements</li> <li>Validating test coverage meets thresholds</li> <li>Ensuring documentation quality standards</li> <li>Multi-step validation pipelines</li> </ul> <p>Validate vs Goal Seek</p> <p><code>validate:</code> is for one-time validation with gap detection and retry logic. <code>goal_seek:</code> is for iterative refinement until a score threshold is met. Use <code>validate:</code> for spec checking, <code>goal_seek:</code> for quality improvement.</p>"},{"location":"workflow-basics/command-types/#common-fields","title":"Common Fields","text":"<p>Several fields are available across all command types:</p> <p>Source: src/config/command.rs:320-401</p> <ul> <li><code>id</code> (string): Unique identifier for referencing command outputs</li> <li><code>timeout</code> (number): Maximum execution time in seconds</li> <li><code>when</code> (string): Conditional execution expression</li> <li><code>on_success</code> (command): Command to run on successful execution</li> <li><code>on_failure</code> (command): Command to run on failed execution</li> <li><code>capture_output</code> (boolean or string): Capture command output (true/false or variable name)</li> <li><code>capture_format</code> (string): Format for captured output (json, text, lines)</li> <li><code>capture_streams</code> (string): Which streams to capture (stdout, stderr, both)</li> <li><code>output_file</code> (string): File to redirect output to</li> </ul>"},{"location":"workflow-basics/command-types/#command-exclusivity","title":"Command Exclusivity","text":"<p>Each workflow step must specify exactly one command type. You cannot combine multiple command types in a single step:</p> Valid \u2713Invalid \u2717 <pre><code>- shell: \"cargo test\"        # \u2713 Only shell command\n- claude: \"/lint\"            # \u2713 Only Claude command\n- analyze:                   # \u2713 Only analyze command\n    max_cache_age: 300\n- goal_seek:                 # \u2713 Only goal_seek command\n    goal: \"Fix tests\"\n    validate: \"...\"\n</code></pre> <pre><code>- shell: \"cargo test\"        # \u2717 Cannot combine shell and claude\n  claude: \"/lint\"\n\n- analyze: {...}             # \u2717 Cannot combine analyze and claude\n  claude: \"/review\"\n\n- goal_seek: {...}           # \u2717 Cannot combine goal_seek and foreach\n  foreach: {...}\n</code></pre> <p>Enforcement: src/config/command.rs:465-476</p>"},{"location":"workflow-basics/command-types/#deprecated-test-command","title":"Deprecated: Test Command","text":"<p>Deprecated Syntax</p> <p>The <code>test:</code> command syntax is deprecated and will be removed in a future version. Use <code>shell:</code> with <code>on_failure:</code> instead for the same functionality.</p> <p>Source: src/config/command.rs:446-463</p> <p>Old (deprecated): <pre><code>- test:\n    command: \"cargo test\"\n    on_failure:\n      claude: \"/debug-test\"\n</code></pre></p> <p>New (recommended): <pre><code>- shell: \"cargo test\"\n  on_failure:\n    claude: \"/debug-test\"\n</code></pre></p> <p>The migration is straightforward: replace <code>test:</code> \u2192 <code>command:</code> with <code>shell:</code> and keep all other fields unchanged.</p>"},{"location":"workflow-basics/command-types/#see-also","title":"See Also","text":"<ul> <li>Available Fields - Complete reference of all command fields</li> <li>Goal Seeking Operations - Advanced goal-seeking patterns</li> <li>Parallel Iteration with Foreach - Foreach best practices</li> <li>Implementation Validation - Validation strategies</li> <li>Variables - Variable interpolation in commands</li> <li>Error Handling - Error handling strategies</li> </ul>"},{"location":"workflow-basics/complete-example/","title":"Complete Example","text":""},{"location":"workflow-basics/complete-example/#complete-example","title":"Complete Example","text":"<p>Here's a complete workflow demonstrating Prodigy's core features in a single file. This example combines environment configuration, workflow commands, and custom merge behavior.</p> <p>Source: Based on <code>workflows/implement.yml</code></p> <pre><code># Environment configuration\nenv:\n  RUST_BACKTRACE: 1              # Standard environment variable\n\nenv_files:\n  - .env                         # Load variables from .env file (dotenv format)\n\nprofiles:\n  ci:                            # Activate with: prodigy run workflow.yml --profile ci\n    CI: \"true\"\n    VERBOSE: \"true\"\n\n# Workflow commands\ncommands:\n  - shell: \"cargo fmt --check\"\n  - shell: \"cargo clippy -- -D warnings\"\n  - shell: \"cargo test --all\"\n  - claude: \"/prodigy-lint\"\n\n# Custom merge workflow (simplified format)\nmerge:\n  - shell: \"cargo test\"          # Validate before merging\n  - claude: \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"\n</code></pre>"},{"location":"workflow-basics/complete-example/#key-features-demonstrated","title":"Key Features Demonstrated","text":"<ol> <li>Environment Variables (lines 2-4): Define variables available to all commands</li> <li> <p>See Environment Variables for details</p> </li> <li> <p>Environment Files (lines 6-7): Load variables from <code>.env</code> files in dotenv format</p> </li> <li> <p>See Environment Files for syntax</p> </li> <li> <p>Profiles (lines 9-12): Define environment sets activated via <code>--profile</code> flag</p> </li> <li>Example: <code>prodigy run workflow.yml --profile ci</code></li> <li> <p>See Environment Profiles for advanced usage</p> </li> <li> <p>Workflow Commands (lines 15-19): Execute shell and Claude commands sequentially</p> </li> <li> <p>See Command Types for all command types</p> </li> <li> <p>Custom Merge Workflow (lines 22-24): Customize the merge-back process</p> </li> <li>Important: Always include both <code>${merge.source_branch}</code> and <code>${merge.target_branch}</code> parameters</li> <li>The simplified array format is shown here (supported by <code>MergeWorkflow</code>)</li> <li>See next section for the full configuration format</li> </ol>"},{"location":"workflow-basics/complete-example/#alternative-merge-format-with-timeout","title":"Alternative Merge Format (with timeout)","text":"<p>The merge block also supports a configuration format with timeout:</p> <pre><code>merge:\n  commands:\n    - shell: \"cargo test\"\n    - claude: \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"\n  timeout: 600                   # Optional: timeout in seconds (10 minutes)\n</code></pre> <p>Source: <code>src/config/mapreduce.rs:86-124</code></p>"},{"location":"workflow-basics/complete-example/#real-world-example","title":"Real-World Example","text":"<p>For a production-grade workflow with validation and error handling, see the implementation workflow:</p> <p>File: <code>workflows/implement.yml</code> (lines 32-41)</p> <pre><code>merge:\n  # Step 1: Merge master into worktree\n  - claude: \"/prodigy-merge-master\"\n\n  # Step 2: Run CI checks and fix any issues\n  - claude: \"/prodigy-ci\"\n\n  # Step 3: Merge worktree back to original branch\n  - claude: \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"\n</code></pre> <p>This demonstrates best practices: - Sync with upstream before merging back - Validate changes with CI checks - Use proper merge parameters</p>"},{"location":"workflow-basics/complete-example/#next-steps","title":"Next Steps","text":"<ul> <li>Error Handling - Add <code>on_failure</code> handlers for robust workflows</li> <li>Variable Interpolation - Use dynamic values in commands</li> <li>Advanced Features - Explore goal-seeking, validation, and more</li> </ul>"},{"location":"workflow-basics/conditional-execution/","title":"Conditional Execution","text":"<p>Prodigy provides powerful conditional execution features that allow workflows to make intelligent decisions about which steps to run and how to handle errors. These features enable workflows to adapt to different scenarios, handle failures gracefully, and execute complex logic.</p>"},{"location":"workflow-basics/conditional-execution/#when-clauses","title":"When Clauses","text":"<p>When clauses allow you to conditionally execute workflow steps based on boolean expressions. Steps with a <code>when</code> clause are only executed if the expression evaluates to <code>true</code>.</p>"},{"location":"workflow-basics/conditional-execution/#syntax","title":"Syntax","text":"<pre><code># Source: src/config/command.rs:388\n- shell: \"cargo build --release\"\n  when: \"${env} == 'production'\"\n</code></pre>"},{"location":"workflow-basics/conditional-execution/#expression-features","title":"Expression Features","text":"<p>When clauses support:</p> <ul> <li>Variable interpolation: <code>${variable_name}</code></li> <li>Comparison operators: <code>==</code>, <code>!=</code>, <code>&lt;</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&gt;=</code></li> <li>Logical operators: <code>&amp;&amp;</code> (and), <code>||</code> (or), <code>!</code> (not)</li> <li>Parentheses: For grouping expressions</li> <li>Step results: Access previous step outcomes</li> </ul> <p>Expression Evaluation Gotcha</p> <p>Undefined variables in when clauses evaluate to <code>false</code> (not an error). This allows safe checking for optional variables but can cause unexpected skips if you mistype a variable name.</p>"},{"location":"workflow-basics/conditional-execution/#available-variables","title":"Available Variables","text":"<p>Workflow Variables: <pre><code># Source: src/cook/workflow/conditional_tests.rs:72-86\nwhen: \"${flag} == true\"              # Boolean variable\nwhen: \"${score} &gt;= 80\"               # Numeric variable\nwhen: \"${env} == 'production'\"       # String variable\n</code></pre></p> <p>Step Results: <pre><code># Source: src/cook/workflow/conditional_tests.rs:94-98\nwhen: \"${build.success}\"             # Did step succeed?\nwhen: \"${build.exit_code} == 0\"      # Check exit code\nwhen: \"${build.output} contains 'OK'\" # Check output\n</code></pre></p> <p>Environment Variables: <pre><code>when: \"${CI} == 'true'\"              # Environment variables are accessible\n</code></pre></p> <p>Note</p> <p>Undefined variables evaluate to <code>false</code>. Use this behavior to safely check for optional variables.</p>"},{"location":"workflow-basics/conditional-execution/#examples","title":"Examples","text":"<p>Skip deployment on non-main branches: <pre><code># Source: src/cook/workflow/conditional_tests.rs:108-109\n- shell: \"git rev-parse --abbrev-ref HEAD\"\n  capture_output: \"branch\"\n\n- shell: \"./deploy.sh\"\n  when: \"${branch} == 'main'\"\n</code></pre></p> <p>Run tests only if code changed: <pre><code>- shell: \"git diff --name-only HEAD~1 | grep '\\.rs$'\"\n  capture_output: \"rust_changed\"\n\n- shell: \"cargo test\"\n  when: \"${rust_changed} != ''\"\n</code></pre></p> <p>Complex conditions with multiple checks: <pre><code># Source: src/cook/workflow/conditional_tests.rs:111-121\n- shell: \"cargo bench\"\n  when: \"(${coverage} &gt;= 80 || ${override}) &amp;&amp; ${branch} == 'main'\"\n</code></pre></p> <p>Conditional execution based on step results: <pre><code># Source: src/cook/workflow/conditional_tests.rs:56\n- shell: \"cargo build\"\n  id: \"build\"\n\n- shell: \"cargo test\"\n  when: \"${build.success} &amp;&amp; ${coverage} &gt;= 80\"\n</code></pre></p>"},{"location":"workflow-basics/conditional-execution/#on-failure-handlers","title":"On Failure Handlers","text":"<p>On failure handlers specify what to do when a command fails. They provide flexible error handling strategies ranging from simple error ignoring to complex recovery logic.</p>"},{"location":"workflow-basics/conditional-execution/#simple-syntax-variants","title":"Simple Syntax Variants","text":"<p>Ignore errors: <pre><code># Source: src/cook/workflow/on_failure.rs:72\n- shell: \"cargo clippy -- -D warnings\"\n  on_failure: false  # Continue workflow even if command fails\n</code></pre></p> <p>Single recovery command: <pre><code># Source: workflows/debug.yml:3-5\n- shell: \"just test\"\n  on_failure:\n    claude: \"/prodigy-debug-test-failure --output ${shell.output}\"\n</code></pre></p> <p>Multiple recovery commands: <pre><code>- shell: \"cargo test\"\n  on_failure:\n    - shell: \"cargo clean\"\n    - shell: \"cargo test --verbose\"\n</code></pre></p>"},{"location":"workflow-basics/conditional-execution/#advanced-configuration","title":"Advanced Configuration","text":"<p>For more control over failure handling, use the advanced syntax:</p> <p>Advanced Error Recovery</p> <p>The advanced syntax provides fine-grained control over retry behavior and workflow continuation:</p> <pre><code># Source: src/cook/workflow/on_failure.rs:85-105\n- shell: \"cargo test\"\n  on_failure:\n    shell: \"echo 'Tests failed, trying cleanup...'\"\n    fail_workflow: false       # (1)!\n    retry_original: true       # (2)!\n    max_retries: 3            # (3)!\n\n1. Continue workflow even if the handler can't recover\n2. Automatically retry the original command after handler completes\n3. Maximum number of retry attempts (prevents infinite loops)\n</code></pre> <p>With Claude recovery: <pre><code># Source: workflows/implement.yml:19-23\n- shell: \"just test\"\n  on_failure:\n    claude: \"/prodigy-debug-test-failure --spec $ARG --output ${shell.output}\"\n    max_attempts: 5\n    fail_workflow: false  # Continue workflow even if tests can't be fixed\n</code></pre></p>"},{"location":"workflow-basics/conditional-execution/#detailed-handler-configuration","title":"Detailed Handler Configuration","text":"<p>For complex scenarios, use the detailed handler configuration:</p> <p>Full Handler Configuration</p> <p>This example shows all available handler configuration options:</p> <pre><code># Source: src/cook/workflow/on_failure.rs:25-49\n- shell: \"cargo test\"\n  on_failure:\n    strategy: recovery        # (1)!\n    timeout: 300              # (2)!\n    handler_failure_fatal: false  # (3)!\n    fail_workflow: false      # (4)!\n    commands:\n      - shell: \"cargo clean\"\n        continue_on_error: true\n      - claude: \"/fix-tests ${shell.output}\"\n    capture:\n      error_details: \"${handler.output}\"\n\n1. Choose recovery strategy: recovery, fallback, cleanup, or custom\n2. Timeout for handler execution (prevents hanging handlers)\n3. If handler fails, continue to next command instead of failing workflow\n4. After handler completes, continue workflow instead of failing\n</code></pre>"},{"location":"workflow-basics/conditional-execution/#handler-strategies","title":"Handler Strategies","text":"<p>Choosing the Right Strategy</p> <p>Different strategies communicate intent and help with workflow maintenance:</p> <ul> <li>recovery: Attempt to fix the problem and retry</li> <li>fallback: Try an alternative approach</li> <li>cleanup: Clean up resources before failing</li> <li>custom: Execute domain-specific error handling</li> </ul> RecoveryFallbackCleanupCustom <p>Try to fix the problem and continue:</p> <pre><code># Source: src/cook/workflow/on_failure.rs:10-22\n- shell: \"cargo build\"\n  on_failure:\n    strategy: recovery\n    commands:\n      - claude: \"/fix-compilation-errors ${shell.output}\"\n</code></pre> <p>Use an alternative approach when the primary method fails:</p> <pre><code>- shell: \"docker build -t myapp .\"\n  on_failure:\n    strategy: fallback\n    commands:\n      - shell: \"podman build -t myapp .\"\n</code></pre> <p>Clean up resources before failing:</p> <pre><code>- shell: \"run-integration-tests.sh\"\n  on_failure:\n    strategy: cleanup\n    fail_workflow: true\n    commands:\n      - shell: \"docker-compose down\"\n      - shell: \"rm -rf test-data/\"\n</code></pre> <p>Execute custom handler logic:</p> <pre><code>- shell: \"deploy-to-production.sh\"\n  on_failure:\n    strategy: custom\n    commands:\n      - shell: \"rollback-deployment.sh\"\n      - shell: \"notify-team.sh\"\n</code></pre>"},{"location":"workflow-basics/conditional-execution/#real-world-examples","title":"Real-World Examples","text":"<p>Auto-fix test failures with Claude: <pre><code># Source: workflows/coverage-simplified.yml:15-19\n- shell: \"just test\"\n  on_failure:\n    claude: \"/prodigy-debug-test-failure --spec ${coverage.spec} --output ${shell.output}\"\n    max_attempts: 3\n    fail_workflow: false  # Continue workflow even if tests can't be fixed\n</code></pre></p> <p>Auto-fix linting issues: <pre><code># Source: workflows/complex-build-pipeline.yml:16-19\n- shell: \"cargo clippy -- -D warnings\"\n  capture_output: \"clippy_warnings\"\n  on_failure:\n    claude: \"/prodigy-fix-clippy-warnings '${clippy_warnings}'\"\n</code></pre></p> <p>Nested error handling: <pre><code># Source: workflows/complex-build-pipeline.yml:7-13\n- shell: \"cargo check\"\n  on_success:\n    shell: \"cargo build --release\"\n    on_success:\n      shell: \"cargo test --release\"\n      on_failure:\n        claude: \"/prodigy-debug-and-fix '${shell.output}'\"\n</code></pre></p>"},{"location":"workflow-basics/conditional-execution/#on-success-handlers","title":"On Success Handlers","text":"<p>On success handlers execute additional steps after a command completes successfully. They're useful for post-processing, notifications, or chaining dependent operations.</p>"},{"location":"workflow-basics/conditional-execution/#syntax_1","title":"Syntax","text":"<pre><code># Source: src/config/command.rs:375-376\n- shell: \"cargo build --release\"\n  on_success:\n    shell: \"notify-team.sh 'Build successful'\"\n</code></pre>"},{"location":"workflow-basics/conditional-execution/#chaining-operations","title":"Chaining Operations","text":"<p>Success handlers can be chained to create sequential workflows:</p> <pre><code># Source: workflows/complex-build-pipeline.yml:8-11\n- shell: \"cargo check\"\n  on_success:\n    shell: \"cargo build --release\"\n    on_success:\n      shell: \"cargo test --release\"\n</code></pre>"},{"location":"workflow-basics/conditional-execution/#use-cases","title":"Use Cases","text":"<p>Notifications: <pre><code>- shell: \"cargo test\"\n  on_success:\n    shell: \"echo 'All tests passed!' | mail -s 'Test Report' team@example.com\"\n</code></pre></p> <p>Post-processing: <pre><code>- shell: \"cargo build --release\"\n  on_success:\n    shell: \"strip target/release/myapp\"  # Strip debug symbols\n</code></pre></p> <p>Conditional deployment: <pre><code>- shell: \"cargo test --release\"\n  on_success:\n    shell: \"./deploy.sh\"\n    when: \"${branch} == 'main'\"\n</code></pre></p>"},{"location":"workflow-basics/conditional-execution/#combining-conditional-features","title":"Combining Conditional Features","text":"<p>When clauses, on_failure, and on_success can be combined to create sophisticated workflows:</p>"},{"location":"workflow-basics/conditional-execution/#execution-order","title":"Execution Order","text":"<pre><code>flowchart TD\n    Start[Workflow Step] --&gt; WhenCheck{when clause&lt;br/&gt;present?}\n    WhenCheck --&gt;|No| Execute[Execute Command]\n    WhenCheck --&gt;|Yes| EvalWhen{Evaluate&lt;br/&gt;when clause}\n\n    EvalWhen --&gt;|true| Execute\n    EvalWhen --&gt;|false| Skip[Skip Step]\n\n    Execute --&gt; Result{Command&lt;br/&gt;Result}\n\n    Result --&gt;|Success| SuccessCheck{on_success&lt;br/&gt;defined?}\n    Result --&gt;|Failure| FailCheck{on_failure&lt;br/&gt;defined?}\n\n    SuccessCheck --&gt;|Yes| RunSuccess[Execute on_success]\n    SuccessCheck --&gt;|No| Next[Next Step]\n\n    FailCheck --&gt;|Yes| RunFail[Execute on_failure]\n    FailCheck --&gt;|No| Fail[Fail Workflow]\n\n    RunSuccess --&gt; SuccessResult{Handler&lt;br/&gt;Result}\n    SuccessResult --&gt;|Success| Next\n    SuccessResult --&gt;|Failure| Fail\n\n    RunFail --&gt; FailResult{Handler&lt;br/&gt;Result}\n    FailResult --&gt; FailWorkflow{fail_workflow&lt;br/&gt;setting}\n    FailWorkflow --&gt;|true| Fail\n    FailWorkflow --&gt;|false| Next\n\n    Skip --&gt; Next\n\n    style Execute fill:#e1f5ff\n    style RunSuccess fill:#e8f5e9\n    style RunFail fill:#fff3e0\n    style Fail fill:#ffebee\n    style Next fill:#f3e5f5</code></pre> <p>Figure: Conditional execution flow showing when clause evaluation, command execution, and success/failure handler logic.</p> <p>The execution order follows these steps:</p> <ol> <li>when clause evaluated first</li> <li>If <code>when</code> is true (or absent), command executes</li> <li>Based on result:</li> <li>Success \u2192 on_success handler runs (if present)</li> <li>Failure \u2192 on_failure handler runs (if present)</li> </ol>"},{"location":"workflow-basics/conditional-execution/#combined-example","title":"Combined Example","text":"<pre><code># Source: workflows/implement-with-tests.yml:25-35\n- shell: \"cargo test\"\n  when: \"${code_changed} == 'true'\"\n  capture_output: \"test_output\"\n  commit_required: false\n  on_failure:\n    # If tests fail, debug and fix them\n    claude: \"/prodigy-debug-test-failures '${test_output}'\"\n    commit_required: true\n    on_success:\n      # After fixing, verify tests pass\n      shell: \"cargo test\"\n      commit_required: false\n</code></pre>"},{"location":"workflow-basics/conditional-execution/#complex-build-pipeline","title":"Complex Build Pipeline","text":"<pre><code># Full pipeline with all conditional features\n- shell: \"git diff --name-only HEAD~1 | grep '\\\\.rs$'\"\n  capture_output: \"rust_changed\"\n\n- shell: \"cargo check\"\n  when: \"${rust_changed} != ''\"\n  on_success:\n    shell: \"cargo build --release\"\n    on_failure:\n      claude: \"/fix-compilation-errors ${shell.output}\"\n      max_attempts: 3\n    on_success:\n      shell: \"cargo test --release\"\n      on_failure:\n        claude: \"/fix-test-failures ${shell.output}\"\n      on_success:\n        shell: \"./deploy.sh\"\n        when: \"${branch} == 'main'\"\n</code></pre>"},{"location":"workflow-basics/conditional-execution/#best-practices","title":"Best Practices","text":"<p>Use when clauses for pre-conditions</p> <p>Use <code>when</code> clauses to check pre-conditions before executing expensive operations: <pre><code>- shell: \"expensive-operation.sh\"\n  when: \"${pre_check.success} &amp;&amp; ${env} == 'production'\"\n</code></pre></p> <p>Prefer on_failure to test: syntax (deprecated)</p> <p>The <code>test:</code> command syntax is deprecated. Use <code>shell:</code> with <code>on_failure:</code> instead: <pre><code># Deprecated (src/config/command.rs:446-463)\ntest:\n  command: \"cargo test\"\n  on_failure: true\n\n# Preferred\nshell: \"cargo test\"\non_failure: false  # Or specify recovery logic\n</code></pre></p> <p>Capture output for debugging</p> <p>Always capture command output when using failure handlers: <pre><code>- shell: \"cargo test\"\n  capture_output: \"test_output\"\n  on_failure:\n    claude: \"/debug-failures ${test_output}\"\n</code></pre></p> <p>Set fail_workflow appropriately</p> <p>Use <code>fail_workflow: false</code> when the handler can recover, <code>true</code> when it's informational: <pre><code>- shell: \"cargo test\"\n  on_failure:\n    claude: \"/fix-tests\"\n    fail_workflow: false  # Handler attempts recovery\n\n- shell: \"deploy.sh\"\n  on_failure:\n    shell: \"rollback.sh\"\n    fail_workflow: true   # Rollback then fail\n</code></pre></p> <p>Use step IDs for clarity</p> <p>When referencing step results in when clauses, use step IDs: <pre><code>- id: \"build\"\n  shell: \"cargo build\"\n\n- shell: \"cargo test\"\n  when: \"${build.success}\"\n</code></pre></p>"},{"location":"workflow-basics/conditional-execution/#debugging-conditional-expressions","title":"Debugging Conditional Expressions","text":"<p>When a <code>when</code> clause doesn't evaluate as expected:</p> <ol> <li> <p>Check variable values: Use a debug step to print variables    <pre><code>- shell: \"echo 'branch=${branch}, env=${env}'\"\n</code></pre></p> </li> <li> <p>Verify variable interpolation: Ensure variables are properly captured    <pre><code>- shell: \"git rev-parse --abbrev-ref HEAD\"\n  capture_output: \"branch\"  # Required for ${branch} to be available\n</code></pre></p> </li> <li> <p>Test expressions incrementally: Break complex expressions into parts    <pre><code># Instead of:\nwhen: \"${a} &amp;&amp; ${b} &amp;&amp; ${c}\"\n\n# Try:\nwhen: \"${a}\"\n# Then add: when: \"${a} &amp;&amp; ${b}\"\n# Finally: when: \"${a} &amp;&amp; ${b} &amp;&amp; ${c}\"\n</code></pre></p> </li> <li> <p>Remember undefined behavior: Undefined variables evaluate to <code>false</code> <pre><code># Source: src/cook/workflow/conditional_tests.rs:128-137\nwhen: \"${might_not_exist}\"  # Evaluates to false if not defined\n</code></pre></p> </li> </ol>"},{"location":"workflow-basics/conditional-execution/#related-topics","title":"Related Topics","text":"<ul> <li>Variables - Learn about variable interpolation and capture</li> <li>Error Handling - Broader error handling strategies</li> <li>Workflow Structure - Understanding workflow execution flow</li> </ul>"},{"location":"workflow-basics/environment-configuration/","title":"Environment Configuration","text":""},{"location":"workflow-basics/environment-configuration/#environment-configuration","title":"Environment Configuration","text":"<p>Environment variables can be configured at multiple levels in Prodigy workflows, providing flexible control over workflow execution across different environments and contexts.</p> <p>Source: Configuration structures defined in <code>src/cook/environment/config.rs:11-144</code>, workflow configuration in <code>src/config/workflow.rs:12-39</code>.</p>"},{"location":"workflow-basics/environment-configuration/#overview","title":"Overview","text":"<p>Prodigy supports comprehensive environment configuration including: - Global environment variables with static, dynamic, and conditional values - Secret management with automatic log masking - Environment profiles for different deployment contexts - Environment files (.env format) - Step-level environment overrides - Variable interpolation in commands</p> <p>This subsection provides a quick introduction to environment configuration. For comprehensive coverage, see the Environment chapter.</p>"},{"location":"workflow-basics/environment-configuration/#global-environment-variables","title":"Global Environment Variables","text":"<p>Define environment variables in the <code>env:</code> block at the workflow root. Variables can be static strings, dynamically computed from commands, or conditionally set based on expressions.</p> <p>Source: <code>workflows/environment-example.yml:4-18</code></p> <pre><code>env:\n  # Static environment variables\n  NODE_ENV: production\n  API_URL: https://api.example.com\n\n  # Dynamic environment variable (computed from command)\n  WORKERS:\n    command: \"nproc 2&gt;/dev/null || echo 4\"\n    cache: true\n\n  # Conditional environment variable\n  DEPLOY_ENV:\n    condition: \"${branch} == 'main'\"\n    when_true: \"production\"\n    when_false: \"staging\"\n</code></pre> <p>Type Definitions (<code>src/cook/environment/config.rs:38-81</code>): - <code>EnvValue::Static(String)</code> - Static string value - <code>EnvValue::Dynamic(DynamicEnv)</code> - Computed from command with optional caching - <code>EnvValue::Conditional(ConditionalEnv)</code> - Based on condition expression</p>"},{"location":"workflow-basics/environment-configuration/#environment-files","title":"Environment Files","text":"<p>Load environment variables from .env format files using the <code>env_files:</code> field.</p> <p>Source: <code>workflows/environment-example.yml:25-27</code>, <code>src/cook/environment/config.rs:22-23</code></p> <pre><code>env_files:\n  - .env.production\n  - .env.local\n</code></pre> <p>Variables from env files are merged with global env variables, with explicit env: block taking precedence.</p> <p>For detailed information about .env file format and loading behavior, see Environment Files.</p>"},{"location":"workflow-basics/environment-configuration/#secrets-management","title":"Secrets Management","text":"<p>Secret environment variables are automatically masked in all log output, preventing accidental credential leaks.</p> <p>Source: <code>workflows/mapreduce-env-example.yml:23-26</code>, <code>src/cook/environment/config.rs:86-112</code></p> <pre><code>secrets:\n  # Simple secret reference\n  API_KEY: \"${env:SECRET_API_KEY}\"\n\n  # Provider-based secret\n  API_TOKEN:\n    provider: env\n    key: \"GITHUB_TOKEN\"\n</code></pre> <p>Secret Providers (<code>src/cook/environment/config.rs:101-112</code>): - <code>env</code> - Environment variable - <code>file</code> - File-based secret - <code>vault</code> - HashiCorp Vault - <code>aws</code> - AWS Secrets Manager - Custom providers</p> <p>For comprehensive secrets management documentation, see Secrets Management.</p>"},{"location":"workflow-basics/environment-configuration/#environment-profiles","title":"Environment Profiles","text":"<p>Profiles enable different environment configurations for various deployment contexts (development, staging, production).</p> <p>Source: <code>workflows/environment-example.yml:30-39</code>, <code>src/cook/environment/config.rs:116-124</code></p> <pre><code>profiles:\n  development:\n    description: \"Development environment with debug enabled\"\n    NODE_ENV: development\n    API_URL: http://localhost:3000\n    DEBUG: \"true\"\n\n  production:\n    description: \"Production environment\"\n    NODE_ENV: production\n    API_URL: https://api.example.com\n    DEBUG: \"false\"\n</code></pre> <p>Activate a profile: <pre><code># Via command line flag\nprodigy run workflow.yml --profile development\n\n# Via environment variable\nexport PRODIGY_PROFILE=production\nprodigy run workflow.yml\n</code></pre></p> <p>For detailed profile configuration and best practices, see Environment Profiles.</p>"},{"location":"workflow-basics/environment-configuration/#step-level-environment-overrides","title":"Step-Level Environment Overrides","text":"<p>Commands can define their own environment variables that override global and profile settings.</p> <p>Source: <code>workflows/environment-example.yml:47-60</code>, <code>src/cook/environment/config.rs:126-144</code></p> <pre><code>commands:\n  - name: \"Build frontend\"\n    shell: \"echo 'Building with NODE_ENV='$NODE_ENV\"\n    env:\n      BUILD_TARGET: production\n      OPTIMIZE: \"true\"\n    working_dir: ./frontend\n\n  - name: \"Run tests\"\n    shell: \"pytest\"\n    env:\n      PYTHONPATH: \"./src:./tests\"\n      TEST_ENV: \"true\"\n    working_dir: ./backend\n    temporary: true  # Environment restored after this step\n</code></pre> <p>Step Environment Fields (<code>src/cook/environment/config.rs:128-144</code>): - <code>env: HashMap&lt;String, String&gt;</code> - Step-specific variables - <code>working_dir: Option&lt;PathBuf&gt;</code> - Working directory for this step - <code>clear_env: bool</code> - Clear parent environment before applying step env - <code>temporary: bool</code> - Restore environment after step completes</p> <p>For detailed step-level configuration, see Per-Command Environment Overrides.</p>"},{"location":"workflow-basics/environment-configuration/#variable-interpolation","title":"Variable Interpolation","text":"<p>Environment variables can be referenced in commands using two syntaxes: <code>$VAR</code> or <code>${VAR}</code>.</p> <p>Source: <code>workflows/mapreduce-env-example.yml:44-80</code></p> <pre><code>env:\n  PROJECT_NAME: \"my-project\"\n  OUTPUT_DIR: \"output\"\n\ncommands:\n  # Both syntaxes work\n  - shell: \"echo Starting $PROJECT_NAME\"\n  - shell: \"echo Output directory: ${OUTPUT_DIR}\"\n\n  # Use ${} for complex expressions\n  - shell: \"cp summary.json ${OUTPUT_DIR}/${PROJECT_NAME}-report.json\"\n</code></pre> <p>Variable interpolation is available in: - Shell commands - Claude commands - File paths - MapReduce configuration (max_parallel, timeout, etc.)</p> <p>For comprehensive variable interpolation documentation, see Variable Interpolation.</p>"},{"location":"workflow-basics/environment-configuration/#environment-precedence","title":"Environment Precedence","text":"<p>When the same variable is defined at multiple levels, precedence is:</p> <ol> <li>Step-level env (highest priority)</li> <li>Profile env</li> <li>Global env</li> <li>System environment (lowest priority)</li> </ol> <p>Source: Precedence logic in <code>src/cook/environment/builder.rs:48-53</code>, tests in <code>tests/environment_workflow_test.rs:62-132</code></p> <p>For detailed precedence rules and examples, see Environment Precedence.</p>"},{"location":"workflow-basics/environment-configuration/#mapreduce-environment-variables","title":"MapReduce Environment Variables","text":"<p>Environment variables are available across all MapReduce workflow phases: setup, map, reduce, and merge.</p> <p>Source: <code>workflows/mapreduce-env-example.yml:1-95</code>, <code>src/config/mapreduce.rs:24-38</code></p> <pre><code>name: mapreduce-workflow\nmode: mapreduce\n\nenv:\n  PROJECT_NAME: \"my-project\"\n  MAX_RETRIES: \"3\"\n\nsetup:\n  - shell: \"echo Starting $PROJECT_NAME\"\n\nmap:\n  agent_template:\n    - claude: \"/process --project $PROJECT_NAME --retries $MAX_RETRIES\"\n\nreduce:\n  - shell: \"echo Completed $PROJECT_NAME workflow\"\n\nmerge:\n  commands:\n    - shell: \"echo Merging $PROJECT_NAME changes\"\n</code></pre> <p>For comprehensive MapReduce environment documentation, see MapReduce Environment Variables.</p>"},{"location":"workflow-basics/environment-configuration/#complete-example","title":"Complete Example","text":"<p>This example demonstrates multiple environment features working together.</p> <p>Source: Complete workflow in <code>workflows/environment-example.yml:1-70</code></p> <pre><code># Global environment with static, dynamic, and conditional values\nenv:\n  NODE_ENV: production\n  API_URL: https://api.example.com\n\n  WORKERS:\n    command: \"nproc 2&gt;/dev/null || echo 4\"\n    cache: true\n\n  DEPLOY_ENV:\n    condition: \"${branch} == 'main'\"\n    when_true: \"production\"\n    when_false: \"staging\"\n\n# Secrets (masked in logs)\nsecrets:\n  API_KEY: \"${env:SECRET_API_KEY}\"\n\n# Environment files\nenv_files:\n  - .env.production\n\n# Profiles\nprofiles:\n  development:\n    NODE_ENV: development\n    API_URL: http://localhost:3000\n    DEBUG: \"true\"\n\n# Commands with step-level overrides\ncommands:\n  - name: \"Build\"\n    shell: \"npm run build\"\n    env:\n      BUILD_TARGET: production\n    working_dir: ./frontend\n\n  - name: \"Deploy\"\n    shell: \"echo Deploying to $DEPLOY_ENV\"\n</code></pre>"},{"location":"workflow-basics/environment/","title":"Environment Variables","text":"<p>Prodigy provides comprehensive environment variable management for workflows, enabling parameterization, secrets management, and environment-specific configurations.</p>"},{"location":"workflow-basics/environment/#overview","title":"Overview","text":"<p>Environment variables in Prodigy allow you to:</p> <ul> <li>Define workflow-wide variables accessible in all commands</li> <li>Securely manage sensitive credentials with automatic masking</li> <li>Configure environment-specific settings using profiles</li> <li>Load variables from <code>.env</code> files</li> <li>Use dynamic and conditional variables</li> <li>Reference variables across all workflow phases</li> </ul>"},{"location":"workflow-basics/environment/#variable-precedence","title":"Variable Precedence","text":"<p>Variables can be defined in multiple locations. When the same variable is defined in multiple places, Prodigy uses this precedence order (highest to lowest):</p> <ol> <li>Profile variables - Activated with <code>--profile</code> flag</li> <li>Workflow <code>env</code> block - Defined in workflow YAML</li> <li>Environment files - Loaded from <code>.env</code> files (later files override earlier)</li> <li>Parent process environment - Inherited from shell</li> </ol> <p>This hierarchy allows you to set sensible defaults while providing runtime overrides when needed.</p> <pre><code>graph TD\n    Start[Variable Reference: $API_URL] --&gt; Profile{Profile&lt;br/&gt;variable?}\n    Profile --&gt;|Yes| UseProfile[Use profile value]\n    Profile --&gt;|No| WorkflowEnv{Workflow&lt;br/&gt;env block?}\n\n    WorkflowEnv --&gt;|Yes| UseWorkflow[Use workflow value]\n    WorkflowEnv --&gt;|No| EnvFile{Environment&lt;br/&gt;file?}\n\n    EnvFile --&gt;|Yes| UseEnvFile[Use env file value]\n    EnvFile --&gt;|No| ParentEnv{Parent&lt;br/&gt;process env?}\n\n    ParentEnv --&gt;|Yes| UseParent[Use parent value]\n    ParentEnv --&gt;|No| Error[Error: Variable not found]\n\n    UseProfile --&gt; End[Value resolved]\n    UseWorkflow --&gt; End\n    UseEnvFile --&gt; End\n    UseParent --&gt; End\n\n    style Profile fill:#e1f5ff\n    style WorkflowEnv fill:#fff3e0\n    style EnvFile fill:#f3e5f5\n    style ParentEnv fill:#e8f5e9\n    style Error fill:#ffebee</code></pre> <p>Figure: Variable resolution follows precedence from profile \u2192 workflow env \u2192 env files \u2192 parent environment.</p>"},{"location":"workflow-basics/environment/#defining-environment-variables","title":"Defining Environment Variables","text":"<p>Environment variables are defined in the <code>env</code> block at the workflow root:</p> Basic environment variables<pre><code># Source: workflows/environment-example.yml\nenv:\n  # Static variables\n  NODE_ENV: production\n  API_URL: https://api.example.com\n  PROJECT_NAME: \"my-project\"\n  VERSION: \"1.0.0\"\n\ncommands:\n  - shell: \"echo Building $PROJECT_NAME version $VERSION\"\n  - shell: \"curl $API_URL/health\"\n</code></pre>"},{"location":"workflow-basics/environment/#variable-types","title":"Variable Types","text":""},{"location":"workflow-basics/environment/#static-variables","title":"Static Variables","text":"<p>Simple key-value pairs for constant values:</p> <pre><code># Source: workflows/mapreduce-env-example.yml:8-11\nenv:\n  PROJECT_NAME: \"example-project\"\n  PROJECT_CONFIG: \"config.yml\"\n  FEATURES_PATH: \"features\"\n</code></pre>"},{"location":"workflow-basics/environment/#dynamic-variables","title":"Dynamic Variables","text":"<p>Computed from command output at workflow start:</p> <pre><code># Source: workflows/environment-example.yml:10-12\nenv:\n  WORKERS:\n    command: \"nproc 2&gt;/dev/null || echo 4\"\n    cache: true\n</code></pre> <p>Dynamic variables are evaluated once and cached for workflow duration when <code>cache: true</code>.</p>"},{"location":"workflow-basics/environment/#conditional-variables","title":"Conditional Variables","text":"<p>Values that depend on expressions:</p> <pre><code># Source: workflows/environment-example.yml:14-18\nenv:\n  DEPLOY_ENV:\n    condition: \"${branch} == 'main'\"\n    when_true: \"production\"\n    when_false: \"staging\"\n</code></pre>"},{"location":"workflow-basics/environment/#variable-interpolation","title":"Variable Interpolation","text":"<p>Prodigy supports two interpolation syntaxes for flexibility:</p> <pre><code># Source: workflows/mapreduce-env-example.yml:43-46\ncommands:\n  # Simple syntax\n  - shell: \"echo Starting $PROJECT_NAME workflow\"\n\n  # Bracketed syntax (more explicit)\n  - shell: \"echo Created output directory: ${OUTPUT_DIR}\"\n\n  # In Claude commands\n  - claude: \"/analyze --project $PROJECT_NAME --config ${PROJECT_CONFIG}\"\n</code></pre> <p>When to use bracketed syntax:</p> <ul> <li>When variable name is followed by alphanumeric characters: <code>${VAR}_suffix</code></li> <li>For clarity in complex expressions: <code>${map.results}</code></li> <li>Inside quoted strings: <code>\"Path: ${OUTPUT_DIR}/file\"</code></li> </ul> <p>Prefer Bracketed Syntax</p> <p>Using <code>${VAR}</code> instead of <code>$VAR</code> prevents ambiguity when variables are adjacent to other characters. For example, <code>$VARsuffix</code> may be interpreted as a variable named <code>VARsuffix</code>, while <code>${VAR}suffix</code> is unambiguous.</p>"},{"location":"workflow-basics/environment/#secrets-management","title":"Secrets Management","text":"<p>Secrets are automatically masked in all output, logs, and error messages to prevent credential leaks.</p>"},{"location":"workflow-basics/environment/#defining-secrets","title":"Defining Secrets","text":"<pre><code># Source: workflows/mapreduce-env-example.yml:22-25\nenv:\n  API_TOKEN:\n    secret: true\n    value: \"${GITHUB_TOKEN}\"\n</code></pre> <p>Secrets can reference environment variables from the parent process using <code>${ENV_VAR}</code> syntax.</p> <p>Common Mistake: Forgetting secret: true</p> <p>The most common mistake is defining sensitive values without marking them as secrets:</p> <pre><code># \u274c Wrong - will appear in logs\nenv:\n  API_KEY: \"sk-abc123\"\n\n# \u2705 Correct - automatically masked\nenv:\n  API_KEY:\n    secret: true\n    value: \"sk-abc123\"\n</code></pre>"},{"location":"workflow-basics/environment/#alternative-secrets-syntax","title":"Alternative Secrets Syntax","text":"<pre><code># Source: workflows/environment-example.yml:21-23\nsecrets:\n  API_KEY: \"${env:SECRET_API_KEY}\"\n</code></pre> <p>The <code>secrets</code> block is an alternative to inline <code>secret: true</code> definitions.</p>"},{"location":"workflow-basics/environment/#advanced-secret-providers","title":"Advanced: Secret Providers","text":"<p>Prodigy supports multiple secret providers for integration with external secret management systems:</p> <pre><code># Source: src/cook/environment/config.rs:99-112\nenv:\n  # Environment variable provider (default)\n  API_TOKEN:\n    secret: true              # (1)!\n    value: \"${GITHUB_TOKEN}\"  # (2)!\n\n  # File-based secrets\n  DATABASE_PASSWORD:\n    secret: true\n    provider: file            # (3)!\n    key: \"/run/secrets/db_password\"  # (4)!\n\n  # HashiCorp Vault integration\n  VAULT_TOKEN:\n    secret: true\n    provider: vault           # (5)!\n    key: \"secret/data/myapp/token\"  # (6)!\n    version: \"v2\"             # (7)!\n\n  # AWS Secrets Manager\n  AWS_SECRET:\n    secret: true\n    provider: aws             # (8)!\n    key: \"myapp/prod/api-key\"  # (9)!\n</code></pre> <ol> <li>Marks value as secret for automatic masking</li> <li>References environment variable from parent process</li> <li>File provider reads secret from filesystem</li> <li>Path to file containing the secret value</li> <li>HashiCorp Vault integration for centralized secrets</li> <li>Vault path to the secret</li> <li>KV secrets engine version (v1 or v2)</li> <li>AWS Secrets Manager integration</li> <li>Secret name/ARN in AWS Secrets Manager</li> </ol> <p>Provider Availability</p> <p>Secret provider support depends on configuration. The <code>env</code> and <code>file</code> providers are always available. Vault and AWS providers require additional setup.</p>"},{"location":"workflow-basics/environment/#automatic-masking","title":"Automatic Masking","text":"<p>Secrets are masked in:</p> <ul> <li>Command output (stdout/stderr)</li> <li>Error messages and stack traces</li> <li>Event logs and checkpoints</li> <li>Workflow summaries</li> <li>MapReduce agent logs</li> </ul> <p>Example output with masking:</p> <pre><code>$ curl -H 'Authorization: Bearer ***' https://api.example.com\n</code></pre> <pre><code>flowchart LR\n    Cmd[Execute Command] --&gt; Output[Generate Output]\n    Output --&gt; Scan{Contains&lt;br/&gt;secret value?}\n\n    Scan --&gt;|Yes| Mask[Replace with ***]\n    Scan --&gt;|No| Pass[Pass through]\n\n    Mask --&gt; Log[Write to log]\n    Pass --&gt; Log\n\n    Log --&gt; Display[Display to user]\n\n    style Cmd fill:#e1f5ff\n    style Scan fill:#fff3e0\n    style Mask fill:#ffebee\n    style Pass fill:#e8f5e9</code></pre> <p>Figure: Secret masking automatically replaces sensitive values with <code>***</code> in all output streams.</p> <p>Secret Security</p> <p>Always mark sensitive values as secrets. Without the <code>secret: true</code> flag, values will appear in logs and may be exposed.</p>"},{"location":"workflow-basics/environment/#profiles","title":"Profiles","text":"<p>Profiles enable environment-specific configurations for development, staging, and production environments.</p>"},{"location":"workflow-basics/environment/#defining-profiles","title":"Defining Profiles","text":"<pre><code># Source: workflows/mapreduce-env-example.yml:28-39\nenv:\n  DEBUG_MODE: \"false\"           # (1)!\n  TIMEOUT_SECONDS: \"300\"        # (2)!\n  OUTPUT_DIR: \"output\"          # (3)!\n\nprofiles:\n  development:\n    description: \"Development environment with debug enabled\"  # (4)!\n    DEBUG_MODE: \"true\"          # (5)!\n    TIMEOUT_SECONDS: \"60\"       # (6)!\n    OUTPUT_DIR: \"dev-output\"\n\n  production:\n    description: \"Production environment\"\n    DEBUG_MODE: \"false\"\n    TIMEOUT_SECONDS: \"300\"\n    OUTPUT_DIR: \"prod-output\"\n</code></pre> <ol> <li>Default values used when no profile is activated</li> <li>Timeout for operations in seconds</li> <li>Output directory for workflow results</li> <li>Optional description shown in help text</li> <li>Profile values override default env values</li> <li>Development uses shorter timeout for faster feedback</li> </ol>"},{"location":"workflow-basics/environment/#activating-profiles","title":"Activating Profiles","text":"<pre><code># Use default values (no profile)\nprodigy run workflow.yml\n\n# Activate development profile\nprodigy run workflow.yml --profile development\n\n# Activate production profile\nprodigy run workflow.yml --profile production\n</code></pre> <p>Profile variables override default <code>env</code> values. Variables not defined in the profile inherit default values.</p> <p>Profile Best Practice</p> <p>Use profiles to separate environment-specific configuration (development, staging, production) rather than maintaining multiple workflow files. This ensures consistency while allowing environment-specific overrides.</p>"},{"location":"workflow-basics/environment/#environment-files","title":"Environment Files","text":"<p>Load variables from <code>.env</code> format files for external configuration. Environment files support standard <code>.env</code> format and can be used for external secrets management and configuration.</p>"},{"location":"workflow-basics/environment/#defining-environment-files","title":"Defining Environment Files","text":"<pre><code># Source: workflows/environment-example.yml:26-27\nenv_files:\n  - .env.production\n  - .env.local\n</code></pre> <p>Multiple files can be specified, with later files overriding earlier ones for the same variable names.</p>"},{"location":"workflow-basics/environment/#env-file-format","title":".env File Format","text":".env.production<pre><code># Database configuration\nDATABASE_URL=postgres://localhost/mydb\nDATABASE_POOL_SIZE=10\n\n# API settings\nAPI_KEY=sk-abc123xyz\nAPI_TIMEOUT=30\n\n# Feature flags\nENABLE_CACHING=true\n</code></pre> <p>Supported Formats</p> <p>Environment files follow standard <code>.env</code> format with <code>KEY=VALUE</code> pairs. Lines starting with <code>#</code> are treated as comments. No spaces are allowed around the <code>=</code> sign.</p>"},{"location":"workflow-basics/environment/#variable-precedence-with-environment-files","title":"Variable Precedence with Environment Files","text":"<p>When variables are defined in multiple locations, Prodigy uses this precedence (highest to lowest):</p> <ol> <li>Profile variables (<code>--profile</code> flag) - Highest priority</li> <li>Workflow <code>env</code> block - Workflow-defined variables</li> <li>Environment files - Later files override earlier files</li> <li>Parent process environment - Lowest priority</li> </ol> <p>This precedence order ensures that explicit workflow configuration takes precedence over external sources, while profiles provide runtime overrides.</p> <p>Precedence Example</p> <p>Given these definitions:</p> <pre><code>env_files:\n  - .env.base        # API_URL=http://localhost\n  - .env.production  # API_URL=https://prod.api.com\n\nenv:\n  API_URL: https://staging.api.com\n\nprofiles:\n  prod:\n    API_URL: https://api.example.com\n</code></pre> <p>Resolution: - No profile: <code>https://staging.api.com</code> (workflow env) - With <code>--profile prod</code>: <code>https://api.example.com</code> (profile)</p>"},{"location":"workflow-basics/environment/#usage-in-workflow-phases","title":"Usage in Workflow Phases","text":"<p>Environment variables are available in all workflow phases:</p>"},{"location":"workflow-basics/environment/#standard-workflows","title":"Standard Workflows","text":"<pre><code># Source: workflows/environment-example.yml:42-52\ncommands:\n  - name: \"Show environment\"\n    shell: \"echo NODE_ENV=$NODE_ENV API_URL=$API_URL\"\n\n  - name: \"Build frontend\"\n    shell: \"echo 'Building with NODE_ENV='$NODE_ENV\"\n    env:\n      BUILD_TARGET: production\n      OPTIMIZE: \"true\"\n    working_dir: ./frontend\n</code></pre>"},{"location":"workflow-basics/environment/#mapreduce-setup-phase","title":"MapReduce Setup Phase","text":"<pre><code># Source: workflows/mapreduce-env-example.yml:42-49\nsetup:\n  - shell: \"echo Starting $PROJECT_NAME workflow\"\n  - shell: \"mkdir -p $OUTPUT_DIR\"\n  - shell: \"echo Created output directory: ${OUTPUT_DIR}\"\n  - shell: \"echo Debug mode: $DEBUG_MODE\"\n</code></pre>"},{"location":"workflow-basics/environment/#mapreduce-map-phase","title":"MapReduce Map Phase","text":"<p>Environment variables are available in agent templates:</p> <pre><code># Source: workflows/mapreduce-env-example.yml:56-68\nmap:\n  agent_template:\n    # In Claude commands\n    - claude: \"/process-item '${item.name}' --project $PROJECT_NAME\"\n\n    # In shell commands\n    - shell: \"echo Processing ${item.name} for $PROJECT_NAME\"\n    - shell: \"echo Output: $OUTPUT_DIR\"\n\n    # In failure handlers\n    - shell: \"timeout ${TIMEOUT_SECONDS}s ./process.sh\"\n      on_failure:\n        - claude: \"/fix-issue --max-retries $MAX_RETRIES\"\n</code></pre> <p>MapReduce Agent Isolation</p> <p>Each MapReduce agent runs in an isolated git worktree with its own execution context. Environment variables defined in the workflow are automatically inherited by all agents. Secret masking is maintained across agent boundaries to ensure credentials remain protected.</p>"},{"location":"workflow-basics/environment/#mapreduce-reduce-phase","title":"MapReduce Reduce Phase","text":"<pre><code># Source: workflows/mapreduce-env-example.yml:72-79\nreduce:\n  - shell: \"echo Aggregating results for $PROJECT_NAME\"\n  - claude: \"/summarize ${map.results} --format $REPORT_FORMAT\"\n  - shell: \"cp summary.$REPORT_FORMAT $OUTPUT_DIR/${PROJECT_NAME}-summary.$REPORT_FORMAT\"\n  - shell: \"echo Processed ${map.successful}/${map.total} items\"\n</code></pre>"},{"location":"workflow-basics/environment/#merge-phase","title":"Merge Phase","text":"<pre><code># Source: workflows/mapreduce-env-example.yml:82-93\nmerge:\n  commands:\n    - shell: \"echo Merging changes for $PROJECT_NAME\"\n    - claude: \"/validate-merge --branch ${merge.source_branch} --project $PROJECT_NAME\"\n    - shell: \"echo Merge completed for ${PROJECT_NAME}\"\n</code></pre>"},{"location":"workflow-basics/environment/#per-step-environment","title":"Per-Step Environment","text":"<p>Override or add variables for specific commands:</p> <pre><code># Source: workflows/environment-example.yml:54-60\ncommands:\n  - name: \"Run tests\"\n    shell: \"pytest tests/\"\n    env:\n      PYTHONPATH: \"./src:./tests\"\n      TEST_ENV: \"true\"\n    working_dir: ./backend\n    temporary: true  # Environment restored after this step\n</code></pre> <p>Options:</p> <ul> <li><code>temporary: true</code> - Restore environment after step completes</li> <li><code>clear_env: true</code> - Clear all inherited variables, use only step-specific ones</li> </ul>"},{"location":"workflow-basics/environment/#best-practices","title":"Best Practices","text":"<p>Use Secrets for Sensitive Data</p> <p>Always mark API keys, tokens, passwords, and credentials as secrets to enable automatic masking.</p> <p>Parameterize Project-Specific Values</p> <p>Use environment variables instead of hardcoding paths, URLs, and configuration values. This improves portability and maintainability.</p> <p>Document Required Variables</p> <p>Add comments in workflow files documenting expected variables and their purposes.</p> <p>Use Profiles for Environments</p> <p>Separate development, staging, and production configurations using profiles rather than maintaining separate workflow files.</p> <p>Prefer Bracketed Syntax</p> <p>Use <code>${VAR}</code> instead of <code>$VAR</code> for explicitness and to avoid ambiguity in complex expressions.</p>"},{"location":"workflow-basics/environment/#common-patterns","title":"Common Patterns","text":""},{"location":"workflow-basics/environment/#project-configuration","title":"Project Configuration","text":"<pre><code>env:\n  PROJECT_NAME: \"my-app\"\n  VERSION: \"1.2.3\"\n  BUILD_DIR: \"dist\"\n  RELEASE_CHANNEL: \"stable\"\n</code></pre>"},{"location":"workflow-basics/environment/#api-integration","title":"API Integration","text":"<pre><code>env:\n  API_URL: \"https://api.example.com\"\n  API_TIMEOUT: \"30\"\n\nsecrets:\n  API_KEY: \"${env:EXTERNAL_API_KEY}\"\n\ncommands:\n  - shell: \"curl -H 'Authorization: Bearer $API_KEY' $API_URL/data\"\n</code></pre>"},{"location":"workflow-basics/environment/#multi-environment-configuration","title":"Multi-Environment Configuration","text":"<pre><code>env:\n  APP_ENV: \"development\"\n  LOG_LEVEL: \"debug\"\n\nprofiles:\n  staging:\n    APP_ENV: \"staging\"\n    LOG_LEVEL: \"info\"\n\n  production:\n    APP_ENV: \"production\"\n    LOG_LEVEL: \"warn\"\n</code></pre>"},{"location":"workflow-basics/environment/#feature-flags","title":"Feature Flags","text":"<pre><code>env:\n  ENABLE_CACHING: \"true\"\n  ENABLE_ANALYTICS: \"false\"\n  MAX_WORKERS: \"4\"\n\ncommands:\n  - shell: |\n      if [ \"$ENABLE_CACHING\" = \"true\" ]; then\n        echo \"Caching enabled\"\n      fi\n</code></pre>"},{"location":"workflow-basics/environment/#troubleshooting","title":"Troubleshooting","text":""},{"location":"workflow-basics/environment/#variable-not-found","title":"Variable Not Found","text":"<p>Symptom: <code>$VAR</code> appears literally in output or command fails with \"command not found\"</p> <p>Cause: Variable not defined or incorrect interpolation syntax</p> <p>Solution:</p> <ol> <li>Verify variable is defined in <code>env</code> block or environment file</li> <li>Check spelling and case (variable names are case-sensitive)</li> <li>Ensure proper interpolation syntax (<code>$VAR</code> or <code>${VAR}</code>)</li> <li>Use <code>--profile</code> flag if variable is profile-specific</li> </ol>"},{"location":"workflow-basics/environment/#secret-not-masked","title":"Secret Not Masked","text":"<p>Symptom: Sensitive value appears in logs or output</p> <p>Cause: Variable not marked as secret</p> <p>Solution:</p> <pre><code># Before (not masked)\nenv:\n  API_KEY: \"sk-abc123\"\n\n# After (masked)\nenv:\n  API_KEY:\n    secret: true\n    value: \"sk-abc123\"\n</code></pre>"},{"location":"workflow-basics/environment/#profile-variables-not-applied","title":"Profile Variables Not Applied","text":"<p>Symptom: Default values used instead of profile values</p> <p>Cause: Profile not activated with <code>--profile</code> flag</p> <p>Solution:</p> <pre><code># Activate profile\nprodigy run workflow.yml --profile production\n</code></pre>"},{"location":"workflow-basics/environment/#environment-file-not-loaded","title":"Environment File Not Loaded","text":"<p>Symptom: Variables from <code>.env</code> file not available</p> <p>Cause: File path incorrect or file doesn't exist</p> <p>Solution:</p> <ol> <li>Verify file path is relative to workflow file location</li> <li>Check file exists: <code>ls .env.production</code></li> <li>Verify file syntax (KEY=VALUE format, no spaces around <code>=</code>)</li> </ol>"},{"location":"workflow-basics/environment/#see-also","title":"See Also","text":"<ul> <li>Workflow Structure - Overall workflow configuration</li> <li>Variables and Interpolation - Advanced variable interpolation</li> <li>Command Types - Using variables in different command types</li> </ul>"},{"location":"workflow-basics/error-handling/","title":"Error Handling","text":"<p>Prodigy provides comprehensive error handling at both the workflow level (for MapReduce jobs) and the command level (for individual workflow steps). This chapter covers the practical features available for handling failures gracefully.</p>"},{"location":"workflow-basics/error-handling/#command-level-error-handling","title":"Command-Level Error Handling","text":"<p>Command-level error handling allows you to specify what happens when a single workflow step fails. Use the <code>on_failure</code> configuration to define recovery, cleanup, or fallback strategies.</p> <pre><code>flowchart TD\n    Start[Execute Command] --&gt; Success{Success?}\n    Success --&gt;|Yes| NextStep[Next Command]\n    Success --&gt;|No| Handler{on_failure&lt;br/&gt;defined?}\n\n    Handler --&gt;|No| FailWorkflow[Fail Workflow]\n    Handler --&gt;|Yes| RunHandler[Execute Handler]\n\n    RunHandler --&gt; HandlerSuccess{Handler&lt;br/&gt;Success?}\n    HandlerSuccess --&gt;|No| HandlerFatal{handler_failure_fatal?}\n    HandlerFatal --&gt;|Yes| FailWorkflow\n    HandlerFatal --&gt;|No| CheckRetry\n\n    HandlerSuccess --&gt;|Yes| CheckRetry{max_attempts&lt;br/&gt;&gt; 1?}\n    CheckRetry --&gt;|Yes| RetryCmd[Retry Original Command]\n    RetryCmd --&gt; AttemptCheck{Attempts&lt;br/&gt;exhausted?}\n    AttemptCheck --&gt;|No| Start\n    AttemptCheck --&gt;|Yes| FinalCheck{fail_workflow?}\n\n    CheckRetry --&gt;|No| FinalCheck\n    FinalCheck --&gt;|Yes| FailWorkflow\n    FinalCheck --&gt;|No| NextStep\n\n    style Success fill:#e8f5e9\n    style Handler fill:#fff3e0\n    style HandlerSuccess fill:#e1f5ff\n    style FailWorkflow fill:#ffebee\n    style NextStep fill:#e8f5e9</code></pre> <p>Figure: Command-level error handling flow showing handler execution, retry logic, and failure propagation.</p>"},{"location":"workflow-basics/error-handling/#simple-forms","title":"Simple Forms","text":"<p>For basic error handling, use the simplest form that meets your needs:</p> <pre><code># Ignore errors - don't fail the workflow\n- shell: \"optional-cleanup.sh\"\n  on_failure: true\n\n# Single recovery command (shell or claude)\n- shell: \"npm install\"\n  on_failure: \"npm cache clean --force\"\n\n- shell: \"cargo clippy\"\n  on_failure: \"/fix-warnings\"\n\n# Multiple recovery commands\n- shell: \"build-project\"\n  on_failure:\n    - \"cleanup-artifacts\"\n    - \"/diagnose-build-errors\"\n    - \"retry-build\"\n</code></pre>"},{"location":"workflow-basics/error-handling/#advanced-configuration","title":"Advanced Configuration","text":"<p>For more control over error handling behavior:</p> <p>Automatic Retry Behavior</p> <p>Setting <code>max_attempts &gt; 1</code> automatically enables retry of the original command after the handler runs. You don't need a separate <code>retry_original</code> flag.</p> <pre><code>- shell: \"cargo clippy\"\n  on_failure:\n    claude: \"/fix-warnings ${shell.output}\"  # (1)!\n    fail_workflow: false     # (2)!\n    max_attempts: 3          # (3)!\n\n1. Handler receives command output for analysis\n2. Continue workflow even if handler fails\n3. Retry original command up to 3 times (auto-retry when &gt; 1)\n</code></pre> <p>Available Fields: - <code>shell</code> - Shell command to run on failure - <code>claude</code> - Claude command to run on failure - <code>fail_workflow</code> - Whether to fail the entire workflow (default: <code>false</code>) - <code>max_attempts</code> - Maximum retry attempts for the original command (default: <code>1</code>) - <code>max_retries</code> - Alternative name for <code>max_attempts</code> (both are supported for backward compatibility)</p> <p>Source: FailureHandlerConfig struct in src/cook/workflow/on_failure.rs:26</p> <p>Notes: - When <code>max_attempts &gt; 1</code>, Prodigy automatically retries the original command after running the failure handler (the deprecated <code>retry_original</code> flag is no longer needed) - Retry behavior is now controlled by the <code>max_attempts</code>/<code>max_retries</code> value, not a separate flag - You can specify both <code>shell</code> and <code>claude</code> commands - they will execute in sequence - By default, having a handler means the workflow continues even if the step fails</p> <p>Migration from <code>retry_original</code>: Previously you used <code>retry_original: true</code> with <code>max_retries: 3</code>. Now just use <code>max_attempts: 3</code> (retry is implicit when &gt; 1). Both <code>max_attempts</code> and <code>max_retries</code> are supported as aliases for backward compatibility.</p>"},{"location":"workflow-basics/error-handling/#detailed-handler-configuration","title":"Detailed Handler Configuration","text":"<p>For complex error handling scenarios with multiple commands and fine-grained control:</p> <pre><code>- shell: \"deploy-production\"\n  on_failure:\n    strategy: recovery        # Options: recovery, fallback, cleanup, custom\n    timeout: 300             # Handler timeout in seconds\n    handler_failure_fatal: true  # Fail workflow if handler fails\n    fail_workflow: false     # Don't fail workflow if step fails\n    capture:                 # Capture handler output to variables\n      error_log: \"handler_output\"\n      rollback_status: \"rollback_result\"\n    commands:\n      - shell: \"rollback-deployment\"\n        continue_on_error: true\n      - claude: \"/analyze-deployment-failure\"\n      - shell: \"notify-team\"\n</code></pre> <p>Handler Configuration Fields: - <code>strategy</code> - Handler strategy (recovery, fallback, cleanup, custom) - <code>timeout</code> - Handler timeout in seconds - <code>handler_failure_fatal</code> - Fail workflow if handler fails - <code>fail_workflow</code> - Whether to fail the entire workflow - <code>capture</code> - Map of variable names to capture from handler output (e.g., <code>error_log: \"handler_output\"</code>). Note: capture applies to the handler's combined output, not individual commands. - <code>commands</code> - List of handler commands to execute</p> <p>Handler Strategies: - <code>recovery</code> - Try to fix the problem and retry (default) - <code>fallback</code> - Use an alternative approach - <code>cleanup</code> - Clean up resources - <code>custom</code> - Custom handler logic</p> <p>Handler Command Fields: - <code>shell</code> or <code>claude</code> - The command to execute - <code>continue_on_error</code> - Continue to next handler command even if this fails</p> <p>Simpler Alternative: For basic cases, you can use the Advanced format shown earlier instead of the detailed handler configuration. The Advanced format allows <code>shell</code> and <code>claude</code> fields directly without wrapping in a <code>commands</code> array: <code>on_failure: { shell: \"command\", fail_workflow: false, max_attempts: 3 }</code>.</p>"},{"location":"workflow-basics/error-handling/#success-handling","title":"Success Handling","text":"<p>Execute commands when a step succeeds. The <code>on_success</code> field accepts a full WorkflowStep configuration with all available fields.</p> <p>Simple Form: <pre><code>- shell: \"deploy-staging\"\n  on_success:\n    shell: \"notify-success\"\n    claude: \"/update-deployment-docs\"\n</code></pre></p> <p>Advanced Form with Full WorkflowStep Configuration: <pre><code>- shell: \"build-production\"\n  on_success:\n    claude: \"/update-build-metrics\"\n    timeout: 60              # Success handler timeout\n    capture: \"metrics\"       # Capture output to variable\n    working_dir: \"dist\"      # Run in specific directory\n    when: \"${build.target} == 'release'\"  # Conditional execution\n</code></pre></p> <p>Note: The <code>on_success</code> handler supports all WorkflowStep fields including <code>timeout</code>, <code>capture</code>, <code>working_dir</code>, <code>when</code>, and nested <code>on_failure</code> handlers.</p> <p>Common Use Cases:</p> <p>Success handlers are useful for post-processing actions that should only occur when a step completes successfully: - Notifications: Send success notifications to teams via Slack, email, or other channels - Metrics Updates: Update deployment metrics, dashboard statistics, or monitoring systems - Downstream Workflows: Trigger dependent workflows or pipelines - Artifact Archiving: Archive build artifacts, logs, or generated files for later use - External System Updates: Update issue trackers, deployment records, or configuration management systems</p> <p>The handler receives access to step outputs via the <code>capture</code> field, allowing you to process results or pass data to subsequent steps. For example, using <code>capture: \"metrics\"</code> creates a <code>${metrics}</code> variable containing the handler output, which can be used in later workflow steps for processing or decision-making.</p>"},{"location":"workflow-basics/error-handling/#commit-requirements","title":"Commit Requirements","text":"<p>Specify whether a workflow step must create a git commit:</p> <pre><code>- claude: \"/implement-feature\"\n  commit_required: true   # Fail if no commit is made\n</code></pre> <p>This is useful for ensuring that Claude commands that are expected to make code changes actually do so.</p>"},{"location":"workflow-basics/error-handling/#workflow-level-error-policy-mapreduce","title":"Workflow-Level Error Policy (MapReduce)","text":"<p>For MapReduce workflows, you can configure workflow-level error policies that control how the entire job responds to failures. This is separate from command-level error handling and only applies to MapReduce mode.</p>"},{"location":"workflow-basics/error-handling/#basic-configuration","title":"Basic Configuration","text":"<pre><code># Source: src/cook/workflow/error_policy.rs:13-31\n# Example: workflows/mkdocs-drift.yml:89-93\nname: process-items\nmode: mapreduce\n\nerror_policy:\n  # What to do when a work item fails\n  on_item_failure: dlq      # Options: dlq, retry, skip, stop, custom:&lt;handler_name&gt;\n\n  # Continue processing after failures\n  continue_on_failure: true\n\n  # Stop after this many failures\n  max_failures: 10\n\n  # Stop if failure rate exceeds threshold (0.0 to 1.0)\n  failure_threshold: 0.2    # Stop if 20% of items fail\n\n  # How to report errors\n  error_collection: aggregate  # Options: aggregate, immediate, batched\n</code></pre> <p>Item Failure Actions: - <code>dlq</code> - Send failed items to Dead Letter Queue for later retry (default) - <code>retry</code> - Retry the item immediately with backoff (if retry_config is set) - <code>skip</code> - Skip the failed item and continue - <code>stop</code> - Stop the entire workflow on first failure - <code>custom:&lt;name&gt;</code> - Use a custom failure handler (not yet implemented)</p> <p>Error Collection Strategies: - <code>aggregate</code> - Collect all errors and report at the end (default) - <code>immediate</code> - Report errors as they occur - <code>batched</code> - Report errors in batches of N items (e.g., <code>batched: { size: 10 }</code>)</p> <p>Source: ErrorCollectionStrategy enum in src/cook/workflow/error_policy.rs:33-44</p>"},{"location":"workflow-basics/error-handling/#circuit-breaker","title":"Circuit Breaker","text":"<p>Prevent cascading failures by opening a circuit after consecutive failures:</p> <pre><code># Source: src/cook/workflow/error_policy.rs:48-56 (CircuitBreakerConfig struct)\nerror_policy:\n  circuit_breaker:\n    failure_threshold: 5      # Open circuit after 5 consecutive failures\n    success_threshold: 2      # Close circuit after 2 successes\n    timeout: 30s             # Duration in humantime format (e.g., 30s, 1m, 500ms)\n    half_open_requests: 3    # Test requests in half-open state\n</code></pre> <p>Note: The <code>timeout</code> field uses humantime format supporting <code>1s</code>, <code>100ms</code>, <code>2m</code>, <code>30s</code> for duration parsing.</p>"},{"location":"workflow-basics/error-handling/#circuit-breaker-states","title":"Circuit Breaker States","text":"<p>The circuit breaker operates in three states to protect against cascading failures:</p> <pre><code>stateDiagram-v2\n    [*] --&gt; Closed: Initial State\n\n    Closed --&gt; Open: failure_threshold&lt;br/&gt;consecutive failures\n    Open --&gt; HalfOpen: timeout expires\n    HalfOpen --&gt; Closed: success_threshold&lt;br/&gt;successes\n    HalfOpen --&gt; Open: Any test request fails\n\n    note right of Closed\n        Normal Operation\n        Track failures\n        Process all requests\n    end note\n\n    note right of Open\n        Rejecting Requests\n        Fail fast\n        Wait for timeout\n    end note\n\n    note right of HalfOpen\n        Testing Recovery\n        Limited test requests\n        Verify health\n    end note</code></pre> <p>Figure: Circuit breaker state transitions showing how the breaker protects against cascading failures.</p> <p>Circuit Breaker Opens After Consecutive Failures</p> <p>The circuit opens after <code>failure_threshold</code> consecutive failures, not total failures. A single success resets the failure counter. This means the circuit breaker is designed to detect sustained problems, not intermittent errors.</p> <p>State Behaviors:</p> <ol> <li>Closed (Normal Operation)</li> <li>All requests are processed normally</li> <li>Failures are tracked; consecutive failures increment the failure counter</li> <li> <p>Transitions to Open after <code>failure_threshold</code> consecutive failures</p> </li> <li> <p>Open (Rejecting Requests)</p> </li> <li>All requests are immediately rejected without attempting execution</li> <li>Prevents further load on a failing dependency</li> <li> <p>Transitions to HalfOpen after <code>timeout</code> duration expires</p> </li> <li> <p>HalfOpen (Testing Recovery)</p> </li> <li>Allows a limited number of test requests (<code>half_open_requests</code>) to verify recovery</li> <li>If test requests succeed (reaching <code>success_threshold</code>), transitions back to Closed</li> <li>If any test request fails, transitions back to Open and resets the timeout</li> </ol> <p>Monitoring Circuit Breaker State:</p> <p>To check the circuit breaker state during MapReduce execution, monitor the event logs:</p> <pre><code># View circuit breaker events for a job\nprodigy events ls --job-id &lt;job_id&gt; | grep -i circuit\n\n# Follow circuit state changes in real-time\nprodigy events follow --job-id &lt;job_id&gt;\n</code></pre> <p>Circuit breaker state transitions are logged as <code>CircuitOpen</code> and <code>CircuitClosed</code> events, allowing you to track when the circuit opens due to failures and when it recovers.</p> <p>Note: The <code>events</code> CLI commands are defined but currently have stub implementations. Event data is stored in <code>~/.prodigy/events/{repo_name}/{job_id}/</code> and can be inspected directly as JSONL files.</p>"},{"location":"workflow-basics/error-handling/#circuit-breaker-example","title":"Circuit Breaker Example","text":"<pre><code>error_policy:\n  circuit_breaker:\n    failure_threshold: 3\n    success_threshold: 2\n    timeout: \"30s\"\n    half_open_requests: 1\n\n- shell: \"curl https://api.example.com/health\"\n  # After 3 failures, circuit opens\n  # After 30s, allows 1 test request\n  # After 2 successes, circuit closes\n</code></pre>"},{"location":"workflow-basics/error-handling/#retry-configuration-with-backoff","title":"Retry Configuration with Backoff","text":"<p>Configure automatic retry behavior for failed items:</p> <p>Choosing a Backoff Strategy</p> <ul> <li>Exponential: Best for external APIs and network calls (prevents overwhelming services)</li> <li>Linear: Good for predictable resource constraints</li> <li>Fixed: Use for operations with consistent retry cost</li> <li>Fibonacci: Balanced approach between linear and exponential</li> </ul> <pre><code># Source: src/cook/workflow/error_policy.rs:92-99, 108-120\nerror_policy:\n  on_item_failure: retry    # (1)!\n  retry_config:\n    max_attempts: 3         # (2)!\n    backoff:\n      exponential:\n        initial: 1s         # (3)!\n        multiplier: 2       # (4)!\n\n1. Use retry strategy instead of sending to DLQ\n2. Total attempts including the original (3 means 1 original + 2 retries)\n3. Starting delay before first retry\n4. Multiply delay by this factor for each retry (1s, 2s, 4s...)\n</code></pre> <p>Backoff Strategy Options:</p> <p>The backoff strategy is configured using enum variant syntax. Each variant has specific fields defined by the <code>BackoffStrategy</code> enum in <code>src/cook/workflow/error_policy.rs:108-120</code>.</p> <pre><code># Source: src/cook/workflow/error_policy.rs:110\n# Fixed delay between retries\n# Always waits the same duration\nbackoff:\n  fixed:\n    delay: 1s\n\n# Source: src/cook/workflow/error_policy.rs:112-115\n# Linear increase in delay\n# Calculates: delay = initial + (retry_count * increment)\n# Example with initial=1s, increment=500ms:\n#   Retry 1: 1s + (1 * 500ms) = 1.5s\n#   Retry 2: 1s + (2 * 500ms) = 2s\n#   Retry 3: 1s + (3 * 500ms) = 2.5s\nbackoff:\n  linear:\n    initial: 1s\n    increment: 500ms\n\n# Source: src/cook/workflow/error_policy.rs:117\n# Exponential backoff (recommended)\n# Calculates: delay = initial * (multiplier ^ retry_count)\n# Example: 1s, 2s, 4s, 8s...\nbackoff:\n  exponential:\n    initial: 1s\n    multiplier: 2\n\n# Source: src/cook/workflow/error_policy.rs:119\n# Fibonacci sequence delays\n# Calculates: delay = initial * fibonacci(retry_count)\n# Example: 1s, 1s, 2s, 3s, 5s...\nbackoff:\n  fibonacci:\n    initial: 1s\n</code></pre> <p>Important: All duration values use humantime format (e.g., <code>1s</code>, <code>100ms</code>, <code>2m</code>, <code>30s</code>), not milliseconds.</p> <p>Note: All duration values use humantime format (e.g., <code>1s</code>, <code>100ms</code>, <code>2m</code>, <code>30s</code>) for consistency. This applies to both BackoffStrategy delays and CircuitBreakerConfig timeout.</p>"},{"location":"workflow-basics/error-handling/#advanced-retry-configuration","title":"Advanced Retry Configuration","text":"<p>Prodigy has two retry configuration systems with different capabilities:</p> <p>1. Workflow-Level RetryConfig (<code>src/cook/workflow/error_policy.rs:92-99</code>)    - Used in MapReduce <code>error_policy.retry_config</code>    - Fields: <code>max_attempts</code>, <code>backoff</code> (BackoffStrategy enum)    - Simpler configuration for workflow-level retries</p> <p>2. Command-Level RetryConfig (<code>src/cook/retry_v2.rs:16-52</code>)    - Used for individual command retry logic    - Additional fields: <code>jitter</code>, <code>max_delay</code>, <code>retry_budget</code>, <code>retry_on</code>    - More advanced features for fine-grained control</p> <p>Jitter (Command-Level Only):</p> <p>Add randomization to retry delays to prevent thundering herd problems. This feature is available in <code>retry_v2.rs</code> RetryConfig but not yet in workflow-level error_policy RetryConfig.</p> <pre><code># Source: src/cook/retry_v2.rs:33-39\n# Note: This syntax is for command-level retry configuration\nretry_config:\n  attempts: 5              # Max retry attempts\n  initial_delay: 1s        # Starting delay\n  max_delay: 60s          # Cap on delay (prevents unbounded growth)\n  jitter: true            # Enable jitter\n  jitter_factor: 0.1      # Jitter randomization (0.0 to 1.0)\n  backoff: exponential    # Backoff strategy type\n</code></pre> <p>Workflow-Level vs Command-Level Retry</p> <p>The workflow-level <code>error_policy.retry_config</code> (used in MapReduce) does not currently support <code>jitter</code>, <code>max_delay</code>, or <code>retry_budget</code> fields. These advanced features are only available in the command-level retry configuration (<code>retry_v2.rs</code>).</p>"},{"location":"workflow-basics/error-handling/#error-metrics","title":"Error Metrics","text":"<p>Prodigy automatically tracks error metrics for MapReduce jobs using the <code>ErrorMetrics</code> structure:</p> <p>Available Fields: - <code>total_items</code> - Total number of work items processed - <code>successful</code> - Number of items that completed successfully - <code>failed</code> - Number of items that failed - <code>skipped</code> - Number of items that were skipped - <code>failure_rate</code> - Percentage of failures (0.0 to 1.0) - <code>error_types</code> - Map of error types to their frequency counts - <code>failure_patterns</code> - Detected recurring error patterns with suggested remediation</p> <p>Source: ErrorMetrics struct in src/cook/workflow/error_policy.rs:196</p> <p>Accessing Metrics:</p> <p>Access metrics during execution or after completion to understand job health:</p> <pre><code># In your reduce phase\nreduce:\n  - shell: \"echo 'Processed ${map.successful}/${map.total} items'\"\n  - shell: \"echo 'Failure rate: ${map.failure_rate}'\"\n</code></pre> <p>You can also access metrics programmatically via the Prodigy API or through CLI commands like <code>prodigy events</code> to view detailed error statistics.</p> <p>Pattern Detection:</p> <p>Prodigy automatically detects recurring error patterns when an error type occurs 3 or more times. The following error types receive specific remediation suggestions in the <code>failure_patterns</code> field:</p> <ul> <li>Timeout errors \u2192 \"Consider increasing agent_timeout_secs in timeout_config\"</li> <li>Network errors \u2192 \"Check network connectivity and retry settings\"</li> <li>Permission errors \u2192 \"Verify file permissions and access rights\"</li> </ul> <p>All other error types receive a generic suggestion: \"Review error logs for more details.\"</p> <p>These suggestions help diagnose and resolve systemic issues in MapReduce jobs.</p> <p>Note: Only the three error types listed above receive specific remediation suggestions. All other error types (such as compilation errors, runtime panics, or custom application errors) receive the generic \"Review error logs\" suggestion.</p> <p>Source: Pattern detection logic in src/cook/workflow/error_policy.rs:478-489</p>"},{"location":"workflow-basics/error-handling/#dead-letter-queue-dlq","title":"Dead Letter Queue (DLQ)","text":"<p>The Dead Letter Queue stores failed work items from MapReduce jobs for later retry or analysis. This is only available for MapReduce workflows, not regular workflows.</p>"},{"location":"workflow-basics/error-handling/#sending-items-to-dlq","title":"Sending Items to DLQ","text":"<p>Configure your MapReduce workflow to use DLQ:</p> <pre><code>mode: mapreduce\nerror_policy:\n  on_item_failure: dlq\n</code></pre> <p>Failed items are automatically sent to the DLQ with: - Original work item data - Failure reason and error message - Timestamp of failure - Attempt history - JSON log location for debugging</p>"},{"location":"workflow-basics/error-handling/#retrying-failed-items","title":"Retrying Failed Items","text":"<p>Use the CLI to retry failed items:</p> <pre><code># Retry all failed items for a job\nprodigy dlq retry &lt;job_id&gt;\n\n# Retry with custom parallelism (default: 5)\nprodigy dlq retry &lt;job_id&gt; --max-parallel 10\n\n# Dry run to see what would be retried\nprodigy dlq retry &lt;job_id&gt; --dry-run\n</code></pre> <p>DLQ Retry Features: - Streams items to avoid memory issues with large queues - Respects original workflow's max_parallel setting (unless overridden) - Preserves correlation IDs for tracking - Updates DLQ state (removes successful, keeps failed) - Supports interruption and resumption - Shared across worktrees for centralized failure tracking</p>"},{"location":"workflow-basics/error-handling/#view-dlq-contents","title":"View DLQ Contents","text":"<pre><code># Show failed items\nprodigy dlq show &lt;job_id&gt;\n\n# Get JSON format\nprodigy dlq show &lt;job_id&gt; --format json\n</code></pre>"},{"location":"workflow-basics/error-handling/#dlq-storage","title":"DLQ Storage","text":"<p>DLQ data is stored in: <pre><code>~/.prodigy/dlq/{repo_name}/{job_id}/\n</code></pre></p> <p>This centralized storage allows multiple worktrees to share the same DLQ.</p>"},{"location":"workflow-basics/error-handling/#best-practices","title":"Best Practices","text":""},{"location":"workflow-basics/error-handling/#choosing-the-right-error-handling-level","title":"Choosing the Right Error Handling Level","text":"<p>Understanding when to use command-level versus workflow-level error handling is crucial for building robust workflows.</p> <p>Two Levels of Error Handling</p> <p>Prodigy provides error handling at two distinct levels, and you can use both in the same workflow for defense in depth. Command-level handlers respond to specific step failures, while workflow-level policies apply consistent rules across all MapReduce items.</p> Aspect Command-Level (<code>on_failure</code>) Workflow-Level (<code>error_policy</code>) Scope Single workflow step Entire MapReduce job Availability All workflow modes MapReduce mode only Use Case Step-specific recovery logic Consistent handling across all items Retry Control Per-command retry with <code>max_attempts</code> Per-item retry with backoff strategies Failure Action Custom handler commands DLQ, retry, skip, or stop Circuit Breaker Not available Available with configurable thresholds Best For Targeted recovery, cleanup, notifications Batch processing, rate limiting, cascading failure prevention"},{"location":"workflow-basics/error-handling/#when-to-use-command-level-error-handling","title":"When to Use Command-Level Error Handling","text":"<ul> <li>Recovery: Use <code>on_failure</code> to fix issues and retry (e.g., clearing cache before reinstalling)</li> <li>Cleanup: Use <code>strategy: cleanup</code> to clean up resources after failures</li> <li>Fallback: Use <code>strategy: fallback</code> for alternative approaches</li> <li>Notifications: Use handler commands to notify teams of failures</li> <li>Step-Specific Logic: When different steps need different error handling strategies</li> </ul>"},{"location":"workflow-basics/error-handling/#when-to-use-workflow-level-error-policy","title":"When to Use Workflow-Level Error Policy","text":"<ul> <li>MapReduce jobs: Use error_policy for consistent failure handling across all work items</li> <li>Failure thresholds: Use max_failures or failure_threshold to prevent runaway jobs</li> <li>Circuit breakers: Use when external dependencies might fail cascading</li> <li>DLQ: Use for large batch jobs where you want to retry failures separately</li> <li>Rate Limiting: Use backoff strategies to avoid overwhelming external services</li> <li>Batch Processing: When processing hundreds or thousands of items with similar error patterns</li> </ul>"},{"location":"workflow-basics/error-handling/#error-information-available","title":"Error Information Available","text":"<p>When a command fails, you can access error information in handler commands:</p> <pre><code>- shell: \"risky-command\"\n  on_failure:\n    claude: \"/analyze-error ${shell.output}\"\n</code></pre> <p>The <code>${shell.output}</code> variable contains the command's stdout/stderr output.</p>"},{"location":"workflow-basics/error-handling/#common-patterns","title":"Common Patterns","text":"<p>Cleanup and Retry</p> <p>This pattern is useful when failures are caused by corrupted caches or stale state:</p> <pre><code>- shell: \"npm install\"\n  on_failure:\n    - \"npm cache clean --force\"    # (1)!\n    - \"rm -rf node_modules\"         # (2)!\n    - \"npm install\"                 # (3)!\n\n1. Clean npm cache to remove corrupted entries\n2. Remove node_modules to ensure clean state\n3. Retry installation from scratch\n</code></pre> <p>Conditional Recovery: <pre><code>- shell: \"cargo test\"\n  on_failure:\n    claude: \"/fix-failing-tests\"\n  max_attempts: 3\n  fail_workflow: false\n</code></pre></p> <p>Critical Step with Notification: <pre><code>- shell: \"deploy-production\"\n  on_failure:\n    commands:\n      - shell: \"rollback-deployment\"\n      - shell: \"notify-team 'Deployment failed'\"\n    fail_workflow: true   # Still fail workflow after cleanup\n</code></pre></p> <p>Resilient API Integration: <pre><code>error_policy:\n  retry_config:\n    max_attempts: 5\n    backoff:\n      exponential:\n        initial: \"2s\"\n        multiplier: 2.0\n        max_delay: \"60s\"\n    jitter: true\n\n  circuit_breaker:\n    failure_threshold: 3\n    success_threshold: 2\n    timeout: \"30s\"\n\n- shell: \"curl -f https://api.example.com/data\"\n  on_failure:\n    shell: \"echo 'API unavailable, will retry with backoff'\"\n</code></pre></p> <p>MapReduce with DLQ: <pre><code>mode: mapreduce\n\nerror_policy:\n  on_item_failure: dlq\n  max_failures: 10\n  failure_threshold: 0.1  # Stop at 10% failure rate\n\nmap:\n  input: \"items.json\"\n  json_path: \"$[*]\"\n  agent_template:\n    - claude: \"/process ${item}\"\n      timeout: 300\n      on_failure:\n        shell: \"echo 'Item ${item.id} failed, sent to DLQ'\"\n</code></pre></p> <p>Progressive Error Handling: <pre><code>- shell: \"cargo test\"\n  on_failure:\n    - claude: \"/analyze-test-failure ${shell.stderr}\"\n    - shell: \"cargo clean\"\n    - shell: \"cargo test\"  # Retry after clean\n      on_failure:\n        - claude: \"/deep-analysis ${shell.stderr}\"\n        - shell: \"notify-team.sh 'Tests still failing after retry'\"\n</code></pre></p> <p>Combined Error Handling Strategies (MapReduce):</p> <p>For complex MapReduce workflows, combine multiple error handling features:</p> <p>Defense in Depth: Multi-Layer Error Handling</p> <p>This example demonstrates how to combine command-level and workflow-level error handling for maximum resilience:</p> <pre><code># Process API endpoints with comprehensive error handling\nmode: mapreduce\nerror_policy:\n  on_item_failure: retry          # (1)!\n  continue_on_failure: true       # (2)!\n  max_failures: 50                # (3)!\n  failure_threshold: 0.15         # (4)!\n\n  # Retry with exponential backoff\n  retry_config:\n    max_attempts: 3               # (5)!\n    backoff:\n      type: exponential\n      initial: 2s\n      multiplier: 2               # (6)!\n\n  # Protect against cascading failures\n  circuit_breaker:\n    failure_threshold: 10         # (7)!\n    success_threshold: 3\n    timeout: 60s\n    half_open_requests: 5\n\n  # Report errors in batches of 10\n  error_collection:\n    batched:\n      size: 10                    # (8)!\n\nmap:\n  agent_template:\n    - claude: \"/process-endpoint ${item.path}\"\n      on_failure:\n        # Item-level recovery before workflow-level retry\n        claude: \"/diagnose-api-error ${shell.output}\"\n        max_attempts: 2           # (9)!\n\n1. Try immediate retry first (workflow-level)\n2. Don't stop entire job on failures\n3. Stop if 50 total failures occur\n4. Stop if 15% of items fail (runaway protection)\n5. Retry each failed item up to 3 times\n6. Delays: 2s, 4s, 8s between retries\n7. Open circuit after 10 consecutive failures\n8. Report errors in batches to reduce noise\n9. Item-level handler can retry twice before workflow-level retry\n</code></pre> <p>This configuration provides multiple layers of protection: 1. Item-level error handlers for immediate recovery attempts 2. Automatic retry with exponential backoff for transient failures 3. Circuit breaker to prevent overwhelming failing dependencies 4. Failure thresholds to stop runaway jobs early 5. Batched error reporting to reduce noise</p>"},{"location":"workflow-basics/error-handling/#see-also","title":"See Also","text":"<ul> <li>Conditional Execution - Using conditions with error handlers</li> <li>Dead Letter Queue - DLQ details and retry</li> <li>MapReduce Workflows - Error handling at scale</li> <li>Command Types - Commands supporting error handlers</li> </ul>"},{"location":"workflow-basics/full-workflow-structure/","title":"Full Workflow Structure","text":""},{"location":"workflow-basics/full-workflow-structure/#full-workflow-structure","title":"Full Workflow Structure","text":"<p>For more complex workflows, use the full format with explicit configuration:</p> <pre><code># Full format with environment and merge configuration\ncommands:\n  - shell: \"cargo build\"\n  - claude: \"/prodigy-test\"\n\n# Global environment variables (available to all commands)\nenv:\n  NODE_ENV: production\n  API_URL: https://api.example.com\n\n# Secret environment variables (masked in logs)\nsecrets:\n  API_KEY: \"${env:SECRET_API_KEY}\"\n\n# Environment files to load (.env format)\nenv_files:\n  - .env.production\n\n# Environment profiles (switch contexts easily)\nprofiles:\n  development:\n    NODE_ENV: development\n    DEBUG: \"true\"\n\n# Custom merge workflow (for worktree integration)\n# Simplified format (direct array of commands)\nmerge:\n  - shell: \"git fetch origin\"\n  - claude: \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"\n\n# OR with timeout (use config object format)\nmerge:\n  commands:\n    - shell: \"git fetch origin\"\n    - shell: \"git merge origin/main\"\n    - shell: \"cargo test\"\n    - claude: \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"\n  timeout: 600  # Timeout in seconds for entire merge phase\n</code></pre> <p>Source: Merge workflow structure from <code>src/config/mapreduce.rs:86-124</code></p>"},{"location":"workflow-basics/full-workflow-structure/#merge-workflow-formats","title":"Merge Workflow Formats","text":"<p>Prodigy supports two formats for merge workflows:</p> <ol> <li> <p>Direct Array Format - For simple merge operations without timeout:    <pre><code>merge:\n  - shell: \"git fetch origin\"\n  - claude: \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"\n</code></pre></p> </li> <li> <p>Config Object Format - When you need to specify a timeout:    <pre><code>merge:\n  commands:\n    - shell: \"git fetch origin\"\n    - claude: \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"\n  timeout: 600  # Optional timeout in seconds\n</code></pre></p> </li> </ol> <p>When to Use Custom Merge Workflows:</p> <p>Custom merge workflows execute when merging worktree changes back to the main branch. Use them for: - Pre-merge validation and testing - Automatic conflict resolution - Running CI checks before merge - Cleaning up temporary files</p> <p>If no custom merge workflow is specified, Prodigy uses default merge behavior.</p> <p>Source: Deserializer implementation in <code>src/config/mapreduce.rs:96-124</code></p>"},{"location":"workflow-basics/full-workflow-structure/#merge-context-variables","title":"Merge Context Variables","text":"<p>The following variables are available in merge workflows:</p> <ul> <li><code>${merge.worktree}</code> - Name of the worktree being merged</li> <li><code>${merge.source_branch}</code> - Source branch (worktree branch)</li> <li><code>${merge.target_branch}</code> - Target branch (your original branch when workflow started)</li> <li><code>${merge.session_id}</code> - Session ID for correlation and debugging</li> </ul> <p>Example with all variables: <pre><code>merge:\n  - shell: |\n      echo \"Merging worktree: ${merge.worktree}\"\n      echo \"From: ${merge.source_branch}\"\n      echo \"To: ${merge.target_branch}\"\n      echo \"Session: ${merge.session_id}\"\n  - claude: \"/validate-merge --branch ${merge.source_branch}\"\n  - claude: \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"\n</code></pre></p> <p>Source: Variable substitution from <code>tests/merge_workflow_integration.rs:274-290</code></p>"},{"location":"workflow-basics/full-workflow-structure/#real-world-examples","title":"Real-World Examples","text":"<p>Example 1: Pre-merge Validation (from <code>workflows/implement.yml:32-42</code>): <pre><code>merge:\n  - claude: \"/prodigy-merge-master\"  # Merge main into worktree first\n  - claude: \"/prodigy-ci\"             # Run CI checks and auto-fix issues\n  - claude: \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"\n</code></pre></p> <p>Example 2: Cleanup and Testing (from <code>workflows/workflow-syntax-drift.yml:38-49</code>): <pre><code>merge:\n  - shell: \"rm -rf .prodigy/syntax-analysis\"\n  - shell: \"git add -A &amp;&amp; git commit -m 'chore: cleanup temp files' || true\"\n  - shell: \"git fetch origin\"\n  - claude: \"/prodigy-merge-master\"\n  - claude: \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"\n</code></pre></p> <p>Example 3: With Environment Variables (from <code>workflows/mapreduce-env-example.yml:82-94</code>): <pre><code>merge:\n  commands:\n    - shell: \"echo Merging changes for $PROJECT_NAME\"\n    - claude: \"/validate-merge --branch ${merge.source_branch} --project $PROJECT_NAME\"\n    - shell: \"echo Merge completed\"\n  timeout: 600\n</code></pre></p>"},{"location":"workflow-basics/full-workflow-structure/#see-also","title":"See Also","text":"<ul> <li>Merge Workflows - Detailed merge workflow documentation</li> <li>Environment Configuration - Environment variables and secrets</li> <li>Command Types - Available command types in workflows</li> </ul>"},{"location":"workflow-basics/merge-workflows/","title":"Merge Workflows","text":""},{"location":"workflow-basics/merge-workflows/#merge-workflows","title":"Merge Workflows","text":"<p>Merge workflows execute when merging worktree changes back to the main branch. This feature enables custom validation, testing, and conflict resolution before integrating changes.</p> <p>When to use merge workflows: - Run tests before merging - Validate code quality - Handle merge conflicts automatically - Sync with upstream changes</p>"},{"location":"workflow-basics/merge-workflows/#configuration-formats","title":"Configuration Formats","text":"<p>Merge workflows support two configuration formats (src/config/mapreduce.rs:96-123):</p> <p>1. Simplified Format (direct array of commands, no timeout support):</p> <pre><code>merge:\n  - shell: \"git fetch origin\"\n  - shell: \"cargo test\"\n  - claude: \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"\n</code></pre> <p>2. Full Format (config object with commands array and optional timeout):</p> <pre><code>merge:\n  commands:\n    - shell: \"git fetch origin\"\n    - shell: \"git merge origin/main\"\n    - shell: \"cargo test\"\n    - claude: \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"\n  timeout: 600  # Optional: timeout for entire merge phase (seconds)\n</code></pre> <p>Use the simplified format for quick merge workflows without timeouts. Use the full format when you need timeout control for long-running merge operations.</p> <p>Important: Always pass both <code>${merge.source_branch}</code> and <code>${merge.target_branch}</code> to the <code>/prodigy-merge-worktree</code> command. This ensures the merge targets the branch you were on when you started the workflow, not a hardcoded main/master branch.</p>"},{"location":"workflow-basics/merge-workflows/#available-merge-variables","title":"Available Merge Variables","text":"<p>The following variables are available exclusively within merge workflow commands. Variable interpolation happens before command execution, and these variables are NOT available in setup/map/reduce phases:</p> <ul> <li><code>${merge.worktree}</code> - Worktree name (e.g., \"prodigy-session-abc123\")</li> <li><code>${merge.source_branch}</code> - Source branch (worktree branch)</li> <li><code>${merge.target_branch}</code> - Target branch (the branch you were on when workflow started)</li> <li><code>${merge.session_id}</code> - Session ID for correlation</li> </ul> <p>Merge workflows also have access to all workflow environment variables defined in the env block, including profile-specific values and secrets.</p>"},{"location":"workflow-basics/merge-workflows/#claude-merge-streaming","title":"Claude Merge Streaming","text":"<p>Claude commands in merge workflows respect verbosity settings (src/worktree/merge_orchestrator.rs:521-534):</p> <ul> <li>Use <code>-v</code> flag for real-time streaming output</li> <li>Set <code>PRODIGY_CLAUDE_CONSOLE_OUTPUT=true</code> to force streaming regardless of verbosity</li> <li>Default behavior shows clean minimal output</li> </ul> <p>This provides full visibility into Claude's merge operations and tool invocations.</p>"},{"location":"workflow-basics/merge-workflows/#real-world-examples","title":"Real-World Examples","text":"<p>Pre-merge CI validation (workflows/implement.yml:33-41):</p> <pre><code>merge:\n  - claude: \"/prodigy-merge-master\"  # Merge main into worktree first\n  - claude: \"/prodigy-ci\"  # Run all CI checks\n  - claude: \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"\n</code></pre> <p>Environment-aware merge (workflows/mapreduce-env-example.yml:83-93):</p> <pre><code>merge:\n  commands:\n    - shell: \"echo Merging changes for $PROJECT_NAME\"\n    - shell: \"echo Debug mode was: $DEBUG_MODE\"\n    - claude: \"/validate-merge --branch ${merge.source_branch} --project $PROJECT_NAME\"\n</code></pre> <p>Documentation workflow with cleanup (workflows/book-docs-drift.yml:93-100):</p> <pre><code>merge:\n  commands:\n    - shell: \"git fetch origin\"\n    - claude: \"/prodigy-merge-master --project ${PROJECT_NAME}\"\n    - claude: \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"\n</code></pre>"},{"location":"workflow-basics/next-steps/","title":"Next Steps","text":""},{"location":"workflow-basics/next-steps/#next-steps","title":"Next Steps","text":"<p>Now that you understand basic workflows, explore these topics:</p>"},{"location":"workflow-basics/next-steps/#within-workflow-basics","title":"Within Workflow Basics","text":"<p>Continue learning about workflow fundamentals:</p> <ul> <li> <p>Command Types - Learn about all available command types (claude, shell, goal_seek, foreach) and when to use each one</p> </li> <li> <p>Command-Level Options - Advanced command configuration including timeout management, output capture, error handling, and conditional execution</p> </li> <li> <p>Full Workflow Structure - Complete workflow format with explicit configuration for complex automation pipelines</p> </li> <li> <p>Environment Configuration - Configure environment variables at multiple levels (workflow-wide, step-specific, profiles)</p> </li> </ul>"},{"location":"workflow-basics/next-steps/#advanced-topics","title":"Advanced Topics","text":"<p>Ready to level up? Explore these advanced features:</p> <ul> <li> <p>Advanced Features - Conditional execution, parallel processing with foreach, goal-seeking operations with validation, and complex control flow</p> </li> <li> <p>Environment Configuration - Comprehensive guide to environment variables, secrets management, profiles, and per-command overrides</p> </li> <li> <p>Error Handling - Workflow-level and command-level error handling strategies for building resilient automation</p> </li> <li> <p>MapReduce Workflows - Massive parallel processing with setup/map/reduce phases for large-scale tasks like codebase transformations</p> </li> <li> <p>Variable Interpolation - Capture command outputs, reference work items, access map results, and build dynamic workflows with variables</p> </li> </ul>"},{"location":"workflow-basics/variables/","title":"Variables and Interpolation","text":"<p>Prodigy provides a powerful variable system that enables dynamic workflows with captured outputs, nested field access, and flexible interpolation.</p>"},{"location":"workflow-basics/variables/#overview","title":"Overview","text":"<p>Variables in Prodigy allow you to: - Capture command outputs in multiple formats (JSON, text, lines, numbers) - Reference work items in MapReduce workflows - Access nested fields with dot notation - Provide default values for missing variables - Use both <code>${var}</code> and <code>$var</code> syntax</p>"},{"location":"workflow-basics/variables/#variable-categories","title":"Variable Categories","text":"<pre><code>graph TD\n    Variables[Variable System] --&gt; Standard[Standard Variables]\n    Variables --&gt; MapReduce[MapReduce Variables]\n    Variables --&gt; Merge[Merge Variables]\n    Variables --&gt; Computed[Computed Variables]\n\n    Standard --&gt; Item[\"item, item_index, item_total\"]\n    Standard --&gt; Workflow[\"workflow.name, workflow.id\"]\n    Standard --&gt; Step[\"step.name, step.index\"]\n    Standard --&gt; Last[\"last.output, last.exit_code\"]\n\n    MapReduce --&gt; MapResults[\"map.results, map.successful\"]\n    MapReduce --&gt; Worker[\"worker.id\"]\n\n    Merge --&gt; MergeVars[\"merge.worktree, merge.source_branch\"]\n\n    Computed --&gt; Env[\"env.VAR\"]\n    Computed --&gt; File[\"file:path\"]\n    Computed --&gt; Cmd[\"cmd:command\"]\n    Computed --&gt; Date[\"date:format\"]\n    Computed --&gt; UUID[\"uuid\"]\n\n    style Standard fill:#e1f5ff\n    style MapReduce fill:#fff3e0\n    style Merge fill:#f3e5f5\n    style Computed fill:#e8f5e9</code></pre> <p>Figure: Variable system overview showing four main categories and their available variables.</p>"},{"location":"workflow-basics/variables/#standard-variables","title":"Standard Variables","text":"<p>Available in all execution modes:</p> <p>Work Item Variables:</p> <ul> <li><code>${item}</code> - Current work item in foreach/MapReduce</li> <li><code>${item_index}</code> - Zero-based index of current item</li> <li><code>${item_total}</code> - Total number of items being processed</li> </ul> <p>Workflow Variables:</p> <ul> <li><code>${workflow.name}</code> - Workflow identifier</li> <li><code>${workflow.id}</code> - Unique workflow execution ID</li> <li><code>${workflow.iteration}</code> - Current iteration number</li> </ul> <p>Step Variables:</p> <p>Source: <code>src/cook/workflow/variables.rs:10-42</code></p> <ul> <li><code>${step.name}</code> - Name of the currently executing step</li> <li><code>${step.index}</code> - Zero-based index of the current step in the workflow</li> </ul> <p>Previous Command Results:</p> <ul> <li><code>${last.output}</code> - Output from the previous command</li> <li><code>${last.exit_code}</code> - Exit code from the previous command</li> </ul>"},{"location":"workflow-basics/variables/#mapreduce-variables","title":"MapReduce Variables","text":"<p>Available in MapReduce workflows:</p> <ul> <li><code>${map.results}</code> - Aggregated results from all map agents</li> <li><code>${map.successful}</code> - Count of successfully processed items</li> <li><code>${map.failed}</code> - Count of failed items</li> <li><code>${map.total}</code> - Total item count</li> <li><code>${map.key}</code> - Optional key for grouping or identifying map outputs (rarely used)</li> <li><code>${worker.id}</code> - Identifier for parallel worker processing the current item</li> </ul> <p>String Representation</p> <p>MapReduce aggregate variables (<code>${map.successful}</code>, <code>${map.failed}</code>, <code>${map.total}</code>) contain string representations of counts, not numeric values. If you need to perform arithmetic operations, convert them to numbers first.</p> <p>Source: <code>src/cook/execution/mapreduce/utils.rs:114-116</code></p>"},{"location":"workflow-basics/variables/#merge-variables","title":"Merge Variables","text":"<p>Available in merge workflows:</p> <ul> <li><code>${merge.worktree}</code> - Name of worktree being merged</li> <li><code>${merge.source_branch}</code> - Source branch for merge</li> <li><code>${merge.target_branch}</code> - Target branch for merge</li> <li><code>${merge.session_id}</code> - Session ID for correlation</li> </ul>"},{"location":"workflow-basics/variables/#output-capture","title":"Output Capture","text":"<p>Capture command outputs for use in subsequent steps:</p> <p>When to Use Capture</p> <p>Use output capture when you need to:</p> <ul> <li>Pass data between commands (commit SHA, file paths, configuration)</li> <li>Make decisions based on command results (<code>when:</code> conditions)</li> <li>Access structured data (JSON) with nested field access</li> <li>Track metadata (exit codes, duration, success status)</li> </ul> <pre><code># Source: examples/capture-json-processing.yml\n# Capture as string (default)\n- shell: \"git rev-parse HEAD\"\n  capture_output: commit_sha                # (1)!\n\n# Capture as JSON\n- shell: \"cat items.json\"\n  capture_output: items\n  capture_format: json                      # (2)!\n\n# Capture as lines array\n- shell: \"find src -name '*.rs'\"\n  capture_output: rust_files\n  capture_format: lines                     # (3)!\n\n# Capture as number\n- shell: \"wc -l &lt; file.txt\"\n  capture_output: line_count\n  capture_format: number                    # (4)!\n\n# Capture as boolean\n- shell: \"git diff --quiet &amp;&amp; echo true || echo false\"\n  capture_output: repo_clean\n  capture_format: boolean                   # (5)!\n\n1. Default format - captures raw text output\n2. Parses JSON to enable nested field access with dot notation\n3. Splits output into array (one element per line)\n4. Parses numeric value for arithmetic operations\n5. Parses boolean for conditional logic\n</code></pre>"},{"location":"workflow-basics/variables/#capture-formats","title":"Capture Formats","text":"<p>The following capture formats are supported (defined in <code>src/cook/workflow/variables.rs:253-265</code>):</p> <ul> <li>string - Raw text output (default)</li> <li>json - Parse JSON and access nested fields with dot notation</li> <li>lines - Split output into array of lines (one per line break)</li> <li>number - Parse numeric value (integer or float)</li> <li>boolean - Parse true/false value from output</li> </ul>"},{"location":"workflow-basics/variables/#capture-metadata","title":"Capture Metadata","text":"<p>Additional metadata available for captured outputs:</p> <ul> <li><code>${var.exit_code}</code> - Command exit status</li> <li><code>${var.success}</code> - Boolean success flag</li> <li><code>${var.duration}</code> - Execution time</li> <li><code>${var.stderr}</code> - Error output</li> </ul>"},{"location":"workflow-basics/variables/#interpolation-syntax","title":"Interpolation Syntax","text":""},{"location":"workflow-basics/variables/#basic-interpolation","title":"Basic Interpolation","text":"<pre><code># Both syntaxes work for simple variables\n- shell: \"echo $VARIABLE\"\n- shell: \"echo ${VARIABLE}\"\n</code></pre> <p>Unbraced $VAR Syntax Limitation</p> <p>The unbraced <code>$VAR</code> syntax only works for simple identifiers (A-Z, a-z, 0-9, _). For nested field access like <code>map.total</code>, you MUST use braced syntax <code>${map.total}</code>. The syntax <code>$map.total</code> will fail because the unbraced parser cannot handle dots.</p> <pre><code># \u274c WRONG - unbraced syntax with dots fails\n- shell: \"echo $map.total\"\n\n# \u2705 CORRECT - braced syntax required for nested access\n- shell: \"echo ${map.total}\"\n</code></pre> <p>Source: <code>src/cook/execution/interpolation.rs:33-36</code></p>"},{"location":"workflow-basics/variables/#nested-field-access","title":"Nested Field Access","text":"<pre><code># Source: src/cook/execution/interpolation.rs:420-522\n# Access nested JSON fields - always use braced syntax\n- shell: \"echo ${item.metadata.priority}\"       # (1)!\n- claude: \"/process ${user.config.api_url}\"     # (2)!\n\n1. Dot notation for nested object fields\n2. Works with any captured JSON output\n</code></pre> <p>Nested Field Example</p> <p>Given this captured JSON: <pre><code>{\n  \"user\": {\n    \"name\": \"Alice\",\n    \"config\": {\n      \"api_url\": \"https://api.example.com\"\n    }\n  }\n}\n</code></pre></p> <p>Access nested fields: <pre><code>- shell: \"echo Name: ${user.name}\"              # Output: Name: Alice\n- shell: \"echo URL: ${user.config.api_url}\"     # Output: URL: https://api.example.com\n</code></pre></p>"},{"location":"workflow-basics/variables/#default-values","title":"Default Values","text":"<pre><code># Source: src/cook/execution/interpolation.rs:274-288\n# Provide default if variable missing (using :- syntax)\n- shell: \"echo ${PORT:-8080}\"                   # (1)!\n- claude: \"/deploy ${environment:-dev}\"         # (2)!\n\n1. If PORT not set, use 8080\n2. If environment not set, use \"dev\"\n</code></pre> <p>Default Value Syntax</p> <p>Use the <code>:-</code> syntax (with colon and dash) for default values:</p> <ul> <li>Correct: <code>${VAR:-default}</code> - Uses \"default\" if VAR is unset or empty</li> <li>Incorrect: <code>${VAR-default}</code> - Only works if VAR is unset (not if empty)</li> <li>Incorrect: <code>${VAR:default}</code> - Not supported, will cause parsing error</li> </ul> <p>The <code>:-</code> operator is the recommended and most robust option.</p>"},{"location":"workflow-basics/variables/#array-access","title":"Array Access","text":"<pre><code># Source: src/cook/execution/interpolation.rs:420-522\n# Access array elements - two syntaxes supported\n- shell: \"echo ${items[0]}\"      # Bracket notation (preferred)\n- shell: \"echo ${items.0}\"       # Dot notation with number (also supported)\n- shell: \"echo ${items[1]}\"      # Second element\n- shell: \"echo ${items.1}\"       # Same as items[1]\n</code></pre> <p>Array Indexing Syntax</p> <p>Prodigy supports two notations for array access:</p> <ul> <li>Bracket syntax <code>${items[0]}</code> - Preferred for clarity and consistency</li> <li>Dot notation <code>${items.0}</code> - Also supported, resolves to the same element</li> </ul> <p>Important: Negative indices like <code>${items[-1]}</code> are NOT supported. Use explicit positive indices or compute the index in a shell command.</p>"},{"location":"workflow-basics/variables/#advanced-features","title":"Advanced Features","text":""},{"location":"workflow-basics/variables/#computed-variables","title":"Computed Variables","text":"<p>Source: <code>src/cook/execution/variables.rs:1-1378</code></p> <p>Prodigy supports computed variables that dynamically generate values at runtime:</p> <p>Computed Variable Use Cases</p> <p>Computed variables are ideal for:</p> <ul> <li>Dynamic configuration: Read config files at runtime (<code>file:config.json</code>)</li> <li>Environment-specific values: Access environment variables (<code>env.DATABASE_URL</code>)</li> <li>Command integration: Embed command output (<code>cmd:git rev-parse HEAD</code>)</li> <li>Timestamps: Generate dated filenames (<code>${date:%Y%m%d}</code>)</li> <li>Unique IDs: Create correlation IDs (<code>${uuid}</code>)</li> </ul> Environment VariablesFile ContentCommand OutputJSON ExtractionDate FormattingUUID Generation <pre><code># Access environment variables\n- shell: \"echo Database: ${env.DATABASE_URL}\"\n- shell: \"echo User: ${env.USER}\"\n- shell: \"echo Path: ${env.PATH}\"\n</code></pre> <p>Access system environment variables using the <code>env:</code> prefix.</p> <pre><code># Read file content into variable\n- shell: \"echo Version: ${file:/path/to/VERSION}\"\n- claude: \"/analyze ${file:config.json}\"\n</code></pre> <p>Read file contents at runtime. Useful for configuration files and version strings.</p> <pre><code># Execute command and capture output\n- shell: \"echo Commit: ${cmd:git rev-parse HEAD}\"\n- shell: \"echo Branch: ${cmd:git branch --show-current}\"\n</code></pre> <p>Execute commands inline and use their output. Cached for performance.</p> <pre><code># Extract value from JSON file using path\n- shell: \"echo Name: ${json:name:package.json}\"\n- shell: \"echo Version: ${json:version:package.json}\"\n</code></pre> <p>Extract specific fields from JSON files without loading the entire file.</p> <pre><code># Generate formatted date strings\n- shell: \"echo Today: ${date:%Y-%m-%d}\"\n- shell: \"echo Timestamp: ${date:%Y%m%d_%H%M%S}\"\n</code></pre> <p>Generate timestamps using strftime format codes.</p> <pre><code># Generate unique identifiers\n- shell: \"echo Request ID: ${uuid}\"\n- shell: \"echo Session: ${uuid}\"\n</code></pre> <p>Generate unique UUIDs (v4). Each use generates a new UUID.</p> <p>Performance: Computed Variable Caching</p> <p>Expensive operations (<code>file:</code>, <code>cmd:</code>) are cached with an LRU cache (100 entry limit) to improve performance. However, <code>${uuid}</code> always generates a new value for each use.</p> <p>Source: <code>src/cook/execution/variables.rs:415-421, 662-669</code></p>"},{"location":"workflow-basics/variables/#aggregate-functions-enhanced-variable-system","title":"Aggregate Functions (Enhanced Variable System)","text":"<p>Source: <code>src/cook/execution/variables.rs:44-97</code></p> <p>For advanced workflows, Prodigy provides aggregate functions for data processing:</p> <p>Statistical Functions:</p> <ul> <li><code>Count(filter?)</code> - Count items (with optional filter)</li> <li><code>Sum</code> - Sum numeric values</li> <li><code>Average</code> - Calculate mean</li> <li><code>Min</code> - Find minimum value</li> <li><code>Max</code> - Find maximum value</li> <li><code>Median</code> - Calculate median</li> <li><code>StdDev</code> - Standard deviation</li> <li><code>Variance</code> - Calculate variance</li> </ul> <p>Collection Functions:</p> <ul> <li><code>Collect</code> - Gather values into array</li> <li><code>Unique</code> - Get unique values</li> <li><code>Concat</code> - Concatenate strings</li> <li><code>Merge</code> - Merge objects</li> <li><code>Flatten</code> - Flatten nested arrays</li> <li><code>Sort(ascending|descending)</code> - Sort values</li> <li><code>GroupBy</code> - Group by key</li> </ul> <p>Enhanced Variable System</p> <p>Aggregate functions are part of the enhanced variable system designed for advanced workflows. Consult the source code or examples for detailed usage patterns.</p>"},{"location":"workflow-basics/variables/#variable-scoping","title":"Variable Scoping","text":"<p>Source: <code>src/cook/execution/variables.rs:382-407</code></p> <p>Prodigy uses a three-tier variable scoping system with precedence rules:</p> <pre><code>flowchart TD\n    Start[Variable Resolution] --&gt; CheckLocal{Variable in&lt;br/&gt;Local Scope?}\n    CheckLocal --&gt;|Yes| UseLocal[Use Local Value&lt;br/&gt;Highest Precedence]\n    CheckLocal --&gt;|No| CheckPhase{Variable in&lt;br/&gt;Phase Scope?}\n\n    CheckPhase --&gt;|Yes| UsePhase[Use Phase Value&lt;br/&gt;Middle Precedence]\n    CheckPhase --&gt;|No| CheckGlobal{Variable in&lt;br/&gt;Global Scope?}\n\n    CheckGlobal --&gt;|Yes| UseGlobal[Use Global Value&lt;br/&gt;Lowest Precedence]\n    CheckGlobal --&gt;|No| Error[Error: Variable Not Found]\n\n    UseLocal --&gt; Return[Return Value]\n    UsePhase --&gt; Return\n    UseGlobal --&gt; Return\n\n    style CheckLocal fill:#e1f5ff\n    style CheckPhase fill:#fff3e0\n    style CheckGlobal fill:#f3e5f5\n    style Error fill:#ffebee</code></pre> <p>Figure: Variable resolution flow showing scope precedence from local (highest) to global (lowest).</p> <p>Scope Hierarchy (highest to lowest precedence):</p> <ol> <li>Local Scope - Step-level variables (highest precedence)</li> <li>Set within a specific command or step</li> <li>Only visible within that step</li> <li> <p>Overrides phase and global variables</p> </li> <li> <p>Phase Scope - Phase-level variables (middle precedence)</p> </li> <li>Available within a workflow phase (setup, map, reduce)</li> <li>Shared across steps in the same phase</li> <li> <p>Overrides global variables</p> </li> <li> <p>Global Scope - Workflow-level variables (lowest precedence)</p> </li> <li>Available throughout the entire workflow</li> <li>Set at workflow start or in environment</li> <li>Can be overridden by phase or local variables</li> </ol> <p>Example:</p> <pre><code># Global scope\nenv:\n  ENVIRONMENT: \"production\"\n\nsetup:\n  # Phase scope - overrides global\n  - shell: \"export ENVIRONMENT=staging\"\n    capture_output: phase_env\n\nmap:\n  agent_template:\n    # Local scope - overrides phase and global\n    - shell: \"ENVIRONMENT=dev npm test\"\n</code></pre>"},{"location":"workflow-basics/variables/#variable-aliases","title":"Variable Aliases","text":"<p>Prodigy supports aliases for backward compatibility:</p> <p>Syntax Aliases:</p> <ul> <li><code>$item</code> \u2192 <code>${item}</code> (both syntaxes work for simple identifiers)</li> <li><code>$workflow_name</code> \u2192 <code>${workflow.name}</code> (legacy snake_case maps to dot notation)</li> </ul> <p>Backward Compatibility Aliases:</p> <p>Source: <code>src/cook/workflow/variables.rs:10-42</code></p> <ul> <li><code>$ARG</code> or <code>$ARGUMENT</code> \u2192 <code>${item.value}</code> (legacy item value access)</li> <li><code>$FILE</code> or <code>$FILE_PATH</code> \u2192 <code>${item.path}</code> (legacy item path access)</li> </ul> <p>These aliases exist for backward compatibility with older workflow formats.</p> <p>Use Modern Syntax</p> <p>New workflows should prefer: - Dot notation (<code>${workflow.name}</code>) over legacy underscore variables (<code>$workflow_name</code>) - Braced syntax (<code>${var}</code>) over unbraced (<code>$var</code>) for consistency - Direct item access (<code>${item.value}</code>) over aliases (<code>$ARG</code>)</p>"},{"location":"workflow-basics/variables/#examples","title":"Examples","text":""},{"location":"workflow-basics/variables/#capturing-and-using-json-output","title":"Capturing and Using JSON Output","text":"<pre><code># Source: examples/capture-json-processing.yml:8-14\n- shell: \"jq -c '{name, version}' package.json\"\n  capture_output: pkg\n  capture_format: json\n\n- shell: \"echo Building ${pkg.name} version ${pkg.version}\"\n</code></pre>"},{"location":"workflow-basics/variables/#conditional-variable-usage","title":"Conditional Variable Usage","text":"<pre><code>- shell: \"npm run build\"\n  when: \"${environment} == 'production'\"\n  capture_output: build_output\n\n- claude: \"/analyze-build ${build_output}\"\n  when: \"${build_output.success} == true\"\n</code></pre>"},{"location":"workflow-basics/variables/#mapreduce-variable-flow","title":"MapReduce Variable Flow","text":"<pre><code># Source: src/cook/workflow/variables.rs:738-750\nmode: mapreduce\n\nmap:\n  agent_template:\n    - claude: \"/process ${item.path}\"\n      capture_output: result\n\nreduce:\n  - shell: \"echo Processing ${map.total} files\"\n  - shell: \"echo Successful: ${map.successful}\"\n  - claude: \"/summarize ${map.results}\"\n</code></pre>"},{"location":"workflow-basics/variables/#common-patterns","title":"Common Patterns","text":""},{"location":"workflow-basics/variables/#using-capture-metadata","title":"Using Capture Metadata","text":"<p>Access metadata from captured outputs:</p> <pre><code>- shell: \"npm test\"\n  capture_output: test_results\n\n- shell: \"echo Tests completed in ${test_results.duration}ms\"\n  when: \"${test_results.success} == true\"\n\n- shell: \"echo Test failures: ${test_results.stderr}\"\n  when: \"${test_results.exit_code} != 0\"\n</code></pre>"},{"location":"workflow-basics/variables/#default-values-for-optional-configuration","title":"Default Values for Optional Configuration","text":"<p>Provide fallbacks when variables may not be set:</p> <pre><code># Source: src/cook/execution/interpolation.rs:274-288\n- shell: \"echo API URL: ${API_URL:-http://localhost:3000}\"\n- shell: \"echo Environment: ${ENVIRONMENT:-development}\"\n- shell: \"echo Timeout: ${TIMEOUT:-300} seconds\"\n</code></pre>"},{"location":"workflow-basics/variables/#see-also","title":"See Also","text":"<ul> <li>Command Types - Commands that can capture output</li> <li>Environment Variables - Environment-specific configuration</li> <li>MapReduce Workflows - Using variables in parallel workflows</li> <li>Conditional Execution - Conditional logic with variables</li> </ul>"},{"location":"workflow-basics/workflow-structure/","title":"Workflow Structure","text":"<p>Prodigy workflows are YAML files that define commands to execute. This page explains the basic structure and how workflows are executed.</p>"},{"location":"workflow-basics/workflow-structure/#two-formats","title":"Two Formats","text":"<p>Prodigy supports two workflow formats, allowing you to start simple and add complexity as needed.</p> <p>Choosing a Format</p> <p>Start with the simple array format for basic automation. Switch to the full object format when you need environment variables, secrets, or custom merge workflows. You can convert between formats at any time.</p>"},{"location":"workflow-basics/workflow-structure/#simple-array-format","title":"Simple Array Format","text":"<p>The simplest workflow is just an array of commands:</p> <pre><code># Source: examples/standard-workflow.yml:3-13\n- shell: echo \"Starting code analysis...\"\n- shell: find . -name \"*.rs\" -type f | wc -l | xargs -I {} echo \"Found {} Rust files\"\n- shell: echo \"Running cargo check...\"\n- shell: cargo check --quiet 2&gt;&amp;1 || echo \"Check completed\"\n- shell: echo \"Workflow complete\"\n</code></pre> <p>This format is ideal for: - Quick automation scripts - Simple sequential tasks - Learning Prodigy basics</p> <p>Execution: Commands run sequentially, one after another. Each command must complete before the next starts.</p>"},{"location":"workflow-basics/workflow-structure/#full-object-format","title":"Full Object Format","text":"<p>For more control, use the full format with explicit configuration:</p> <pre><code># Source: src/config/workflow.rs:12-39\ncommands:                    # (1)!\n  - shell: \"cargo build\"\n  - claude: \"/prodigy-test\"\n  - shell: \"cargo test\"\n\n# Optional: Global environment variables\nenv:                         # (2)!\n  NODE_ENV: production\n  API_URL: https://api.example.com\n\n# Optional: Custom merge workflow\nmerge:                       # (3)!\n  - shell: \"git fetch origin\"\n  - claude: \"/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}\"\n</code></pre> <ol> <li>Array of commands to execute sequentially</li> <li>Environment variables available to all commands</li> <li>Custom commands to run when merging worktree changes</li> </ol> <p>This format enables: - Environment variable configuration - Secret management - Custom merge workflows - Profile switching (dev/staging/prod)</p> <p>When to use: When you need environment variables, secrets, or custom merge behavior.</p>"},{"location":"workflow-basics/workflow-structure/#sequential-execution-model","title":"Sequential Execution Model","text":"<p>Standard workflows execute commands sequentially - each command completes before the next starts.</p> <pre><code>- shell: \"cargo build\"          # Runs first, completes\n- shell: \"cargo test\"           # Runs second, completes\n- claude: \"/prodigy-analyze\"    # Runs third, completes\n</code></pre> <p>Key behaviors: - Commands execute in the order listed - If a command fails, the workflow stops (unless <code>allow_failure: true</code>) - Variables from earlier commands are available to later commands - Each command's output can be captured and used downstream</p> <p>Failure Handling</p> <p>By default, a failing command stops the entire workflow. Use <code>allow_failure: true</code> on individual commands if you want to continue execution after failures. See Command-Level Options for details.</p> <p>Source: Sequential execution logic in <code>src/cook/workflow/executor/orchestration.rs</code></p> <p>MapReduce Workflows</p> <p>MapReduce workflows (<code>mode: mapreduce</code>) use a different execution model with parallel processing. Instead of sequential command execution, they process work items in parallel across multiple agents. See the MapReduce Guide for details.</p>"},{"location":"workflow-basics/workflow-structure/#top-level-fields","title":"Top-Level Fields","text":"<p>The full workflow format supports these top-level fields:</p> Field Required Description <code>commands</code> Yes Array of commands to execute <code>mode</code> No Workflow execution mode (<code>standard</code> or <code>mapreduce</code>) - defaults to <code>standard</code> <code>env</code> No Global environment variables for all commands <code>secrets</code> No Secret environment variables (masked in logs) <code>env_files</code> No Environment files to load (<code>.env</code> format) <code>profiles</code> No Environment profiles for different contexts (dev/staging/prod) <code>merge</code> No Custom merge workflow for worktree integration <code>name</code> No Workflow name (defaults to \"default\") <p>Source: Field definitions from <code>src/config/workflow.rs:12-39</code></p> <p>See Available Fields for detailed documentation of each field.</p>"},{"location":"workflow-basics/workflow-structure/#basic-examples","title":"Basic Examples","text":"<p>Example 1: Simple Test Workflow</p> <pre><code># Source: examples/standard-workflow.yml:3-5\n- shell: echo \"Starting code analysis...\"\n- shell: cargo check --quiet 2&gt;&amp;1 || echo \"Check completed\"\n- shell: echo \"Workflow complete\"\n</code></pre> <p>Example 2: Workflow with Environment Variables</p> <pre><code># Source: examples/capture-conditional-flow.yml:4-5\nname: conditional-deployment\nmode: standard\n\ncommands:\n  - shell: \"cargo test\"\n  - shell: \"cargo build --release\"\n\nenv:\n  RUST_BACKTRACE: \"1\"\n  BUILD_ENV: production\n</code></pre> <p>Example 3: Workflow with Variable Capture</p> <p>Commands can capture output for use in later commands:</p> <pre><code># Source: examples/capture-conditional-flow.yml:26-28\ncommands:\n  - shell: \"grep '^version' Cargo.toml | cut -d'\\\"' -f2 || echo '0.0.0'\"\n    capture: \"current_version\"    # (1)!\n\n  - shell: \"echo 'Building version ${current_version}'\"  # (2)!\n</code></pre> <ol> <li>Capture command output into a variable</li> <li>Use the captured variable in subsequent commands</li> </ol> <p>See Command Types for details on variable capture and other command features.</p>"},{"location":"workflow-basics/workflow-structure/#execution-flow-diagram","title":"Execution Flow Diagram","text":"<pre><code>flowchart TD\n    Start([Start Workflow]) --&gt; Load[Load workflow.yml]\n    Load --&gt; Worktree{Worktree&lt;br/&gt;needed?}\n    Worktree --&gt;|Yes| Create[Create isolated worktree]\n    Worktree --&gt;|No| Cmd1\n    Create --&gt; Cmd1[Execute Command 1]\n\n    Cmd1 --&gt; Check1{Success?}\n    Check1 --&gt;|Yes| Cmd2[Execute Command 2]\n    Check1 --&gt;|No| Allow1{allow_failure?}\n    Allow1 --&gt;|Yes| Cmd2\n    Allow1 --&gt;|No| Stop1[Stop Workflow]\n\n    Cmd2 --&gt; Check2{Success?}\n    Check2 --&gt;|Yes| CmdN[Execute Command N]\n    Check2 --&gt;|No| Allow2{allow_failure?}\n    Allow2 --&gt;|Yes| CmdN\n    Allow2 --&gt;|No| Stop2[Stop Workflow]\n\n    CmdN --&gt; Complete[All commands complete]\n    Complete --&gt; Merge{Worktree&lt;br/&gt;used?}\n    Merge --&gt;|Yes| Prompt[Prompt for merge]\n    Merge --&gt;|No| End\n    Prompt --&gt; End([End Workflow])\n    Stop1 --&gt; End\n    Stop2 --&gt; End\n\n    style Start fill:#e8f5e9\n    style Load fill:#e1f5ff\n    style Complete fill:#e8f5e9\n    style End fill:#e8f5e9\n    style Stop1 fill:#ffebee\n    style Stop2 fill:#ffebee</code></pre> <p>Figure: Sequential workflow execution showing command-by-command processing with failure handling.</p>"},{"location":"workflow-basics/workflow-structure/#format-detection","title":"Format Detection","text":"<p>Prodigy automatically detects which format you're using:</p> <pre><code>// Source: src/config/workflow.rs:41-74\n// Deserializer tries formats in order:\n// 1. Commands (simple array)\n// 2. Full (object with env/merge/etc)\n// 3. WithCommandsField (legacy object format)\n</code></pre> <p>You don't need to specify the format - just write YAML and Prodigy handles it.</p>"},{"location":"workflow-basics/workflow-structure/#see-also","title":"See Also","text":"<ul> <li>Command Types - Learn about <code>shell</code>, <code>claude</code>, <code>goal_seek</code>, and <code>foreach</code> commands</li> <li>Command-Level Options - Options like <code>capture</code>, <code>when</code>, <code>allow_failure</code></li> <li>Environment Configuration - Using environment variables and secrets</li> <li>Full Workflow Structure - Complete details on the full format with merge workflows</li> <li>Complete Example - A comprehensive workflow showcasing all features</li> </ul>"}]}