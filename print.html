<!DOCTYPE HTML>
<html lang="en" class="rust sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Prodigy Documentation</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="AI-powered workflow orchestration for development teams">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "rust";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('rust')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Prodigy Documentation</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/iepathos/prodigy" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>Prodigy is an AI-powered workflow orchestration tool that enables development teams to automate complex tasks using Claude AI through structured YAML workflows.</p>
<h2 id="what-is-prodigy"><a class="header" href="#what-is-prodigy">What is Prodigy?</a></h2>
<p>Prodigy combines the power of Claude AI with workflow orchestration to:</p>
<ul>
<li><strong>Automate repetitive development tasks</strong> - Code reviews, refactoring, testing</li>
<li><strong>Process work in parallel</strong> - MapReduce-style parallel execution across git worktrees</li>
<li><strong>Maintain quality</strong> - Built-in validation, error handling, and retry mechanisms</li>
<li><strong>Track changes</strong> - Full git integration with automatic commits and merge workflows</li>
</ul>
<h2 id="quick-start"><a class="header" href="#quick-start">Quick Start</a></h2>
<p>Create a simple workflow in <code>workflow.yml</code>:</p>
<pre><code class="language-yaml">- shell: "cargo build"
- shell: "cargo test"
  on_failure:
    claude: "/fix-failing-tests"
- shell: "cargo clippy"
</code></pre>
<p>Run it:</p>
<pre><code class="language-bash">prodigy run workflow.yml
</code></pre>
<h2 id="key-concepts"><a class="header" href="#key-concepts">Key Concepts</a></h2>
<ul>
<li><strong>Workflows</strong>: YAML files defining sequences of commands</li>
<li><strong>Commands</strong>: Shell commands, Claude AI invocations, or control flow</li>
<li><strong>Variables</strong>: Dynamic values captured and interpolated across steps</li>
<li><strong>MapReduce</strong>: Parallel processing across multiple git worktrees</li>
<li><strong>Validation</strong>: Automatic testing and quality checks</li>
</ul>
<h2 id="next-steps"><a class="header" href="#next-steps">Next Steps</a></h2>
<ul>
<li><a href="workflow-basics.html">Workflow Basics</a> - Learn workflow fundamentals</li>
<li><a href="commands.html">Command Types</a> - Explore available command types</li>
<li><a href="examples.html">Examples</a> - See real-world workflows</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="workflow-basics"><a class="header" href="#workflow-basics">Workflow Basics</a></h1>
<p>This chapter covers the fundamentals of creating Prodigy workflows. You’ll learn about workflow structure, basic commands, and configuration options.</p>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>Prodigy workflows are YAML files that define a sequence of commands to execute. They can be as simple as a list of shell commands or as complex as parallel MapReduce jobs.</p>
<p><strong>Two Main Workflow Types:</strong></p>
<ul>
<li><strong>Standard Workflows</strong>: Sequential command execution (covered here)</li>
<li><strong>MapReduce Workflows</strong>: Parallel processing with map/reduce phases (see <a href="mapreduce.html">MapReduce chapter</a>)</li>
</ul>
<h2 id="simple-workflows"><a class="header" href="#simple-workflows">Simple Workflows</a></h2>
<p>The simplest workflow is just an array of commands:</p>
<pre><code class="language-yaml"># Simple array format - just list your commands
- shell: "echo 'Starting workflow...'"
- claude: "/prodigy-analyze"
- shell: "cargo test"
</code></pre>
<p>This executes each command sequentially. No additional configuration needed.</p>
<h2 id="full-workflow-structure"><a class="header" href="#full-workflow-structure">Full Workflow Structure</a></h2>
<p>For more complex workflows, use the full format with explicit configuration:</p>
<pre><code class="language-yaml"># Full format with environment and merge configuration
commands:
  - shell: "cargo build"
  - claude: "/prodigy-test"

# Global environment variables (available to all commands)
env:
  NODE_ENV: production
  API_URL: https://api.example.com

# Secret environment variables (masked in logs)
secrets:
  API_KEY: "${env:SECRET_API_KEY}"

# Environment files to load (.env format)
env_files:
  - .env.production

# Environment profiles (switch contexts easily)
profiles:
  development:
    NODE_ENV: development
    DEBUG: "true"

# Custom merge workflow (for worktree integration)
merge:
  - shell: "git fetch origin"
  - claude: "/merge-worktree ${merge.source_branch}"
  timeout: 600  # Optional timeout in seconds
</code></pre>
<h2 id="available-fields"><a class="header" href="#available-fields">Available Fields</a></h2>
<p>Standard workflows support these top-level fields:</p>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Required</th><th>Description</th></tr></thead><tbody>
<tr><td><code>commands</code></td><td>Array</td><td>Yes*</td><td>List of commands to execute sequentially</td></tr>
<tr><td><code>env</code></td><td>Map</td><td>No</td><td>Global environment variables</td></tr>
<tr><td><code>secrets</code></td><td>Map</td><td>No</td><td>Secret environment variables (masked in logs)</td></tr>
<tr><td><code>env_files</code></td><td>Array</td><td>No</td><td>Paths to .env files to load</td></tr>
<tr><td><code>profiles</code></td><td>Map</td><td>No</td><td>Named environment profiles</td></tr>
<tr><td><code>merge</code></td><td>Object</td><td>No</td><td>Custom merge workflow for worktree integration</td></tr>
</tbody></table>
</div>
<p><strong>Note:</strong> <code>commands</code> is only required in the full format. Simple array format doesn’t use the <code>commands</code> key.</p>
<h2 id="command-types"><a class="header" href="#command-types">Command Types</a></h2>
<p>Prodigy supports several types of commands in workflows. <strong>Each command step must specify exactly one command type</strong> - they are mutually exclusive.</p>
<h3 id="core-commands"><a class="header" href="#core-commands">Core Commands</a></h3>
<p><strong><code>shell:</code></strong> - Execute shell commands</p>
<pre><code class="language-yaml">- shell: "cargo build --release"
- shell: "npm install"
</code></pre>
<p><strong><code>claude:</code></strong> - Invoke Claude Code commands</p>
<pre><code class="language-yaml">- claude: "/prodigy-lint"
- claude: "/analyze codebase"
</code></pre>
<h3 id="advanced-commands"><a class="header" href="#advanced-commands">Advanced Commands</a></h3>
<ul>
<li><strong><code>goal_seek:</code></strong> - Goal-seeking operations with validation (see <a href="advanced.html">Advanced Features</a>)</li>
<li><strong><code>foreach:</code></strong> - Iterate over lists with nested commands (see <a href="advanced.html">Advanced Features</a>)</li>
<li><strong><code>validate:</code></strong> - Validates that implementation matches spec/plan requirements (see <a href="commands.html">Commands</a>)</li>
<li><strong><code>write_file:</code></strong> - Write content to files with format validation (see <a href="commands.html">Commands</a>)</li>
<li><strong><code>analyze:</code></strong> - Run analysis handlers for coverage, complexity metrics, etc. (see <a href="commands.html">Commands</a>)</li>
</ul>
<p><strong>Deprecated:</strong></p>
<ul>
<li><strong><code>test:</code></strong> - Deprecated in favor of <code>shell:</code> with <code>on_failure:</code> handlers</li>
</ul>
<p>For detailed information on each command type and their fields, see the <a href="commands.html">Command Types chapter</a>.</p>
<h2 id="command-level-options"><a class="header" href="#command-level-options">Command-Level Options</a></h2>
<p>All command types support additional fields for advanced control:</p>
<h3 id="basic-options"><a class="header" href="#basic-options">Basic Options</a></h3>
<pre><code class="language-yaml">- shell: "cargo test"
  id: "run-tests"              # Step identifier for output referencing
  commit_required: true        # Expect git commit after this step
  timeout: 300                 # Timeout in seconds
  cwd: "./subproject"          # Set working directory (alias: working_dir)
  output_file: "test-results.txt"  # Save output to file
</code></pre>
<p><strong>Working Directory Control:</strong></p>
<ul>
<li><code>cwd</code> (or <code>working_dir</code>) - Changes working directory for the command</li>
<li>Supports variable interpolation: <code>cwd: "${PROJECT_DIR}/subdir"</code></li>
<li>Relative paths are relative to workflow file location</li>
</ul>
<h3 id="conditional-execution"><a class="header" href="#conditional-execution">Conditional Execution</a></h3>
<p>Run commands based on conditions:</p>
<pre><code class="language-yaml">- shell: "deploy.sh"
  when: "${branch} == 'main'"  # Only run on main branch
</code></pre>
<h3 id="error-handling"><a class="header" href="#error-handling">Error Handling</a></h3>
<p>Handle failures gracefully:</p>
<pre><code class="language-yaml">- shell: "risky-command"
  on_failure:
    shell: "cleanup.sh"        # Run on failure
  on_success:
    shell: "notify.sh"         # Run on success
</code></pre>
<h3 id="output-capture"><a class="header" href="#output-capture">Output Capture</a></h3>
<p>Capture command output to variables for use in subsequent commands:</p>
<pre><code class="language-yaml">- shell: "git rev-parse HEAD"
  id: "get-commit"
  capture_output: "commit_hash"  # Variable name to capture output
  capture_format: "string"       # Format type (see below)
  capture_streams: "stdout"      # Which streams to capture (default)
</code></pre>
<p><strong>Capture formats:</strong></p>
<ul>
<li><code>string</code> - Raw output as string (default)</li>
<li><code>json</code> - Parse output as JSON object</li>
<li><code>lines</code> - Split output into array of lines</li>
<li><code>number</code> - Parse output as number</li>
<li><code>boolean</code> - Parse output as true/false</li>
</ul>
<p><strong>Stream control:</strong></p>
<ul>
<li><code>capture_streams</code> - Controls which output streams to capture:
<ul>
<li><code>stdout</code> - Capture standard output only (default)</li>
<li><code>stderr</code> - Capture error output only</li>
<li><code>both</code> - Capture both stdout and stderr combined</li>
</ul>
</li>
</ul>
<pre><code class="language-yaml"># Example: Capture stderr separately for error analysis
- shell: "cargo build 2&gt;&amp;1"
  capture_output: "build_output"
  capture_streams: "both"
</code></pre>
<p><strong>Output capture with metadata:</strong></p>
<p>When using <code>capture_output</code> with a variable name, the command output is stored in that variable, and additional metadata is automatically captured as sub-fields:</p>
<ul>
<li><code>${varname}</code> - The captured output (formatted according to <code>capture_format</code>)</li>
<li><code>${varname.exit_code}</code> - The command’s exit code (0 for success)</li>
<li><code>${varname.success}</code> - Boolean indicating command success (true/false)</li>
<li><code>${varname.duration}</code> - Command execution duration in seconds</li>
<li><code>${varname.stderr}</code> - Standard error output (when using <code>capture_streams: "both"</code>)</li>
</ul>
<p>Example:</p>
<pre><code class="language-yaml">- shell: "cargo test"
  capture_output: "test_results"
  capture_format: "string"

- shell: "echo 'Exit code: ${test_results.exit_code}'"
- shell: "echo 'Success: ${test_results.success}'"
- shell: "echo 'Duration: ${test_results.duration}s'"
</code></pre>
<p>For comprehensive coverage of these options, see:</p>
<ul>
<li><a href="advanced.html">Advanced Features</a> - Conditional execution, output capture, timeouts</li>
<li><a href="error-handling.html">Error Handling</a> - on_failure and on_success handlers</li>
<li><a href="variables.html">Variables</a> - Variable interpolation and capture formats</li>
</ul>
<h2 id="environment-configuration"><a class="header" href="#environment-configuration">Environment Configuration</a></h2>
<p>Environment variables can be configured at multiple levels:</p>
<h3 id="global-environment-variables"><a class="header" href="#global-environment-variables">Global Environment Variables</a></h3>
<p><strong>Basic (Static) Variables:</strong></p>
<pre><code class="language-yaml">env:
  NODE_ENV: production
  DATABASE_URL: postgres://localhost/mydb
</code></pre>
<p><strong>Advanced Variable Types:</strong></p>
<p>Prodigy supports three types of environment variable values:</p>
<ol>
<li><strong>Static</strong> - Plain string values (shown above)</li>
<li><strong>Dynamic</strong> - Execute commands to compute values (with caching support)</li>
<li><strong>Conditional</strong> - Use expressions to compute values based on conditions</li>
</ol>
<p><strong>Dynamic Variables:</strong></p>
<p>Execute shell commands to generate values:</p>
<pre><code class="language-yaml">env:
  GIT_COMMIT:
    command: "git rev-parse HEAD"
    cache: true  # Cache result for performance
  BUILD_TIME:
    command: "date -u +%Y-%m-%dT%H:%M:%SZ"
    cache: false  # Recompute each time
</code></pre>
<p><strong>Conditional Variables:</strong></p>
<p>Use expressions to set values based on conditions:</p>
<pre><code class="language-yaml">env:
  LOG_LEVEL:
    condition: "${CI} == 'true'"
    if_true: "debug"
    if_false: "info"
  API_URL:
    condition: "${NODE_ENV} == 'production'"
    if_true: "https://api.example.com"
    if_false: "http://localhost:3000"
</code></pre>
<h3 id="secret-variables"><a class="header" href="#secret-variables">Secret Variables</a></h3>
<p>Secret variables are masked in logs for security. Prodigy supports multiple secret providers:</p>
<p><strong>Environment Variable Provider:</strong></p>
<pre><code class="language-yaml">secrets:
  API_KEY: "${env:SECRET_API_KEY}"
  DB_PASSWORD: "${env:DATABASE_PASSWORD}"
</code></pre>
<p><strong>File Provider:</strong></p>
<p>Read secrets from files:</p>
<pre><code class="language-yaml">secrets:
  SSH_KEY: "${file:/path/to/ssh/key}"
  TLS_CERT: "${file:~/.config/certs/client.pem}"
</code></pre>
<p><strong>Vault Provider:</strong></p>
<p>Retrieve secrets from HashiCorp Vault:</p>
<pre><code class="language-yaml">secrets:
  DATABASE_PASSWORD: "${vault:secret/data/myapp/db}"
  API_TOKEN: "${vault:auth/token/myapp}"
</code></pre>
<p><strong>AWS Secrets Manager Provider:</strong></p>
<p>Fetch secrets from AWS:</p>
<pre><code class="language-yaml">secrets:
  RDS_PASSWORD: "${aws:prod/database/password}"
  API_KEY: "${aws:prod/api/key}"
</code></pre>
<p><strong>Custom Provider:</strong></p>
<p>Use custom secret providers:</p>
<pre><code class="language-yaml">secrets:
  CUSTOM_SECRET: "${custom:my-provider:secret-name}"
</code></pre>
<p>All secret values are masked in logs, error messages, and output regardless of provider type.</p>
<h3 id="environment-files"><a class="header" href="#environment-files">Environment Files</a></h3>
<p>Load variables from .env files:</p>
<pre><code class="language-yaml">env_files:
  - .env
  - .env.production
</code></pre>
<h3 id="environment-profiles"><a class="header" href="#environment-profiles">Environment Profiles</a></h3>
<p>Switch between different environment contexts:</p>
<pre><code class="language-yaml">profiles:
  development:
    NODE_ENV: development
    DEBUG: "true"
    API_URL: http://localhost:3000

  production:
    NODE_ENV: production
    DEBUG: "false"
    API_URL: https://api.example.com
</code></pre>
<p>Activate a profile with: <code>prodigy run --profile development</code></p>
<h3 id="step-level-environment-overrides"><a class="header" href="#step-level-environment-overrides">Step-Level Environment Overrides</a></h3>
<p>Individual commands can override or add environment variables:</p>
<pre><code class="language-yaml">- shell: "npm test"
  env:
    NODE_ENV: test
    DEBUG: "true"

- shell: "cargo build"
  env:
    RUST_BACKTRACE: full
</code></pre>
<p>Step-level environment variables override global variables for that command only.</p>
<p><strong>Clearing Parent Environment:</strong></p>
<p>Use <code>clear_env</code> to start with a clean environment (no inherited global variables):</p>
<pre><code class="language-yaml">- shell: "isolated-command"
  env:
    ONLY_VAR: "value"
    clear_env: true  # Start fresh, ignore global env
</code></pre>
<p>This is useful for commands that need complete isolation from the global environment. When <code>clear_env: true</code>, only the step-level environment variables are available to the command. Default is <code>false</code> (inherit global environment).</p>
<p>For more details, see the <a href="environment.html">Environment Variables chapter</a>.</p>
<h2 id="merge-workflows"><a class="header" href="#merge-workflows">Merge Workflows</a></h2>
<p>Merge workflows execute when merging worktree changes back to the main branch. This feature enables custom validation, testing, and conflict resolution before integrating changes.</p>
<p><strong>When to use merge workflows:</strong></p>
<ul>
<li>Run tests before merging</li>
<li>Validate code quality</li>
<li>Handle merge conflicts automatically</li>
<li>Sync with upstream changes</li>
</ul>
<pre><code class="language-yaml">merge:
  - shell: "git fetch origin"
  - shell: "git merge origin/main"
  - shell: "cargo test"
  - claude: "/prodigy-merge-worktree ${merge.source_branch}"
  timeout: 600  # Optional: timeout for entire merge phase (seconds)
</code></pre>
<p><strong>Available merge variables:</strong></p>
<ul>
<li><code>${merge.worktree}</code> - Worktree name (e.g., “prodigy-session-abc123”)</li>
<li><code>${merge.source_branch}</code> - Source branch (worktree branch)</li>
<li><code>${merge.target_branch}</code> - Target branch (usually main/master)</li>
<li><code>${merge.session_id}</code> - Session ID for correlation</li>
</ul>
<p>These variables are only available within the merge workflow context.</p>
<h2 id="complete-example"><a class="header" href="#complete-example">Complete Example</a></h2>
<p>Here’s a complete workflow combining multiple features:</p>
<pre><code class="language-yaml"># Environment configuration
env:
  RUST_BACKTRACE: 1

env_files:
  - .env

profiles:
  ci:
    CI: "true"
    VERBOSE: "true"

# Workflow commands
commands:
  - shell: "cargo fmt --check"
  - shell: "cargo clippy -- -D warnings"
  - shell: "cargo test --all"
  - claude: "/prodigy-lint"

# Custom merge workflow
merge:
  - shell: "cargo test"
  - claude: "/prodigy-merge-worktree ${merge.source_branch}"
</code></pre>
<h2 id="next-steps-1"><a class="header" href="#next-steps-1">Next Steps</a></h2>
<p>Now that you understand basic workflows, explore these topics:</p>
<ul>
<li><strong><a href="commands.html">Command Types</a></strong> - Detailed guide to all command types and options</li>
<li><strong><a href="advanced.html">Advanced Features</a></strong> - Conditional execution, output capture, goal seeking, and more</li>
<li><strong><a href="environment.html">Environment Variables</a></strong> - Advanced environment configuration</li>
<li><strong><a href="error-handling.html">Error Handling</a></strong> - Handle failures gracefully</li>
<li><strong><a href="mapreduce.html">MapReduce Workflows</a></strong> - Parallel processing for large-scale tasks</li>
<li><strong><a href="variables.html">Variables</a></strong> - Variable interpolation and usage</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mapreduce-workflows"><a class="header" href="#mapreduce-workflows">MapReduce Workflows</a></h1>
<h2 id="quick-start-1"><a class="header" href="#quick-start-1">Quick Start</a></h2>
<p>Want to get started with MapReduce? Here’s a minimal working example:</p>
<pre><code class="language-yaml">name: my-first-mapreduce
mode: mapreduce

# Generate work items
setup:
  - shell: "echo '[{\"id\": 1, \"name\": \"task-1\"}, {\"id\": 2, \"name\": \"task-2\"}]' &gt; items.json"

# Process items in parallel
map:
  input: "items.json"
  json_path: "$[*]"
  agent_template:
    - shell: "echo Processing ${item.name}"
  max_parallel: 5

# Aggregate results
reduce:
  - shell: "echo Completed ${map.successful}/${map.total} items"
</code></pre>
<p>Run it:</p>
<pre><code class="language-bash">prodigy run workflow.yml
</code></pre>
<p>That’s it! Now let’s explore the full capabilities.</p>
<h2 id="complete-structure"><a class="header" href="#complete-structure">Complete Structure</a></h2>
<pre><code class="language-yaml">name: parallel-processing
mode: mapreduce

# Optional setup phase
setup:
  - shell: "generate-work-items.sh"
  - shell: "debtmap analyze . --output items.json"

# Map phase: Process items in parallel
map:
  # Input source (JSON file or command)
  input: "items.json"

  # JSONPath expression to extract items
  json_path: "$.items[*]"

  # Agent template (commands run for each item)
  # Modern syntax: Commands directly under agent_template
  agent_template:
    - claude: "/process '${item}'"
    - shell: "test ${item.path}"
      on_failure:
        claude: "/fix-issue '${item}'"

  # DEPRECATED: Nested 'commands' syntax (still supported)
  # agent_template:
  #   commands:
  #     - claude: "/process '${item}'"

  # Maximum parallel agents (can use environment variables)
  max_parallel: 10  # or max_parallel: "$MAX_WORKERS"

  # Optional: Filter items
  filter: "item.score &gt;= 5"

  # Optional: Sort items
  sort_by: "item.priority DESC"

  # Optional: Limit number of items
  max_items: 100

  # Optional: Skip items
  offset: 10

  # Optional: Deduplicate by field
  distinct: "item.id"

  # Optional: Agent timeout in seconds
  agent_timeout_secs: 300

  # Optional: Advanced timeout configuration (alternative to agent_timeout_secs)
  # timeout_config:
  #   default: "5m"
  #   per_command: "2m"

# Reduce phase: Aggregate results
# Modern syntax: Commands directly under reduce
reduce:
  - claude: "/summarize ${map.results}"
  - shell: "echo 'Processed ${map.successful}/${map.total} items'"

# DEPRECATED: Nested 'commands' syntax (still supported)
# reduce:
#   commands:
#     - claude: "/summarize ${map.results}"

# Optional: Custom merge workflow (supports two formats)
merge:
  # Simple array format
  - shell: "git fetch origin"
  - claude: "/merge-worktree ${merge.source_branch}"
  - shell: "cargo test"

# OR full format with timeout
# merge:
#   commands:
#     - shell: "git fetch origin"
#     - claude: "/merge-worktree ${merge.source_branch}"
#   timeout: 600  # Timeout in seconds

# Error handling policy
error_policy:
  on_item_failure: dlq  # dlq, retry, skip, stop, or custom handler name
  continue_on_failure: true
  max_failures: 5
  failure_threshold: 0.2  # 20% failure rate
  error_collection: aggregate  # aggregate, immediate, or batched:N

  # Circuit breaker configuration
  circuit_breaker:
    failure_threshold: 5      # Open circuit after N failures
    success_threshold: 2      # Close circuit after N successes
    timeout: "60s"           # Duration before attempting half-open (humantime format: "1s", "1m", "5m")
    half_open_requests: 3    # Test requests in half-open state

  # Retry configuration with backoff
  retry_config:
    max_attempts: 3
    # BackoffStrategy is an enum - use one variant:
    backoff:
      exponential:
        initial: "1s"
        multiplier: 2.0

# Convenience fields (alternative to nested error_policy)
# These top-level fields map to error_policy for simpler syntax
on_item_failure: dlq
continue_on_failure: true
max_failures: 5
</code></pre>
<h2 id="environment-variables-in-configuration"><a class="header" href="#environment-variables-in-configuration">Environment Variables in Configuration</a></h2>
<p>Many MapReduce configuration fields support environment variable interpolation, allowing you to parameterize workflows:</p>
<p><strong>Supported Fields:</strong></p>
<ul>
<li><code>max_parallel</code> - Control parallelism dynamically</li>
<li><code>agent_timeout_secs</code> - Adjust timeouts per environment</li>
<li><code>setup.timeout</code> - Configure setup phase timeouts</li>
<li><code>merge.timeout</code> - Control merge operation timeouts</li>
<li>Any string field in your workflow</li>
</ul>
<p><strong>Usage Example:</strong></p>
<pre><code class="language-yaml">name: configurable-mapreduce
mode: mapreduce

# Define environment variables (see Variables chapter for details)
env:
  MAX_WORKERS: "10"
  AGENT_TIMEOUT: "300"

map:
  input: "items.json"
  json_path: "$[*]"
  max_parallel: "$MAX_WORKERS"      # Use environment variable
  agent_timeout_secs: "$AGENT_TIMEOUT"
  agent_template:
    - claude: "/process ${item}"

setup:
  timeout: "$SETUP_TIMEOUT"  # Can also reference env vars
  commands:
    - shell: "prepare-data.sh"
</code></pre>
<p><strong>Running with Different Values:</strong></p>
<pre><code class="language-bash"># Development: Lower parallelism
MAX_WORKERS=5 AGENT_TIMEOUT=600 prodigy run workflow.yml

# Production: Higher parallelism
MAX_WORKERS=20 AGENT_TIMEOUT=300 prodigy run workflow.yml
</code></pre>
<p>See the <a href="./variables.html">Variables chapter</a> for comprehensive environment variable documentation including profiles, secrets, and advanced usage.</p>
<h2 id="backoff-strategies"><a class="header" href="#backoff-strategies">Backoff Strategies</a></h2>
<p>The <code>retry_config.backoff</code> field uses an enum-based configuration. Choose ONE of the following strategies:</p>
<h3 id="fixed-delay"><a class="header" href="#fixed-delay">Fixed Delay</a></h3>
<p>Retry with a constant delay between attempts:</p>
<pre><code class="language-yaml">retry_config:
  max_attempts: 3
  backoff:
    fixed:
      delay: "1s"  # Constant delay (e.g., "1s", "500ms", "2m")
</code></pre>
<h3 id="linear-backoff"><a class="header" href="#linear-backoff">Linear Backoff</a></h3>
<p>Increase delay linearly with each attempt:</p>
<pre><code class="language-yaml">retry_config:
  max_attempts: 5
  backoff:
    linear:
      initial: "1s"      # First retry delay
      increment: "500ms" # Add this much for each subsequent retry
</code></pre>
<p>Example delays: 1s, 1.5s, 2s, 2.5s, 3s</p>
<h3 id="exponential-backoff"><a class="header" href="#exponential-backoff">Exponential Backoff</a></h3>
<p>Double (or multiply) the delay with each attempt:</p>
<pre><code class="language-yaml">retry_config:
  max_attempts: 4
  backoff:
    exponential:
      initial: "1s"    # First retry delay
      multiplier: 2.0  # Multiply delay by this factor each time
</code></pre>
<p>Example delays (multiplier=2.0): 1s, 2s, 4s, 8s</p>
<h3 id="fibonacci-backoff"><a class="header" href="#fibonacci-backoff">Fibonacci Backoff</a></h3>
<p>Use Fibonacci sequence for delays (gradual exponential growth):</p>
<pre><code class="language-yaml">retry_config:
  max_attempts: 6
  backoff:
    fibonacci:
      initial: "1s"  # Base delay unit
</code></pre>
<p>Example delays: 1s, 1s, 2s, 3s, 5s, 8s</p>
<p><strong>Important Notes:</strong></p>
<ul>
<li>All duration fields use <a href="https://docs.rs/humantime/latest/humantime/">humantime format</a> via <a href="https://docs.rs/humantime-serde/">humantime_serde</a>: “1s”, “500ms”, “2m”, “1h30m”, “1.5s”</li>
<li>The humantime_serde library enables flexible duration parsing - you can use units like “s”, “ms”, “m”, “h” and combine them (e.g., “1h30m”)</li>
<li>The <code>backoff</code> field is an <strong>enum</strong> - use ONE variant, not a <code>type</code> discriminator</li>
<li>Use <code>max_attempts</code> to limit total retries (there is no <code>max_delay</code> field)</li>
<li>Choose strategy based on your use case:
<ul>
<li><strong>Fixed</strong>: Predictable timing, good for known transient issues</li>
<li><strong>Linear</strong>: Gradual increase, good for slowly-recovering services</li>
<li><strong>Exponential</strong>: Fast backoff, good for rate limiting and congestion</li>
<li><strong>Fibonacci</strong>: Balanced growth, good for general-purpose retries</li>
</ul>
</li>
</ul>
<h2 id="error-collection-strategies"><a class="header" href="#error-collection-strategies">Error Collection Strategies</a></h2>
<p>The <code>error_collection</code> field controls how errors are reported during workflow execution:</p>
<pre><code class="language-yaml">error_policy:
  # Collect all errors and report at workflow end (default)
  error_collection: aggregate

  # OR: Report each error immediately as it occurs
  error_collection: immediate

  # OR: Report errors in batches of N items
  error_collection: batched:10
</code></pre>
<p><strong>Strategy Behaviors:</strong></p>
<ul>
<li><code>aggregate</code> - Collect all errors in memory and report at the end
<ul>
<li>Use for: Final summary reports, batch processing where individual failures don’t need immediate attention</li>
<li>Trade-off: Low noise, but you won’t see errors until completion</li>
</ul>
</li>
<li><code>immediate</code> - Log/report each error as soon as it happens
<ul>
<li>Use for: Debugging, development, real-time monitoring</li>
<li>Trade-off: More verbose, but immediate visibility</li>
</ul>
</li>
<li><code>batched:N</code> - Report errors in batches of N items
<ul>
<li>Use for: Progress updates without spam, monitoring long-running jobs</li>
<li>Trade-off: Balance between noise and visibility (e.g., <code>batched:10</code> reports every 10 failures)</li>
</ul>
</li>
</ul>
<h2 id="setup-phase-advanced"><a class="header" href="#setup-phase-advanced">Setup Phase (Advanced)</a></h2>
<p>The setup phase supports two formats: simple array OR full configuration object.</p>
<pre><code class="language-yaml"># Simple array format
setup:
  - shell: "prepare-data.sh"
  - shell: "analyze-codebase.sh"

# Full configuration format with timeout and capture
setup:
  commands:
    - shell: "prepare-data.sh"
    - shell: "analyze-codebase.sh"

  # Timeout for entire setup phase (seconds, can use environment variables)
  timeout: 300  # or timeout: "$SETUP_TIMEOUT"

  # Capture outputs from setup commands
  capture_outputs:
    # Simple format (shorthand - captures stdout with defaults)
    file_count: 0  # Capture stdout from command at index 0

    # Detailed CaptureConfig format
    analysis_result:
      command_index: 1
      source: stdout           # stdout, stderr, both, combined
      json_path: "$.result"    # Extract JSON field
      max_size: 1048576        # Max bytes (1MB)
      default: "{}"            # Fallback if extraction fails
      multiline: preserve      # preserve, join, first_line, last_line, array
</code></pre>
<p><strong>Setup Phase Fields:</strong></p>
<ul>
<li><code>commands</code> - Array of commands to execute (or use simple array format at top level)</li>
<li><code>timeout</code> - Timeout for entire setup phase in seconds</li>
<li><code>capture_outputs</code> - Map of variable names to command outputs (Simple or Detailed format)</li>
</ul>
<h3 id="capture-configuration-formats"><a class="header" href="#capture-configuration-formats">Capture Configuration Formats</a></h3>
<p>Prodigy supports two CaptureConfig formats:</p>
<p><strong>Simple Format</strong> - Shorthand for common cases:</p>
<pre><code class="language-yaml">capture_outputs:
  file_count: 0  # Captures stdout from command at index 0 with default settings
</code></pre>
<p><strong>Detailed Format</strong> - Full control over capture behavior:</p>
<pre><code class="language-yaml">capture_outputs:
  analysis_result:
    command_index: 1                # Which command to capture from
    source: stdout                  # stdout, stderr, both, combined
    pattern: "version: (\\d+\\.\\d+)" # Regex extraction (optional)
    json_path: "$.result"           # JSON path extraction (optional)
    max_size: 1048576               # Max bytes to capture (optional, default 1MB)
    default: "{}"                   # Fallback value if extraction fails (optional)
    multiline: preserve             # How to handle multiple lines (optional)
</code></pre>
<p><strong>CaptureConfig Fields:</strong></p>
<ul>
<li><code>command_index</code> - Zero-based index of command to capture from (required)</li>
<li><code>source</code> - Where to capture from (optional, default: stdout):
<ul>
<li><code>stdout</code> - Capture standard output only</li>
<li><code>stderr</code> - Capture standard error only</li>
<li><code>both</code> - Capture both with labels (stdout:\n…\nstderr:\n…)</li>
<li><code>combined</code> - Interleave stdout and stderr in order</li>
</ul>
</li>
<li><code>pattern</code> - Regex pattern for extraction (optional, use with capture groups)</li>
<li><code>json_path</code> - JSONPath expression for JSON extraction (optional, e.g., “$.items[*].name”)</li>
<li><code>max_size</code> - Maximum bytes to capture (optional, default: 1MB)</li>
<li><code>default</code> - Fallback value if extraction fails (optional)</li>
<li><code>multiline</code> - How to handle multiple output lines (optional, default: preserve):
<ul>
<li><code>preserve</code> - Keep all lines with newlines</li>
<li><code>join</code> - Join lines with spaces (useful for single-line summaries)</li>
<li><code>first_line</code> - Take only first line (useful for version strings)</li>
<li><code>last_line</code> - Take only last line (useful for final status)</li>
<li><code>array</code> - Return as JSON array of lines (useful for lists)</li>
</ul>
</li>
</ul>
<p><strong>Capture Configuration Examples:</strong></p>
<pre><code class="language-yaml"># Extract version number from output
capture_outputs:
  version:
    command_index: 0
    pattern: "version: (\\d+\\.\\d+\\.\\d+)"
    multiline: first_line

# Parse JSON result
capture_outputs:
  items:
    command_index: 1
    source: stdout
    json_path: "$.items[*]"

# Capture error messages for debugging
capture_outputs:
  errors:
    command_index: 2
    source: stderr
    multiline: array

# Get file count as single number
capture_outputs:
  count:
    command_index: 3
    pattern: "(\\d+) files"
    multiline: first_line
    default: "0"
</code></pre>
<p><strong>When to use Simple vs Detailed:</strong></p>
<ul>
<li>Use <strong>Simple</strong> when you only need stdout with default settings</li>
<li>Use <strong>Detailed</strong> when you need:
<ul>
<li>Pattern extraction with regex</li>
<li>JSON parsing with JSONPath</li>
<li>Custom source (stderr, both, combined)</li>
<li>Multiline handling options</li>
<li>Fallback values with <code>default</code></li>
</ul>
</li>
</ul>
<h2 id="global-storage-architecture"><a class="header" href="#global-storage-architecture">Global Storage Architecture</a></h2>
<p>MapReduce workflows use a global storage architecture located in <code>~/.prodigy/</code> (not <code>.prodigy/</code> in your project). This enables:</p>
<ul>
<li><strong>Cross-worktree event aggregation</strong>: Multiple worktrees working on the same job share event logs</li>
<li><strong>Persistent state management</strong>: Job checkpoints survive worktree cleanup</li>
<li><strong>Centralized monitoring</strong>: All job data accessible from a single location</li>
<li><strong>Efficient storage</strong>: Deduplication across worktrees</li>
</ul>
<h3 id="storage-locations"><a class="header" href="#storage-locations">Storage Locations</a></h3>
<pre><code>~/.prodigy/
├── events/
│   └── {repo_name}/          # Events grouped by repository
│       └── {job_id}/         # Job-specific events
│           └── events-{timestamp}.jsonl  # Event log files
├── dlq/
│   └── {repo_name}/          # DLQ grouped by repository
│       └── {job_id}/         # Job-specific failed items
└── state/
    └── {repo_name}/          # State grouped by repository
        └── mapreduce/        # MapReduce job states
            └── jobs/
                └── {job_id}/ # Job-specific checkpoints
</code></pre>
<h2 id="event-tracking"><a class="header" href="#event-tracking">Event Tracking</a></h2>
<p>All MapReduce execution events are logged to <code>~/.prodigy/events/{repo_name}/{job_id}/</code> for debugging and monitoring:</p>
<p><strong>Events Tracked:</strong></p>
<ul>
<li>Agent lifecycle events (started, completed, failed)</li>
<li>Work item processing status</li>
<li>Checkpoint saves for resumption</li>
<li>Error details with correlation IDs</li>
<li>Cross-worktree event aggregation for parallel jobs</li>
</ul>
<p><strong>Event Log Format:</strong>
Events are stored in JSONL (JSON Lines) format, with each line representing a single event:</p>
<pre><code class="language-json">{"timestamp":"2024-01-01T12:00:00Z","event_type":"agent_started","agent_id":"agent-1","item_id":"item-001"}
{"timestamp":"2024-01-01T12:05:00Z","event_type":"agent_completed","agent_id":"agent-1","item_id":"item-001","status":"success"}
</code></pre>
<p><strong>Viewing Events:</strong></p>
<pre><code class="language-bash"># View all events for a job
prodigy events &lt;job_id&gt;

# Stream events in real-time
prodigy events &lt;job_id&gt; --follow
</code></pre>
<h2 id="checkpoint-and-resume"><a class="header" href="#checkpoint-and-resume">Checkpoint and Resume</a></h2>
<p>MapReduce workflows automatically save checkpoints to enable resumption after interruption.</p>
<h3 id="checkpoint-structure"><a class="header" href="#checkpoint-structure">Checkpoint Structure</a></h3>
<p>Checkpoints are stored in <code>~/.prodigy/state/{repo_name}/mapreduce/jobs/{job_id}/</code> and contain:</p>
<pre><code class="language-json">{
  "job_id": "mapreduce-1234567890",
  "workflow_file": "workflow.yml",
  "phase": "map",
  "items_processed": 45,
  "items_total": 100,
  "items_remaining": ["item-046", "item-047", "..."],
  "successful_items": 43,
  "failed_items": 2,
  "started_at": "2024-01-01T12:00:00Z",
  "last_checkpoint_at": "2024-01-01T12:30:00Z"
}
</code></pre>
<h3 id="resume-behavior"><a class="header" href="#resume-behavior">Resume Behavior</a></h3>
<p>When resuming a MapReduce job:</p>
<ol>
<li><strong>Checkpoint Loading</strong>: Prodigy loads the most recent checkpoint from <code>~/.prodigy/state/</code></li>
<li><strong>Work Item Recovery</strong>: Items marked as “in progress” are reset to “pending”</li>
<li><strong>Failed Item Handling</strong>: Previously failed items are moved to DLQ (not retried automatically)</li>
<li><strong>Partial Results</strong>: Successfully processed items are preserved</li>
<li><strong>Phase Continuation</strong>: Job resumes from the phase it was interrupted in</li>
</ol>
<p><strong>Resume Command:</strong></p>
<pre><code class="language-bash"># Resume from checkpoint
prodigy resume-job &lt;job_id&gt;

# Resume with different parallelism
prodigy resume-job &lt;job_id&gt; --max-parallel 20

# Resume and show detailed logs
prodigy resume-job &lt;job_id&gt; -v
</code></pre>
<h2 id="dead-letter-queue-dlq"><a class="header" href="#dead-letter-queue-dlq">Dead Letter Queue (DLQ)</a></h2>
<p>Failed work items are automatically stored in the DLQ for review and retry.</p>
<h3 id="dlq-storage"><a class="header" href="#dlq-storage">DLQ Storage</a></h3>
<p>Failed items are stored in <code>~/.prodigy/dlq/{repo_name}/{job_id}/</code> with this structure:</p>
<pre><code class="language-json">{
  "item_id": "item-047",
  "item_data": {
    "path": "src/module.rs",
    "score": 8,
    "priority": "high"
  },
  "failure_reason": "Command failed: cargo test",
  "error_details": "test failed: expected X but got Y",
  "failed_at": "2024-01-01T12:15:00Z",
  "attempt_count": 3,
  "correlation_id": "agent-7-item-047"
}
</code></pre>
<h3 id="dlq-retry"><a class="header" href="#dlq-retry">DLQ Retry</a></h3>
<p>The <code>prodigy dlq retry</code> command allows you to reprocess failed items:</p>
<pre><code class="language-bash"># Retry all failed items for a job
prodigy dlq retry &lt;job_id&gt;

# Retry with custom parallelism (default: 5)
prodigy dlq retry &lt;job_id&gt; --max-parallel 10

# Dry run to see what would be retried
prodigy dlq retry &lt;job_id&gt; --dry-run

# Verbose output for debugging
prodigy dlq retry &lt;job_id&gt; -v
</code></pre>
<p><strong>DLQ Retry Features:</strong></p>
<ul>
<li>Streams items to avoid memory issues with large queues</li>
<li>Respects original workflow’s <code>max_parallel</code> setting</li>
<li>Preserves correlation IDs for tracking</li>
<li>Updates DLQ state (removes successful, keeps failed)</li>
<li>Supports interruption and resumption</li>
<li>Retried items inherit original workflow configuration</li>
</ul>
<p><strong>DLQ Retry Workflow:</strong></p>
<ol>
<li>Load failed items from <code>~/.prodigy/dlq/{repo_name}/{job_id}/</code></li>
<li>Process items using original workflow’s agent template</li>
<li>Successfully processed items are removed from DLQ</li>
<li>Still-failing items remain in DLQ with updated attempt count</li>
<li>New failures during retry are logged and added to DLQ</li>
</ol>
<h3 id="viewing-dlq-contents"><a class="header" href="#viewing-dlq-contents">Viewing DLQ Contents</a></h3>
<pre><code class="language-bash"># List all failed items
prodigy dlq list &lt;job_id&gt;

# Show details for specific item
prodigy dlq show &lt;job_id&gt; &lt;item_id&gt;

# Clear DLQ after manual fixes
prodigy dlq clear &lt;job_id&gt;
</code></pre>
<h2 id="common-pitfalls"><a class="header" href="#common-pitfalls">Common Pitfalls</a></h2>
<h3 id="incorrect-captureconfig-fields"><a class="header" href="#incorrect-captureconfig-fields">Incorrect CaptureConfig Fields</a></h3>
<p><strong>Problem:</strong> Using <code>format: json</code> in capture_outputs configuration.</p>
<pre><code class="language-yaml"># ❌ WRONG - 'format' field doesn't exist
capture_outputs:
  result:
    command_index: 0
    format: json
</code></pre>
<p><strong>Solution:</strong> Use the correct CaptureConfig fields:</p>
<pre><code class="language-yaml"># ✅ CORRECT - Use json_path for JSON extraction
capture_outputs:
  result:
    command_index: 0
    json_path: "$.result"
    source: stdout
    multiline: preserve
</code></pre>
<h3 id="incorrect-backoff-enum-syntax"><a class="header" href="#incorrect-backoff-enum-syntax">Incorrect Backoff Enum Syntax</a></h3>
<p><strong>Problem:</strong> Using flat <code>type</code> discriminator for backoff strategy.</p>
<pre><code class="language-yaml"># ❌ WRONG - Using 'type' discriminator
retry_config:
  max_attempts: 3
  backoff:
    type: exponential
    initial: "1s"
    multiplier: 2
</code></pre>
<p><strong>Solution:</strong> Use the correct enum variant syntax:</p>
<pre><code class="language-yaml"># ✅ CORRECT - Enum variant with nested fields
retry_config:
  max_attempts: 3
  backoff:
    exponential:
      initial: "1s"
      multiplier: 2.0
</code></pre>
<h3 id="duration-format-errors"><a class="header" href="#duration-format-errors">Duration Format Errors</a></h3>
<p><strong>Problem:</strong> Using numeric values instead of humantime strings.</p>
<pre><code class="language-yaml"># ❌ WRONG - Numbers without units
backoff:
  fixed:
    delay: 1000  # Unclear: milliseconds? seconds?
</code></pre>
<p><strong>Solution:</strong> Always use humantime format strings:</p>
<pre><code class="language-yaml"># ✅ CORRECT - Explicit units
backoff:
  fixed:
    delay: "1s"  # or "1000ms", "1m", "1h30m"
</code></pre>
<p><strong>Note:</strong> All Duration fields (delay, initial, increment, timeout) use <a href="https://docs.rs/humantime-serde/">humantime_serde</a> for serialization, which supports flexible formats:</p>
<ul>
<li>Single units: <code>"1s"</code>, <code>"500ms"</code>, <code>"2m"</code>, <code>"1h"</code></li>
<li>Combined units: <code>"1h30m"</code>, <code>"2m30s"</code></li>
<li>Decimal values: <code>"1.5s"</code>, <code>"0.5m"</code></li>
</ul>
<p><strong>humantime_serde Format:</strong> Throughout this chapter, any field with a Duration type (such as <code>timeout</code>, <code>delay</code>, <code>initial</code>, <code>increment</code>) uses humantime_serde, allowing you to write durations in human-readable format instead of raw numbers. This makes configuration more intuitive and less error-prone.</p>
<h3 id="confusing-simple-vs-detailed-capture"><a class="header" href="#confusing-simple-vs-detailed-capture">Confusing Simple vs Detailed Capture</a></h3>
<p><strong>Problem:</strong> Treating simple capture format as “legacy” or not knowing when to use it.</p>
<p><strong>Solution:</strong></p>
<ul>
<li>Use <strong>Simple</strong> (<code>variable: 0</code>) when you only need stdout with defaults</li>
<li>Use <strong>Detailed</strong> when you need pattern/json extraction, custom source, or multiline handling</li>
<li>Simple format is NOT deprecated - it’s a valid shorthand</li>
</ul>
<h3 id="missing-multiplier-decimal-point"><a class="header" href="#missing-multiplier-decimal-point">Missing Multiplier Decimal Point</a></h3>
<p><strong>Problem:</strong> Using integer multiplier in exponential backoff.</p>
<pre><code class="language-yaml"># ⚠️  POTENTIAL ISSUE - Integer might work but float is safer
backoff:
  exponential:
    multiplier: 2
</code></pre>
<p><strong>Solution:</strong> Use float values explicitly:</p>
<pre><code class="language-yaml"># ✅ CORRECT - Float value
backoff:
  exponential:
    multiplier: 2.0
</code></pre>
<h3 id="incorrect-variable-references"><a class="header" href="#incorrect-variable-references">Incorrect Variable References</a></h3>
<p><strong>Problem:</strong> Using wrong variable names in map/reduce phases.</p>
<pre><code class="language-yaml"># ❌ WRONG - These variables don't exist
reduce:
  - shell: "echo ${results}"  # Should be ${map.results}
  - shell: "echo ${total}"    # Should be ${map.total}
</code></pre>
<p><strong>Solution:</strong> Use the correct variable namespaces (see <a href="./variables.html">Variables chapter</a>):</p>
<pre><code class="language-yaml"># ✅ CORRECT - Proper variable names
reduce:
  - shell: "echo ${map.results}"
  - shell: "echo ${map.successful}/${map.total} items"
</code></pre>
<h3 id="max-delay-field-doesnt-exist"><a class="header" href="#max-delay-field-doesnt-exist">Max Delay Field (Doesn’t Exist)</a></h3>
<p><strong>Problem:</strong> Trying to use <code>max_delay</code> field in backoff configuration.</p>
<pre><code class="language-yaml"># ❌ WRONG - max_delay is not supported
backoff:
  exponential:
    initial: "1s"
    multiplier: 2.0
    max_delay: "60s"  # This field doesn't exist
</code></pre>
<p><strong>Solution:</strong> Use <code>max_attempts</code> to limit retries instead:</p>
<pre><code class="language-yaml"># ✅ CORRECT - Limit via max_attempts
retry_config:
  max_attempts: 5  # Limits total retry attempts
  backoff:
    exponential:
      initial: "1s"
      multiplier: 2.0
</code></pre>
<h3 id="nested-commands-syntax"><a class="header" href="#nested-commands-syntax">Nested Commands Syntax</a></h3>
<p><strong>Problem:</strong> Mixing modern and deprecated syntax.</p>
<pre><code class="language-yaml"># ⚠️  DEPRECATED but still supported
agent_template:
  commands:
    - shell: "test.sh"
</code></pre>
<p><strong>Solution:</strong> Use modern flat syntax:</p>
<pre><code class="language-yaml"># ✅ MODERN - Commands directly under agent_template
agent_template:
  - shell: "test.sh"
  - claude: "/process '${item}'"
</code></pre>
<h2 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h2>
<h3 id="workflow-validation-errors"><a class="header" href="#workflow-validation-errors">Workflow Validation Errors</a></h3>
<p>If you see validation errors when running a MapReduce workflow:</p>
<ol>
<li><strong>Check backoff syntax</strong>: Ensure you’re using enum variants (e.g., <code>exponential: { initial: "1s" }</code>), not <code>type</code> discriminators</li>
<li><strong>Check duration formats</strong>: All duration fields must use humantime format (e.g., <code>"1s"</code>, <code>"500ms"</code>)</li>
<li><strong>Check CaptureConfig fields</strong>: Don’t use <code>format</code> - use <code>json_path</code> or <code>pattern</code> instead</li>
<li><strong>Check variable references</strong>: Use <code>${map.*}</code>, <code>${item.*}</code>, <code>${merge.*}</code> namespaces</li>
</ol>
<h3 id="common-error-messages"><a class="header" href="#common-error-messages">Common Error Messages</a></h3>
<p><strong>“unknown field <code>format</code>”</strong></p>
<ul>
<li>You’re using <code>format</code> in CaptureConfig</li>
<li>Solution: Remove <code>format</code>, use <code>json_path</code> or <code>pattern</code></li>
</ul>
<p><strong>“missing field <code>delay</code>” (in Fixed backoff)</strong></p>
<ul>
<li>You’re using <code>initial</code> instead of <code>delay</code> for Fixed strategy</li>
<li>Solution: Fixed uses <code>delay</code>, not <code>initial</code></li>
</ul>
<p><strong>“data did not match any variant”</strong></p>
<ul>
<li>Your backoff configuration doesn’t match any enum variant</li>
<li>Solution: Check the exact field names for your chosen strategy</li>
</ul>
<p><strong>“invalid value: integer, expected a string”</strong></p>
<ul>
<li>You’re using a number for a duration field</li>
<li>Solution: Use quoted humantime strings (e.g., <code>"1s"</code> instead of <code>1</code>)</li>
</ul>
<h3 id="performance-issues"><a class="header" href="#performance-issues">Performance Issues</a></h3>
<p><strong>Too many parallel agents overwhelming system:</strong></p>
<ul>
<li>Reduce <code>max_parallel</code> value</li>
<li>Use circuit breaker to prevent cascading failures</li>
<li>Monitor with <code>prodigy events &lt;job_id&gt; --follow</code></li>
</ul>
<p><strong>DLQ filling up with same errors:</strong></p>
<ul>
<li>Check DLQ contents: <code>prodigy dlq list &lt;job_id&gt;</code></li>
<li>Fix root cause before retrying</li>
<li>Use <code>error_collection: immediate</code> for faster debugging</li>
</ul>
<h3 id="resume-not-working"><a class="header" href="#resume-not-working">Resume Not Working</a></h3>
<p><strong>Checkpoint not found:</strong></p>
<ul>
<li>Check <code>~/.prodigy/state/{repo_name}/mapreduce/jobs/{job_id}/</code></li>
<li>Ensure you’re using the correct job_id</li>
<li>Run <code>prodigy resume-job &lt;job_id&gt; -v</code> for detailed logs</li>
</ul>
<p><strong>Resume starts from beginning:</strong></p>
<ul>
<li>Checkpoints may be corrupted</li>
<li>Check event logs: <code>prodigy events &lt;job_id&gt;</code></li>
<li>Consider using offset to skip already-processed items</li>
</ul>
<h2 id="performance-tuning"><a class="header" href="#performance-tuning">Performance Tuning</a></h2>
<h3 id="choosing-max_parallel"><a class="header" href="#choosing-max_parallel">Choosing max_parallel</a></h3>
<p>The <code>max_parallel</code> setting controls how many agents run concurrently. Choose based on:</p>
<p><strong>System Resources:</strong></p>
<ul>
<li><strong>CPU-bound tasks</strong> (compilation, analysis): <code>max_parallel = CPU cores * 0.75</code></li>
<li><strong>I/O-bound tasks</strong> (API calls, file operations): <code>max_parallel = CPU cores * 2</code></li>
<li><strong>Memory-intensive tasks</strong>: Lower value to avoid OOM (e.g., <code>max_parallel = 4</code>)</li>
</ul>
<p><strong>Work Item Characteristics:</strong></p>
<ul>
<li><strong>Fast items</strong> (&lt;30s each): Higher parallelism (10-20) for throughput</li>
<li><strong>Slow items</strong> (&gt;5min each): Lower parallelism (3-5) to avoid timeout cascades</li>
<li><strong>Flaky items</strong> (transient failures): Use circuit breaker + lower parallelism</li>
</ul>
<p><strong>Example Configurations:</strong></p>
<pre><code class="language-yaml"># Code review across 100 PRs (API-bound, fast)
map:
  max_parallel: 20
  agent_timeout_secs: 120

# Multi-file refactoring (CPU/memory-bound, slow)
map:
  max_parallel: 4
  agent_timeout_secs: 600

# Test suite execution (flaky, medium)
map:
  max_parallel: 8
  agent_timeout_secs: 300
  error_policy:
    circuit_breaker:
      failure_threshold: 3
</code></pre>
<h3 id="timeout-configuration"><a class="header" href="#timeout-configuration">Timeout Configuration</a></h3>
<p>Choose <code>agent_timeout_secs</code> based on task complexity:</p>
<ul>
<li><strong>Simple tasks</strong> (file operations): 60-120 seconds</li>
<li><strong>Medium tasks</strong> (code analysis): 300 seconds (default)</li>
<li><strong>Complex tasks</strong> (refactoring, tests): 600-1200 seconds</li>
<li><strong>Very slow tasks</strong> (large builds): 1800+ seconds</li>
</ul>
<p><strong>Warning:</strong> Set timeout too low → premature failures. Set too high → hung agents block progress.</p>
<h3 id="circuit-breaker-tuning"><a class="header" href="#circuit-breaker-tuning">Circuit Breaker Tuning</a></h3>
<p>Use circuit breakers to prevent cascading failures:</p>
<pre><code class="language-yaml">error_policy:
  circuit_breaker:
    failure_threshold: 5      # Open after 5 consecutive failures
    success_threshold: 2      # Close after 2 successes in half-open
    timeout: "60s"           # Try again after 1 minute
    half_open_requests: 3    # Test with 3 requests before fully closing
</code></pre>
<p><strong>When to use:</strong></p>
<ul>
<li>External API dependencies (rate limiting, downtime)</li>
<li>Flaky test suites (intermittent failures)</li>
<li>Resource contention (database connections, file locks)</li>
</ul>
<p><strong>Tuning guidelines:</strong></p>
<ul>
<li><strong>Sensitive systems</strong>: Lower <code>failure_threshold</code> (3-5), shorter <code>timeout</code> (30s-1m)</li>
<li><strong>Robust systems</strong>: Higher <code>failure_threshold</code> (10+), longer <code>timeout</code> (5m-10m)</li>
<li><strong>Testing recovery</strong>: Lower <code>half_open_requests</code> (1-2) for faster validation</li>
</ul>
<h2 id="real-world-use-cases"><a class="header" href="#real-world-use-cases">Real-World Use Cases</a></h2>
<h3 id="use-case-1-code-review-across-prs"><a class="header" href="#use-case-1-code-review-across-prs">Use Case 1: Code Review Across PRs</a></h3>
<p>Review all open pull requests in parallel:</p>
<pre><code class="language-yaml">name: review-all-prs
mode: mapreduce

setup:
  - shell: "gh pr list --json number,title,headRefName --limit 100 &gt; prs.json"
  capture_outputs:
    prs: 0

map:
  input: "prs.json"
  json_path: "$[*]"
  agent_template:
    - shell: "gh pr checkout ${item.number}"
    - claude: "/review-pr ${item.number}"
    - shell: "gh pr review ${item.number} --comment --body-file review.md"
  max_parallel: 10
  agent_timeout_secs: 300

reduce:
  - claude: "/summarize-reviews ${map.results}"
  - shell: "echo '✅ Reviewed ${map.successful} PRs'"
</code></pre>
<h3 id="use-case-2-multi-file-refactoring"><a class="header" href="#use-case-2-multi-file-refactoring">Use Case 2: Multi-File Refactoring</a></h3>
<p>Refactor a common pattern across many files:</p>
<pre><code class="language-yaml">name: refactor-error-handling
mode: mapreduce

setup:
  - shell: "rg -l 'unwrap\\(\\)' src/ --json | jq -s 'map({path: .data.path.text})' &gt; files.json"

map:
  input: "files.json"
  json_path: "$[*]"
  agent_template:
    - claude: "/refactor-unwrap ${item.path}"
    - shell: "cargo test --lib -- --test-threads=1"
      on_failure:
        claude: "/fix-tests ${item.path}"
  max_parallel: 4
  agent_timeout_secs: 600

reduce:
  - shell: "cargo test --all"
  - claude: "/verify-refactoring ${map.results}"
</code></pre>
<h3 id="use-case-3-documentation-drift-analysis"><a class="header" href="#use-case-3-documentation-drift-analysis">Use Case 3: Documentation Drift Analysis</a></h3>
<p>Analyze and fix documentation for multiple chapters:</p>
<pre><code class="language-yaml">name: fix-docs-drift
mode: mapreduce

setup:
  - shell: "ls book/src/*.md | jq -R -s 'split(\"\\n\") | map(select(length &gt; 0)) | map({path: .})' &gt; chapters.json"

map:
  input: "chapters.json"
  json_path: "$[*]"
  agent_template:
    - claude: "/analyze-drift ${item.path}"
    - claude: "/fix-drift ${item.path}"
  max_parallel: 8
  filter: "item.path != 'book/src/SUMMARY.md'"

reduce:
  - claude: "/summarize-drift-fixes ${map.results}"
  - shell: "mdbook build book/"
</code></pre>
<h3 id="use-case-4-test-suite-parallelization"><a class="header" href="#use-case-4-test-suite-parallelization">Use Case 4: Test Suite Parallelization</a></h3>
<p>Run test suites in parallel across modules:</p>
<pre><code class="language-yaml">name: parallel-tests
mode: mapreduce

setup:
  - shell: "cargo test --list --format json | jq -s 'map(select(.type == \"test\")) | map({name: .name})' &gt; tests.json"

map:
  input: "tests.json"
  json_path: "$[*]"
  agent_template:
    - shell: "cargo test ${item.name} -- --exact"
  max_parallel: 16
  agent_timeout_secs: 120
  error_policy:
    on_item_failure: dlq
    continue_on_failure: true

reduce:
  - shell: "prodigy dlq list ${job_id}"
  - shell: "echo '✅ ${map.successful}/${map.total} tests passed'"
</code></pre>
<h2 id="workflow-format-comparison"><a class="header" href="#workflow-format-comparison">Workflow Format Comparison</a></h2>
<h3 id="simple-vs-full-configuration"><a class="header" href="#simple-vs-full-configuration">Simple vs Full Configuration</a></h3>
<p>Many workflow sections support both simple (array) and full (object) formats. Here’s when to use each:</p>
<div class="table-wrapper"><table><thead><tr><th>Feature</th><th>Simple Format</th><th>Full Format</th><th>When to Use Simple</th><th>When to Use Full</th></tr></thead><tbody>
<tr><td><strong>setup</strong></td><td><code>setup: [commands]</code></td><td><code>setup: {commands, timeout, capture_outputs}</code></td><td>No timeout or capture needed</td><td>Need timeout or output capture</td></tr>
<tr><td><strong>merge</strong></td><td><code>merge: [commands]</code></td><td><code>merge: {commands, timeout}</code></td><td>Default timeout (5min) OK</td><td>Custom timeout needed</td></tr>
<tr><td><strong>reduce</strong></td><td><code>reduce: [commands]</code></td><td><code>reduce: {commands}</code> (deprecated)</td><td>Always (modern syntax)</td><td>Never (use simple)</td></tr>
<tr><td><strong>agent_template</strong></td><td><code>agent_template: [commands]</code></td><td><code>agent_template: {commands}</code> (deprecated)</td><td>Always (modern syntax)</td><td>Never (use simple)</td></tr>
</tbody></table>
</div>
<p><strong>Migration from Full to Simple:</strong></p>
<pre><code class="language-yaml"># ❌ OLD (deprecated but supported)
agent_template:
  commands:
    - claude: "/process ${item}"

reduce:
  commands:
    - shell: "echo done"

# ✅ NEW (recommended)
agent_template:
  - claude: "/process ${item}"

reduce:
  - shell: "echo done"
</code></pre>
<p><strong>When Full Format is Required:</strong></p>
<pre><code class="language-yaml"># Setup with capture (requires full format)
setup:
  commands:
    - shell: "cargo test --list --format json &gt; tests.json"
  timeout: 300
  capture_outputs:
    test_count:
      command_index: 0
      json_path: "$.length"

# Merge with custom timeout (requires full format)
merge:
  commands:
    - shell: "cargo build --release"
    - claude: "/merge-worktree ${merge.source_branch}"
  timeout: 1200  # 20 minutes
</code></pre>
<p><strong>Best Practice:</strong> Use simple format by default, switch to full only when you need the extra features (timeout, capture_outputs).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="command-types-1"><a class="header" href="#command-types-1">Command Types</a></h1>
<h2 id="1-shell-commands"><a class="header" href="#1-shell-commands">1. Shell Commands</a></h2>
<pre><code class="language-yaml"># Simple shell command
- shell: "cargo test"

# With output capture
- shell: "ls -la | wc -l"
  capture: "file_count"

# With failure handling
- shell: "cargo clippy"
  on_failure:
    claude: "/fix-warnings ${shell.output}"

# With timeout
- shell: "cargo bench"
  timeout: 600  # seconds

# With conditional execution
- shell: "cargo build --release"
  when: "${tests_passed}"
</code></pre>
<h2 id="2-claude-commands"><a class="header" href="#2-claude-commands">2. Claude Commands</a></h2>
<pre><code class="language-yaml"># Simple Claude command
- claude: "/prodigy-analyze"

# With arguments
- claude: "/prodigy-implement-spec ${spec_file}"

# With commit requirement
- claude: "/prodigy-fix-bugs"
  commit_required: true

# With output capture
- claude: "/prodigy-generate-plan"
  capture: "implementation_plan"
</code></pre>
<h2 id="3-goal-seeking-commands"><a class="header" href="#3-goal-seeking-commands">3. Goal-Seeking Commands</a></h2>
<p>Iteratively refine code until a validation threshold is met.</p>
<pre><code class="language-yaml">- goal_seek:
    goal: "Achieve 90% test coverage"
    claude: "/prodigy-coverage --improve"
    validate: "cargo tarpaulin --print-summary | grep 'Coverage' | sed 's/.*Coverage=\\([0-9]*\\).*/score: \\1/'"
    threshold: 90
    max_attempts: 5
    timeout_seconds: 300
    fail_on_incomplete: true
  commit_required: true
</code></pre>
<p><strong>Fields:</strong></p>
<ul>
<li><code>goal</code>: Human-readable description</li>
<li><code>claude</code> or <code>shell</code>: Command to execute for refinement</li>
<li><code>validate</code>: Command that outputs <code>score: N</code> (0-100)</li>
<li><code>threshold</code>: Minimum score to consider complete</li>
<li><code>max_attempts</code>: Maximum refinement iterations</li>
<li><code>timeout_seconds</code>: Optional timeout per attempt</li>
<li><code>fail_on_incomplete</code>: Whether to fail workflow if threshold not met (default: true)</li>
</ul>
<p><strong>Troubleshooting:</strong></p>
<ul>
<li><strong>Threshold not met:</strong> Check that validate command outputs exactly <code>score: N</code> format (0-100)</li>
<li><strong>Not converging:</strong> Use <code>fail_on_incomplete: false</code> for optional quality gates</li>
<li><strong>Debug scores:</strong> Run workflow with verbose mode (<code>-v</code>) to see validation scores each iteration</li>
<li><strong>Max attempts reached:</strong> Increase <code>max_attempts</code> or lower <code>threshold</code> if goal is too ambitious</li>
</ul>
<h2 id="4-foreach-commands"><a class="header" href="#4-foreach-commands">4. Foreach Commands</a></h2>
<p>Iterate over a list with optional parallelism.</p>
<pre><code class="language-yaml">- foreach:
    input: "find . -name '*.rs' -type f"  # Command
    # OR
    # input: ["file1.rs", "file2.rs"]    # List

    parallel: 5  # Number of parallel executions (or true/false)

    do:
      - claude: "/analyze-file ${item}"
      - shell: "cargo check ${item}"

    continue_on_error: true
    max_items: 50
</code></pre>
<p><strong>Variables and Error Handling:</strong></p>
<ul>
<li><strong>${item}</strong>: Current item value available in loop body</li>
<li><strong>continue_on_error: true</strong> (default): Failed items don’t stop the loop</li>
<li><strong>Parallel execution caveat</strong>: Output order is not guaranteed when using <code>parallel</code></li>
<li><strong>No built-in result aggregation</strong>: Use <code>write_file</code> commands to collect results if needed</li>
</ul>
<p><strong>Example with result collection:</strong></p>
<pre><code class="language-yaml">- foreach:
    input: ["module1", "module2", "module3"]
    parallel: 3
    do:
      - shell: "cargo test --package ${item}"
      - write_file:
          path: "results/${item}.txt"
          content: "Test result: ${shell.output}"
          create_dirs: true
</code></pre>
<h2 id="5-write-file-commands"><a class="header" href="#5-write-file-commands">5. Write File Commands</a></h2>
<p>Create or overwrite files with content from variables or literals. Supports text, JSON, and YAML formats with automatic validation and formatting.</p>
<pre><code class="language-yaml"># Write plain text file
- write_file:
    path: "output/result.txt"
    content: "Build completed at ${shell.output}"
    format: text
    mode: "0644"
    create_dirs: true

# Write JSON file with validation
- write_file:
    path: "config/generated.json"
    content: |
      {
        "version": "${version}",
        "timestamp": "${timestamp}",
        "items": ${items_json}
      }
    format: json

# Write YAML file with formatting
- write_file:
    path: ".prodigy/metadata.yml"
    content: |
      workflow: ${workflow.name}
      iteration: ${workflow.iteration}
      results:
        success: ${map.successful}
        total: ${map.total}
    format: yaml
</code></pre>
<p><strong>WriteFileConfig Fields:</strong></p>
<ul>
<li><code>path</code> - File path to write (supports variable interpolation)</li>
<li><code>content</code> - Content to write (supports variable interpolation)</li>
<li><code>format</code> - Output format: <code>text</code> (default), <code>json</code>, <code>yaml</code></li>
<li><code>mode</code> - File permissions in octal (default: “0644”)</li>
<li><code>create_dirs</code> - Create parent directories if they don’t exist (default: false)</li>
</ul>
<p><strong>Format Validation:</strong></p>
<ul>
<li><code>json</code> - Validates JSON syntax and pretty-prints output</li>
<li><code>yaml</code> - Validates YAML syntax and formats output</li>
<li><code>text</code> - Writes content as-is without validation</li>
</ul>
<p><strong>Best Practices:</strong></p>
<ul>
<li><strong>Use format validation for config files</strong>: Set <code>format: json</code> or <code>format: yaml</code> when generating configuration files to catch syntax errors early</li>
<li><strong>Set appropriate permissions</strong>: Use <code>mode</code> field to control file permissions (e.g., <code>"0600"</code> for sensitive files)</li>
<li><strong>Handle nested paths</strong>: Set <code>create_dirs: true</code> when writing to paths that may not exist</li>
<li><strong>Combine with validation</strong>: Use <code>validate</code> field to ensure generated files meet requirements before proceeding</li>
<li><strong>For logs and documentation</strong>: Use <code>format: text</code> to write content as-is without validation overhead</li>
</ul>
<h2 id="6-validation-commands"><a class="header" href="#6-validation-commands">6. Validation Commands</a></h2>
<p>Validate implementation completeness with automatic retry.</p>
<blockquote>
<p><strong>Deprecation Notice</strong>: The <code>command</code> field in ValidationConfig is deprecated. Use <code>shell</code> instead for shell commands or <code>claude</code> for Claude commands. The <code>command</code> field is still supported for backward compatibility but will be removed in a future version.</p>
</blockquote>
<pre><code class="language-yaml">- claude: "/implement-auth-spec"
  validate:
    shell: "debtmap validate --spec auth.md --output result.json"
    result_file: "result.json"
    threshold: 95  # Percentage completion required (default: 100.0)
    timeout: 60
    expected_schema: "validation-schema.json"  # Optional JSON schema

    # What to do if incomplete
    on_incomplete:
      claude: "/complete-implementation ${validation.gaps}"
      max_attempts: 3
      fail_workflow: true
      commit_required: true
      prompt: "Implementation incomplete. Continue?"  # Optional interactive prompt
</code></pre>
<p><strong>ValidationConfig Fields:</strong></p>
<ul>
<li><code>shell</code> or <code>claude</code> - Single validation command (use <code>shell</code>, not deprecated <code>command</code>)</li>
<li><code>commands</code> - Array of commands for multi-step validation</li>
<li><code>result_file</code> - Path to JSON file with validation results</li>
<li><code>threshold</code> - Minimum completion percentage (default: 100.0)</li>
<li><code>timeout</code> - Timeout in seconds</li>
<li><code>expected_schema</code> - JSON schema for validation output structure</li>
</ul>
<p><strong>OnIncompleteConfig Fields:</strong></p>
<ul>
<li><code>shell</code> or <code>claude</code> - Single gap-filling command</li>
<li><code>commands</code> - Array of commands for multi-step gap filling</li>
<li><code>max_attempts</code> - Maximum retry attempts (default: 2)</li>
<li><code>fail_workflow</code> - Whether to fail workflow if validation incomplete (default: true)</li>
<li><code>commit_required</code> - Whether to require commit after gap filling (default: false)</li>
<li><code>prompt</code> - Optional interactive prompt for user guidance</li>
</ul>
<p><strong>Alternative: Array format for multi-step validation</strong></p>
<pre><code class="language-yaml">- claude: "/implement-feature"
  validate:
    # When using array format, ValidationConfig uses default threshold (100.0)
    # and creates a commands array
    - shell: "run-tests.sh"
    - shell: "check-coverage.sh"
    - claude: "/validate-implementation --output validation.json"
      result_file: "validation.json"
</code></pre>
<p><strong>Alternative: Multi-step gap filling</strong></p>
<pre><code class="language-yaml">- claude: "/implement-feature"
  validate:
    shell: "validate.sh"
    result_file: "result.json"
    on_incomplete:
      commands:
        - claude: "/analyze-gaps ${validation.gaps}"
        - shell: "run-fix-script.sh"
        - claude: "/verify-fixes"
      max_attempts: 2
</code></pre>
<hr />
<h2 id="command-reference"><a class="header" href="#command-reference">Command Reference</a></h2>
<h3 id="command-fields"><a class="header" href="#command-fields">Command Fields</a></h3>
<p>All command types support these common fields:</p>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody>
<tr><td><code>id</code></td><td>string</td><td>Unique identifier for referencing outputs</td></tr>
<tr><td><code>timeout</code></td><td>number</td><td>Command timeout in seconds</td></tr>
<tr><td><code>commit_required</code></td><td>boolean</td><td>Whether command should create a git commit</td></tr>
<tr><td><code>when</code></td><td>string</td><td>Conditional execution expression</td></tr>
<tr><td><code>capture</code></td><td>string</td><td>Variable name to capture command output (replaces deprecated <code>capture_output: true</code>)</td></tr>
<tr><td><code>capture_format</code></td><td>enum</td><td>Format: <code>string</code> (default), <code>number</code>, <code>json</code>, <code>lines</code>, <code>boolean</code> (see examples below)</td></tr>
<tr><td><code>capture_streams</code></td><td>string</td><td>Reserved for future YAML syntax - not yet available in workflows</td></tr>
<tr><td><code>on_success</code></td><td>object</td><td>Command to run on success</td></tr>
<tr><td><code>on_failure</code></td><td>object</td><td>OnFailureConfig with nested command, max_attempts, fail_workflow, strategy</td></tr>
<tr><td><code>validate</code></td><td>object</td><td>Validation configuration</td></tr>
<tr><td><code>output_file</code></td><td>string</td><td>Redirect command output to a file</td></tr>
</tbody></table>
</div>
<h3 id="capturestreams-configuration"><a class="header" href="#capturestreams-configuration">CaptureStreams Configuration</a></h3>
<p><strong>Note:</strong> While <code>capture_streams</code> functionality is implemented internally in Prodigy’s execution engine, it is not yet exposed in the YAML workflow syntax. The field exists in the configuration structs but is currently stored as a string placeholder.</p>
<p><strong>Current Approach:</strong> Use the <code>capture</code> and <code>capture_format</code> fields to control output capture:</p>
<pre><code class="language-yaml"># Capture stdout as string (most common use case)
- shell: "cargo test"
  capture: "test_output"
  capture_format: "string"

# Capture exit status as boolean
- shell: "cargo test"
  capture: "test_passed"
  capture_format: "boolean"

# Capture and parse JSON output
- shell: "cargo metadata --format-version 1"
  capture: "project_info"
  capture_format: "json"
</code></pre>
<p><strong>Future Enhancement:</strong> A future version will expose <code>capture_streams</code> in YAML syntax to provide fine-grained control over which streams (stdout, stderr, exit_code, success, duration) are captured. Until then, use the <code>capture</code> and <code>capture_format</code> fields which cover most common use cases.</p>
<h3 id="capture-format-examples"><a class="header" href="#capture-format-examples">Capture Format Examples</a></h3>
<p>The <code>capture_format</code> field controls how captured output is parsed:</p>
<pre><code class="language-yaml"># String format (default) - raw text output
- shell: "git rev-parse HEAD"
  capture: "commit_hash"
  capture_format: "string"

# Number format - parses numeric output
- shell: "wc -l &lt; file.txt"
  capture: "line_count"
  capture_format: "number"

# JSON format - parses JSON output
- shell: "cargo metadata --format-version 1"
  capture: "project_metadata"
  capture_format: "json"

# Lines format - splits output into array of lines
- shell: "git diff --name-only"
  capture: "changed_files"
  capture_format: "lines"

# Boolean format - true if command succeeds, false otherwise
- shell: "grep -q 'pattern' file.txt"
  capture: "pattern_found"
  capture_format: "boolean"
</code></pre>
<h3 id="deprecated-fields"><a class="header" href="#deprecated-fields">Deprecated Fields</a></h3>
<p>These fields are deprecated but still supported for backward compatibility:</p>
<ul>
<li><code>test:</code> - Use <code>shell:</code> with <code>on_failure:</code> instead</li>
<li><code>command:</code> in ValidationConfig - Use <code>shell:</code> instead</li>
<li>Nested <code>commands:</code> in <code>agent_template</code> and <code>reduce</code> - Use direct array format instead</li>
<li>Legacy variable aliases (<code>$ARG</code>, <code>$ARGUMENT</code>, <code>$FILE</code>, <code>$FILE_PATH</code>) - Use modern <code>${item.*}</code> syntax</li>
</ul>
<p><strong>Migration: capture_output to capture</strong></p>
<p>The old <code>capture_output: true/false</code> syntax is deprecated. It used a boolean value to enable/disable output capture, but didn’t specify where the output was stored, making it unclear and harder to reference in later commands.</p>
<p>Old syntax (deprecated):</p>
<pre><code class="language-yaml">- shell: "ls -la | wc -l"
  capture_output: true
</code></pre>
<p>New syntax (recommended):</p>
<pre><code class="language-yaml">- shell: "ls -la | wc -l"
  capture: "file_count"
</code></pre>
<p><strong>Why the change?</strong> The modern <code>capture</code> field requires an explicit variable name, making workflows more maintainable:</p>
<ul>
<li><strong>Explicit is better than implicit</strong>: Variable names are self-documenting</li>
<li><strong>Easier refactoring</strong>: Clear what each command produces</li>
<li><strong>Better error messages</strong>: References to undefined variables are clearer</li>
</ul>
<p>You can then reference the captured value using <code>${file_count}</code> in subsequent commands. The boolean <code>capture_output</code> field is retained for backward compatibility but should not be used in new workflows.</p>
<hr />
<h2 id="technical-notes"><a class="header" href="#technical-notes">Technical Notes</a></h2>
<details>
<summary>Internal Implementation Fields (for contributors)</summary>
<p>The following fields are used internally during workflow execution but are NOT part of the YAML configuration syntax. These are implementation details managed by Prodigy’s execution engine, not user-facing configuration options:</p>
<ul>
<li><code>handler</code> - Internal HandlerStep for execution routing</li>
<li><code>retry</code> - Internal RetryConfig for automatic retry logic</li>
<li><code>env</code> - Not available (use shell environment syntax: <code>ENV=value command</code>)</li>
<li><code>auto_commit</code> - Internal commit tracking</li>
<li><code>commit_config</code> - Internal commit configuration</li>
<li><code>step_validate</code> - Internal validation state</li>
<li><code>skip_validation</code> - Internal validation control</li>
<li><code>validation_timeout</code> - Internal validation timing</li>
<li><code>ignore_validation_failure</code> - Internal validation handling</li>
</ul>
<p>These fields are documented here for reference when working on Prodigy’s source code, but should not be used in workflow YAML files.</p>
</details>
<hr />
<h2 id="cross-references"><a class="header" href="#cross-references">Cross-References</a></h2>
<p>For more information on related topics:</p>
<ul>
<li><strong>Variable Interpolation</strong>: See the Variables chapter for details on using captured outputs like <code>${variable_name}</code> in subsequent commands</li>
<li><strong>Error Handling</strong>: See the Error Handling chapter for advanced <code>on_failure</code> strategies and retry patterns</li>
<li><strong>MapReduce Workflows</strong>: See the MapReduce chapter for large-scale parallel command execution</li>
</ul>
<p><strong>Example: Using Captured Output in Subsequent Commands</strong></p>
<pre><code class="language-yaml"># Capture build output and use it in later commands
- shell: "cargo build --release 2&gt;&amp;1"
  capture: "build_output"
  capture_format: "string"

# Use the captured output in Claude command
- claude: "/analyze-warnings '${build_output}'"
  when: "${build_output contains 'warning'}"

# Store output to file for later analysis
- write_file:
    path: "logs/build-${workflow.iteration}.log"
    content: "${build_output}"
    create_dirs: true
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="variable-interpolation"><a class="header" href="#variable-interpolation">Variable Interpolation</a></h1>
<h2 id="overview-1"><a class="header" href="#overview-1">Overview</a></h2>
<p>Prodigy provides two complementary variable systems:</p>
<ol>
<li><strong>Built-in Variables</strong>: Automatically available based on workflow context (workflow state, step info, work items, etc.)</li>
<li><strong>Custom Captured Variables</strong>: User-defined variables created via the <code>capture:</code> field in commands</li>
</ol>
<p>Both systems use the same <code>${variable.name}</code> interpolation syntax and can be freely mixed in your workflows.</p>
<h2 id="variable-availability-by-phase"><a class="header" href="#variable-availability-by-phase">Variable Availability by Phase</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Variable Category</th><th>Setup</th><th>Map</th><th>Reduce</th><th>Merge</th></tr></thead><tbody>
<tr><td>Standard Variables</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td></tr>
<tr><td>Output Variables</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td></tr>
<tr><td>Item Variables (<code>${item.*}</code>)</td><td>✗</td><td>✓</td><td>✗</td><td>✗</td></tr>
<tr><td>Map Aggregation (<code>${map.total}</code>, etc.)</td><td>✗</td><td>✗</td><td>✓</td><td>✗</td></tr>
<tr><td>Merge Variables</td><td>✗</td><td>✗</td><td>✗</td><td>✓</td></tr>
<tr><td>Custom Captured Variables</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td></tr>
</tbody></table>
</div>
<p><strong>Note</strong>: Using phase-specific variables outside their designated phase (e.g., <code>${item}</code> in reduce phase, <code>${map.results}</code> in map phase) will result in interpolation errors or empty values. Always verify variable availability matches your workflow phase.</p>
<p><strong>Reduce Phase Access to Item Data</strong>: In reduce phase, individual item variables (<code>${item.*}</code>) are not directly available, but you can access all item data through <code>${map.results}</code> which contains the aggregated results from all map agents. This allows you to process item-level information during aggregation.</p>
<h2 id="available-variables"><a class="header" href="#available-variables">Available Variables</a></h2>
<h3 id="standard-variables"><a class="header" href="#standard-variables">Standard Variables</a></h3>
<ul>
<li><code>${workflow.name}</code> - Workflow name</li>
<li><code>${workflow.id}</code> - Workflow unique identifier</li>
<li><code>${workflow.iteration}</code> - Current iteration number</li>
<li><code>${step.name}</code> - Current step name</li>
<li><code>${step.index}</code> - Current step index</li>
</ul>
<p><strong>Note</strong>: For detailed file tracking, use the Git Context Variables section below which includes <code>step.files_changed</code>, <code>step.files_added</code>, <code>step.files_modified</code>, etc.</p>
<h3 id="system-variables"><a class="header" href="#system-variables">System Variables</a></h3>
<p>These variables are automatically available in all workflow contexts:</p>
<ul>
<li><code>${PROJECT_ROOT}</code> - Absolute path to the working directory (repository root)</li>
<li><code>${WORKTREE}</code> - Name of current worktree (automatically set by Prodigy when running in isolated worktree, e.g., session-abc123)</li>
<li><code>${git.branch}</code> - Current git branch name</li>
<li><code>${git.commit}</code> - Current git commit hash (short SHA)</li>
</ul>
<p><strong>Git Context Variables:</strong> The <code>git.branch</code> and <code>git.commit</code> variables are dynamically populated from your repository state when the workflow runs. They are not available if running outside a git repository.</p>
<p><strong>Example Usage:</strong></p>
<pre><code class="language-yaml">- shell: "cd ${PROJECT_ROOT} &amp;&amp; cargo build"
- shell: "echo 'Running on branch: ${git.branch}'"
- shell: "echo 'Commit: ${git.commit}'"
</code></pre>
<h3 id="output-variables"><a class="header" href="#output-variables">Output Variables</a></h3>
<p><strong>Primary Output Variables:</strong></p>
<ul>
<li><code>${shell.output}</code> - Output (stdout) from last shell command</li>
<li><code>${claude.output}</code> - Output from last Claude command</li>
<li><code>${last.output}</code> - Output from last executed command (any type)</li>
<li><code>${last.exit_code}</code> - Exit code from last command</li>
</ul>
<p><strong>Note</strong>: Shell command output is captured in <code>${shell.output}</code>, providing the complete stdout from the last shell command executed.</p>
<p><strong>Legacy/Specialized Output Variables (Deprecated):</strong></p>
<p>These variables are still supported but <strong>deprecated</strong> in favor of custom capture:</p>
<ul>
<li><code>${handler.output}</code> - Output from handler command (used in error handling)</li>
<li><code>${test.output}</code> - Output from test command (used in validation)</li>
<li><code>${goal_seek.output}</code> - Output from goal-seeking command</li>
</ul>
<p><strong>Migration Note</strong>: These variables are automatically set by specific command types but are deprecated. Use custom capture variables (via <code>capture:</code> field) instead for explicit naming and better readability.</p>
<pre><code class="language-yaml"># OLD (deprecated):
- shell: "echo 'Result: ${shell.output}'"

# NEW (preferred):
- shell: "make build"
  capture: "build_result"
- shell: "echo 'Result: ${build_result}'"
</code></pre>
<h3 id="mapreduce-variables"><a class="header" href="#mapreduce-variables">MapReduce Variables</a></h3>
<p><strong>Map Phase Variables</strong> (available in <code>agent_template:</code> commands):</p>
<ul>
<li><code>${item}</code> - Current work item in map phase (scope: map phase only)</li>
<li><code>${item.value}</code> - Value of current item (for simple items)</li>
<li><code>${item.path}</code> - Path field of current item</li>
<li><code>${item.name}</code> - Name field of current item</li>
<li><code>${item.*}</code> - Access any item field using wildcard pattern (e.g., <code>${item.id}</code>, <code>${item.priority}</code>)</li>
<li><code>${item_index}</code> - Index of current item in the list</li>
<li><code>${item_total}</code> - Total number of items being processed</li>
<li><code>${map.key}</code> - Current map key</li>
<li><code>${worker.id}</code> - ID of the current worker agent</li>
</ul>
<p><strong>Reduce Phase Variables</strong> (available in <code>reduce:</code> commands):</p>
<ul>
<li><code>${map.total}</code> - Total items processed across all map agents</li>
<li><code>${map.successful}</code> - Number of successfully processed items</li>
<li><code>${map.failed}</code> - Number of failed items</li>
<li><code>${map.results}</code> - Aggregated results from all map agents (JSON array)</li>
</ul>
<p><strong>Note</strong>: <code>${item}</code> and related item variables are only available within the map phase. The aggregation variables (<code>${map.total}</code>, <code>${map.successful}</code>, <code>${map.failed}</code>, <code>${map.results}</code>) are only available in the reduce phase.</p>
<h4 id="using-mapresults-in-reduce-phase"><a class="header" href="#using-mapresults-in-reduce-phase">Using map.results in Reduce Phase</a></h4>
<p>The <code>${map.results}</code> variable contains the complete aggregated results from all map agents as a JSON array. This variable can be very large (&gt;1MB with many agents) and requires special handling.</p>
<p><strong>Supported Commands:</strong></p>
<p>✓ <strong>write_file commands</strong> - Fully supported with <code>${map.results}</code> interpolation</p>
<pre><code class="language-yaml">reduce:
  - write_file:
      path: "results.json"
      content: "${map.results}"
      format: json
</code></pre>
<p>✗ <strong>shell and Claude commands</strong> - NOT supported for <code>${map.results}</code> (environment variable size limits)</p>
<pre><code class="language-yaml"># DON'T DO THIS - Will fail with E2BIG error
reduce:
  - shell: "echo '${map.results}' &gt; results.json"  # ✗ Fails
  - claude: "/analyze ${map.results}"              # ✗ Fails
</code></pre>
<p><strong>Why the Difference?</strong></p>
<ul>
<li><strong>write_file</strong>: Uses direct interpolation without environment variables. Can handle multi-megabyte variables safely.</li>
<li><strong>shell/Claude</strong>: Must pass variables via environment variables, which have OS limits (typically ~1MB). Large <code>${map.results}</code> exceeds these limits.</li>
</ul>
<p><strong>Best Practices:</strong></p>
<ol>
<li>
<p><strong>Write map.results to file first</strong> using <code>write_file</code>:</p>
<pre><code class="language-yaml">reduce:
  # Step 1: Write results to file using write_file
  - write_file:
      path: "map_results.json"
      content: "${map.results}"
      format: json

  # Step 2: Process file in shell/Claude commands
  - shell: "jq '.[] | select(.success == true)' map_results.json &gt; successful.json"
  - claude: "/analyze-results map_results.json"
</code></pre>
</li>
<li>
<p><strong>Use scalar variables for shell/Claude</strong>:</p>
<pre><code class="language-yaml">reduce:
  # Safe - scalar values are small
  - shell: "echo 'Processed ${map.total} items'"
  - shell: "echo 'Success rate: ${map.successful}/${map.total}'"
  - claude: "/summarize-results --total ${map.total} --failed ${map.failed}"
</code></pre>
</li>
<li>
<p><strong>Format JSON properly</strong> with write_file:</p>
<pre><code class="language-yaml">reduce:
  # Prodigy will serialize map.results as valid JSON
  - write_file:
      path: "results.json"
      content: "${map.results}"
      format: json  # Ensures proper JSON formatting
</code></pre>
</li>
</ol>
<p><strong>Example Workflow:</strong></p>
<pre><code class="language-yaml">name: process-with-results
mode: mapreduce

map:
  input: "items.json"
  json_path: "$.items[*]"
  agent_template:
    - shell: "process ${item.id}"

reduce:
  # Write full results to JSON file
  - write_file:
      path: "all_results.json"
      content: "${map.results}"
      format: json

  # Write summary using scalar variables
  - write_file:
      path: "summary.txt"
      content: |
        Total Items: ${map.total}
        Successful: ${map.successful}
        Failed: ${map.failed}
      format: text

  # Process results file in shell
  - shell: "jq '[.[] | select(.success)] | length' all_results.json &gt; success_count.txt"

  # Analyze with Claude using file reference
  - claude: "/analyze all_results.json"
</code></pre>
<p><strong>Error Messages:</strong></p>
<p>If you try to use <code>${map.results}</code> in shell/Claude commands, you may see:</p>
<ul>
<li><code>E2BIG: Argument list too long</code> - Environment variable exceeded OS limits</li>
<li><code>Failed to interpolate variable</code> - Variable too large for environment</li>
</ul>
<p><strong>Solution</strong>: Always use <code>write_file</code> for <code>${map.results}</code>, then reference the file in subsequent commands.</p>
<h3 id="merge-variables"><a class="header" href="#merge-variables">Merge Variables</a></h3>
<ul>
<li><code>${merge.worktree}</code> - Worktree name</li>
<li><code>${merge.source_branch}</code> - Source branch</li>
<li><code>${merge.target_branch}</code> - Target branch</li>
<li><code>${merge.session_id}</code> - Session ID</li>
</ul>
<h3 id="validation-variables"><a class="header" href="#validation-variables">Validation Variables</a></h3>
<ul>
<li><code>${validation.completion}</code> - Completion percentage (numeric, same as completion_percentage)</li>
<li><code>${validation.completion_percentage}</code> - Completion percentage (numeric, explicit alias)</li>
<li><code>${validation.implemented}</code> - List of implemented features</li>
<li><code>${validation.missing}</code> - Missing requirements</li>
<li><code>${validation.missing_count}</code> - Number of missing requirements (numeric)</li>
<li><code>${validation.gaps}</code> - Gap details</li>
<li><code>${validation.status}</code> - Status (complete/incomplete/failed)</li>
</ul>
<p><strong>Note</strong>: Use <code>validation.completion_percentage</code> in new workflows for clarity. <code>validation.completion</code> is supported as a shorter alias.</p>
<h3 id="git-context-variables"><a class="header" href="#git-context-variables">Git Context Variables</a></h3>
<p><strong>Commit Tracking:</strong></p>
<ul>
<li><code>${step.commits}</code> - Space-separated commit hashes from current step</li>
<li><code>${workflow.commits}</code> - Space-separated commit hashes from entire workflow</li>
</ul>
<p><strong>Note</strong>: These are space-separated strings of commit hashes (e.g., “abc123 def456 ghi789”), not arrays. Use in shell commands that accept commit lists.</p>
<p><strong>Git Status Variables:</strong></p>
<p>These variables reflect the current git repository status (populated by <code>git diff</code>):</p>
<ul>
<li><code>${git.files.modified}</code> - Files with uncommitted changes in working directory</li>
<li><code>${git.files.added}</code> - Files staged for commit</li>
<li><code>${git.files.deleted}</code> - Files deleted in working directory</li>
<li><code>${git.changed_files}</code> - All files with any changes (modified, added, or deleted)</li>
</ul>
<p><strong>Note</strong>: These show repository state, while <code>step.*</code> and <code>workflow.*</code> variables below track changes made during workflow execution.</p>
<p><strong>File Change Tracking:</strong></p>
<ul>
<li><code>${step.files_added}</code> - Space-separated list of files added in current step</li>
<li><code>${step.files_modified}</code> - Space-separated list of files modified in current step</li>
<li><code>${step.files_deleted}</code> - Space-separated list of files deleted in current step</li>
<li><code>${step.files_changed}</code> - Space-separated list of all changed files (union of added, modified, deleted)</li>
<li><code>${step.commit_count}</code> - Number of commits in current step (numeric)</li>
<li><code>${step.insertions}</code> - Lines added in current step (numeric)</li>
<li><code>${step.deletions}</code> - Lines removed in current step (numeric)</li>
</ul>
<p><strong>Workflow Change Tracking:</strong></p>
<ul>
<li><code>${workflow.files_added}</code> - Space-separated list of all files added in entire workflow</li>
<li><code>${workflow.files_modified}</code> - Space-separated list of all files modified in entire workflow</li>
<li><code>${workflow.files_deleted}</code> - Space-separated list of all files deleted in entire workflow</li>
<li><code>${workflow.files_changed}</code> - Space-separated list of all changed files in entire workflow</li>
<li><code>${workflow.commit_count}</code> - Total commits in entire workflow (numeric)</li>
<li><code>${workflow.insertions}</code> - Total lines added in entire workflow (numeric)</li>
<li><code>${workflow.deletions}</code> - Total lines removed in entire workflow (numeric)</li>
</ul>
<p><strong>Example Usage:</strong></p>
<pre><code class="language-yaml"># Check if step made changes
- shell: "echo 'Step made ${step.commit_count} commits'"
- shell: "echo 'Modified files: ${step.files_modified}'"
- shell: "echo 'Code changes: +${step.insertions}/-${step.deletions} lines'"

# Use commit hashes
- shell: "git show ${step.commits}"
- shell: "echo 'All commits: ${workflow.commits}'"

# Use file lists in shell commands
- shell: "for file in ${step.files_changed}; do echo $file; done"
- shell: "wc -l ${step.files_added}"  # Count lines in newly added files
</code></pre>
<h3 id="legacy-variable-aliases"><a class="header" href="#legacy-variable-aliases">Legacy Variable Aliases</a></h3>
<p>These legacy aliases are supported for backward compatibility but should be replaced with modern equivalents:</p>
<ul>
<li><code>$ARG</code> / <code>$ARGUMENT</code> - Legacy aliases for <code>${item.value}</code> (available in WithArguments mode)</li>
<li><code>$FILE</code> / <code>$FILE_PATH</code> - Legacy aliases for <code>${item.path}</code> (available in WithFilePattern mode)</li>
</ul>
<p><strong>Note:</strong> Use the modern <code>${item.*}</code> syntax in new workflows instead of legacy aliases.</p>
<hr />
<h2 id="custom-variable-capture"><a class="header" href="#custom-variable-capture">Custom Variable Capture</a></h2>
<p>Custom capture variables allow you to save command output with explicit names for later use. This is the recommended approach for most workflows instead of relying on automatic output variables.</p>
<h3 id="basic-capture-examples"><a class="header" href="#basic-capture-examples">Basic Capture Examples</a></h3>
<pre><code class="language-yaml"># Capture to custom variable
- shell: "ls -la | wc -l"
  capture: "file_count"
  capture_format: number  # Default: string

# Use in next command
- shell: "echo 'Found ${file_count} files'"
</code></pre>
<h3 id="capture-formats"><a class="header" href="#capture-formats">Capture Formats</a></h3>
<p>The <code>capture_format</code> field determines how output is parsed and stored:</p>
<pre><code class="language-yaml"># String format (default) - stores raw output
- shell: "echo 'Hello World'"
  capture: "greeting"
  capture_format: string
# Access: ${greeting} → "Hello World"

# Number format - parses numeric output
- shell: "echo 42"
  capture: "answer"
  capture_format: number
# Access: ${answer} → 42 (as number, not string)

# Boolean format - converts to true/false
- shell: "[ -f README.md ] &amp;&amp; echo true || echo false"
  capture: "has_readme"
  capture_format: boolean
# Access: ${has_readme} → true or false

# JSON format - parses JSON output
- shell: "echo '{\"name\": \"project\", \"version\": \"1.0\"}'"
  capture: "package_info"
  capture_format: json
# Access nested fields: ${package_info.name} → "project"
# Access nested fields: ${package_info.version} → "1.0"

# Lines format - splits into array by newlines
- shell: "ls *.md"
  capture: "markdown_files"
  capture_format: lines
# Access: ${markdown_files} → array of filenames
</code></pre>
<h3 id="capture-streams"><a class="header" href="#capture-streams">Capture Streams</a></h3>
<p>Control which output streams to capture (useful for detailed command analysis).</p>
<p><strong>Default Behavior</strong>: By default, <code>stdout</code>, <code>exit_code</code>, <code>success</code>, and <code>duration</code> are captured. To customize:</p>
<pre><code class="language-yaml"># Add stderr to default captures
- shell: "cargo test 2&gt;&amp;1"
  capture: "test_results"
  capture_streams:
    stderr: true  # Add stderr to defaults (stdout, exit_code, success, duration)

# Or configure all streams explicitly
- shell: "cargo test 2&gt;&amp;1"
  capture: "test_results"
  capture_streams:
    stdout: true      # Default: true
    stderr: true      # Default: false
    exit_code: true   # Default: true
    success: true     # Default: true
    duration: true    # Default: true

# Access captured stream data
- shell: "echo 'Exit code: ${test_results.exit_code}'"
- shell: "echo 'Success: ${test_results.success}'"
- shell: "echo 'Duration: ${test_results.duration}s'"
</code></pre>
<h3 id="nested-json-field-access"><a class="header" href="#nested-json-field-access">Nested JSON Field Access</a></h3>
<p>For JSON-formatted captures, use dot notation to access nested fields:</p>
<pre><code class="language-yaml"># Example: API response with nested data
- shell: "curl -s https://api.example.com/user/123"
  capture: "user"
  capture_format: json

# Access nested fields with dot notation
- shell: "echo 'Name: ${user.profile.name}'"
- shell: "echo 'Email: ${user.contact.email}'"
- shell: "echo 'City: ${user.address.city}'"
</code></pre>
<p><strong>Error Handling</strong>: Accessing non-existent fields (e.g., <code>${user.missing.field}</code>) will return an error. Ensure your JSON structure matches your field access patterns, or use validation to handle missing fields gracefully.</p>
<h3 id="variable-scope-and-precedence"><a class="header" href="#variable-scope-and-precedence">Variable Scope and Precedence</a></h3>
<p>Variables follow a parent/child scope hierarchy:</p>
<ol>
<li><strong>Local Scope</strong>: Variables defined in the current command block</li>
<li><strong>Parent Scope</strong>: Variables from enclosing blocks (foreach, map phase, etc.)</li>
<li><strong>Built-in Variables</strong>: Standard workflow context variables</li>
</ol>
<p><strong>Precedence</strong>: Local variables override parent scope variables, which override built-in variables.</p>
<p><strong>Scope Lifetime</strong>: Child scopes are temporary. When a child block (foreach loop, map phase agent) completes, its local variables are discarded and the parent scope is automatically restored.</p>
<pre><code class="language-yaml"># Parent scope
- shell: "echo 'outer'"
  capture: "message"

# Child scope (foreach creates new scope)
- foreach:
    items: [1, 2, 3]
    commands:
      # This creates a local 'message' that shadows parent
      - shell: "echo 'inner-${item}'"
        capture: "message"
      - shell: "echo ${message}"  # Uses local 'message'

# After foreach, parent 'message' is still accessible
# The local 'message' from foreach is discarded
- shell: "echo ${message}"  # Uses parent 'message' → "outer"
</code></pre>
<hr />
<h2 id="troubleshooting-variable-interpolation"><a class="header" href="#troubleshooting-variable-interpolation">Troubleshooting Variable Interpolation</a></h2>
<h3 id="common-errors-and-solutions"><a class="header" href="#common-errors-and-solutions">Common Errors and Solutions</a></h3>
<p><strong>Error: “Variable not found: item”</strong></p>
<ul>
<li><strong>Cause</strong>: Using <code>${item}</code> or <code>${item.*}</code> outside the map phase</li>
<li><strong>Solution</strong>: Item variables are only available in <code>agent_template:</code> commands within MapReduce workflows. Move your code to the map phase or use a different variable.</li>
</ul>
<p><strong>Error: “Variable not found: map.results”</strong></p>
<ul>
<li><strong>Cause</strong>: Using <code>${map.results}</code> outside the reduce phase</li>
<li><strong>Solution</strong>: Map aggregation variables are only available in <code>reduce:</code> commands. See the phase availability table above.</li>
</ul>
<p><strong>Error: “Failed to parse JSON field”</strong></p>
<ul>
<li><strong>Cause</strong>: Accessing a non-existent nested field in a JSON capture</li>
<li><strong>Solution</strong>: Verify your JSON structure matches your field access. Use <code>jq</code> or similar tools to inspect the JSON first.</li>
</ul>
<p><strong>Error: “Invalid variable interpolation syntax”</strong></p>
<ul>
<li><strong>Cause</strong>: Missing <code>${}</code> braces or incorrect syntax</li>
<li><strong>Solution</strong>: Always use <code>${variable.name}</code> format. Shell-style <code>$VAR</code> is not supported for Prodigy variables (only for environment variables).</li>
</ul>
<p><strong>Empty/Undefined Variable Value</strong></p>
<ul>
<li><strong>Cause</strong>: Variable hasn’t been set yet, or phase mismatch</li>
<li><strong>Solution</strong>: Check command order and ensure the variable is captured before use. Review the phase availability table.</li>
</ul>
<h3 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h3>
<ol>
<li><strong>Use custom capture for clarity</strong>: Prefer <code>capture: "my_var"</code> over automatic <code>${shell.output}</code></li>
<li><strong>Name variables descriptively</strong>: Use <code>${test_results}</code> not <code>${x}</code></li>
<li><strong>Validate JSON captures</strong>: Test JSON structure before accessing nested fields</li>
<li><strong>Check phase availability</strong>: Ensure variables match your workflow phase</li>
<li><strong>Use appropriate capture formats</strong>: <code>number</code>, <code>json</code>, <code>boolean</code> provide type safety</li>
</ol>
<hr />
<h2 id="see-also"><a class="header" href="#see-also">See Also</a></h2>
<ul>
<li><strong><a href="./commands.html">Command Types</a></strong> - Learn about command types and their capture behavior</li>
<li><strong><a href="./mapreduce.html">MapReduce Workflows</a></strong> - Deep dive into MapReduce variables and phases</li>
<li><strong><a href="./environment.html">Environment Variables</a></strong> - Using environment variables in workflows</li>
<li><strong><a href="./examples.html">Workflow Examples</a></strong> - Practical examples using variables in real workflows</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="environment-configuration-1"><a class="header" href="#environment-configuration-1">Environment Configuration</a></h1>
<p>Prodigy provides flexible environment configuration for workflows, allowing you to manage environment variables, secrets, profiles, and step-specific settings. This chapter explains the user-facing configuration options available in workflow YAML files.</p>
<h2 id="architecture-overview"><a class="header" href="#architecture-overview">Architecture Overview</a></h2>
<p>Prodigy uses a two-layer architecture for environment management:</p>
<ol>
<li><strong>WorkflowConfig</strong>: User-facing YAML configuration with <code>env</code>, <code>secrets</code>, <code>profiles</code>, and <code>env_files</code> fields</li>
<li><strong>EnvironmentConfig</strong>: Internal runtime configuration that extends workflow config with additional features</li>
</ol>
<p>This chapter documents the WorkflowConfig layer - the fields you write in workflow YAML files (<code>env</code>, <code>secrets</code>, <code>env_files</code>, <code>profiles</code>). The EnvironmentConfig is Prodigy’s internal runtime that processes these YAML fields and adds internal-only features like dynamic command-based values and conditional expressions.</p>
<p><strong>Internal vs. User-Facing Capabilities:</strong></p>
<p>The internal <code>EnvironmentConfig</code> supports richer environment value types through the <code>EnvValue</code> enum:</p>
<ul>
<li><code>Static</code>: Simple string values (what WorkflowConfig exposes)</li>
<li><code>Dynamic</code>: Values from command output (internal only)</li>
<li><code>Conditional</code>: Expression-based values (internal only)</li>
</ul>
<p>In workflow YAML, the <code>env</code> field only supports static string values (<code>HashMap&lt;String, String&gt;</code>). The Dynamic and Conditional variants are internal runtime features not exposed in workflow configuration.</p>
<p><strong>Note on Internal Features:</strong> The <code>EnvironmentConfig</code> runtime layer includes a <code>StepEnvironment</code> struct with fields like <code>env</code>, <code>working_dir</code>, <code>clear_env</code>, and <code>temporary</code>. These are internal implementation details not exposed in <code>WorkflowStepCommand</code> YAML syntax. Per-command environment changes must use shell syntax (e.g., <code>ENV=value command</code>).</p>
<hr />
<h2 id="global-environment-variables-1"><a class="header" href="#global-environment-variables-1">Global Environment Variables</a></h2>
<p>Define static environment variables that apply to all commands in your workflow:</p>
<pre><code class="language-yaml"># Global environment variables (static strings only)
env:
  NODE_ENV: production
  PORT: "3000"
  API_URL: https://api.example.com
  DEBUG: "false"

commands:
  - shell: "echo $NODE_ENV"  # Uses global environment
</code></pre>
<p><strong>Important:</strong> The <code>env</code> field at the workflow level only supports static string values. Dynamic or conditional environment variables are handled internally by the runtime but are not directly exposed in workflow YAML.</p>
<p><strong>Environment Inheritance:</strong> Parent process environment variables are always inherited by default. All global environment variables are merged with the parent environment.</p>
<hr />
<h2 id="mapreduce-environment-variables"><a class="header" href="#mapreduce-environment-variables">MapReduce Environment Variables</a></h2>
<p>In MapReduce workflows, environment variables provide powerful parameterization across all phases (setup, map, reduce, and merge). This enables workflows to be reusable across different projects and configurations.</p>
<h3 id="defining-environment-variables-in-mapreduce"><a class="header" href="#defining-environment-variables-in-mapreduce">Defining Environment Variables in MapReduce</a></h3>
<p>Environment variables for MapReduce workflows follow the same global <code>env</code> field structure:</p>
<pre><code class="language-yaml">name: mapreduce-workflow
mode: mapreduce

env:
  # Plain variables for parameterization
  PROJECT_NAME: "prodigy"
  PROJECT_CONFIG: "prodigy.yml"
  FEATURES_PATH: "specs/"
  OUTPUT_DIR: "results"

  # Workflow-specific settings
  MAX_RETRIES: "3"
  TIMEOUT_SECONDS: "300"

setup:
  - shell: "echo Starting $PROJECT_NAME workflow"
  - shell: "mkdir -p $OUTPUT_DIR"

map:
  input: "${FEATURES_PATH}/items.json"
  agent_template:
    - claude: "/process '${item.name}' --config $PROJECT_CONFIG"
    - shell: "test -f $PROJECT_NAME/${item.path}"

reduce:
  - shell: "echo Processed ${map.total} items for $PROJECT_NAME"
  - shell: "cp results.json $OUTPUT_DIR/"

merge:
  commands:
    - shell: "echo Merging $PROJECT_NAME changes"
</code></pre>
<h3 id="variable-interpolation-in-mapreduce"><a class="header" href="#variable-interpolation-in-mapreduce">Variable Interpolation in MapReduce</a></h3>
<p>MapReduce workflows support two interpolation syntaxes:</p>
<ol>
<li><strong><code>$VAR</code></strong> - Shell-style variable reference</li>
<li><strong><code>${VAR}</code></strong> - Bracketed reference (recommended for clarity)</li>
</ol>
<p>Both syntaxes work throughout all workflow phases:</p>
<pre><code class="language-yaml">env:
  PROJECT_NAME: "myproject"
  CONFIG_FILE: "config.yml"

setup:
  - shell: "echo $PROJECT_NAME"           # Shell-style
  - shell: "echo ${PROJECT_NAME}"         # Bracketed
  - shell: "test -f ${CONFIG_FILE}"       # Recommended in paths

map:
  agent_template:
    - claude: "/analyze '${item}' --project $PROJECT_NAME"
</code></pre>
<h3 id="environment-variables-in-all-mapreduce-phases"><a class="header" href="#environment-variables-in-all-mapreduce-phases">Environment Variables in All MapReduce Phases</a></h3>
<h4 id="setup-phase"><a class="header" href="#setup-phase">Setup Phase</a></h4>
<p>Environment variables are available for initialization:</p>
<pre><code class="language-yaml">env:
  WORKSPACE_DIR: "/tmp/workspace"
  INPUT_SOURCE: "https://api.example.com/data"

setup:
  - shell: "mkdir -p $WORKSPACE_DIR"
  - shell: "curl $INPUT_SOURCE -o items.json"
  - shell: "echo Setup complete for ${WORKSPACE_DIR}"
</code></pre>
<h4 id="map-phase"><a class="header" href="#map-phase">Map Phase</a></h4>
<p>Variables are available in agent templates and can be combined with work item variables:</p>
<pre><code class="language-yaml">env:
  PROJECT_ROOT: "/workspace"
  OUTPUT_FORMAT: "json"
  CONFIG_PATH: "config/settings.yml"

map:
  agent_template:
    - claude: "/analyze ${item.file} --root $PROJECT_ROOT --format $OUTPUT_FORMAT"
    - shell: "test -f $PROJECT_ROOT/${item.file}"
    - shell: "mkdir -p $PROJECT_ROOT/results/${item.category}"
    - shell: "cp ${item.file} $PROJECT_ROOT/results/${item.category}/"
</code></pre>
<p>This example shows how environment variables (<code>PROJECT_ROOT</code>, <code>OUTPUT_FORMAT</code>) work together with work item variables (<code>${item.file}</code>, <code>${item.category}</code>) in the map phase.</p>
<h4 id="reduce-phase"><a class="header" href="#reduce-phase">Reduce Phase</a></h4>
<p>Variables work in aggregation commands:</p>
<pre><code class="language-yaml">env:
  PROJECT_NAME: "myproject"
  REPORT_DIR: "reports"

reduce:
  - claude: "/summarize ${map.results} --project $PROJECT_NAME"
  - shell: "mkdir -p $REPORT_DIR"
  - shell: "cp summary.json $REPORT_DIR/${PROJECT_NAME}-summary.json"
</code></pre>
<h4 id="merge-phase"><a class="header" href="#merge-phase">Merge Phase</a></h4>
<p>Variables are available during merge operations:</p>
<pre><code class="language-yaml">env:
  PROJECT_NAME: "myproject"
  NOTIFY_WEBHOOK: "https://hooks.example.com/notify"

merge:
  commands:
    - shell: "echo Merging $PROJECT_NAME changes"
    - claude: "/validate-merge --branch ${merge.source_branch}"
    - shell: "curl -X POST $NOTIFY_WEBHOOK -d 'project=$PROJECT_NAME'"
</code></pre>
<h3 id="secrets-in-mapreduce"><a class="header" href="#secrets-in-mapreduce">Secrets in MapReduce</a></h3>
<p>Sensitive data can be marked as secrets to enable automatic masking:</p>
<pre><code class="language-yaml">env:
  PROJECT_NAME: "myproject"

secrets:
  API_TOKEN:
    provider: env
    key: "GITHUB_TOKEN"

  WEBHOOK_SECRET:
    provider: file
    key: "~/.secrets/webhook.key"

setup:
  - shell: "curl -H 'Authorization: Bearer $API_TOKEN' https://api.github.com/repos"

map:
  agent_template:
    - shell: "curl -X POST $WEBHOOK_URL -H 'X-Secret: $WEBHOOK_SECRET'"
</code></pre>
<p>Secrets are automatically masked in:</p>
<ul>
<li>Standard output/error streams</li>
<li>Claude JSON logs (<code>~/.claude/projects/</code>)</li>
<li>MapReduce event logs (<code>~/.prodigy/events/</code>)</li>
<li>Dead Letter Queue items (<code>~/.prodigy/dlq/</code>)</li>
<li>Checkpoint state files</li>
<li>Git commit messages (if secrets appear in output)</li>
</ul>
<h3 id="profile-support-in-mapreduce"><a class="header" href="#profile-support-in-mapreduce">Profile Support in MapReduce</a></h3>
<p>Profiles enable different configurations for different environments:</p>
<pre><code class="language-yaml">env:
  PROJECT_NAME: "myproject"

profiles:
  development:
    description: "Development environment"
    API_URL: "http://localhost:3000"
    DEBUG: "true"
    TIMEOUT_SECONDS: "60"

  production:
    description: "Production environment"
    API_URL: "https://api.example.com"
    DEBUG: "false"
    TIMEOUT_SECONDS: "300"

map:
  agent_template:
    - shell: "curl $API_URL/data"
    - shell: "timeout ${TIMEOUT_SECONDS}s ./process.sh"
</code></pre>
<p><strong>Note:</strong> The profile infrastructure exists internally (EnvironmentConfig has an <code>active_profile</code> field), but there is currently no CLI flag (like <code>--profile</code>) to activate profiles at runtime. Profiles can be defined in YAML for future use, but cannot be activated in the current version.</p>
<h3 id="reusable-workflows-with-environment-variables"><a class="header" href="#reusable-workflows-with-environment-variables">Reusable Workflows with Environment Variables</a></h3>
<p>Environment variables enable the same workflow to work for different projects:</p>
<pre><code class="language-yaml"># This workflow works for both Prodigy and Debtmap
name: check-book-docs-drift
mode: mapreduce

env:
  # Override these when running the workflow
  PROJECT_NAME: "prodigy"              # or "debtmap"
  PROJECT_CONFIG: "prodigy.yml"        # or "debtmap.yml"
  FEATURES_PATH: "specs/"              # or "features/"

setup:
  - shell: "echo Checking $PROJECT_NAME documentation"
  - shell: "./${PROJECT_NAME} generate-book-items --output items.json"

map:
  input: "items.json"
  agent_template:
    - claude: "/check-drift '${item}' --config $PROJECT_CONFIG"
    - shell: "git diff --exit-code ${item.doc_path}"

reduce:
  - claude: "/summarize-drift ${map.results} --project $PROJECT_NAME"
</code></pre>
<p>Run for different projects:</p>
<pre><code class="language-bash"># For Prodigy
prodigy run workflow.yml

# For Debtmap
env PROJECT_NAME=debtmap PROJECT_CONFIG=debtmap.yml FEATURES_PATH=features/ \
  prodigy run workflow.yml
</code></pre>
<h3 id="best-practices-for-mapreduce-environment-variables"><a class="header" href="#best-practices-for-mapreduce-environment-variables">Best Practices for MapReduce Environment Variables</a></h3>
<ol>
<li>
<p><strong>Parameterize project-specific values</strong>:</p>
<pre><code class="language-yaml">env:
  PROJECT_NAME: "myproject"
  PROJECT_ROOT: "/workspace"
  CONFIG_FILE: "config.yml"
</code></pre>
</li>
<li>
<p><strong>Use consistent naming</strong>:</p>
<ul>
<li>Use UPPER_CASE for environment variables</li>
<li>Use descriptive names (PROJECT_NAME not PN)</li>
<li>Group related variables with prefixes (AWS_<em>, DB_</em>)</li>
</ul>
</li>
<li>
<p><strong>Document variables</strong>:</p>
<pre><code class="language-yaml">env:
  # Project identifier used in logs and reports
  PROJECT_NAME: "prodigy"

  # Path to project configuration file
  PROJECT_CONFIG: "prodigy.yml"

  # Maximum concurrent agents (tune based on resources)
  MAX_PARALLEL: "10"
</code></pre>
</li>
<li>
<p><strong>Use secrets for sensitive data</strong>:</p>
<pre><code class="language-yaml">secrets:
  GITHUB_TOKEN:
    provider: env
    key: "GH_TOKEN"
</code></pre>
</li>
<li>
<p><strong>Prefer <code>${VAR}</code> syntax</strong>:</p>
<pre><code class="language-yaml"># Good - explicit and safe
- shell: "test -f ${CONFIG_PATH}"

# Risky - may fail with special characters
- shell: "test -f $CONFIG_PATH"
</code></pre>
</li>
</ol>
<hr />
<h2 id="environment-files-1"><a class="header" href="#environment-files-1">Environment Files</a></h2>
<p>Load environment variables from <code>.env</code> files:</p>
<pre><code class="language-yaml"># Environment files to load
env_files:
  - .env
  - .env.local
  - config/.env.production

commands:
  - shell: "echo $DATABASE_URL"
</code></pre>
<p><strong>Environment File Format:</strong></p>
<p>Environment files use the standard <code>.env</code> format with <code>KEY=value</code> pairs:</p>
<pre><code class="language-bash"># .env file example
DATABASE_URL=postgresql://localhost:5432/mydb
REDIS_HOST=localhost
REDIS_PORT=6379

# Comments are supported
API_KEY=secret-key-here

# Multi-line values use quotes
PRIVATE_KEY="-----BEGIN PRIVATE KEY-----
MIIEvQIBADANBg...
-----END PRIVATE KEY-----"
</code></pre>
<p><strong>Loading Order and Precedence:</strong></p>
<p>Environment files are loaded in order, with later files overriding earlier files. This enables layered configuration:</p>
<pre><code class="language-yaml">env_files:
  - .env                # Base configuration
  - .env.local          # Local overrides (gitignored)
  - .env.production     # Environment-specific settings
</code></pre>
<p>Example override behavior:</p>
<pre><code class="language-bash"># .env (base)
DATABASE_URL=postgresql://localhost:5432/dev
API_TIMEOUT=30

# .env.production (overrides)
DATABASE_URL=postgresql://prod-server:5432/app
# API_TIMEOUT remains 30 from base file
</code></pre>
<p>Precedence order (highest to lowest):</p>
<ol>
<li>Global <code>env</code> field in workflow YAML</li>
<li>Later files in <code>env_files</code> list</li>
<li>Earlier files in <code>env_files</code> list</li>
<li>Parent process environment</li>
</ol>
<p><strong>Error Handling:</strong></p>
<p>If an env file specified in <code>env_files</code> does not exist or contains invalid syntax, Prodigy will report an error and halt workflow execution. Use absolute paths or paths relative to the workflow file location.</p>
<hr />
<h2 id="secrets-management"><a class="header" href="#secrets-management">Secrets Management</a></h2>
<p>Store sensitive values securely using secret providers:</p>
<pre><code class="language-yaml">secrets:
  # Provider-based secrets (recommended)
  AWS_SECRET:
    provider: aws
    key: "my-app/api-key"

  VAULT_SECRET:
    provider: vault
    key: "secret/data/myapp"
    version: "v2"  # Optional version

  # Environment variable reference
  API_KEY:
    provider: env
    key: "SECRET_API_KEY"

  # File-based secret
  DB_PASSWORD:
    provider: file
    key: "~/.secrets/db.pass"

  # Custom provider (extensible)
  CUSTOM_SECRET:
    provider: "my-custom-provider"
    key: "secret-id"

commands:
  - shell: "echo $API_KEY"  # Secrets are available as environment variables
</code></pre>
<p><strong>Supported Secret Providers:</strong></p>
<ul>
<li><code>env</code> - Reference another environment variable</li>
<li><code>file</code> - Read secret from a file</li>
<li><code>vault</code> - HashiCorp Vault integration (requires Vault setup)</li>
<li><code>aws</code> - AWS Secrets Manager (requires AWS credentials)</li>
<li><code>custom</code> - Custom provider for advanced use cases. Requires implementing custom secret resolution logic in Prodigy’s environment manager. Contact maintainers for extension points.</li>
</ul>
<p><strong>Security Notes:</strong></p>
<ul>
<li>Secrets are masked in logs and output</li>
<li>Secret values are only resolved at runtime</li>
<li>Use secrets for API keys, passwords, tokens, and other sensitive data</li>
</ul>
<hr />
<h2 id="environment-profiles-1"><a class="header" href="#environment-profiles-1">Environment Profiles</a></h2>
<p>Define named environment configurations for different contexts:</p>
<pre><code class="language-yaml"># Define profiles with environment variables
profiles:
  development:
    description: "Development environment with debug enabled"
    NODE_ENV: development
    DEBUG: "true"
    API_URL: http://localhost:3000

  production:
    description: "Production environment configuration"
    NODE_ENV: production
    DEBUG: "false"
    API_URL: https://api.example.com

# Global environment still applies
env:
  APP_NAME: "my-app"

commands:
  - shell: "npm run build"
</code></pre>
<p><strong>Profile Structure:</strong></p>
<p>Profiles use a flat structure where environment variables are defined directly at the profile level (not nested under an <code>env:</code> key). The <code>description</code> field is optional and helps document the profile’s purpose.</p>
<pre><code class="language-yaml">profiles:
  staging:
    description: "Staging environment"  # Optional
    NODE_ENV: staging                   # Direct key-value pairs
    API_URL: https://staging.api.com
    DEBUG: "true"
</code></pre>
<p><strong>Note:</strong> The profile infrastructure exists internally (EnvironmentConfig has an <code>active_profile</code> field), but there is currently no CLI flag (like <code>--profile</code>) to activate profiles at runtime. Profiles can be defined in YAML for future use, but cannot be activated in the current version.</p>
<hr />
<h2 id="per-command-environment-overrides"><a class="header" href="#per-command-environment-overrides">Per-Command Environment Overrides</a></h2>
<p><strong>IMPORTANT:</strong> WorkflowStepCommand does NOT have an <code>env</code> field. All per-command environment changes must use shell syntax.</p>
<p><strong>Note:</strong> The legacy Command struct (structured format) has an <code>env</code> field via CommandMetadata, but the modern WorkflowStepCommand format does not. For workflows using the modern <code>claude:</code>/<code>shell:</code> syntax, use shell-level environment syntax (<code>ENV=value command</code>).</p>
<p>You can override environment variables for individual commands using shell environment syntax:</p>
<pre><code class="language-yaml">env:
  RUST_LOG: info
  API_URL: "https://api.example.com"

# Steps go directly in the workflow
- shell: "cargo run"  # Uses RUST_LOG=info from global env

# Override environment for this command only using shell syntax
- shell: "RUST_LOG=debug cargo run --verbose"

# Change directory and set environment in shell
- shell: "cd frontend &amp;&amp; PATH=./node_modules/.bin:$PATH npm run build"
</code></pre>
<p><strong>Shell-based Environment Techniques:</strong></p>
<ul>
<li><strong>Single variable override:</strong> <code>ENV_VAR=value command</code></li>
<li><strong>Multiple variables:</strong> <code>VAR1=value1 VAR2=value2 command</code></li>
<li><strong>Change directory:</strong> <code>cd path &amp;&amp; command</code></li>
<li><strong>Combine both:</strong> <code>cd path &amp;&amp; ENV_VAR=value command</code></li>
</ul>
<p><strong>Note:</strong> A <code>StepEnvironment</code> struct exists in the internal runtime (<code>EnvironmentConfig</code>), but it is not currently exposed in the WorkflowStepCommand YAML syntax. All per-command environment changes must use shell syntax as shown above.</p>
<hr />
<h2 id="environment-precedence"><a class="header" href="#environment-precedence">Environment Precedence</a></h2>
<p>Environment variables are resolved with the following precedence (highest to lowest):</p>
<ol>
<li><strong>Shell-level overrides</strong> - Using <code>ENV=value command</code> syntax</li>
<li><strong>Global <code>env</code></strong> - Defined at workflow level in YAML</li>
<li><strong>Environment files</strong> - Loaded from <code>env_files</code> (later files override earlier)</li>
<li><strong>Parent environment</strong> - Always inherited from the parent process</li>
</ol>
<p><strong>Note:</strong> The internal <code>EnvironmentConfig</code> runtime also supports profile-based precedence and step-level environment overrides, but these are not exposed in workflow YAML. The profile infrastructure exists internally (<code>active_profile</code> field), but there is no CLI flag to activate profiles. WorkflowStepCommand has no <code>env</code> field.</p>
<p>Example demonstrating precedence:</p>
<pre><code class="language-yaml"># Parent environment: NODE_ENV=local

env_files:
  - .env  # Contains: NODE_ENV=development

env:
  NODE_ENV: production  # Overrides .env file and parent environment

# Steps go directly in the workflow
- shell: "echo $NODE_ENV"  # Prints: production (from global env)

# Override using shell syntax
- shell: "NODE_ENV=staging echo $NODE_ENV"  # Prints: staging (shell override)
</code></pre>
<p>In this example:</p>
<ul>
<li>Parent environment has <code>NODE_ENV=local</code> (lowest precedence)</li>
<li><code>.env</code> file sets <code>NODE_ENV=development</code> (overrides parent)</li>
<li>Global <code>env</code> sets <code>NODE_ENV=production</code> (overrides .env file)</li>
<li>Shell syntax <code>NODE_ENV=staging</code> (overrides everything for that command)</li>
</ul>
<hr />
<h2 id="best-practices-1"><a class="header" href="#best-practices-1">Best Practices</a></h2>
<h3 id="1-use-environment-files-for-configuration"><a class="header" href="#1-use-environment-files-for-configuration">1. Use Environment Files for Configuration</a></h3>
<p>Store configuration in <code>.env</code> files instead of hardcoding in YAML:</p>
<pre><code class="language-yaml"># Good: Load from files
env_files:
  - .env
  - .env.${ENVIRONMENT}

# Avoid: Hardcoding sensitive values
env:
  API_KEY: "hardcoded-key-here"  # Don't do this!
</code></pre>
<h3 id="2-use-secrets-for-sensitive-data"><a class="header" href="#2-use-secrets-for-sensitive-data">2. Use Secrets for Sensitive Data</a></h3>
<p>Always use the <code>secrets</code> field for sensitive information:</p>
<pre><code class="language-yaml"># Good: Use secrets provider
secrets:
  DATABASE_PASSWORD:
    provider: vault
    key: "db/password"

# Bad: Sensitive data in plain env
env:
  DATABASE_PASSWORD: "my-password"  # Don't do this!
</code></pre>
<h3 id="3-leverage-profiles-for-environments"><a class="header" href="#3-leverage-profiles-for-environments">3. Leverage Profiles for Environments</a></h3>
<p>Define profiles for different deployment environments:</p>
<pre><code class="language-yaml">profiles:
  development:
    NODE_ENV: development
    LOG_LEVEL: debug
    API_URL: http://localhost:3000

  production:
    NODE_ENV: production
    LOG_LEVEL: error
    API_URL: https://api.example.com
</code></pre>
<h3 id="4-use-shell-syntax-for-command-specific-overrides"><a class="header" href="#4-use-shell-syntax-for-command-specific-overrides">4. Use Shell Syntax for Command-Specific Overrides</a></h3>
<p>Override global settings for specific commands using shell environment syntax:</p>
<pre><code class="language-yaml">env:
  RUST_LOG: info

# Steps go directly in the workflow
# Most commands use info level
- shell: "cargo run"

# Override for this specific command using shell syntax
- shell: "RUST_LOG=debug cargo run --verbose"
</code></pre>
<h3 id="5-document-your-environment-variables"><a class="header" href="#5-document-your-environment-variables">5. Document Your Environment Variables</a></h3>
<p>Add comments to explain environment variables:</p>
<pre><code class="language-yaml">env:
  # Number of worker threads (adjust based on CPU cores)
  WORKER_COUNT: "4"

  # API rate limit (requests per minute)
  RATE_LIMIT: "1000"

  # Feature flags
  ENABLE_BETA_FEATURES: "false"
</code></pre>
<hr />
<h2 id="common-patterns"><a class="header" href="#common-patterns">Common Patterns</a></h2>
<h3 id="multi-environment-workflows"><a class="header" href="#multi-environment-workflows">Multi-Environment Workflows</a></h3>
<pre><code class="language-yaml"># Load environment-specific configuration
env_files:
  - .env.${ENVIRONMENT}

env:
  APP_NAME: "my-app"

commands:
  - shell: "npm run deploy"
</code></pre>
<h3 id="provider-based-secrets"><a class="header" href="#provider-based-secrets">Provider-Based Secrets</a></h3>
<pre><code class="language-yaml">secrets:
  # Production: use Vault
  API_KEY:
    provider: vault
    key: "api/key"

  # Development: use environment variable
  DEV_API_KEY:
    provider: env
    key: "LOCAL_API_KEY"
</code></pre>
<h3 id="build-matrix-with-profiles"><a class="header" href="#build-matrix-with-profiles">Build Matrix with Profiles</a></h3>
<pre><code class="language-yaml">profiles:
  debug:
    CARGO_PROFILE: debug
    RUST_BACKTRACE: "1"

  release:
    CARGO_PROFILE: release
    RUST_BACKTRACE: "0"

commands:
  - shell: "cargo build --profile ${CARGO_PROFILE}"
</code></pre>
<h3 id="temporary-environment-changes"><a class="header" href="#temporary-environment-changes">Temporary Environment Changes</a></h3>
<pre><code class="language-yaml"># Steps go directly in the workflow
# Set PATH for this command only using shell syntax
- shell: "cd frontend &amp;&amp; PATH=./node_modules/.bin:$PATH ./node_modules/.bin/webpack"

# PATH is back to normal for subsequent commands
- shell: "echo $PATH"
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="configuration"><a class="header" href="#configuration">Configuration</a></h1>
<p>Prodigy supports comprehensive configuration through multiple files with a clear precedence hierarchy. This chapter explains all configuration options and how to use them effectively.</p>
<h2 id="quick-start-2"><a class="header" href="#quick-start-2">Quick Start</a></h2>
<p>Minimal project configuration (<code>.prodigy/config.yml</code>):</p>
<pre><code class="language-yaml">name: my-project
</code></pre>
<p>Minimal workflow configuration (<code>.prodigy/workflow.yml</code>):</p>
<pre><code class="language-yaml">commands:
  - prodigy-code-review
  - prodigy-lint
</code></pre>
<p>That’s all you need to get started! Prodigy provides sensible defaults for everything else.</p>
<h2 id="configuration-file-locations"><a class="header" href="#configuration-file-locations">Configuration File Locations</a></h2>
<p>Prodigy uses a search hierarchy to find configuration files:</p>
<h3 id="workflow-configuration"><a class="header" href="#workflow-configuration">Workflow Configuration</a></h3>
<ol>
<li>
<p><strong>Explicit path via CLI</strong> (highest priority):</p>
<pre><code class="language-bash">prodigy run custom-workflow.yml
</code></pre>
</li>
<li>
<p><strong>Project workflow file</strong>:</p>
<pre><code>.prodigy/workflow.yml
</code></pre>
</li>
<li>
<p><strong>Default configuration</strong> (if no file found):</p>
<ul>
<li>Uses built-in defaults</li>
<li>Minimal workflow with standard commands</li>
</ul>
</li>
</ol>
<h3 id="project-configuration"><a class="header" href="#project-configuration">Project Configuration</a></h3>
<p>Project settings are loaded from:</p>
<pre><code>.prodigy/config.yml
</code></pre>
<h3 id="global-configuration"><a class="header" href="#global-configuration">Global Configuration</a></h3>
<p>Global user settings are stored in:</p>
<pre><code>~/.prodigy/config.yml
</code></pre>
<h3 id="format-support"><a class="header" href="#format-support">Format Support</a></h3>
<p><strong>Supported formats:</strong></p>
<ul>
<li>YAML (<code>.yml</code>, <code>.yaml</code>) - <strong>Recommended and required format</strong></li>
</ul>
<p><strong>Unsupported formats:</strong></p>
<ul>
<li>TOML (<code>.toml</code>) - No longer supported. Prodigy will reject TOML files with an error during validation. See the Migration Guide below for converting to YAML.</li>
<li>JSON (<code>.json</code>) - Not supported</li>
</ul>
<p>All configuration files must use YAML format.</p>
<h2 id="configuration-precedence-rules"><a class="header" href="#configuration-precedence-rules">Configuration Precedence Rules</a></h2>
<p>Prodigy merges configuration from multiple sources with clear precedence:</p>
<h3 id="overall-precedence-hierarchy"><a class="header" href="#overall-precedence-hierarchy">Overall Precedence Hierarchy</a></h3>
<ol>
<li><strong>Environment variables</strong> (highest priority)</li>
<li><strong>Project configuration</strong> (<code>.prodigy/config.yml</code>)</li>
<li><strong>Global configuration</strong> (<code>~/.prodigy/config.yml</code>)</li>
<li><strong>Default values</strong> (lowest priority)</li>
</ol>
<h3 id="specific-settings-precedence"><a class="header" href="#specific-settings-precedence">Specific Settings Precedence</a></h3>
<h4 id="claude-api-key"><a class="header" href="#claude-api-key">Claude API Key</a></h4>
<pre><code>Project config &gt; Global config (with env var overrides) &gt; Defaults
</code></pre>
<p><strong>Important:</strong> Environment variables are merged into global config via the <code>merge_env_vars()</code> function before the project config check, so the actual evaluation order is:</p>
<ol>
<li><strong>Project config</strong> (highest priority) - Explicit project-level API key</li>
<li><strong>Global config with environment variable overrides</strong> - Environment variables override values in <code>~/.prodigy/config.yml</code></li>
<li><strong>Defaults</strong> (lowest priority)</li>
</ol>
<p>Example:</p>
<pre><code class="language-yaml"># .prodigy/config.yml (highest priority - takes precedence over everything)
claude_api_key: "sk-project-key"
</code></pre>
<pre><code class="language-yaml"># ~/.prodigy/config.yml (can be overridden by env vars)
claude_api_key: "sk-global-key"
</code></pre>
<pre><code class="language-bash"># Environment variable (overrides global config, but not project config)
export PRODIGY_CLAUDE_API_KEY="sk-env-key"
</code></pre>
<p>If both <code>~/.prodigy/config.yml</code> has <code>claude_api_key: "sk-global-key"</code> and <code>PRODIGY_CLAUDE_API_KEY="sk-env-key"</code> is set, the environment variable wins for the merged global config. However, if <code>.prodigy/config.yml</code> has a <code>claude_api_key</code>, it overrides both.</p>
<h4 id="auto-commit-setting"><a class="header" href="#auto-commit-setting">Auto-Commit Setting</a></h4>
<pre><code>Project config &gt; Global config &gt; Default (true)
</code></pre>
<h4 id="log-level"><a class="header" href="#log-level">Log Level</a></h4>
<pre><code>Environment variable &gt; Global config &gt; Default (info)
</code></pre>
<p>Example:</p>
<pre><code class="language-bash"># Override log level via environment
export PRODIGY_LOG_LEVEL="debug"
prodigy run workflow.yml
</code></pre>
<h2 id="global-configuration-structure"><a class="header" href="#global-configuration-structure">Global Configuration Structure</a></h2>
<p>Global configuration is stored in <code>~/.prodigy/config.yml</code> and applies to all projects.</p>
<h3 id="globalconfig-fields"><a class="header" href="#globalconfig-fields">GlobalConfig Fields</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr></thead><tbody>
<tr><td><code>prodigy_home</code></td><td>Path</td><td><code>~/.prodigy</code></td><td>Base directory for Prodigy data</td></tr>
<tr><td><code>default_editor</code></td><td>String</td><td>None</td><td>Editor for interactive operations</td></tr>
<tr><td><code>log_level</code></td><td>String</td><td><code>"info"</code></td><td>Logging verbosity (<code>info</code>, <code>debug</code>, <code>trace</code>)</td></tr>
<tr><td><code>claude_api_key</code></td><td>String</td><td>None</td><td>API key for Claude integration</td></tr>
<tr><td><code>max_concurrent_specs</code></td><td>Number</td><td><code>1</code></td><td>Parallel execution limit</td></tr>
<tr><td><code>auto_commit</code></td><td>Boolean</td><td><code>true</code></td><td>Automatic git commits</td></tr>
<tr><td><code>plugins</code></td><td>Object</td><td>None</td><td>Plugin system configuration</td></tr>
</tbody></table>
</div>
<h3 id="example-global-configuration"><a class="header" href="#example-global-configuration">Example Global Configuration</a></h3>
<pre><code class="language-yaml"># ~/.prodigy/config.yml
prodigy_home: /Users/username/.prodigy
default_editor: vim
log_level: info
claude_api_key: sk-ant-your-key-here
max_concurrent_specs: 1
auto_commit: true
</code></pre>
<h2 id="project-configuration-structure"><a class="header" href="#project-configuration-structure">Project Configuration Structure</a></h2>
<p>Project configuration is stored in <code>.prodigy/config.yml</code> and overrides global settings for a specific project.</p>
<h3 id="projectconfig-fields"><a class="header" href="#projectconfig-fields">ProjectConfig Fields</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Required</th><th>Default</th><th>Description</th></tr></thead><tbody>
<tr><td><code>name</code></td><td>String</td><td><strong>Yes</strong></td><td>-</td><td>Project name</td></tr>
<tr><td><code>description</code></td><td>String</td><td>No</td><td>None</td><td>Project description</td></tr>
<tr><td><code>version</code></td><td>String</td><td>No</td><td>None</td><td>Project version</td></tr>
<tr><td><code>spec_dir</code></td><td>Path</td><td>No</td><td><code>"specs"</code></td><td>Directory for specification files</td></tr>
<tr><td><code>claude_api_key</code></td><td>String</td><td>No</td><td>None</td><td>Project-specific API key</td></tr>
<tr><td><code>auto_commit</code></td><td>Boolean</td><td>No</td><td>None</td><td>Project-specific auto-commit setting</td></tr>
<tr><td><code>variables</code></td><td>Map</td><td>No</td><td>None</td><td>Custom project variables as YAML map</td></tr>
</tbody></table>
</div>
<h3 id="example-project-configuration"><a class="header" href="#example-project-configuration">Example Project Configuration</a></h3>
<p><strong>Minimal:</strong></p>
<pre><code class="language-yaml"># .prodigy/config.yml
name: my-project
</code></pre>
<p><strong>Complete:</strong></p>
<pre><code class="language-yaml"># .prodigy/config.yml
name: my-awesome-project
description: A fantastic project using Prodigy
version: 2.1.0
spec_dir: custom-specs
claude_api_key: sk-ant-project-key
auto_commit: false

variables:
  PROJECT_ROOT: /path/to/project
  API_URL: https://api.example.com
</code></pre>
<h2 id="workflow-configuration-1"><a class="header" href="#workflow-configuration-1">Workflow Configuration</a></h2>
<p>Workflow configuration defines commands to execute and their environment.</p>
<h3 id="workflowconfig-structure"><a class="header" href="#workflowconfig-structure">WorkflowConfig Structure</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody>
<tr><td><code>commands</code></td><td>Array</td><td>List of commands to execute (required)</td></tr>
<tr><td><code>env</code></td><td>Object</td><td>Global environment variables</td></tr>
<tr><td><code>secrets</code></td><td>Object</td><td>Secret environment variables (masked in logs)</td></tr>
<tr><td><code>env_files</code></td><td>Array</td><td>List of <code>.env</code> files to load</td></tr>
<tr><td><code>profiles</code></td><td>Object</td><td>Named environment profiles</td></tr>
<tr><td><code>merge</code></td><td>Object</td><td>Custom merge workflow configuration</td></tr>
</tbody></table>
</div>
<h3 id="basic-workflow-examples"><a class="header" href="#basic-workflow-examples">Basic Workflow Examples</a></h3>
<p><strong>Simple command list:</strong></p>
<pre><code class="language-yaml">commands:
  - prodigy-code-review
  - prodigy-implement-spec
  - prodigy-lint
</code></pre>
<p><strong>With environment variables:</strong></p>
<pre><code class="language-yaml">env:
  PROJECT_NAME: prodigy
  VERSION: 1.0.0

commands:
  - prodigy-build
  - prodigy-test
</code></pre>
<p><strong>With secrets (masked in logs):</strong></p>
<pre><code class="language-yaml">env:
  PROJECT_NAME: prodigy

secrets:
  API_KEY:
    secret: true
    value: sk-secret-key-here

commands:
  - prodigy-deploy
</code></pre>
<p><strong>With profiles (environment-specific):</strong></p>
<pre><code class="language-yaml">env:
  PROJECT_NAME: prodigy

  API_URL:
    default: http://localhost:3000
    staging: https://staging.api.com
    prod: https://api.com

commands:
  - prodigy-deploy
</code></pre>
<p>Run with a profile:</p>
<pre><code class="language-bash">prodigy run workflow.yml --profile prod
</code></pre>
<p><strong>With custom merge workflow:</strong></p>
<pre><code class="language-yaml">commands:
  - prodigy-implement-feature

merge:
  commands:
    - shell: "cargo test"
    - shell: "cargo clippy"
    - claude: "/prodigy-merge-worktree ${merge.source_branch}"
  timeout: 600
</code></pre>
<h2 id="storage-configuration"><a class="header" href="#storage-configuration">Storage Configuration</a></h2>
<p>Storage configuration controls where and how Prodigy stores data (events, state, DLQ).</p>
<h3 id="storageconfig-structure"><a class="header" href="#storageconfig-structure">StorageConfig Structure</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr></thead><tbody>
<tr><td><code>backend</code></td><td>String</td><td><code>"file"</code></td><td>Storage backend type (<code>file</code>, <code>memory</code>)</td></tr>
<tr><td><code>connection_pool_size</code></td><td>Number</td><td><code>10</code></td><td>Pool size for database backends</td></tr>
<tr><td><code>retry_policy</code></td><td>Object</td><td>See below</td><td>Retry configuration</td></tr>
<tr><td><code>timeout</code></td><td>Duration</td><td><code>30s</code></td><td>Default operation timeout</td></tr>
<tr><td><code>backend_config</code></td><td>Object</td><td>See below</td><td>Backend-specific settings</td></tr>
<tr><td><code>enable_locking</code></td><td>Boolean</td><td><code>true</code></td><td>Distributed locking</td></tr>
<tr><td><code>enable_cache</code></td><td>Boolean</td><td><code>false</code></td><td>Caching layer</td></tr>
<tr><td><code>cache_config</code></td><td>Object</td><td>See below</td><td>Cache configuration</td></tr>
</tbody></table>
</div>
<h3 id="file-storage-configuration"><a class="header" href="#file-storage-configuration">File Storage Configuration</a></h3>
<p>The file backend stores data in the filesystem (recommended for production).</p>
<h4 id="fileconfig-fields"><a class="header" href="#fileconfig-fields">FileConfig Fields</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr></thead><tbody>
<tr><td><code>base_dir</code></td><td>Path</td><td><code>~/.prodigy</code></td><td>Base directory for storage</td></tr>
<tr><td><code>use_global</code></td><td>Boolean</td><td><code>true</code></td><td>Use global storage (local storage deprecated)</td></tr>
<tr><td><code>enable_file_locks</code></td><td>Boolean</td><td><code>true</code></td><td>File-based locking</td></tr>
<tr><td><code>max_file_size</code></td><td>Number</td><td><code>104857600</code></td><td>Max file size before rotation (100MB, same as MemoryConfig.max_memory default)</td></tr>
<tr><td><code>enable_compression</code></td><td>Boolean</td><td><code>false</code></td><td>Compression for archived files</td></tr>
</tbody></table>
</div>
<h4 id="example-file-storage-configuration"><a class="header" href="#example-file-storage-configuration">Example File Storage Configuration</a></h4>
<pre><code class="language-yaml">storage:
  backend: file
  backend_config:
    base_dir: /custom/prodigy/data
    use_global: true
    enable_file_locks: true
    max_file_size: 104857600  # 100MB
    enable_compression: false
</code></pre>
<h3 id="memory-storage-configuration"><a class="header" href="#memory-storage-configuration">Memory Storage Configuration</a></h3>
<p>The memory backend stores data in RAM (useful for testing).</p>
<h4 id="memoryconfig-fields"><a class="header" href="#memoryconfig-fields">MemoryConfig Fields</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr></thead><tbody>
<tr><td><code>max_memory</code></td><td>Number</td><td><code>104857600</code></td><td>Maximum memory usage (100MB)</td></tr>
<tr><td><code>persist_to_disk</code></td><td>Boolean</td><td><code>false</code></td><td>Enable persistence to disk</td></tr>
<tr><td><code>persistence_path</code></td><td>Path</td><td>None</td><td>Persistence file path</td></tr>
</tbody></table>
</div>
<h4 id="example-memory-storage-configuration"><a class="header" href="#example-memory-storage-configuration">Example Memory Storage Configuration</a></h4>
<pre><code class="language-yaml">storage:
  backend: memory
  backend_config:
    max_memory: 104857600  # 100MB
    persist_to_disk: false
</code></pre>
<h3 id="retry-policy-configuration"><a class="header" href="#retry-policy-configuration">Retry Policy Configuration</a></h3>
<p>Controls retry behavior for failed storage operations.</p>
<h4 id="retrypolicy-fields"><a class="header" href="#retrypolicy-fields">RetryPolicy Fields</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr></thead><tbody>
<tr><td><code>max_retries</code></td><td>Number</td><td><code>3</code></td><td>Maximum retry attempts</td></tr>
<tr><td><code>initial_delay</code></td><td>Duration</td><td><code>1s</code></td><td>Initial retry delay</td></tr>
<tr><td><code>max_delay</code></td><td>Duration</td><td><code>30s</code></td><td>Maximum retry delay</td></tr>
<tr><td><code>backoff_multiplier</code></td><td>Number</td><td><code>2.0</code></td><td>Exponential backoff multiplier</td></tr>
<tr><td><code>jitter</code></td><td>Boolean</td><td><code>true</code></td><td>Enable jitter</td></tr>
</tbody></table>
</div>
<h4 id="example-retry-policy"><a class="header" href="#example-retry-policy">Example Retry Policy</a></h4>
<pre><code class="language-yaml">storage:
  retry_policy:
    max_retries: 5
    initial_delay: 1s
    max_delay: 60s
    backoff_multiplier: 2.0
    jitter: true
</code></pre>
<h3 id="cache-configuration"><a class="header" href="#cache-configuration">Cache Configuration</a></h3>
<p>Optional caching layer for improved performance.</p>
<h4 id="cacheconfig-fields"><a class="header" href="#cacheconfig-fields">CacheConfig Fields</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr></thead><tbody>
<tr><td><code>max_entries</code></td><td>Number</td><td><code>1000</code></td><td>Cache size limit (entries)</td></tr>
<tr><td><code>ttl</code></td><td>Duration</td><td><code>3600s</code></td><td>Cache TTL (1 hour)</td></tr>
<tr><td><code>cache_type</code></td><td>String</td><td><code>"memory"</code></td><td>Cache implementation</td></tr>
</tbody></table>
</div>
<h4 id="example-cache-configuration"><a class="header" href="#example-cache-configuration">Example Cache Configuration</a></h4>
<pre><code class="language-yaml">storage:
  enable_cache: true
  cache_config:
    max_entries: 5000
    ttl: 600s  # 10 minutes
    cache_type: memory
</code></pre>
<h2 id="environment-variables"><a class="header" href="#environment-variables">Environment Variables</a></h2>
<p>Prodigy recognizes the following environment variables:</p>
<h3 id="core-environment-variables"><a class="header" href="#core-environment-variables">Core Environment Variables</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Variable</th><th>Type</th><th>Description</th></tr></thead><tbody>
<tr><td><code>PRODIGY_CLAUDE_API_KEY</code></td><td>String</td><td>Override Claude API key</td></tr>
<tr><td><code>PRODIGY_LOG_LEVEL</code></td><td>String</td><td>Override log level (<code>info</code>, <code>debug</code>, <code>trace</code>)</td></tr>
<tr><td><code>PRODIGY_EDITOR</code></td><td>String</td><td>Override default editor</td></tr>
<tr><td><code>EDITOR</code></td><td>String</td><td>Fallback editor variable</td></tr>
<tr><td><code>PRODIGY_AUTO_COMMIT</code></td><td>Boolean</td><td>Override auto-commit setting (<code>true</code>, <code>false</code>)</td></tr>
</tbody></table>
</div>
<h3 id="storage-environment-variables"><a class="header" href="#storage-environment-variables">Storage Environment Variables</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Variable</th><th>Type</th><th>Description</th></tr></thead><tbody>
<tr><td><code>PRODIGY_STORAGE_TYPE</code></td><td>String</td><td>Storage backend type (<code>file</code>, <code>memory</code>)</td></tr>
<tr><td><code>PRODIGY_STORAGE_BASE_PATH</code></td><td>Path</td><td>Custom storage directory (recommended)</td></tr>
<tr><td><code>PRODIGY_STORAGE_DIR</code></td><td>Path</td><td>Alternative storage directory variable (fallback)</td></tr>
<tr><td><code>PRODIGY_STORAGE_PATH</code></td><td>Path</td><td>Alternative storage directory variable (fallback)</td></tr>
</tbody></table>
</div>
<p><strong>Note:</strong> Multiple environment variable names are supported for the storage base path. They are checked in the following order:</p>
<ol>
<li><code>PRODIGY_STORAGE_BASE_PATH</code> (recommended)</li>
<li><code>PRODIGY_STORAGE_DIR</code> (fallback)</li>
<li><code>PRODIGY_STORAGE_PATH</code> (fallback)</li>
</ol>
<p>Use <code>PRODIGY_STORAGE_BASE_PATH</code> for consistency with the config file field name.</p>
<h3 id="automation-environment-variables"><a class="header" href="#automation-environment-variables">Automation Environment Variables</a></h3>
<p>These variables are set automatically by Prodigy during execution:</p>
<div class="table-wrapper"><table><thead><tr><th>Variable</th><th>Value</th><th>Description</th></tr></thead><tbody>
<tr><td><code>PRODIGY_AUTOMATION</code></td><td><code>"true"</code></td><td>Signals automated execution mode</td></tr>
<tr><td><code>PRODIGY_CLAUDE_STREAMING</code></td><td><code>"true"</code> or <code>"false"</code></td><td>Automatically set by Prodigy based on verbosity level (<code>-v</code> flag). Can be manually set to <code>"false"</code> to disable JSON streaming in CI/CD environments with storage constraints.</td></tr>
</tbody></table>
</div>
<h3 id="examples"><a class="header" href="#examples">Examples</a></h3>
<p><strong>Override log level:</strong></p>
<pre><code class="language-bash">export PRODIGY_LOG_LEVEL=debug
prodigy run workflow.yml
</code></pre>
<p><strong>Set API key:</strong></p>
<pre><code class="language-bash">export PRODIGY_CLAUDE_API_KEY=sk-ant-your-key
prodigy run workflow.yml
</code></pre>
<p><strong>Disable auto-commit:</strong></p>
<pre><code class="language-bash">export PRODIGY_AUTO_COMMIT=false
prodigy run workflow.yml
</code></pre>
<p><strong>Custom storage location:</strong></p>
<pre><code class="language-bash">export PRODIGY_STORAGE_BASE_PATH=/custom/storage
prodigy run workflow.yml
</code></pre>
<h2 id="complete-configuration-examples"><a class="header" href="#complete-configuration-examples">Complete Configuration Examples</a></h2>
<h3 id="minimal-setup"><a class="header" href="#minimal-setup">Minimal Setup</a></h3>
<p><strong>Project config (<code>.prodigy/config.yml</code>):</strong></p>
<pre><code class="language-yaml">name: my-project
</code></pre>
<p><strong>Workflow (<code>.prodigy/workflow.yml</code>):</strong></p>
<pre><code class="language-yaml">commands:
  - prodigy-code-review
  - prodigy-lint
</code></pre>
<h3 id="full-featured-setup"><a class="header" href="#full-featured-setup">Full-Featured Setup</a></h3>
<p><strong>Global config (<code>~/.prodigy/config.yml</code>):</strong></p>
<pre><code class="language-yaml">prodigy_home: /Users/username/.prodigy
default_editor: vim
log_level: info
claude_api_key: sk-ant-global-key
max_concurrent_specs: 1
auto_commit: true
</code></pre>
<p><strong>Project config (<code>.prodigy/config.yml</code>):</strong></p>
<pre><code class="language-yaml">name: production-app
description: Our production application
version: 2.0.0
spec_dir: specifications
auto_commit: false

variables:
  PROJECT_ROOT: /app
  ENV: production
</code></pre>
<p><strong>Workflow with environments (<code>.prodigy/workflow.yml</code>):</strong></p>
<pre><code class="language-yaml">env:
  PROJECT_NAME: production-app
  VERSION: 2.0.0

  DATABASE_URL:
    default: postgres://localhost/dev
    staging: postgres://staging-db/app
    prod: postgres://prod-db/app

secrets:
  DB_PASSWORD:
    secret: true
    value: super-secret

commands:
  - name: prodigy-code-review
    options:
      focus: security
  - shell: "cargo build --release"
  - shell: "cargo test --all"
  - prodigy-deploy

merge:
  commands:
    - shell: "cargo test"
    - shell: "cargo clippy -- -D warnings"
    - claude: "/prodigy-merge-worktree ${merge.source_branch}"
  timeout: 600
</code></pre>
<h2 id="default-values-reference"><a class="header" href="#default-values-reference">Default Values Reference</a></h2>
<p>Quick reference of all default values:</p>
<div class="table-wrapper"><table><thead><tr><th>Setting</th><th>Default Value</th></tr></thead><tbody>
<tr><td><code>log_level</code></td><td><code>"info"</code></td></tr>
<tr><td><code>max_concurrent_specs</code></td><td><code>1</code></td></tr>
<tr><td><code>auto_commit</code></td><td><code>true</code></td></tr>
<tr><td><code>spec_dir</code></td><td><code>"specs"</code></td></tr>
<tr><td><code>storage.backend</code></td><td><code>"file"</code></td></tr>
<tr><td><code>storage.use_global</code></td><td><code>true</code></td></tr>
<tr><td><code>storage.enable_locking</code></td><td><code>true</code></td></tr>
<tr><td><code>storage.enable_cache</code></td><td><code>false</code></td></tr>
<tr><td><code>storage.connection_pool_size</code></td><td><code>10</code></td></tr>
<tr><td><code>storage.timeout</code></td><td><code>30s</code></td></tr>
<tr><td><code>storage.retry_policy.max_retries</code></td><td><code>3</code></td></tr>
<tr><td><code>storage.retry_policy.initial_delay</code></td><td><code>1s</code></td></tr>
<tr><td><code>storage.retry_policy.max_delay</code></td><td><code>30s</code></td></tr>
<tr><td><code>storage.retry_policy.backoff_multiplier</code></td><td><code>2.0</code></td></tr>
<tr><td><code>storage.file.max_file_size</code></td><td><code>104857600</code> (100MB)</td></tr>
<tr><td><code>storage.cache.max_entries</code></td><td><code>1000</code></td></tr>
<tr><td><code>storage.cache.ttl</code></td><td><code>3600s</code> (1 hour)</td></tr>
</tbody></table>
</div>
<h2 id="best-practices-2"><a class="header" href="#best-practices-2">Best Practices</a></h2>
<h3 id="managing-api-keys-securely"><a class="header" href="#managing-api-keys-securely">Managing API Keys Securely</a></h3>
<p><strong>DO:</strong></p>
<ul>
<li>Store API keys in global config (<code>~/.prodigy/config.yml</code>)</li>
<li>Use environment variables for CI/CD environments</li>
<li>Use workflow <code>secrets</code> for sensitive values</li>
<li>Add <code>.prodigy/config.yml</code> to <code>.gitignore</code> if it contains secrets</li>
</ul>
<p><strong>DON’T:</strong></p>
<ul>
<li>Commit API keys to version control</li>
<li>Share project configs containing secrets</li>
<li>Log or print secret values</li>
</ul>
<h3 id="when-to-use-each-configuration-level"><a class="header" href="#when-to-use-each-configuration-level">When to Use Each Configuration Level</a></h3>
<p><strong>Global config (<code>~/.prodigy/config.yml</code>):</strong></p>
<ul>
<li>Personal preferences (editor, log level)</li>
<li>Default Claude API key</li>
<li>System-wide settings</li>
</ul>
<p><strong>Project config (<code>.prodigy/config.yml</code>):</strong></p>
<ul>
<li>Project name and metadata</li>
<li>Project-specific spec directory</li>
<li>Project-specific API key (if needed)</li>
<li>Custom variables shared by team</li>
</ul>
<p><strong>Workflow config (<code>.prodigy/workflow.yml</code>):</strong></p>
<ul>
<li>Command sequences</li>
<li>Environment variables for commands</li>
<li>Environment-specific settings (profiles)</li>
<li>Workflow-level secrets</li>
</ul>
<p><strong>Environment variables:</strong></p>
<ul>
<li>CI/CD overrides</li>
<li>Temporary testing configurations</li>
<li>Dynamic runtime values</li>
</ul>
<h3 id="environment-specific-configurations"><a class="header" href="#environment-specific-configurations">Environment-Specific Configurations</a></h3>
<p>Use profiles for different environments:</p>
<pre><code class="language-yaml">env:
  # Common variables
  PROJECT_NAME: my-app

  # Environment-specific
  API_URL:
    default: http://localhost:3000
    dev: http://localhost:3000
    staging: https://staging.api.com
    prod: https://api.com

  LOG_LEVEL:
    default: debug
    prod: info

commands:
  - prodigy-deploy
</code></pre>
<p>Run for specific environment:</p>
<pre><code class="language-bash">prodigy run workflow.yml --profile prod
</code></pre>
<h2 id="troubleshooting-1"><a class="header" href="#troubleshooting-1">Troubleshooting</a></h2>
<h3 id="common-configuration-errors"><a class="header" href="#common-configuration-errors">Common Configuration Errors</a></h3>
<p><strong>Error: “Unsupported configuration file format”</strong></p>
<ul>
<li><strong>Cause:</strong> Using TOML or JSON instead of YAML</li>
<li><strong>Solution:</strong> Convert your config to YAML format (<code>.yml</code> or <code>.yaml</code>)</li>
</ul>
<p><strong>Error: “Failed to read configuration file”</strong></p>
<ul>
<li><strong>Cause:</strong> File doesn’t exist or permissions issue</li>
<li><strong>Solution:</strong> Check file path and permissions</li>
</ul>
<p><strong>Error: “Failed to parse configuration”</strong></p>
<ul>
<li><strong>Cause:</strong> Invalid YAML syntax</li>
<li><strong>Solution:</strong> Validate YAML syntax (use <code>yamllint</code> or online validator)</li>
</ul>
<p><strong>Error: “Claude API key not found”</strong></p>
<ul>
<li><strong>Cause:</strong> No API key configured</li>
<li><strong>Solution:</strong> Set <code>claude_api_key</code> in global or project config, or use <code>PRODIGY_CLAUDE_API_KEY</code> environment variable</li>
</ul>
<h3 id="validating-configuration"><a class="header" href="#validating-configuration">Validating Configuration</a></h3>
<p>Check your configuration syntax:</p>
<pre><code class="language-bash"># Validate YAML syntax
yamllint .prodigy/workflow.yml

# Test configuration loading
prodigy run workflow.yml --dry-run
</code></pre>
<h3 id="configuration-precedence-debugging"><a class="header" href="#configuration-precedence-debugging">Configuration Precedence Debugging</a></h3>
<p>To see which configuration values are being used:</p>
<pre><code class="language-bash"># Enable debug logging
export PRODIGY_LOG_LEVEL=debug
prodigy run workflow.yml -v
</code></pre>
<p>This will show which config files are loaded and how values are merged.</p>
<h2 id="migration-guide-toml-to-yaml"><a class="header" href="#migration-guide-toml-to-yaml">Migration Guide: TOML to YAML</a></h2>
<p>If you’re upgrading from an older version that used TOML:</p>
<p><strong>Old TOML format (<code>.prodigy/config.toml</code>):</strong></p>
<pre><code class="language-toml">name = "my-project"
description = "My project"

[variables]
PROJECT_ROOT = "/app"
</code></pre>
<p><strong>New YAML format (<code>.prodigy/config.yml</code>):</strong></p>
<pre><code class="language-yaml">name: my-project
description: My project

variables:
  PROJECT_ROOT: /app
</code></pre>
<p><strong>Key differences:</strong></p>
<ul>
<li>Use <code>:</code> instead of <code>=</code> for assignments</li>
<li>Indentation matters (use 2 spaces)</li>
<li>No need for <code>[section]</code> headers (use nested structure)</li>
<li>Strings usually don’t need quotes (unless they contain special characters)</li>
</ul>
<h2 id="related-documentation"><a class="header" href="#related-documentation">Related Documentation</a></h2>
<ul>
<li><a href="./workflows.html">Workflow System</a> - Learn about workflow execution</li>
<li><a href="./mapreduce.html">MapReduce</a> - Configure MapReduce workflows</li>
<li><a href="./cli-reference.html">CLI Reference</a> - Command-line options</li>
<li><a href="./storage.html">Storage System</a> - Understanding storage backends</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="advanced-features"><a class="header" href="#advanced-features">Advanced Features</a></h1>
<p>This chapter covers advanced workflow features for building sophisticated automation pipelines. These features enable conditional execution, parallel processing, validation, and complex control flow.</p>
<hr />
<h2 id="conditional-execution-1"><a class="header" href="#conditional-execution-1">Conditional Execution</a></h2>
<p>Control when commands execute based on expressions or previous command results.</p>
<h3 id="expression-based-conditions"><a class="header" href="#expression-based-conditions">Expression-Based Conditions</a></h3>
<p>Use the <code>when</code> field to conditionally execute commands based on variable values:</p>
<pre><code class="language-yaml"># Execute only when variable is true
- shell: "cargo build --release"
  when: "${tests_passed}"

# Execute based on complex expression
- shell: "deploy.sh"
  when: "${environment == 'production' &amp;&amp; tests_passed}"
</code></pre>
<h4 id="expression-syntax-for-when-clauses"><a class="header" href="#expression-syntax-for-when-clauses">Expression Syntax for When Clauses</a></h4>
<p>The <code>when</code> clause supports a flexible expression syntax for conditional logic:</p>
<p><strong>Variable Interpolation:</strong></p>
<ul>
<li>Use <code>${variable}</code> to reference captured outputs or environment variables</li>
<li>Variables are evaluated in the context of previous command results</li>
<li>Boolean variables are evaluated as truthy/falsy values</li>
</ul>
<p><strong>Comparison Operators:</strong></p>
<ul>
<li><code>==</code> - Equality comparison (e.g., <code>${status == 'success'}</code>)</li>
<li><code>!=</code> - Inequality comparison (e.g., <code>${exit_code != 0}</code>)</li>
<li><code>&gt;</code> - Greater than (e.g., <code>${score &gt; 80}</code>)</li>
<li><code>&lt;</code> - Less than (e.g., <code>${errors &lt; 5}</code>)</li>
<li><code>&gt;=</code> - Greater than or equal to (e.g., <code>${coverage &gt;= 90}</code>)</li>
<li><code>&lt;=</code> - Less than or equal to (e.g., <code>${warnings &lt;= 10}</code>)</li>
<li><code>contains</code> - String matching (e.g., <code>${output contains 'success'}</code>)</li>
</ul>
<p><strong>Logical Operators:</strong></p>
<ul>
<li><code>&amp;&amp;</code> - Logical AND (e.g., <code>${tests_passed &amp;&amp; build_succeeded}</code>)</li>
<li><code>||</code> - Logical OR (e.g., <code>${is_dev || is_staging}</code>)</li>
</ul>
<p><strong>Type Coercion:</strong></p>
<ul>
<li>String values: Non-empty strings are truthy, empty strings are falsy</li>
<li>Numeric values: Non-zero numbers are truthy, zero is falsy</li>
<li>Boolean values: <code>true</code> is truthy, <code>false</code> is falsy</li>
</ul>
<p><strong>Complex Expressions:</strong></p>
<pre><code class="language-yaml"># Multiple conditions with logical operators
- shell: "deploy.sh"
  when: "${environment == 'production' &amp;&amp; tests_passed &amp;&amp; coverage &gt;= 80}"

# Nested logic with parentheses
- shell: "run-checks.sh"
  when: "${(is_pr || is_main) &amp;&amp; tests_passed}"

# Comparing captured outputs
- shell: "notify-team.sh"
  when: "${test-step.exit_code == 0 &amp;&amp; build-step.success}"
</code></pre>
<h3 id="on-success-handlers"><a class="header" href="#on-success-handlers">On Success Handlers</a></h3>
<p>Execute follow-up commands when a command succeeds:</p>
<pre><code class="language-yaml">- shell: "cargo test"
  on_success:
    shell: "cargo bench"
</code></pre>
<p><strong>Note</strong>: The <code>on_success</code> field supports any workflow step command with all its features, including nested conditionals, output capture, validation, and error handlers. You can create complex success workflows by combining multiple handlers or using <code>when</code> clauses for sophisticated control flow.</p>
<p><strong>Complex On Success Example:</strong></p>
<pre><code class="language-yaml">- shell: "cargo build --release"
  on_success:
    claude: "/verify-build-artifacts"
    validate:
      shell: "check-binary-size.sh"
      threshold: 100
    on_failure:
      claude: "/optimize-binary-size"
      max_attempts: 2
</code></pre>
<h3 id="on-failure-handlers"><a class="header" href="#on-failure-handlers">On Failure Handlers</a></h3>
<p>Handle failures with automatic remediation:</p>
<pre><code class="language-yaml">- shell: "cargo clippy"
  on_failure:
    claude: "/fix-warnings"
    max_attempts: 3
    fail_workflow: false
    commit_required: true
</code></pre>
<p>The <code>on_failure</code> configuration supports:</p>
<ul>
<li><code>max_attempts</code>: Maximum retry attempts (default: 3)</li>
<li><code>fail_workflow</code>: Whether to fail entire workflow on final failure (default: false)</li>
<li><code>commit_required</code>: Whether the remediation command should create a git commit (default: true)</li>
</ul>
<p><strong>Note</strong>: These defaults come from the <code>TestDebugConfig</code> which provides sensible defaults for error recovery workflows.</p>
<p><strong>Multi-Step Remediation:</strong></p>
<p>For complex error recovery, use the <code>commands</code> array to execute multiple remediation steps in sequence:</p>
<pre><code class="language-yaml">- shell: "cargo test"
  on_failure:
    commands:
      - claude: "/analyze-test-failures"
      - shell: "cargo fmt"
      - claude: "/verify-fixes"
    max_attempts: 3
    fail_workflow: true
</code></pre>
<h3 id="exit-code-handlers"><a class="header" href="#exit-code-handlers">Exit Code Handlers</a></h3>
<p>Map specific exit codes to different actions using <code>on_exit_code</code>:</p>
<pre><code class="language-yaml">- shell: "cargo test"
  on_exit_code:
    1: {claude: "/fix-test-failures"}
    2: {shell: "retry-flaky-tests.sh"}
    101: {claude: "/fix-compilation-errors"}
    255: {fail_workflow: true}
</code></pre>
<p>This allows fine-grained control over error handling based on the specific exit code returned by a command.</p>
<h3 id="nested-conditionals"><a class="header" href="#nested-conditionals">Nested Conditionals</a></h3>
<p>Chain multiple levels of conditional execution:</p>
<pre><code class="language-yaml">- shell: "cargo check"
  on_success:
    shell: "cargo build --release"
    on_success:
      shell: "cargo test --release"
      on_failure:
        claude: "/debug-failures '${shell.output}'"
</code></pre>
<hr />
<h2 id="output-capture-and-variable-management"><a class="header" href="#output-capture-and-variable-management">Output Capture and Variable Management</a></h2>
<p>Capture command output in different formats for use in subsequent steps.</p>
<h3 id="capture-variable"><a class="header" href="#capture-variable">Capture Variable</a></h3>
<p>Capture output to a named variable using the <code>capture_output</code> field:</p>
<pre><code class="language-yaml"># Capture as string (backward compatible)
- shell: "git rev-parse HEAD"
  capture_output: "commit_hash"

# Reference in later steps
- shell: "echo 'Commit: ${commit_hash}'"
</code></pre>
<h3 id="capture-formats-1"><a class="header" href="#capture-formats-1">Capture Formats</a></h3>
<p>Control how output is parsed with <code>capture_format</code>:</p>
<pre><code class="language-yaml"># String (default) - trimmed output as single string
- shell: "git rev-parse HEAD"
  capture_output: "commit_hash"
  capture_format: "string"

# Number - parse output as number
- shell: "wc -l &lt; file.txt"
  capture_output: "line_count"
  capture_format: "number"

# JSON - parse output as JSON object
- shell: "cargo metadata --format-version 1"
  capture_output: "metadata"
  capture_format: "json"

# Lines - split output into array of lines
- shell: "find . -name '*.rs'"
  capture_output: "rust_files"
  capture_format: "lines"

# Boolean - parse "true"/"false" as boolean
- shell: "test -f README.md &amp;&amp; echo true || echo false"
  capture_output: "has_readme"
  capture_format: "boolean"
</code></pre>
<h3 id="stream-capture-control"><a class="header" href="#stream-capture-control">Stream Capture Control</a></h3>
<p>The <code>capture_streams</code> field supports two formats:</p>
<p><strong>Simple String Format</strong> - For basic stream selection:</p>
<pre><code class="language-yaml"># Capture only stdout (default)
- shell: "cargo build"
  capture_output: "build_log"
  capture_streams: "stdout"

# Capture only stderr
- shell: "cargo test"
  capture_output: "errors"
  capture_streams: "stderr"

# Capture both streams
- shell: "npm install"
  capture_output: "full_output"
  capture_streams: "both"
</code></pre>
<p>Use the string format when you only need to capture output from specific streams.</p>
<p><strong>Advanced Struct Format</strong> - For fine-grained control with metadata:</p>
<pre><code class="language-yaml">- shell: "cargo test"
  capture_output: "test_results"
  capture_streams:
    stdout: true
    stderr: true
    exit_code: true
    success: true
    duration: true
</code></pre>
<p>Use the struct format when you need additional metadata alongside the captured output:</p>
<ul>
<li><code>exit_code</code>: Capture the command’s exit code</li>
<li><code>success</code>: Capture whether the command succeeded (true/false)</li>
<li><code>duration</code>: Capture how long the command took to execute</li>
</ul>
<p>This is particularly useful for validation workflows where you need to make decisions based on command success or timing information.</p>
<p><strong>Format Flexibility</strong>: The <code>capture_streams</code> field is flexible—you can choose either format based on your needs. Use the simple string format (<code>"stdout"</code>, <code>"stderr"</code>, <code>"both"</code>) when you only need basic stream selection, or the struct format when you need metadata like <code>exit_code</code>, <code>success</code>, and <code>duration</code> alongside the output. Prodigy uses Rust’s untagged enum deserialization, allowing you to choose either format based on your needs without any special syntax.</p>
<h3 id="output-file-redirection"><a class="header" href="#output-file-redirection">Output File Redirection</a></h3>
<p>Write command output directly to a file:</p>
<pre><code class="language-yaml"># Redirect output to file
- shell: "cargo test --verbose"
  output_file: "test-results.txt"

# File is written to working directory
# Can be combined with capture_output to save and use output
</code></pre>
<h3 id="working-directory-control"><a class="header" href="#working-directory-control">Working Directory Control</a></h3>
<p>Control the working directory for individual commands using the <code>working_dir</code> field. This allows you to execute commands in different directories without changing your workflow’s base directory:</p>
<pre><code class="language-yaml"># Run frontend build in frontend directory
- shell: "npm install"
  working_dir: "./frontend"

# Run backend build in backend directory
- shell: "cargo build"
  working_dir: "./backend"

# Run tests in a subdirectory
- shell: "pytest"
  working_dir: "./tests"
</code></pre>
<p>The <code>working_dir</code> path is relative to the workflow’s root directory. This is particularly useful for:</p>
<ul>
<li><strong>Monorepo workflows</strong>: Building different packages in their respective directories</li>
<li><strong>Multi-language projects</strong>: Running language-specific tools in appropriate directories</li>
<li><strong>Isolated testing</strong>: Running tests in dedicated test directories</li>
<li><strong>Subproject operations</strong>: Working with nested projects without changing global context</li>
</ul>
<p><strong>Note</strong>: The <code>working_dir</code> field is also available with the alias <code>cwd</code> for compatibility with other tools.</p>
<h3 id="step-level-environment-overrides-1"><a class="header" href="#step-level-environment-overrides-1">Step-Level Environment Overrides</a></h3>
<p>Configure environment variables and settings for individual steps using the step environment configuration:</p>
<pre><code class="language-yaml"># Set environment variables for a specific command
- shell: "npm test"
  env:
    NODE_ENV: test
    DEBUG: "*"
  working_dir: "./frontend"
  clear_env: false  # Keep parent environment variables
  inherit: true     # Inherit workflow-level environment
</code></pre>
<p><strong>Step Environment Fields:</strong></p>
<ul>
<li><code>env</code>: Key-value pairs of environment variables to set</li>
<li><code>working_dir</code>: Directory to execute the command in</li>
<li><code>clear_env</code>: Whether to clear all parent environment variables (default: false)</li>
<li><code>inherit</code>: Whether to inherit workflow-level environment variables (default: true)</li>
</ul>
<p><strong>Use Cases:</strong></p>
<ul>
<li>Set test-specific environment variables without affecting other steps</li>
<li>Configure tool-specific settings (DEBUG, LOG_LEVEL, etc.)</li>
<li>Run commands with isolated environments for reproducibility</li>
<li>Override workflow-level environment for specific operations</li>
</ul>
<hr />
<h2 id="step-identification"><a class="header" href="#step-identification">Step Identification</a></h2>
<p>Assign unique IDs to steps for explicit output referencing. This is particularly useful in complex workflows where multiple steps produce outputs and you need to reference specific results.</p>
<h3 id="available-step-reference-fields"><a class="header" href="#available-step-reference-fields">Available Step Reference Fields</a></h3>
<p>When you assign an ID to a step, you can reference multiple fields from that step’s execution:</p>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Description</th><th>Example</th></tr></thead><tbody>
<tr><td><code>${step-id.output}</code></td><td>string</td><td>Captured output</td><td><code>${test-step.output}</code></td></tr>
<tr><td><code>${step-id.exit_code}</code></td><td>number</td><td>Exit code</td><td><code>${build.exit_code}</code></td></tr>
<tr><td><code>${step-id.success}</code></td><td>boolean</td><td>Success status</td><td><code>${lint.success}</code></td></tr>
<tr><td><code>${step-id.duration}</code></td><td>number</td><td>Execution time (seconds)</td><td><code>${bench.duration}</code></td></tr>
</tbody></table>
</div>
<h3 id="basic-step-ids"><a class="header" href="#basic-step-ids">Basic Step IDs</a></h3>
<pre><code class="language-yaml">- shell: "cargo test"
  id: "test-step"
  capture_output: "test_results"

# Reference step output by ID
- shell: "echo 'Tests: ${test-step.output}'"

# Reference step exit code
- shell: "echo 'Exit code: ${test-step.exit_code}'"
</code></pre>
<h3 id="when-to-use-step-ids"><a class="header" href="#when-to-use-step-ids">When to Use Step IDs</a></h3>
<p><strong>1. Complex Workflows with Multiple Parallel Paths</strong></p>
<p>When you have multiple steps producing similar outputs, IDs make references unambiguous:</p>
<pre><code class="language-yaml">- shell: "cargo test --lib"
  id: "unit-tests"
  capture_output: "results"

- shell: "cargo test --test integration"
  id: "integration-tests"
  capture_output: "results"

# Clear reference to specific test results
- claude: "/analyze-failures '${unit-tests.output}'"
  when: "${unit-tests.exit_code} != 0"

- claude: "/analyze-failures '${integration-tests.output}'"
  when: "${integration-tests.exit_code} != 0"
</code></pre>
<p><strong>2. Debugging Specific Steps</strong></p>
<p>Step IDs help identify which step produced problematic output:</p>
<pre><code class="language-yaml">- shell: "npm run build"
  id: "build"
  capture_output: "build_log"

- shell: "npm run lint"
  id: "lint"
  capture_output: "lint_log"

- shell: "npm test"
  id: "test"
  capture_output: "test_log"

# Reference specific logs for debugging
- claude: "/debug-build-failure '${build.output}'"
  when: "${build.exit_code} != 0"
</code></pre>
<p><strong>3. Conditional Execution Based on Specific Step Outputs</strong></p>
<p>Use step IDs to create complex conditional logic:</p>
<pre><code class="language-yaml">- shell: "cargo clippy"
  id: "clippy-check"
  capture_output: "warnings"
  capture_format: "lines"

- shell: "cargo fmt --check"
  id: "format-check"
  capture_output: "format_issues"

# Only proceed if both checks pass
- shell: "cargo build --release"
  when: "${clippy-check.exit_code} == 0 &amp;&amp; ${format-check.exit_code} == 0"

# Fix clippy warnings if present
- claude: "/fix-clippy-warnings '${clippy-check.output}'"
  when: "${clippy-check.exit_code} != 0"
  on_failure:
    claude: "/analyze-clippy-fix-failures"
</code></pre>
<p><strong>4. Combining with Validation and Error Handlers</strong></p>
<p>Step IDs enable sophisticated error handling patterns:</p>
<pre><code class="language-yaml">- shell: "cargo test --format json"
  id: "test-run"
  capture_output: "test_results"
  capture_format: "json"
  validate:
    shell: "check-coverage.sh"
    threshold: 80
    on_incomplete:
      claude: "/improve-coverage '${test-run.output}'"
      max_attempts: 3
  on_failure:
    claude: "/debug-test-failures '${test-run.output}'"
</code></pre>
<hr />
<h2 id="timeout-configuration-1"><a class="header" href="#timeout-configuration-1">Timeout Configuration</a></h2>
<p>Set execution timeouts at the command level:</p>
<pre><code class="language-yaml"># Command-level timeout (in seconds)
- shell: "cargo bench"
  timeout: 600  # 10 minutes

# Timeout for long-running operations
- claude: "/analyze-codebase"
  timeout: 1800  # 30 minutes
</code></pre>
<h3 id="timeout-with-environment-variables"><a class="header" href="#timeout-with-environment-variables">Timeout with Environment Variables</a></h3>
<p>Timeout fields support environment variable references for flexible configuration:</p>
<pre><code class="language-yaml"># Use environment variable for timeout
- shell: "cargo bench"
  timeout: $BENCH_TIMEOUT

# With default value
- claude: "/analyze-codebase"
  timeout: ${ANALYSIS_TIMEOUT:-1800}

# Different timeouts per environment
- shell: "run-tests.sh"
  timeout: ${TEST_TIMEOUT:-300}  # 5 minutes default
</code></pre>
<p><strong>Note</strong>: Timeouts are only supported at the individual command level, not for MapReduce agents.</p>
<hr />
<h2 id="implementation-validation"><a class="header" href="#implementation-validation">Implementation Validation</a></h2>
<p>Validate that implementations meet requirements using the <code>validate</code> field.</p>
<h3 id="basic-validation"><a class="header" href="#basic-validation">Basic Validation</a></h3>
<p>Run validation commands after a step completes:</p>
<pre><code class="language-yaml">- claude: "/implement-feature"
  validate:
    shell: "cargo test"
    threshold: 100  # Require 100% completion
</code></pre>
<h3 id="validation-with-claude"><a class="header" href="#validation-with-claude">Validation with Claude</a></h3>
<p>Use Claude to validate implementation quality:</p>
<pre><code class="language-yaml">- shell: "generate-code.sh"
  validate:
    claude: "/verify-implementation"
    threshold: 95
</code></pre>
<h3 id="multi-step-validation"><a class="header" href="#multi-step-validation">Multi-Step Validation</a></h3>
<p>Run multiple validation commands in sequence:</p>
<pre><code class="language-yaml">- claude: "/refactor"
  validate:
    commands:
      - shell: "cargo test"
      - shell: "cargo clippy"
      - shell: "cargo fmt --check"
    threshold: 100
</code></pre>
<p><strong>Convenience Syntax</strong>: For simple cases, you can use an array format directly:</p>
<pre><code class="language-yaml">- claude: "/refactor"
  validate:
    - shell: "cargo test"
    - shell: "cargo clippy"
    - shell: "cargo fmt --check"
</code></pre>
<h3 id="validation-with-result-files"><a class="header" href="#validation-with-result-files">Validation with Result Files</a></h3>
<p>Read validation results from a file instead of stdout:</p>
<pre><code class="language-yaml">- claude: "/implement-feature"
  validate:
    shell: "run-validator.sh"
    result_file: "validation-results.json"
    threshold: 95
</code></pre>
<p><strong>When to Use result_file:</strong></p>
<p>The <code>result_file</code> option is useful when you need to separate validation output from command logs:</p>
<ul>
<li><strong>Complex JSON Output</strong>: Validation produces structured JSON that shouldn’t be mixed with logs</li>
<li><strong>Separate Concerns</strong>: Keep validation results separate from command stdout/stderr</li>
<li><strong>Additional Logging</strong>: Validation command produces diagnostic output alongside results</li>
<li><strong>Debugging</strong>: Preserve validation output in a file for later inspection</li>
</ul>
<p>The file should contain JSON matching the validation result schema with fields like <code>completion_percentage</code>, <code>status</code>, <code>gaps</code>, etc.</p>
<h3 id="handling-incomplete-implementations"><a class="header" href="#handling-incomplete-implementations">Handling Incomplete Implementations</a></h3>
<p>Automatically remediate when validation fails:</p>
<p><strong>Convenience Array Syntax</strong> - For simple remediation workflows:</p>
<pre><code class="language-yaml">- claude: "/implement-spec"
  validate:
    shell: "check-completeness.sh"
    threshold: 100
    on_incomplete:
      - claude: "/fill-gaps"
      - shell: "cargo fmt"
</code></pre>
<p><strong>Verbose Configuration</strong> - For complex cases requiring additional control:</p>
<pre><code class="language-yaml">- claude: "/implement-spec"
  validate:
    shell: "check-completeness.sh"
    threshold: 100
    on_incomplete:
      claude: "/fill-gaps"
      max_attempts: 3
      fail_workflow: true
      commit_required: true
</code></pre>
<p>The <code>on_incomplete</code> configuration supports:</p>
<ul>
<li><code>claude</code>: Claude command to execute for gap-filling</li>
<li><code>shell</code>: Shell command to execute for gap-filling</li>
<li><code>commands</code>: Array of commands to execute</li>
<li><code>max_attempts</code>: Maximum remediation attempts (default: 1)</li>
<li><code>fail_workflow</code>: Whether to fail workflow if remediation fails (default: true)</li>
<li><code>commit_required</code>: Whether remediation command should create a commit (default: false)</li>
</ul>
<hr />
<h2 id="parallel-iteration-with-foreach"><a class="header" href="#parallel-iteration-with-foreach">Parallel Iteration with Foreach</a></h2>
<p>Process multiple items in parallel using the <code>foreach</code> command.</p>
<h3 id="basic-foreach"><a class="header" href="#basic-foreach">Basic Foreach</a></h3>
<p>Iterate over a list of items:</p>
<pre><code class="language-yaml">- foreach:
    foreach: ["a", "b", "c"]
    do:
      - shell: "process ${item}"
</code></pre>
<h3 id="dynamic-item-lists"><a class="header" href="#dynamic-item-lists">Dynamic Item Lists</a></h3>
<p>Generate items from a command:</p>
<pre><code class="language-yaml">- foreach:
    foreach: "find . -name '*.rs'"
    do:
      - shell: "rustfmt ${item}"
</code></pre>
<h3 id="parallel-execution"><a class="header" href="#parallel-execution">Parallel Execution</a></h3>
<p>Control parallelism with the <code>parallel</code> field. It accepts both boolean and numeric values:</p>
<p><strong>Boolean - Automatic Parallelism:</strong></p>
<pre><code class="language-yaml">- foreach:
    foreach: "ls *.txt"
    parallel: true  # Use all available cores
    do:
      - shell: "analyze ${item}"
</code></pre>
<p><strong>Number - Explicit Concurrency Limit:</strong></p>
<pre><code class="language-yaml">- foreach:
    foreach: "ls *.txt"
    parallel: 5  # Process 5 items concurrently
    do:
      - shell: "analyze ${item}"
</code></pre>
<p>Use <code>true</code> for automatic parallelism based on available resources, or specify a number to limit concurrent execution.</p>
<h3 id="error-handling-1"><a class="header" href="#error-handling-1">Error Handling</a></h3>
<p>Continue processing remaining items on failure:</p>
<pre><code class="language-yaml">- foreach:
    foreach: ["test1", "test2", "test3"]
    continue_on_error: true
    do:
      - shell: "run-test ${item}"
</code></pre>
<h3 id="limiting-items"><a class="header" href="#limiting-items">Limiting Items</a></h3>
<p>Process only a subset of items:</p>
<pre><code class="language-yaml">- foreach:
    foreach: "find . -name '*.log'"
    max_items: 10  # Process first 10 items only
    do:
      - shell: "compress ${item}"
</code></pre>
<hr />
<h2 id="goal-seeking-operations"><a class="header" href="#goal-seeking-operations">Goal-Seeking Operations</a></h2>
<p>Iteratively refine implementations until they meet validation criteria.</p>
<h3 id="basic-goal-seek"><a class="header" href="#basic-goal-seek">Basic Goal Seek</a></h3>
<p>Define a goal and validation command using either <code>shell</code> or <code>claude</code>:</p>
<pre><code class="language-yaml"># Using shell command
- goal_seek:
    goal: "All tests pass"
    shell: "cargo fix"
    validate: "cargo test"
    threshold: 100
</code></pre>
<pre><code class="language-yaml"># Using Claude command
- goal_seek:
    goal: "Code quality improved"
    claude: "/fix-issues"
    validate: "quality-check.sh"
    threshold: 95
</code></pre>
<p>The goal-seeking operation will:</p>
<ol>
<li>Run the command (shell or claude)</li>
<li>Run the validation</li>
<li>Retry if validation threshold not met</li>
<li>Stop when goal achieved or max attempts reached</li>
</ol>
<h3 id="advanced-goal-seek-configuration"><a class="header" href="#advanced-goal-seek-configuration">Advanced Goal Seek Configuration</a></h3>
<p>Control iteration behavior:</p>
<pre><code class="language-yaml">- goal_seek:
    goal: "Code passes all quality checks"
    shell: "auto-fix.sh"
    validate: "quality-check.sh"
    threshold: 95
    max_attempts: 5
    timeout_seconds: 300
    fail_on_incomplete: true
</code></pre>
<hr />
<h2 id="best-practices-3"><a class="header" href="#best-practices-3">Best Practices</a></h2>
<h3 id="1-use-meaningful-variable-names"><a class="header" href="#1-use-meaningful-variable-names">1. Use Meaningful Variable Names</a></h3>
<pre><code class="language-yaml"># Good
- shell: "cargo test --format json"
  capture_output: "test_results"
  capture_format: "json"

# Avoid
- shell: "cargo test --format json"
  capture_output: "x"
</code></pre>
<h3 id="2-set-appropriate-timeouts"><a class="header" href="#2-set-appropriate-timeouts">2. Set Appropriate Timeouts</a></h3>
<pre><code class="language-yaml"># Set timeouts for potentially long-running operations
- shell: "npm install"
  timeout: 300

- claude: "/analyze-large-codebase"
  timeout: 1800
</code></pre>
<h3 id="3-handle-failures-gracefully"><a class="header" href="#3-handle-failures-gracefully">3. Handle Failures Gracefully</a></h3>
<pre><code class="language-yaml"># Provide automatic remediation
- shell: "cargo test"
  on_failure:
    claude: "/fix-failing-tests"
    max_attempts: 2
    fail_workflow: true
</code></pre>
<h3 id="4-validate-critical-changes"><a class="header" href="#4-validate-critical-changes">4. Validate Critical Changes</a></h3>
<pre><code class="language-yaml"># Ensure implementation meets requirements
- claude: "/implement-feature"
  validate:
    commands:
      - shell: "cargo test"
      - shell: "cargo clippy -- -D warnings"
    threshold: 100
    on_incomplete:
      claude: "/fix-issues"
      max_attempts: 3
</code></pre>
<h3 id="5-use-step-ids-for-complex-workflows"><a class="header" href="#5-use-step-ids-for-complex-workflows">5. Use Step IDs for Complex Workflows</a></h3>
<pre><code class="language-yaml"># Make output references explicit
- shell: "git diff --stat"
  id: "git-changes"
  capture_output: "diff"

- claude: "/review-changes '${git-changes.output}'"
  id: "code-review"
</code></pre>
<hr />
<h2 id="common-patterns-1"><a class="header" href="#common-patterns-1">Common Patterns</a></h2>
<h3 id="test-fix-verify-loop"><a class="header" href="#test-fix-verify-loop">Test-Fix-Verify Loop</a></h3>
<pre><code class="language-yaml">- shell: "cargo test"
  on_failure:
    claude: "/fix-tests"
    on_success:
      shell: "cargo test --release"
</code></pre>
<h3 id="parallel-processing-with-aggregation"><a class="header" href="#parallel-processing-with-aggregation">Parallel Processing with Aggregation</a></h3>
<pre><code class="language-yaml">- foreach:
    foreach: "find src -name '*.rs'"
    parallel: 10
    do:
      - shell: "analyze-file ${item}"
        capture_output: "analysis_${item}"

- shell: "aggregate-results.sh"
</code></pre>
<h3 id="gradual-quality-improvement"><a class="header" href="#gradual-quality-improvement">Gradual Quality Improvement</a></h3>
<pre><code class="language-yaml">- goal_seek:
    goal: "Code quality score above 90"
    shell: "auto-improve.sh"
    validate: "quality-check.sh"
    threshold: 90
    max_attempts: 5
  on_success:
    shell: "git commit -m 'Improved code quality'"
</code></pre>
<h3 id="conditional-deployment"><a class="header" href="#conditional-deployment">Conditional Deployment</a></h3>
<pre><code class="language-yaml">- shell: "cargo test"
  capture_output: "test_results"
  capture_format: "json"

- shell: "deploy.sh"
  when: "${test_results.passed == test_results.total}"
  on_success:
    shell: "notify-success.sh"
  on_failure:
    shell: "rollback.sh"
</code></pre>
<h3 id="multi-stage-validation"><a class="header" href="#multi-stage-validation">Multi-Stage Validation</a></h3>
<pre><code class="language-yaml">- claude: "/implement-feature"
  validate:
    commands:
      - shell: "cargo build"
      - shell: "cargo test"
      - shell: "cargo clippy"
      - shell: "cargo fmt --check"
    threshold: 100
    on_incomplete:
      commands:
        - claude: "/fix-build-errors"
        - shell: "cargo fmt"
      max_attempts: 3
      fail_workflow: true
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="advanced-git-context"><a class="header" href="#advanced-git-context">Advanced Git Context</a></h1>
<p>Advanced git context features enable powerful filtering and formatting of git information in your workflows. This chapter covers automatic git tracking, variable modifiers, format options, and pattern filtering.</p>
<h2 id="overview-2"><a class="header" href="#overview-2">Overview</a></h2>
<p>Prodigy automatically tracks git changes throughout workflow execution and exposes them through variables. No configuration is needed—git context variables are available out-of-the-box in any git repository. You can access file changes, commits, and modification statistics at both the step and workflow level.</p>
<h2 id="how-git-tracking-works"><a class="header" href="#how-git-tracking-works">How Git Tracking Works</a></h2>
<h3 id="automatic-tracking"><a class="header" href="#automatic-tracking">Automatic Tracking</a></h3>
<p>Git context is automatically tracked when you run workflows in a git repository:</p>
<ul>
<li><strong>GitChangeTracker</strong> is initialized at workflow start</li>
<li>Each step’s changes are tracked between <code>begin_step</code> and <code>complete_step</code></li>
<li>Variables are automatically available for interpolation in all commands</li>
<li>No YAML configuration needed—tracking happens transparently</li>
</ul>
<h3 id="when-tracking-is-active"><a class="header" href="#when-tracking-is-active">When Tracking is Active</a></h3>
<p>Git tracking is active in:</p>
<ul>
<li>Regular workflows running in git repositories</li>
<li>MapReduce setup, map, and reduce phases</li>
<li>Child worktrees created for map agents</li>
</ul>
<p>Git tracking is <strong>not</strong> active in:</p>
<ul>
<li>Non-git repositories</li>
<li>Workflows without git integration</li>
</ul>
<h2 id="git-context-variables-1"><a class="header" href="#git-context-variables-1">Git Context Variables</a></h2>
<h3 id="step-level-variables"><a class="header" href="#step-level-variables">Step-Level Variables</a></h3>
<p>Track changes made during the current step:</p>
<pre><code class="language-yaml"># Access files changed in this step
- shell: "echo Changed: ${step.files_changed}"
- shell: "echo Added: ${step.files_added}"
- shell: "echo Modified: ${step.files_modified}"
- shell: "echo Deleted: ${step.files_deleted}"

# Access commit information
- shell: "echo Commits: ${step.commits}"
- shell: "echo Commit count: ${step.commit_count}"

# Access modification statistics
- shell: "echo Insertions: ${step.insertions}"
- shell: "echo Deletions: ${step.deletions}"
</code></pre>
<h3 id="workflow-level-variables"><a class="header" href="#workflow-level-variables">Workflow-Level Variables</a></h3>
<p>Track cumulative changes across all steps:</p>
<pre><code class="language-yaml"># Access all files changed in workflow
- shell: "echo Changed: ${workflow.files_changed}"
- shell: "echo Added: ${workflow.files_added}"
- shell: "echo Modified: ${workflow.files_modified}"
- shell: "echo Deleted: ${workflow.files_deleted}"

# Access all commits
- shell: "echo Commits: ${workflow.commits}"
- shell: "echo Commit count: ${workflow.commit_count}"

# Access total modifications
- shell: "echo Insertions: ${workflow.insertions}"
- shell: "echo Deletions: ${workflow.deletions}"
</code></pre>
<h3 id="variable-reference"><a class="header" href="#variable-reference">Variable Reference</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Variable</th><th>Scope</th><th>Description</th></tr></thead><tbody>
<tr><td><code>step.files_added</code></td><td>Step</td><td>Files added in current step</td></tr>
<tr><td><code>step.files_modified</code></td><td>Step</td><td>Files modified in current step</td></tr>
<tr><td><code>step.files_deleted</code></td><td>Step</td><td>Files deleted in current step</td></tr>
<tr><td><code>step.files_changed</code></td><td>Step</td><td>All files changed (added + modified + deleted)</td></tr>
<tr><td><code>step.commits</code></td><td>Step</td><td>Commit SHAs from current step</td></tr>
<tr><td><code>step.commit_count</code></td><td>Step</td><td>Number of commits in current step</td></tr>
<tr><td><code>step.insertions</code></td><td>Step</td><td>Lines added in current step</td></tr>
<tr><td><code>step.deletions</code></td><td>Step</td><td>Lines deleted in current step</td></tr>
<tr><td><code>workflow.files_added</code></td><td>Workflow</td><td>All files added in workflow</td></tr>
<tr><td><code>workflow.files_modified</code></td><td>Workflow</td><td>All files modified in workflow</td></tr>
<tr><td><code>workflow.files_deleted</code></td><td>Workflow</td><td>All files deleted in workflow</td></tr>
<tr><td><code>workflow.files_changed</code></td><td>Workflow</td><td>All files changed in workflow</td></tr>
<tr><td><code>workflow.commits</code></td><td>Workflow</td><td>All commit SHAs in workflow</td></tr>
<tr><td><code>workflow.commit_count</code></td><td>Workflow</td><td>Total commits in workflow</td></tr>
<tr><td><code>workflow.insertions</code></td><td>Workflow</td><td>Total lines added in workflow</td></tr>
<tr><td><code>workflow.deletions</code></td><td>Workflow</td><td>Total lines deleted in workflow</td></tr>
</tbody></table>
</div>
<h2 id="pattern-filtering"><a class="header" href="#pattern-filtering">Pattern Filtering</a></h2>
<p>Filter git context variables using glob patterns with the <code>:pattern</code> modifier syntax:</p>
<h3 id="basic-pattern-filtering"><a class="header" href="#basic-pattern-filtering">Basic Pattern Filtering</a></h3>
<pre><code class="language-yaml"># Only Rust files added in this step
- shell: "echo ${step.files_added:*.rs}"

# Only source files changed in workflow
- shell: "echo ${workflow.files_changed:src/**/*.rs}"

# Multiple file types
- shell: "echo ${step.files_modified:**/*.{rs,toml}}"

# Module files only
- shell: "echo ${workflow.files_added:**/mod.rs}"
</code></pre>
<h3 id="pattern-syntax"><a class="header" href="#pattern-syntax">Pattern Syntax</a></h3>
<p>Use glob patterns to match files precisely:</p>
<ul>
<li><code>*</code> - Match any characters except <code>/</code></li>
<li><code>**</code> - Match any characters including <code>/</code></li>
<li><code>?</code> - Match single character</li>
<li><code>{a,b}</code> - Match either <code>a</code> or <code>b</code></li>
<li><code>[abc]</code> - Match character class</li>
</ul>
<p><strong>Note</strong>: Prodigy uses glob patterns only. Regular expressions (regex) are not supported for pattern filtering. Use glob syntax for all file matching operations.</p>
<p><strong>Examples:</strong></p>
<pre><code class="language-yaml"># Match Rust and TOML files
- shell: "echo ${step.files_changed:**/*.{rs,toml}}"

# Match module files in src/
- shell: "echo ${workflow.files_added:src/**/mod.rs}"

# Match integration tests
- shell: "echo ${step.files_modified:tests/integration/**}"

# Match any test files
- shell: "echo ${workflow.files_changed:**/*_test.rs}"
</code></pre>
<h3 id="combining-filters"><a class="header" href="#combining-filters">Combining Filters</a></h3>
<p>For complex filtering, use multiple variable references or shell commands:</p>
<pre><code class="language-yaml"># Pass different file sets to different commands
- shell: "cargo fmt $(echo ${step.files_changed:*.rs})"
- shell: "markdownlint $(echo ${step.files_changed:*.md})"

# Combine with shell filtering
- shell: |
    files="${workflow.files_changed:src/**/*.rs}"
    echo "$files" | grep -v test | xargs cargo clippy
</code></pre>
<h2 id="format-modifiers"><a class="header" href="#format-modifiers">Format Modifiers</a></h2>
<p>Customize how git context variables are formatted:</p>
<h3 id="default-format-space-separated"><a class="header" href="#default-format-space-separated">Default Format (Space-Separated)</a></h3>
<p>By default, variables are space-separated:</p>
<pre><code class="language-yaml">- shell: "echo ${step.files_changed}"
# Output: src/main.rs src/lib.rs tests/test.rs
</code></pre>
<h3 id="json-format"><a class="header" href="#json-format">JSON Format</a></h3>
<p>Use <code>:json</code> for JSON array output:</p>
<pre><code class="language-yaml">- shell: "echo ${step.files_added:json}"
# Output: ["src/main.rs","src/lib.rs","tests/test.rs"]

# Parse with jq
- shell: "echo ${workflow.commits:json} | jq -r '.[]'"
</code></pre>
<h3 id="newline-separated-format"><a class="header" href="#newline-separated-format">Newline-Separated Format</a></h3>
<p>Use <code>:lines</code> or <code>:newline</code> for one item per line:</p>
<pre><code class="language-yaml">- shell: "echo ${step.files_changed:lines}"
# Output:
# src/main.rs
# src/lib.rs
# tests/test.rs

# Useful with xargs
- shell: "echo ${workflow.files_modified:lines} | xargs -I {} cp {} backup/"
</code></pre>
<h3 id="comma-separated-format"><a class="header" href="#comma-separated-format">Comma-Separated Format</a></h3>
<p>Use <code>:csv</code> or <code>:comma</code> for comma-separated output:</p>
<pre><code class="language-yaml">- shell: "echo ${step.files_added:csv}"
# Output: src/main.rs,src/lib.rs,tests/test.rs
</code></pre>
<h3 id="combining-format-and-pattern"><a class="header" href="#combining-format-and-pattern">Combining Format and Pattern</a></h3>
<p>Apply both pattern filtering and format modifiers using the <code>:pattern:format</code> syntax:</p>
<pre><code class="language-yaml"># JSON array of Rust files
- shell: "echo ${step.files_changed:*.rs:json}"

# Newline-separated source files
- shell: "echo ${workflow.files_added:src/**:lines}"

# CSV of modified test files
- shell: "echo ${step.files_modified:**/*_test.rs:csv}"
</code></pre>
<p><strong>Note</strong>: The syntax supports exactly one pattern and one format modifier per variable reference. You cannot chain multiple patterns or formats. For more complex filtering, use shell commands to post-process the output.</p>
<h2 id="use-cases"><a class="header" href="#use-cases">Use Cases</a></h2>
<h3 id="code-review-workflows"><a class="header" href="#code-review-workflows">Code Review Workflows</a></h3>
<p>Review only source code changes:</p>
<pre><code class="language-yaml">- claude: "/review-changes"
  args:
    files: "${step.files_changed:src/**/*.rs}"

- shell: |
    echo "Reviewing ${step.commit_count} commits"
    echo "Changed files: ${step.files_changed:src/**/*.rs:lines}"
</code></pre>
<h3 id="documentation-updates"><a class="header" href="#documentation-updates">Documentation Updates</a></h3>
<p>Work with documentation changes:</p>
<pre><code class="language-yaml">- claude: "/update-docs"
  args:
    docs: "${workflow.files_changed:**/*.md}"

- shell: "markdownlint ${step.files_modified:*.md}"

# Check if docs were updated
- when: "${workflow.files_changed:**/*.md}"
  shell: "echo Documentation was updated"
</code></pre>
<h3 id="test-verification"><a class="header" href="#test-verification">Test Verification</a></h3>
<p>Focus on test-related changes:</p>
<pre><code class="language-yaml"># Run tests for changed test files
- shell: "cargo test $(echo ${step.files_changed:**/*_test.rs})"

# Verify test coverage
- when: "${step.files_added:tests/**}"
  claude: "/verify-test-coverage"
</code></pre>
<h3 id="conditional-execution-2"><a class="header" href="#conditional-execution-2">Conditional Execution</a></h3>
<p>Use git context in conditional logic:</p>
<pre><code class="language-yaml"># Only run if Rust files changed
- when: "${step.files_changed:*.rs}"
  shell: "cargo clippy"

# Run different linters based on changes
- when: "${workflow.files_changed:*.rs}"
  shell: "cargo fmt --check"

- when: "${workflow.files_changed:*.md}"
  shell: "markdownlint **/*.md"

# Check commit count
- when: "${step.commit_count} &gt; 1"
  shell: "echo Multiple commits detected"
</code></pre>
<h3 id="mapreduce-workflows-1"><a class="header" href="#mapreduce-workflows-1">MapReduce Workflows</a></h3>
<p>Git context works across MapReduce phases:</p>
<pre><code class="language-yaml">name: review-changes
mode: mapreduce

setup:
  # Workflow-level tracking starts here
  - shell: "git diff main --name-only &gt; changed-files.txt"
  - shell: "echo Setup modified: ${step.files_changed}"

map:
  input: "changed-files.txt"
  agent_template:
    # Each agent has its own step tracking
    - claude: "/review ${item}"
    - shell: "echo Agent changed: ${step.files_changed}"

reduce:
  # Access workflow-level changes from all agents
  - shell: "echo Total changes: ${workflow.files_changed}"
  - shell: "echo Total commits: ${workflow.commit_count}"
</code></pre>
<h2 id="best-practices-4"><a class="header" href="#best-practices-4">Best Practices</a></h2>
<ul>
<li><strong>Use Pattern Filtering</strong>: Filter variables to only relevant files using glob patterns</li>
<li><strong>Choose Appropriate Format</strong>: Use <code>:json</code> for parsing, <code>:lines</code> for iteration, default for simple commands</li>
<li><strong>Scope Appropriately</strong>: Use <code>step.*</code> for current changes, <code>workflow.*</code> for cumulative tracking</li>
<li><strong>Combine with Conditionals</strong>: Use <code>when:</code> to execute steps only when relevant files change</li>
<li><strong>Test Your Patterns</strong>: Verify patterns match intended files with <code>echo ${var:pattern}</code></li>
<li><strong>Document Intent</strong>: Add comments explaining why specific patterns are used</li>
</ul>
<h2 id="performance-considerations"><a class="header" href="#performance-considerations">Performance Considerations</a></h2>
<ul>
<li>Git operations are performed once per step and cached</li>
<li>Pattern filtering is efficient using compiled glob patterns</li>
<li>Variables are computed on-demand during interpolation</li>
<li>Workflow-level tracking maintains cumulative state without re-scanning</li>
</ul>
<h2 id="troubleshooting-2"><a class="header" href="#troubleshooting-2">Troubleshooting</a></h2>
<h3 id="pattern-not-matching-files"><a class="header" href="#pattern-not-matching-files">Pattern Not Matching Files</a></h3>
<p><strong>Issue</strong>: Your pattern doesn’t match any files</p>
<pre><code class="language-yaml"># Debug: Echo the unfiltered variable first
- shell: "echo All files: ${step.files_changed}"
- shell: "echo Filtered: ${step.files_changed:*.rs}"
</code></pre>
<p><strong>What happens</strong>: When a pattern matches no files, the variable resolves to an empty string. This is not an error—it’s expected behavior when no files match the pattern.</p>
<pre><code class="language-yaml"># Example: No Python files in a Rust project
- shell: "echo Python files: ${step.files_changed:*.py}"
# Output: Python files:
# (empty - no error)

# Use conditionals to handle empty results
- when: "${step.files_changed:*.rs}"
  shell: "cargo fmt"  # Only runs if Rust files changed
</code></pre>
<p><strong>Solution</strong>: Use <code>git ls-files</code> to verify file paths match your pattern, or use conditionals to handle empty results gracefully</p>
<h3 id="empty-variables"><a class="header" href="#empty-variables">Empty Variables</a></h3>
<p><strong>Issue</strong>: Git context variables are empty</p>
<p><strong>Possible causes:</strong></p>
<ul>
<li>Not running in a git repository</li>
<li>No commits have been made in the current step</li>
<li>Pattern filter is too restrictive</li>
</ul>
<p><strong>Solution</strong>: Check if git tracking is active and verify patterns</p>
<h3 id="format-modifier-not-working"><a class="header" href="#format-modifier-not-working">Format Modifier Not Working</a></h3>
<p><strong>Issue</strong>: Format modifier produces unexpected output</p>
<pre><code class="language-yaml"># Verify correct syntax: variable:pattern:format
- shell: "echo ${step.files_added:*.rs:json}"
</code></pre>
<p><strong>Solution</strong>: Ensure proper syntax with colon separators and valid format name (json, lines, csv)</p>
<h3 id="variables-not-interpolating"><a class="header" href="#variables-not-interpolating">Variables Not Interpolating</a></h3>
<p><strong>Issue</strong>: Variables appear as literal strings like <code>${step.files_changed}</code></p>
<p><strong>Solution</strong>: Ensure you’re using proper YAML syntax and the command supports interpolation</p>
<h2 id="see-also-1"><a class="header" href="#see-also-1">See Also</a></h2>
<ul>
<li><a href="variables.html">Variables and Interpolation</a> - Basic variable usage and interpolation syntax</li>
<li><a href="workflow-basics.html">Workflow Basics</a> - Git integration fundamentals and workflow structure</li>
<li><a href="mapreduce.html">MapReduce Workflows</a> - Using git context in parallel jobs</li>
<li><a href="conditionals.html">Conditional Execution</a> - Using git context with <code>when:</code> conditions</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="workflow-composition"><a class="header" href="#workflow-composition">Workflow Composition</a></h1>
<p>Prodigy provides powerful composition features that enable building complex workflows from reusable components. This chapter covers importing workflows, using templates, defining parameters, and composing workflows through inheritance.</p>
<h2 id="overview-3"><a class="header" href="#overview-3">Overview</a></h2>
<p>Workflow composition allows you to:</p>
<ul>
<li><strong>Import</strong> shared workflow configurations from other files</li>
<li><strong>Extend</strong> base workflows to inherit common configurations</li>
<li><strong>Use templates</strong> from a registry for standardized patterns</li>
<li><strong>Define parameters</strong> with type validation for flexible workflows</li>
<li><strong>Execute sub-workflows</strong> in parallel or sequentially</li>
<li><strong>Set defaults</strong> for common parameter values</li>
</ul>
<p>These features promote code reuse, maintainability, and consistency across your automation workflows.</p>
<h2 id="workflow-imports"><a class="header" href="#workflow-imports">Workflow Imports</a></h2>
<p>Import external workflow files to reuse configurations and share common patterns across multiple workflows.</p>
<h3 id="basic-import"><a class="header" href="#basic-import">Basic Import</a></h3>
<p>Import an entire workflow file:</p>
<pre><code class="language-yaml">name: my-workflow
imports:
  - path: ./common/utilities.yml
</code></pre>
<p>This imports all content from <code>utilities.yml</code> into your workflow.</p>
<h3 id="import-with-alias"><a class="header" href="#import-with-alias">Import with Alias</a></h3>
<p>Use an alias to reference imported workflows:</p>
<pre><code class="language-yaml">imports:
  - path: ./common/utilities.yml
    alias: utils
</code></pre>
<p>The alias provides a namespace for the imported content, preventing naming conflicts.</p>
<h3 id="selective-import"><a class="header" href="#selective-import">Selective Import</a></h3>
<p>Import only specific items from a workflow file:</p>
<pre><code class="language-yaml">imports:
  - path: ./common/validators.yml
    selective:
      - validate_output
      - validate_schema
</code></pre>
<p>Selective import configuration is validated and stored correctly (see <code>mod.rs:63-64</code>). The filtering logic for applying selective imports is implemented but currently logs items without full integration into the merge process (see <code>composer.rs:321-334</code>).</p>
<h3 id="multiple-imports"><a class="header" href="#multiple-imports">Multiple Imports</a></h3>
<p>You can import from multiple files:</p>
<pre><code class="language-yaml">imports:
  - path: ./common/setup.yml
    alias: setup
  - path: ./common/cleanup.yml
    alias: cleanup
  - path: ./shared/validators.yml
    selective:
      - validate_config
</code></pre>
<p>Imports are processed in order, and later imports can override earlier ones.</p>
<h2 id="workflow-extension-inheritance"><a class="header" href="#workflow-extension-inheritance">Workflow Extension (Inheritance)</a></h2>
<p>Extend a base workflow to inherit its configuration. Child workflows override parent values, allowing you to customize specific aspects while maintaining common configuration.</p>
<h3 id="basic-extension"><a class="header" href="#basic-extension">Basic Extension</a></h3>
<pre><code class="language-yaml">name: production-workflow
extends: base-workflow
</code></pre>
<p>This workflow inherits all configuration from <code>base-workflow</code>.</p>
<h3 id="base-resolution-paths"><a class="header" href="#base-resolution-paths">Base Resolution Paths</a></h3>
<p>Prodigy searches for base workflows in the following locations (in order):</p>
<ol>
<li><code>./bases/&lt;name&gt;.yml</code></li>
<li><code>./templates/&lt;name&gt;.yml</code></li>
<li><code>./workflows/&lt;name&gt;.yml</code></li>
<li><code>./&lt;name&gt;.yml</code></li>
</ol>
<p>For example, <code>extends: base-workflow</code> will look for:</p>
<ul>
<li><code>./bases/base-workflow.yml</code></li>
<li><code>./templates/base-workflow.yml</code></li>
<li><code>./workflows/base-workflow.yml</code></li>
<li><code>./base-workflow.yml</code></li>
</ul>
<h3 id="override-behavior"><a class="header" href="#override-behavior">Override Behavior</a></h3>
<p>Child workflows override parent configuration:</p>
<pre><code class="language-yaml"># bases/base-workflow.yml
name: base
commands:
  - shell: "echo 'Setup'"
  - shell: "echo 'Base task'"
defaults:
  timeout: 300
  verbose: false

# production.yml
name: production
extends: base-workflow
commands:
  - shell: "echo 'Production task'"  # Overrides base commands
defaults:
  verbose: true  # Overrides base default
</code></pre>
<p>Child workflows override parent configuration at the field level. The child’s <code>commands</code> array completely replaces the parent’s commands (not merged). Similarly, if the child defines <code>defaults</code>, the entire defaults HashMap is replaced - individual defaults are not merged. In this example, the <code>production</code> workflow completely replaces the <code>commands</code> array. If <code>production</code> provides defaults with only <code>verbose: true</code>, it would NOT inherit the <code>timeout: 300</code> from base - it would need to re-specify both defaults to keep them.</p>
<h2 id="template-system"><a class="header" href="#template-system">Template System</a></h2>
<p>Templates provide reusable workflow patterns that can be instantiated with different parameters. Templates can be stored in a registry or loaded from files.</p>
<h3 id="template-sources"><a class="header" href="#template-sources">Template Sources</a></h3>
<p>Templates can come from three sources:</p>
<h4 id="1-registry-templates"><a class="header" href="#1-registry-templates">1. Registry Templates</a></h4>
<p>Store templates in a central registry (fully implemented with <code>FileTemplateStorage</code>):</p>
<pre><code class="language-yaml">name: my-workflow
template:
  name: refactor-base
  source: refactor-base  # Registry name
  with:
    style: modular
    target: src/
</code></pre>
<p>Templates are loaded from the registry using <code>TemplateRegistry</code> with <code>FileTemplateStorage</code> backend. Templates are cached in memory after first load for performance.</p>
<h4 id="2-file-templates"><a class="header" href="#2-file-templates">2. File Templates</a></h4>
<p>Load templates from local files (fully implemented via <code>TemplateSource::File</code> enum):</p>
<pre><code class="language-yaml">template:
  name: ci-pipeline
  source: ./templates/ci-pipeline.yml
  with:
    environment: staging
</code></pre>
<h4 id="3-url-templates-planned"><a class="header" href="#3-url-templates-planned">3. URL Templates (Planned)</a></h4>
<p>Future support for remote templates:</p>
<pre><code class="language-yaml">template:
  name: shared-workflow
  source: https://example.com/templates/workflow.yml
</code></pre>
<p><em>Note: URL template sources are not yet implemented. The system returns an explicit error message if a URL source is attempted.</em></p>
<h3 id="template-parameters"><a class="header" href="#template-parameters">Template Parameters</a></h3>
<p>Pass parameters to templates using the <code>with</code> field:</p>
<pre><code class="language-yaml">template:
  name: refactor-base
  source: refactor-base
  with:
    style: functional
    max_complexity: 5
    target_dir: src/
</code></pre>
<p>Parameter validation is fully implemented with type checking for all six parameter types (string, number, boolean, array, object, any) via <code>validate_parameter_value</code> (<code>mod.rs:252-279</code>). Parameters are stored and applied to workflow metadata. Template-specific parameter substitution throughout commands is logged but not fully integrated (see <code>apply_template_params</code> in <code>composer.rs:336-347</code>).</p>
<h3 id="template-overrides"><a class="header" href="#template-overrides">Template Overrides</a></h3>
<p>Override specific template values:</p>
<pre><code class="language-yaml">template:
  name: test-workflow
  source: test-base
  with:
    parallel: true
  override:
    timeout: 600
    retry_count: 5
</code></pre>
<p>The <code>override</code> field allows you to modify template values after parameter substitution.</p>
<h3 id="template-registry-management"><a class="header" href="#template-registry-management">Template Registry Management</a></h3>
<h4 id="registering-templates"><a class="header" href="#registering-templates">Registering Templates</a></h4>
<p>Register a template for reuse:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use prodigy::cook::workflow::{ComposableWorkflow, TemplateRegistry};

let registry = TemplateRegistry::new();
let template = ComposableWorkflow::from_config(/* ... */);

registry.register_template("my-template".to_string(), template).await?;
<span class="boring">}</span></code></pre></pre>
<h4 id="listing-templates"><a class="header" href="#listing-templates">Listing Templates</a></h4>
<p>View all available templates:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let templates = registry.list().await?;
for template in templates {
    println!("{}: {} (v{})", template.name,
             template.description.unwrap_or_default(),
             template.version);
}
<span class="boring">}</span></code></pre></pre>
<h4 id="searching-by-tags"><a class="header" href="#searching-by-tags">Searching by Tags</a></h4>
<p>Find templates by tags:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let refactor_templates = registry.search_by_tags(&amp;["refactor".to_string()]).await?;
<span class="boring">}</span></code></pre></pre>
<h4 id="deleting-templates"><a class="header" href="#deleting-templates">Deleting Templates</a></h4>
<p>Remove a template from the registry:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>registry.delete("old-template").await?;
<span class="boring">}</span></code></pre></pre>
<h3 id="template-metadata"><a class="header" href="#template-metadata">Template Metadata</a></h3>
<p>Templates can include metadata for better organization:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use prodigy::cook::workflow::composition::registry::TemplateMetadata;

let metadata = TemplateMetadata {
    description: Some("Refactoring workflow for Rust code".to_string()),
    author: Some("DevOps Team".to_string()),
    version: "2.0.0".to_string(),
    tags: vec!["refactor".to_string(), "rust".to_string()],
    created_at: chrono::Utc::now(),
    updated_at: chrono::Utc::now(),
};

registry.register_template_with_metadata(
    "refactor-rust".to_string(),
    template,
    metadata
).await?;
<span class="boring">}</span></code></pre></pre>
<h2 id="parameter-definitions"><a class="header" href="#parameter-definitions">Parameter Definitions</a></h2>
<p>Define parameters with type validation to create flexible, reusable workflows.</p>
<h3 id="parameter-types"><a class="header" href="#parameter-types">Parameter Types</a></h3>
<p>Prodigy supports the following parameter types:</p>
<ul>
<li><code>string</code> - Text values</li>
<li><code>number</code> - Numeric values</li>
<li><code>boolean</code> - True/false values</li>
<li><code>array</code> - List of values</li>
<li><code>object</code> - Structured data</li>
<li><code>any</code> - Any JSON value</li>
</ul>
<h3 id="required-parameters"><a class="header" href="#required-parameters">Required Parameters</a></h3>
<p>Define parameters that must be provided:</p>
<pre><code class="language-yaml">parameters:
  required:
    - name: target_file
      type: string
      description: File to process
    - name: iteration_count
      type: number
      description: Number of iterations to run
    - name: enabled
      type: boolean
      description: Enable feature flag
</code></pre>
<h3 id="optional-parameters"><a class="header" href="#optional-parameters">Optional Parameters</a></h3>
<p>Define optional parameters with default values:</p>
<pre><code class="language-yaml">parameters:
  optional:
    - name: style
      type: string
      description: Processing style
      default: "functional"
    - name: timeout
      type: number
      description: Timeout in seconds
      default: 300
    - name: verbose
      type: boolean
      description: Enable verbose output
      default: false
</code></pre>
<h3 id="parameter-validation"><a class="header" href="#parameter-validation">Parameter Validation</a></h3>
<p>Prodigy supports two types of parameter validation:</p>
<h4 id="type-validation-fully-implemented"><a class="header" href="#type-validation-fully-implemented">Type Validation (Fully Implemented)</a></h4>
<p>All parameter types are validated automatically:</p>
<pre><code class="language-yaml">parameters:
  required:
    - name: count
      type: number
      description: Item count
    - name: enabled
      type: boolean
      description: Feature flag
    - name: files
      type: array
      description: File list
</code></pre>
<p>Type validation is fully implemented for all six parameter types (string, number, boolean, array, object, any) in the <code>validate_parameter_value</code> method.</p>
<h4 id="custom-validation-expressions-planned"><a class="header" href="#custom-validation-expressions-planned">Custom Validation Expressions (Planned)</a></h4>
<p>Custom validation expressions can be defined but evaluation is not yet implemented:</p>
<pre><code class="language-yaml">parameters:
  required:
    - name: email
      type: string
      description: Email address
      validation: "^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$"
    - name: count
      type: number
      description: Item count
      validation: "value &gt;= 1 &amp;&amp; value &lt;= 100"
</code></pre>
<p><em>Note: Custom validation expressions are stored and logged when present (see <code>validate_parameter_value</code>, <code>mod.rs:269-276</code>), but the actual expression evaluation engine is not yet implemented. This means validation fields are preserved in the configuration but not enforced during parameter validation. Type validation IS fully enforced for all parameter types.</em></p>
<h3 id="array-and-object-parameters"><a class="header" href="#array-and-object-parameters">Array and Object Parameters</a></h3>
<p>Define complex parameter types:</p>
<pre><code class="language-yaml">parameters:
  required:
    - name: files
      type: array
      description: List of files to process
    - name: config
      type: object
      description: Configuration object
  optional:
    - name: tags
      type: array
      description: Tags to apply
      default: []
</code></pre>
<h3 id="type-validation"><a class="header" href="#type-validation">Type Validation</a></h3>
<p>Prodigy automatically validates parameter types:</p>
<pre><code class="language-yaml"># Valid
parameters:
  target_file: "src/main.rs"
  iteration_count: 5
  enabled: true

# Invalid - type mismatch
parameters:
  target_file: "src/main.rs"
  iteration_count: "five"  # Error: expected number, got string
  enabled: true
</code></pre>
<h3 id="parameter-substitution"><a class="header" href="#parameter-substitution">Parameter Substitution</a></h3>
<p>Use parameters in commands with <code>${parameter_name}</code> syntax:</p>
<pre><code class="language-yaml">parameters:
  required:
    - name: target_dir
      type: string
      description: Target directory
commands:
  - shell: "cd ${target_dir} &amp;&amp; cargo build"
  - claude: "/analyze ${target_dir}"
</code></pre>
<h2 id="default-values"><a class="header" href="#default-values">Default Values</a></h2>
<p>Set default parameter values at the workflow level:</p>
<pre><code class="language-yaml">defaults:
  timeout: 300
  retry_count: 3
  verbose: false
  environment: development
</code></pre>
<p>Default values are validated, stored, and integrated into the composition flow (<code>composer.rs:85-87</code>). The function <code>apply_defaults</code> is called during composition but the actual application logic to merge defaults with parameters has a TODO (<code>composer.rs:210-221</code>). The infrastructure is in place but the merge logic needs implementation.</p>
<p>When implemented, defaults will be applied before parameter validation and can be overridden by:</p>
<ol>
<li>Values in the <code>parameters</code> section</li>
<li>Values passed at workflow invocation time</li>
<li>Template <code>override</code> fields</li>
</ol>
<p>Defaults interact with parameters as follows:</p>
<ul>
<li>If a required parameter has a default, it’s not strictly required</li>
<li>Optional parameter defaults take precedence over workflow defaults</li>
<li>Workflow defaults provide fallback values for any parameter</li>
</ul>
<h2 id="sub-workflows"><a class="header" href="#sub-workflows">Sub-Workflows</a></h2>
<p>Execute child workflows as part of a parent workflow. Sub-workflows can run in parallel and have their own parameters and outputs.</p>
<p><em>Implementation Status: Sub-workflow configuration, validation (<code>validate_sub_workflows</code> in <code>composer.rs:381-395</code>), and composition are fully implemented. Sub-workflow definitions work correctly and are validated. The <code>SubWorkflowExecutor</code> structure exists (<code>sub_workflow.rs:181-227</code>) but execution integration with the main workflow executor runtime is in progress.</em></p>
<h3 id="basic-sub-workflow"><a class="header" href="#basic-sub-workflow">Basic Sub-Workflow</a></h3>
<pre><code class="language-yaml">workflows:
  process_files:
    source: ./workflows/process.yml
    parameters:
      parallel: true
</code></pre>
<h3 id="sub-workflow-parameters"><a class="header" href="#sub-workflow-parameters">Sub-Workflow Parameters</a></h3>
<p>Pass parameters to sub-workflows:</p>
<pre><code class="language-yaml">workflows:
  validate_code:
    source: ./workflows/validate.yml
    parameters:
      strict_mode: true
      max_warnings: 5
</code></pre>
<h3 id="inputoutput-mapping"><a class="header" href="#inputoutput-mapping">Input/Output Mapping</a></h3>
<p>Map inputs and outputs between parent and sub-workflows:</p>
<pre><code class="language-yaml">workflows:
  transform_data:
    source: ./workflows/transform.yml
    inputs:
      source_files: "file_list"  # Map parent variable to sub-workflow input
      config: "transform_config"
    outputs:
      - processed_count  # Expose sub-workflow output to parent
      - error_log
</code></pre>
<h3 id="parallel-execution-1"><a class="header" href="#parallel-execution-1">Parallel Execution</a></h3>
<p>Run multiple sub-workflows in parallel:</p>
<pre><code class="language-yaml">workflows:
  run_tests:
    source: ./workflows/test.yml
    parallel: true
    parameters:
      suite: unit

  run_linting:
    source: ./workflows/lint.yml
    parallel: true
</code></pre>
<p>Both workflows execute concurrently.</p>
<h3 id="error-handling-2"><a class="header" href="#error-handling-2">Error Handling</a></h3>
<p>Control sub-workflow error behavior:</p>
<pre><code class="language-yaml">workflows:
  optional_task:
    source: ./workflows/optional.yml
    continue_on_error: true  # Parent continues if this fails

  critical_task:
    source: ./workflows/critical.yml
    continue_on_error: false  # Parent fails if this fails (default)
</code></pre>
<h3 id="timeouts-and-working-directory"><a class="header" href="#timeouts-and-working-directory">Timeouts and Working Directory</a></h3>
<p>Configure execution constraints:</p>
<pre><code class="language-yaml">workflows:
  long_running:
    source: ./workflows/process.yml
    timeout: 600  # 10 minutes in seconds
    working_dir: ./build/  # Execute from this directory
</code></pre>
<h2 id="composition-metadata"><a class="header" href="#composition-metadata">Composition Metadata</a></h2>
<p>Prodigy tracks metadata about workflow composition for debugging and dependency analysis.</p>
<h3 id="tracked-information"><a class="header" href="#tracked-information">Tracked Information</a></h3>
<p>The composition system tracks:</p>
<ul>
<li><strong>Sources</strong>: All workflow files involved in composition</li>
<li><strong>Templates</strong>: Templates used and their sources</li>
<li><strong>Parameters</strong>: Parameters applied during composition</li>
<li><strong>Composed At</strong>: Timestamp of composition</li>
<li><strong>Dependencies</strong>: Full dependency graph with types</li>
</ul>
<h3 id="dependency-types"><a class="header" href="#dependency-types">Dependency Types</a></h3>
<p>The system tracks four types of dependencies:</p>
<ul>
<li><code>Import</code> - Files imported with <code>imports:</code></li>
<li><code>Extends</code> - Base workflows referenced with <code>extends:</code></li>
<li><code>Template</code> - Templates used with <code>template:</code></li>
<li><code>SubWorkflow</code> - Sub-workflows defined in <code>workflows:</code></li>
</ul>
<h3 id="dependency-graph"><a class="header" href="#dependency-graph">Dependency Graph</a></h3>
<p>Prodigy builds a complete dependency graph during composition:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use prodigy::cook::workflow::{WorkflowComposer, ComposedWorkflow};

let composed: ComposedWorkflow = composer.compose(&amp;source, params).await?;

// Access composition metadata
println!("Sources: {:?}", composed.metadata.sources);
println!("Templates: {:?}", composed.metadata.templates);
println!("Dependencies: {:?}", composed.metadata.dependencies);
<span class="boring">}</span></code></pre></pre>
<h3 id="circular-dependency-detection"><a class="header" href="#circular-dependency-detection">Circular Dependency Detection</a></h3>
<p>The composer automatically detects circular dependencies:</p>
<pre><code class="language-yaml"># workflow-a.yml
name: workflow-a
extends: workflow-b

# workflow-b.yml
name: workflow-b
extends: workflow-a  # Error: circular dependency detected
</code></pre>
<p>The composer uses a DFS-based cycle detection algorithm that maintains visited nodes and a recursion stack. If a node is encountered that’s already in the recursion stack, a circular dependency is detected and composition fails with a clear error message (see <code>DependencyResolver::has_cycle</code> in <code>composer.rs:471-494</code>). This prevents infinite loops during composition.</p>
<h2 id="complete-examples"><a class="header" href="#complete-examples">Complete Examples</a></h2>
<h3 id="example-1-basic-import-with-alias"><a class="header" href="#example-1-basic-import-with-alias">Example 1: Basic Import with Alias</a></h3>
<pre><code class="language-yaml">name: integration-test
imports:
  - path: ./common/setup.yml
    alias: setup
  - path: ./common/assertions.yml
    alias: assert

commands:
  - shell: "npm install"
  - shell: "npm test"
</code></pre>
<h3 id="example-2-selective-import"><a class="header" href="#example-2-selective-import">Example 2: Selective Import</a></h3>
<pre><code class="language-yaml">name: validation-workflow
imports:
  - path: ./validators/all.yml
    selective:
      - validate_yaml
      - validate_json
      - validate_toml

commands:
  - shell: "find . -name '*.yml' -exec validate {}"
</code></pre>
<h3 id="example-3-template-from-registry"><a class="header" href="#example-3-template-from-registry">Example 3: Template from Registry</a></h3>
<pre><code class="language-yaml">name: refactor-project
template:
  name: rust-refactor
  source: rust-refactor  # From registry
  with:
    max_complexity: 5
    style: functional
    target: src/

commands:
  - claude: "/analyze-debt"
</code></pre>
<h3 id="example-4-template-with-override"><a class="header" href="#example-4-template-with-override">Example 4: Template with Override</a></h3>
<pre><code class="language-yaml">name: custom-ci
template:
  name: ci-base
  source: ./templates/ci.yml
  with:
    environment: production
  override:
    timeout: 1800
    retry_count: 3
</code></pre>
<h3 id="example-5-workflow-with-parameters"><a class="header" href="#example-5-workflow-with-parameters">Example 5: Workflow with Parameters</a></h3>
<pre><code class="language-yaml">name: deploy-service
parameters:
  required:
    - name: environment
      type: string
      description: Deployment environment
      # validation: "^(dev|staging|prod)$"  # Custom validation not yet implemented
      # Note: Type validation is fully working - the type: string declaration ensures
      # only strings are accepted, rejecting numbers or booleans
    - name: version
      type: string
      description: Version to deploy
  optional:
    - name: rollback_on_error
      type: boolean
      description: Auto-rollback on deployment failure
      default: true

commands:
  - shell: "deploy --env ${environment} --version ${version}"
  - shell: "verify-deployment ${environment}"
</code></pre>
<h3 id="example-6-complex-composition"><a class="header" href="#example-6-complex-composition">Example 6: Complex Composition</a></h3>
<pre><code class="language-yaml">name: full-pipeline
extends: base-pipeline

imports:
  - path: ./common/docker.yml
    alias: docker
  - path: ./common/kubernetes.yml
    alias: k8s

template:
  name: ci-cd-base
  source: ci-cd-base
  with:
    registry: docker.io
    namespace: my-app

parameters:
  required:
    - name: branch
      type: string
      description: Git branch
    - name: tag
      type: string
      description: Docker tag
  optional:
    - name: skip_tests
      type: boolean
      description: Skip test execution
      default: false

defaults:
  timeout: 600
  retry_count: 2

workflows:
  run_tests:
    source: ./workflows/test.yml
    parallel: true
    parameters:
      suite: all
    continue_on_error: false

  build_image:
    source: ./workflows/docker-build.yml
    parameters:
      tag: "${tag}"
    timeout: 900

commands:
  - shell: "echo 'Pipeline completed for ${branch}'"
</code></pre>
<h3 id="example-7-sub-workflow-execution"><a class="header" href="#example-7-sub-workflow-execution">Example 7: Sub-Workflow Execution</a></h3>
<pre><code class="language-yaml">name: multi-stage-pipeline
workflows:
  stage1_build:
    source: ./workflows/build.yml
    parameters:
      target: release
    outputs:
      - artifact_path

  stage2_test:
    source: ./workflows/test.yml
    parallel: true
    inputs:
      artifact: "artifact_path"
    continue_on_error: false

  stage3_deploy:
    source: ./workflows/deploy.yml
    inputs:
      artifact: "artifact_path"
    timeout: 600
    working_dir: ./deployment/

commands:
  - shell: "echo 'All stages completed'"
</code></pre>
<h3 id="example-8-parameterized-template"><a class="header" href="#example-8-parameterized-template">Example 8: Parameterized Template</a></h3>
<pre><code class="language-yaml">name: analyze-codebase
template:
  name: code-analysis
  source: code-analysis
  with:
    language: rust
    strictness: high
    output_format: json
  override:
    max_parallel: 4

parameters:
  required:
    - name: target_dir
      type: string
      description: Directory to analyze
  optional:
    - name: exclude_patterns
      type: array
      description: Patterns to exclude
      default: ["target/", "node_modules/"]

commands:
  - claude: "/analyze ${target_dir} --exclude ${exclude_patterns}"
</code></pre>
<h2 id="best-practices-5"><a class="header" href="#best-practices-5">Best Practices</a></h2>
<h3 id="when-to-use-each-feature"><a class="header" href="#when-to-use-each-feature">When to Use Each Feature</a></h3>
<ul>
<li><strong>Imports</strong>: Share common configurations (environment variables, setup steps)</li>
<li><strong>Extends</strong>: Create workflow hierarchies with base configurations</li>
<li><strong>Templates</strong>: Standardize workflows across projects and teams</li>
<li><strong>Parameters</strong>: Make workflows flexible and reusable</li>
<li><strong>Sub-workflows</strong>: Break complex workflows into manageable pieces</li>
</ul>
<h3 id="organizing-reusable-workflows"><a class="header" href="#organizing-reusable-workflows">Organizing Reusable Workflows</a></h3>
<p>Create a clear directory structure:</p>
<pre><code>project/
├── workflows/          # Main workflows
├── bases/             # Base workflows for inheritance
├── templates/         # Template workflows
└── common/            # Shared configurations for import
    ├── setup.yml
    ├── cleanup.yml
    └── validators.yml
</code></pre>
<h3 id="template-registry-management-1"><a class="header" href="#template-registry-management-1">Template Registry Management</a></h3>
<ul>
<li>Use descriptive template names</li>
<li>Version templates semantically (1.0.0, 2.0.0)</li>
<li>Tag templates for discoverability (e.g., “refactor”, “ci”, “test”)</li>
<li>Document required parameters in template descriptions</li>
<li>Test templates before registering them</li>
</ul>
<h4 id="template-registry-setup"><a class="header" href="#template-registry-setup">Template Registry Setup</a></h4>
<p>Place templates in a central <code>templates/</code> directory in your project. Use <code>FileTemplateStorage</code> with your project’s template directory:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use prodigy::cook::workflow::composition::registry::{TemplateRegistry, FileTemplateStorage};

let storage = FileTemplateStorage::new("./templates");
let registry = TemplateRegistry::new_with_storage(storage);
<span class="boring">}</span></code></pre></pre>
<p>Templates are cached in memory after first load for performance. The registry automatically discovers templates by scanning for <code>.yml</code> files in the template directory. Metadata files (<code>.meta.json</code>) are detected and filtered out during discovery. Templates are loaded on-demand and cached (see <code>is_template_file</code> and <code>extract_template_name</code> helper functions in <code>registry.rs:303-324</code>).</p>
<p><strong>Template File Structure:</strong></p>
<p>Templates must be <code>.yml</code> files in the base directory, with optional <code>.meta.json</code> files for metadata. For example:</p>
<pre><code>templates/
├── refactor-base.yml           # Template workflow definition
├── refactor-base.meta.json     # Optional metadata (description, version, tags)
├── ci-pipeline.yml
└── ci-pipeline.meta.json
</code></pre>
<p>The registry loads the workflow definition from <code>.yml</code> files and metadata from the corresponding <code>.meta.json</code> sidecar files if present.</p>
<h3 id="avoiding-circular-dependencies"><a class="header" href="#avoiding-circular-dependencies">Avoiding Circular Dependencies</a></h3>
<ul>
<li>Keep inheritance chains shallow (max 2-3 levels)</li>
<li>Use imports for shared utilities, not mutual dependencies</li>
<li>Design base workflows to be self-contained</li>
<li>Document dependency relationships</li>
</ul>
<h3 id="testing-composed-workflows"><a class="header" href="#testing-composed-workflows">Testing Composed Workflows</a></h3>
<p>Test workflows at each composition level:</p>
<ol>
<li><strong>Unit level</strong>: Test individual workflows</li>
<li><strong>Integration level</strong>: Test workflows with imports</li>
<li><strong>Composition level</strong>: Test fully composed workflows with all features</li>
</ol>
<h3 id="performance-considerations-1"><a class="header" href="#performance-considerations-1">Performance Considerations</a></h3>
<p><strong>Workflow Loader Caching:</strong></p>
<p>The workflow loader caches parsed workflows in memory to avoid re-parsing the same files during composition. This means importing the same workflow multiple times has minimal overhead (see <code>WorkflowLoader</code> cache in <code>composer.rs:399-435</code>). The cache is maintained in a <code>Mutex&lt;HashMap&gt;</code> for thread-safe access during parallel composition operations.</p>
<h3 id="naming-conventions"><a class="header" href="#naming-conventions">Naming Conventions</a></h3>
<p>Use consistent naming:</p>
<ul>
<li>Base workflows: <code>base-{purpose}.yml</code></li>
<li>Templates: <code>{category}-{purpose}.yml</code></li>
<li>Shared utilities: <code>{function}-utils.yml</code></li>
<li>Parameters: Use snake_case (<code>target_file</code>, not <code>targetFile</code>)</li>
</ul>
<h2 id="troubleshooting-3"><a class="header" href="#troubleshooting-3">Troubleshooting</a></h2>
<h3 id="circular-dependency-errors"><a class="header" href="#circular-dependency-errors">Circular Dependency Errors</a></h3>
<p><strong>Error</strong>: <code>Circular dependency detected in workflow composition</code></p>
<p><strong>Cause</strong>: Workflows reference each other in a loop</p>
<p><strong>Solution</strong>: Review your <code>extends</code> and <code>imports</code> to break the cycle:</p>
<pre><code class="language-yaml"># Bad: A extends B, B extends A
# Good: A and B both extend base-workflow
</code></pre>
<h3 id="template-not-found-errors"><a class="header" href="#template-not-found-errors">Template Not Found Errors</a></h3>
<p><strong>Error</strong>: <code>Template 'my-template' not found in registry</code></p>
<p><strong>Cause</strong>: Template doesn’t exist or hasn’t been registered</p>
<p><strong>Solution</strong>:</p>
<ol>
<li>List available templates: <code>registry.list().await?</code></li>
<li>Register the template before using it</li>
<li>Check template name spelling</li>
</ol>
<h3 id="parameter-validation-failures"><a class="header" href="#parameter-validation-failures">Parameter Validation Failures</a></h3>
<p><strong>Error</strong>: <code>Required parameter 'target' not provided</code></p>
<p><strong>Cause</strong>: Missing required parameter</p>
<p><strong>Solution</strong>: Provide all required parameters:</p>
<pre><code class="language-yaml">parameters:
  target: "src/"
  iterations: 5
</code></pre>
<p><strong>Error</strong>: <code>Type mismatch: expected Number, got String</code></p>
<p><strong>Cause</strong>: Parameter value doesn’t match declared type</p>
<p><strong>Solution</strong>: Ensure parameter types match:</p>
<pre><code class="language-yaml"># Wrong: count: "5"
# Right: count: 5
</code></pre>
<h3 id="import-path-resolution"><a class="header" href="#import-path-resolution">Import Path Resolution</a></h3>
<p><strong>Error</strong>: <code>Failed to load import from "./common/utils.yml"</code></p>
<p><strong>Cause</strong>: Import path doesn’t exist</p>
<p><strong>Solution</strong>:</p>
<ol>
<li>Use paths relative to workflow file</li>
<li>Verify file exists at the specified location</li>
<li>Check file permissions</li>
</ol>
<h3 id="base-workflow-not-found"><a class="header" href="#base-workflow-not-found">Base Workflow Not Found</a></h3>
<p><strong>Error</strong>: <code>Base workflow 'base' not found</code></p>
<p><strong>Cause</strong>: Base workflow not in standard locations</p>
<p><strong>Solution</strong>: Place base workflow in one of:</p>
<ul>
<li><code>./bases/base.yml</code></li>
<li><code>./templates/base.yml</code></li>
<li><code>./workflows/base.yml</code></li>
<li><code>./base.yml</code></li>
</ul>
<p>Or use a full path in <code>extends</code>.</p>
<h2 id="related-chapters"><a class="header" href="#related-chapters">Related Chapters</a></h2>
<ul>
<li><a href="workflow-basics.html">Workflow Basics</a> - Fundamental workflow concepts</li>
<li><a href="commands.html">Commands</a> - Command types and execution</li>
<li><a href="variables.html">Variables</a> - Variable interpolation and substitution</li>
<li><a href="examples.html">Examples</a> - Additional workflow examples</li>
</ul>
<h2 id="further-reading"><a class="header" href="#further-reading">Further Reading</a></h2>
<ul>
<li>Source code: <code>src/cook/workflow/composition/</code></li>
<li>Integration tests: <code>tests/workflow_composition_test.rs</code></li>
<li>Template registry: <code>src/cook/workflow/composition/registry.rs</code></li>
<li>Dependency resolution: <code>src/cook/workflow/composition/composer.rs</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="retry-configuration"><a class="header" href="#retry-configuration">Retry Configuration</a></h1>
<p>Prodigy provides sophisticated retry mechanisms with multiple backoff strategies to handle transient failures gracefully. The retry system supports both command-level and workflow-level configurations with fine-grained control over retry behavior.</p>
<h2 id="overview-4"><a class="header" href="#overview-4">Overview</a></h2>
<p>Prodigy has two retry systems that work together:</p>
<ol>
<li><strong>Enhanced Retry System</strong> - Rich, configurable retry with multiple backoff strategies, jitter, circuit breakers, and conditional retry (from <code>src/cook/retry_v2.rs</code>)</li>
<li><strong>Workflow-Level Retry</strong> - Simpler retry configuration for workflow-level error policies (from <code>src/cook/workflow/error_policy.rs</code>)</li>
</ol>
<p>This chapter focuses on the enhanced retry system which provides comprehensive retry capabilities.</p>
<h2 id="retryconfig-structure"><a class="header" href="#retryconfig-structure">RetryConfig Structure</a></h2>
<p>The <code>RetryConfig</code> struct controls retry behavior with the following fields:</p>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr></thead><tbody>
<tr><td><code>attempts</code></td><td><code>u32</code></td><td><code>3</code></td><td>Maximum number of retry attempts</td></tr>
<tr><td><code>backoff</code></td><td><code>BackoffStrategy</code></td><td><code>Exponential</code></td><td>Strategy for calculating delays between retries</td></tr>
<tr><td><code>initial_delay</code></td><td><code>Duration</code></td><td><code>1s</code></td><td>Initial delay before first retry</td></tr>
<tr><td><code>max_delay</code></td><td><code>Duration</code></td><td><code>30s</code></td><td>Maximum delay between any two retries</td></tr>
<tr><td><code>jitter</code></td><td><code>bool</code></td><td><code>false</code></td><td>Whether to add randomness to delays</td></tr>
<tr><td><code>jitter_factor</code></td><td><code>f64</code></td><td><code>0.3</code></td><td>Amount of jitter (0.0 to 1.0)</td></tr>
<tr><td><code>retry_on</code></td><td><code>Vec&lt;ErrorMatcher&gt;</code></td><td><code>[]</code></td><td>Retry only on specific error types (empty = retry all)</td></tr>
<tr><td><code>retry_budget</code></td><td><code>Option&lt;Duration&gt;</code></td><td><code>None</code></td><td>Maximum total time for all retry attempts</td></tr>
<tr><td><code>on_failure</code></td><td><code>FailureAction</code></td><td><code>Stop</code></td><td>Action to take after all retries exhausted</td></tr>
</tbody></table>
</div>
<h3 id="duration-format"><a class="header" href="#duration-format">Duration Format</a></h3>
<p>All duration fields use the <code>humantime</code> format. Examples:</p>
<ul>
<li><code>1s</code> - 1 second</li>
<li><code>30s</code> - 30 seconds</li>
<li><code>5m</code> - 5 minutes</li>
<li><code>100ms</code> - 100 milliseconds</li>
</ul>
<h2 id="basic-retry-configuration"><a class="header" href="#basic-retry-configuration">Basic Retry Configuration</a></h2>
<p>The simplest retry configuration uses default values:</p>
<pre><code class="language-yaml">retry:
  attempts: 3
</code></pre>
<p>This will:</p>
<ul>
<li>Retry up to 3 times</li>
<li>Use exponential backoff (base 2.0)</li>
<li>Start with 1 second delay</li>
<li>Cap delays at 30 seconds</li>
</ul>
<h2 id="backoff-strategies-1"><a class="header" href="#backoff-strategies-1">Backoff Strategies</a></h2>
<p>Prodigy supports five backoff strategies for controlling delay between retries:</p>
<h3 id="fixed-backoff"><a class="header" href="#fixed-backoff">Fixed Backoff</a></h3>
<p>Same delay for every retry attempt:</p>
<pre><code class="language-yaml">retry:
  attempts: 5
  backoff: fixed
  initial_delay: 2s
</code></pre>
<p><strong>Delay sequence</strong>: 2s, 2s, 2s, 2s, 2s</p>
<p><strong>Use case</strong>: Simple retry logic when you want consistent delays.</p>
<h3 id="linear-backoff-1"><a class="header" href="#linear-backoff-1">Linear Backoff</a></h3>
<p>Delay increases by a fixed amount each retry:</p>
<pre><code class="language-yaml">retry:
  attempts: 4
  backoff:
    linear:
      increment: 2s
  initial_delay: 1s
</code></pre>
<p><strong>Delay sequence</strong>: 1s, 3s, 5s, 7s</p>
<p>The <code>increment</code> field specifies the additional delay added on each retry attempt. In this example, the delay starts at 1s and increases by 2s on each retry.</p>
<p><strong>Use case</strong>: Gradual backoff when you expect quick recovery.</p>
<h3 id="exponential-backoff-1"><a class="header" href="#exponential-backoff-1">Exponential Backoff</a></h3>
<p>Delay doubles (or increases by <code>base</code> factor) each retry:</p>
<pre><code class="language-yaml">retry:
  attempts: 5
  backoff:
    exponential:
      base: 2.0
  initial_delay: 1s
  max_delay: 30s
</code></pre>
<p><strong>Delay sequence</strong>: 1s, 2s, 4s, 8s, 16s</p>
<p><strong>Use case</strong>: Most common strategy - backs off quickly to avoid overwhelming failing services. Default strategy.</p>
<h3 id="fibonacci-backoff-1"><a class="header" href="#fibonacci-backoff-1">Fibonacci Backoff</a></h3>
<p>Delay follows Fibonacci sequence:</p>
<pre><code class="language-yaml">retry:
  attempts: 6
  backoff: fibonacci
  initial_delay: 1s
</code></pre>
<p><strong>Delay sequence</strong>: 1s, 1s, 2s, 3s, 5s, 8s</p>
<p><strong>Use case</strong>: Smoother increase than exponential, good for distributed systems.</p>
<h3 id="custom-backoff"><a class="header" href="#custom-backoff">Custom Backoff</a></h3>
<p>Specify exact delays for each retry:</p>
<pre><code class="language-yaml">retry:
  attempts: 4
  backoff:
    custom:
      delays:
        - secs: 1
          nanos: 0
        - secs: 2
          nanos: 0
        - secs: 5
          nanos: 0
        - secs: 10
          nanos: 0
</code></pre>
<p><strong>Delay sequence</strong>: Exactly as specified</p>
<p><strong>Use case</strong>: When you need precise control over timing (e.g., aligning with rate limit windows).</p>
<p><strong>Note</strong>: Custom backoff delays require explicit Duration format with <code>secs</code> and <code>nanos</code> fields. The <code>delays</code> field does not use humantime format unlike other duration fields in RetryConfig. Once the custom delays array is exhausted, any remaining retry attempts will use the <code>max_delay</code> value.</p>
<h2 id="backoff-strategy-comparison"><a class="header" href="#backoff-strategy-comparison">Backoff Strategy Comparison</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Strategy</th><th>Attempt 1</th><th>Attempt 2</th><th>Attempt 3</th><th>Attempt 4</th><th>Attempt 5</th><th>Best For</th></tr></thead><tbody>
<tr><td>Fixed (2s)</td><td>2s</td><td>2s</td><td>2s</td><td>2s</td><td>2s</td><td>Simple retry</td></tr>
<tr><td>Linear (+2s)</td><td>1s</td><td>3s</td><td>5s</td><td>7s</td><td>9s</td><td>Gradual backoff</td></tr>
<tr><td>Exponential (base 2.0)</td><td>1s</td><td>2s</td><td>4s</td><td>8s</td><td>16s</td><td>Most failures</td></tr>
<tr><td>Fibonacci</td><td>1s</td><td>1s</td><td>2s</td><td>3s</td><td>5s</td><td>Distributed systems</td></tr>
</tbody></table>
</div>
<h2 id="jitter-for-distributed-systems"><a class="header" href="#jitter-for-distributed-systems">Jitter for Distributed Systems</a></h2>
<p>Jitter adds randomness to retry delays to prevent the “thundering herd” problem where many clients retry at the same time.</p>
<pre><code class="language-yaml">retry:
  attempts: 5
  backoff:
    exponential:
      base: 2.0
  initial_delay: 10s
  jitter: true
  jitter_factor: 0.5
</code></pre>
<p>With <code>jitter_factor: 0.5</code>:</p>
<ul>
<li>A 10s delay becomes a random delay between <strong>7.5s and 12.5s</strong></li>
<li>A 20s delay becomes a random delay between <strong>15s and 25s</strong></li>
</ul>
<p>The jitter is applied as: <code>delay + random(-delay * factor / 2, +delay * factor / 2)</code></p>
<p>The implementation uses Rust’s <code>random_range</code> with inclusive bounds on both ends. For example, with a 10s delay and factor 0.5: <code>10s + random(-2.5s, +2.5s)</code> = 7.5s to 12.5s</p>
<p><strong>When to use jitter</strong>:</p>
<ul>
<li>Multiple clients accessing the same service</li>
<li>Distributed systems with many workers</li>
<li>Rate-limited APIs</li>
<li>Preventing synchronized retry storms</li>
</ul>
<h2 id="conditional-retry-with-error-matchers"><a class="header" href="#conditional-retry-with-error-matchers">Conditional Retry with Error Matchers</a></h2>
<p>By default, Prodigy retries all errors. Use <code>retry_on</code> to retry only specific error types:</p>
<p><strong>Note</strong>: All error matching is case-insensitive. Error messages are normalized to lowercase before pattern comparison.</p>
<h3 id="network-errors"><a class="header" href="#network-errors">Network Errors</a></h3>
<pre><code class="language-yaml">retry:
  attempts: 3
  retry_on:
    - network
</code></pre>
<p>Matches errors containing: <code>network</code>, <code>connection</code>, <code>refused</code>, <code>unreachable</code></p>
<h3 id="timeout-errors"><a class="header" href="#timeout-errors">Timeout Errors</a></h3>
<pre><code class="language-yaml">retry:
  attempts: 5
  retry_on:
    - timeout
  backoff:
    exponential:
      base: 2.0
  initial_delay: 2s
</code></pre>
<p>Matches errors containing: <code>timeout</code>, <code>timed out</code></p>
<h3 id="server-errors-http-5xx"><a class="header" href="#server-errors-http-5xx">Server Errors (HTTP 5xx)</a></h3>
<pre><code class="language-yaml">retry:
  attempts: 3
  retry_on:
    - server_error
</code></pre>
<p>Matches errors containing: <code>500</code>, <code>502</code>, <code>503</code>, <code>504</code>, <code>server error</code></p>
<h3 id="rate-limit-errors"><a class="header" href="#rate-limit-errors">Rate Limit Errors</a></h3>
<pre><code class="language-yaml">retry:
  attempts: 5
  retry_on:
    - rate_limit
  initial_delay: 60s
</code></pre>
<p>Matches errors containing: <code>rate limit</code>, <code>429</code>, <code>too many requests</code></p>
<h3 id="custom-error-patterns"><a class="header" href="#custom-error-patterns">Custom Error Patterns</a></h3>
<p>Use regex patterns for custom error matching:</p>
<pre><code class="language-yaml">retry:
  attempts: 3
  retry_on:
    - pattern: "database connection|pool exhausted"
    - pattern: "ECONNRESET"
</code></pre>
<p><strong>Note</strong>: Pattern matchers use Rust regex syntax. If the pattern is not valid regex, it will silently fail to match any errors (returns false on regex compilation error). Always test your patterns to ensure they’re valid.</p>
<h3 id="multiple-error-types"><a class="header" href="#multiple-error-types">Multiple Error Types</a></h3>
<p>Retry on any matching error type:</p>
<pre><code class="language-yaml">retry:
  attempts: 5
  retry_on:
    - network
    - timeout
    - rate_limit
  backoff: fibonacci
  initial_delay: 1s
</code></pre>
<h2 id="retry-budget"><a class="header" href="#retry-budget">Retry Budget</a></h2>
<p>A retry budget limits the total time spent on retries to prevent indefinite retry loops:</p>
<pre><code class="language-yaml">retry:
  attempts: 10
  retry_budget: 5m
  backoff:
    exponential:
      base: 2.0
  initial_delay: 1s
</code></pre>
<p>In this example:</p>
<ul>
<li>Allows up to 10 retry attempts</li>
<li><strong>BUT</strong> stops retrying if total time exceeds 5 minutes</li>
<li>Useful for preventing workflows from hanging indefinitely</li>
</ul>
<p><strong>Without retry budget</strong>: Exponential backoff with 10 attempts could take hours
<strong>With retry budget</strong>: Guarantees workflow fails within 5 minutes</p>
<h2 id="failure-actions"><a class="header" href="#failure-actions">Failure Actions</a></h2>
<p>Configure what happens after all retries are exhausted:</p>
<h3 id="stop-workflow-default"><a class="header" href="#stop-workflow-default">Stop Workflow (Default)</a></h3>
<pre><code class="language-yaml">retry:
  attempts: 3
  on_failure: stop
</code></pre>
<p>Stops the entire workflow execution on final failure.</p>
<h3 id="continue-with-next-step"><a class="header" href="#continue-with-next-step">Continue with Next Step</a></h3>
<pre><code class="language-yaml">retry:
  attempts: 3
  on_failure: continue
</code></pre>
<p>Logs the failure but continues workflow execution.</p>
<h3 id="fallback-command"><a class="header" href="#fallback-command">Fallback Command</a></h3>
<pre><code class="language-yaml">retry:
  attempts: 3
  on_failure:
    fallback:
      command: "echo 'Primary command failed, using fallback'"
</code></pre>
<p>Executes a fallback command if primary command fails after all retries.</p>
<h2 id="complete-examples-1"><a class="header" href="#complete-examples-1">Complete Examples</a></h2>
<h3 id="retry-network-requests"><a class="header" href="#retry-network-requests">Retry Network Requests</a></h3>
<pre><code class="language-yaml">retry:
  attempts: 5
  backoff:
    exponential:
      base: 2.0
  initial_delay: 1s
  max_delay: 30s
  jitter: true
  jitter_factor: 0.3
  retry_on:
    - network
    - timeout
    - server_error
  retry_budget: 2m
</code></pre>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Retries up to 5 times on network/timeout/server errors</li>
<li>Exponential backoff: 1s, 2s, 4s, 8s, 16s (with jitter)</li>
<li>Total retry time limited to 2 minutes</li>
<li>Other errors (like syntax errors) fail immediately</li>
</ul>
<h3 id="retry-rate-limited-apis"><a class="header" href="#retry-rate-limited-apis">Retry Rate-Limited APIs</a></h3>
<pre><code class="language-yaml">retry:
  attempts: 3
  backoff: fibonacci
  initial_delay: 60s
  max_delay: 300s
  retry_on:
    - rate_limit
  retry_budget: 15m
</code></pre>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Waits 60s, 60s, 120s between retries (Fibonacci sequence)</li>
<li>Only retries on rate limit errors (429, “rate limit exceeded”)</li>
<li>Gives up after 15 minutes total</li>
</ul>
<h3 id="retry-flaky-tests"><a class="header" href="#retry-flaky-tests">Retry Flaky Tests</a></h3>
<pre><code class="language-yaml">retry:
  attempts: 3
  backoff: fixed
  initial_delay: 500ms
  on_failure: continue
</code></pre>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Quick retries with 500ms between attempts</li>
<li>Fixed delay (doesn’t increase)</li>
<li>Continues workflow even if test fails 3 times</li>
</ul>
<h3 id="retry-with-circuit-breaker"><a class="header" href="#retry-with-circuit-breaker">Retry with Circuit Breaker</a></h3>
<p>The <code>RetryExecutor</code> supports circuit breakers to prevent cascading failures. This example shows the <strong>programmatic Rust API</strong> for advanced use cases. For workflow-level circuit breaker configuration using YAML, see the <a href="./error-handling.html">Error Handling</a> chapter which covers <code>error_policy</code> circuit breaker settings.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use prodigy::cook::retry_v2::{RetryConfig, RetryExecutor};
use std::time::Duration;

let config = RetryConfig {
    attempts: 5,
    ..Default::default()
};

let executor = RetryExecutor::new(config)
    .with_circuit_breaker(
        3,                             // Open after 3 consecutive failures
        Duration::from_secs(30)        // Stay open for 30 seconds
    );

let result = executor
    .execute_with_retry(|| async {
        // Your operation here
        Ok("success")
    }, "my-operation")
    .await?;
<span class="boring">}</span></code></pre></pre>
<p><strong>Circuit breaker states</strong>:</p>
<ul>
<li><strong>Closed</strong>: Normal operation, requests pass through</li>
<li><strong>Open</strong>: Too many failures, immediately reject requests</li>
<li><strong>Half-Open</strong>: After timeout, test with limited requests</li>
</ul>
<h2 id="workflow-level-vs-command-level-retry"><a class="header" href="#workflow-level-vs-command-level-retry">Workflow-Level vs Command-Level Retry</a></h2>
<p>Prodigy has two retry systems that serve different purposes:</p>
<h3 id="field-name-differences"><a class="header" href="#field-name-differences">Field Name Differences</a></h3>
<p>The two retry systems use different field names and structures:</p>
<div class="table-wrapper"><table><thead><tr><th>Feature</th><th>retry_v2::RetryConfig</th><th>error_policy::RetryConfig</th></tr></thead><tbody>
<tr><td>Max attempts</td><td><code>attempts</code></td><td><code>max_attempts</code></td></tr>
<tr><td>Backoff</td><td><code>backoff</code> (enum variants)</td><td><code>backoff</code> (enum with nested fields)</td></tr>
<tr><td>Exponential base</td><td><code>exponential: { base }</code></td><td><code>exponential: { initial, multiplier }</code></td></tr>
<tr><td>Linear increment</td><td><code>linear: { increment }</code></td><td><code>linear: { initial, increment }</code></td></tr>
<tr><td>Fixed delay</td><td><code>backoff: fixed</code></td><td><code>fixed: { delay }</code></td></tr>
</tbody></table>
</div>
<h3 id="command-level-retry-enhanced"><a class="header" href="#command-level-retry-enhanced">Command-Level Retry (Enhanced)</a></h3>
<p>Configured per command using <code>RetryConfig</code>:</p>
<pre><code class="language-yaml">commands:
  - shell: "curl https://api.example.com/data"
    retry:
      attempts: 5
      backoff: exponential
      retry_on: [network, timeout]
</code></pre>
<p><strong>Features</strong>:</p>
<ul>
<li>Rich backoff strategies (exponential, fibonacci, custom)</li>
<li>Jitter support</li>
<li>Conditional retry (error matchers)</li>
<li>Retry budget</li>
<li>Circuit breaker integration</li>
<li>Detailed retry metrics</li>
</ul>
<h3 id="workflow-level-retry-error-policy"><a class="header" href="#workflow-level-retry-error-policy">Workflow-Level Retry (Error Policy)</a></h3>
<p>Configured in workflow error policy from <code>error_policy.rs</code>:</p>
<pre><code class="language-yaml">error_policy:
  on_item_failure: retry
  retry_config:
    max_attempts: 3
    backoff:
      exponential:
        initial: 1s
        multiplier: 2.0
</code></pre>
<p><strong>Features</strong>:</p>
<ul>
<li>Simpler configuration</li>
<li>Workflow-level failure handling</li>
<li>Integrates with DLQ (Dead Letter Queue)</li>
<li>Circuit breaker support</li>
<li>Error metrics and pattern detection</li>
</ul>
<p><strong>When to use each</strong>:</p>
<ul>
<li><strong>Command-level</strong>: For specific commands that need sophisticated retry logic</li>
<li><strong>Workflow-level</strong>: For consistent retry behavior across all workflow items in MapReduce jobs</li>
</ul>
<h3 id="command-metadata-override"><a class="header" href="#command-metadata-override">Command Metadata Override</a></h3>
<p>Individual commands can override workflow-level retry settings:</p>
<pre><code class="language-yaml"># Workflow default
retry:
  attempts: 3

# Command override
commands:
  - claude: "/analyze ${item}"
    metadata:
      retries: 5  # This command gets 5 attempts instead of 3
</code></pre>
<p>From <code>src/config/command.rs:135</code>.</p>
<h2 id="retry-metrics-and-observability"><a class="header" href="#retry-metrics-and-observability">Retry Metrics and Observability</a></h2>
<p>The retry system tracks metrics for monitoring:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let executor = RetryExecutor::new(config);
// ... execute operations ...
let metrics = executor.metrics().await;

println!("Total attempts: {}", metrics.total_attempts);
println!("Successful: {}", metrics.successful_attempts);
println!("Failed: {}", metrics.failed_attempts);
println!("Retry history: {:?}", metrics.retries);
<span class="boring">}</span></code></pre></pre>
<p>Metrics include:</p>
<ul>
<li><code>total_attempts</code> - Total number of attempts made</li>
<li><code>successful_attempts</code> - Number of successful operations</li>
<li><code>failed_attempts</code> - Number of failed operations</li>
<li><code>retries</code> - Vec of (attempt_number, delay) pairs</li>
</ul>
<h2 id="best-practices-6"><a class="header" href="#best-practices-6">Best Practices</a></h2>
<h3 id="choosing-a-backoff-strategy"><a class="header" href="#choosing-a-backoff-strategy">Choosing a Backoff Strategy</a></h3>
<ol>
<li><strong>Use Exponential for most cases</strong> - Default strategy, works well for most failures</li>
<li><strong>Use Fibonacci for distributed systems</strong> - Smoother curve, avoids overwhelming services</li>
<li><strong>Use Linear for quick recovery scenarios</strong> - When failures are brief and predictable</li>
<li><strong>Use Fixed only for very specific cases</strong> - When you know exact timing requirements</li>
<li><strong>Use Custom for rate-limited APIs</strong> - Align retries with API rate limit windows</li>
</ol>
<h3 id="setting-appropriate-timeouts"><a class="header" href="#setting-appropriate-timeouts">Setting Appropriate Timeouts</a></h3>
<ul>
<li><strong>initial_delay</strong>: Start small (1-2s) for transient errors, larger (30-60s) for rate limits</li>
<li><strong>max_delay</strong>: Cap at reasonable time (30s for interactive, 5m for background jobs)</li>
<li><strong>retry_budget</strong>: Always set this to prevent infinite retry loops
<ul>
<li>Interactive operations: 1-5 minutes</li>
<li>Background jobs: 15-30 minutes</li>
<li>Critical operations: 1 hour</li>
</ul>
</li>
</ul>
<h3 id="using-jitter"><a class="header" href="#using-jitter">Using Jitter</a></h3>
<p>Enable jitter when:</p>
<ul>
<li>✅ Multiple clients/workers accessing the same service</li>
<li>✅ Distributed systems with parallel execution</li>
<li>✅ Rate-limited APIs</li>
<li>❌ Single client applications</li>
<li>❌ When exact timing is critical</li>
</ul>
<h3 id="conditional-retry"><a class="header" href="#conditional-retry">Conditional Retry</a></h3>
<p>Always use <code>retry_on</code> to avoid retrying permanent failures:</p>
<pre><code class="language-yaml"># ❌ BAD: Retries syntax errors forever
retry:
  attempts: 10
  backoff: exponential

# ✅ GOOD: Only retries transient failures
retry:
  attempts: 10
  retry_on: [network, timeout, server_error]
  backoff: exponential
</code></pre>
<h3 id="retry-budget-guidelines"><a class="header" href="#retry-budget-guidelines">Retry Budget Guidelines</a></h3>
<p>Set retry budgets based on operation criticality:</p>
<div class="table-wrapper"><table><thead><tr><th>Operation Type</th><th>Suggested Budget</th><th>Reasoning</th></tr></thead><tbody>
<tr><td>User-facing API calls</td><td>5-10 seconds</td><td>User is waiting</td></tr>
<tr><td>Background sync</td><td>5-15 minutes</td><td>Can afford patience</td></tr>
<tr><td>Critical data operations</td><td>30-60 minutes</td><td>Must complete</td></tr>
<tr><td>Non-critical tasks</td><td>1-5 minutes</td><td>Fail fast</td></tr>
</tbody></table>
</div>
<h2 id="troubleshooting-4"><a class="header" href="#troubleshooting-4">Troubleshooting</a></h2>
<h3 id="retries-taking-too-long"><a class="header" href="#retries-taking-too-long">Retries Taking Too Long</a></h3>
<p><strong>Problem</strong>: Workflow hangs during retries</p>
<p><strong>Solutions</strong>:</p>
<ol>
<li>Add a <code>retry_budget</code> to cap total retry time</li>
<li>Reduce <code>max_delay</code> to speed up retry cycles</li>
<li>Reduce <code>attempts</code> to fail faster</li>
<li>Use <code>linear</code> or <code>fibonacci</code> instead of <code>exponential</code></li>
</ol>
<h3 id="not-retrying-when-expected"><a class="header" href="#not-retrying-when-expected">Not Retrying When Expected</a></h3>
<p><strong>Problem</strong>: Operation fails without retrying</p>
<p><strong>Causes</strong>:</p>
<ol>
<li>Error doesn’t match <code>retry_on</code> patterns</li>
<li>Already at max <code>attempts</code></li>
<li><code>retry_budget</code> exhausted</li>
<li>Circuit breaker is open</li>
</ol>
<p><strong>Debug</strong>:</p>
<ul>
<li>Remove <code>retry_on</code> to retry all errors</li>
<li>Check error message matches patterns (case-insensitive)</li>
<li>Increase <code>retry_budget</code> or <code>attempts</code></li>
<li>Check circuit breaker status</li>
</ul>
<h3 id="thundering-herd-problem"><a class="header" href="#thundering-herd-problem">Thundering Herd Problem</a></h3>
<p><strong>Problem</strong>: Many workers retry at the same time, overwhelming service</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-yaml">retry:
  attempts: 5
  jitter: true
  jitter_factor: 0.5  # 50% jitter range
  backoff: fibonacci   # Smoother than exponential
</code></pre>
<h3 id="rate-limit-issues"><a class="header" href="#rate-limit-issues">Rate Limit Issues</a></h3>
<p><strong>Problem</strong>: Keep hitting rate limits despite retries</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-yaml">retry:
  attempts: 3
  retry_on:
    - rate_limit
  backoff:
    custom:
      delays:
        - secs: 60
          nanos: 0
        - secs: 120
          nanos: 0
        - secs: 300
          nanos: 0
  retry_budget: 15m
</code></pre>
<h2 id="related-topics"><a class="header" href="#related-topics">Related Topics</a></h2>
<ul>
<li><a href="./error-handling.html">Error Handling</a> - Overall error handling strategy</li>
<li><a href="./workflow-configuration.html">Workflow Configuration</a> - Workflow-level settings</li>
<li><a href="./mapreduce.html">MapReduce</a> - Retry in MapReduce workflows</li>
<li><a href="./dead-letter-queue.html">Dead Letter Queue</a> - Handling failed items</li>
</ul>
<h2 id="implementation-references"><a class="header" href="#implementation-references">Implementation References</a></h2>
<ul>
<li>Enhanced retry system: <code>src/cook/retry_v2.rs:14-461</code></li>
<li>Workflow error policy: <code>src/cook/workflow/error_policy.rs:91-129</code></li>
<li>Command metadata: <code>src/config/command.rs:135</code></li>
<li>Circuit breaker: <code>src/cook/retry_v2.rs:325-397</code></li>
<li>Error matchers: <code>src/cook/retry_v2.rs:100-151</code></li>
<li>Retry metrics: <code>src/cook/retry_v2.rs:399-422</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="error-handling-3"><a class="header" href="#error-handling-3">Error Handling</a></h1>
<p>Prodigy provides comprehensive error handling at both the workflow level (for MapReduce jobs) and the command level (for individual workflow steps). This chapter covers the practical features available for handling failures gracefully.</p>
<hr />
<h2 id="command-level-error-handling"><a class="header" href="#command-level-error-handling">Command-Level Error Handling</a></h2>
<p>Command-level error handling allows you to specify what happens when a single workflow step fails. Use the <code>on_failure</code> configuration to define recovery, cleanup, or fallback strategies.</p>
<h3 id="simple-forms"><a class="header" href="#simple-forms">Simple Forms</a></h3>
<p>For basic error handling, use the simplest form that meets your needs:</p>
<pre><code class="language-yaml"># Ignore errors - don't fail the workflow
- shell: "optional-cleanup.sh"
  on_failure: true

# Single recovery command (shell or claude)
- shell: "npm install"
  on_failure: "npm cache clean --force"

- shell: "cargo clippy"
  on_failure: "/fix-warnings"

# Multiple recovery commands
- shell: "build-project"
  on_failure:
    - "cleanup-artifacts"
    - "/diagnose-build-errors"
    - "retry-build"
</code></pre>
<h3 id="advanced-configuration"><a class="header" href="#advanced-configuration">Advanced Configuration</a></h3>
<p>For more control over error handling behavior:</p>
<pre><code class="language-yaml">- shell: "cargo clippy"
  on_failure:
    claude: "/fix-warnings ${shell.output}"
    fail_workflow: false     # Continue workflow even if handler fails
    max_attempts: 3          # Retry original command up to 3 times (setting &gt; 1 enables auto-retry)
</code></pre>
<p><strong>Available Fields:</strong></p>
<ul>
<li><code>shell</code> - Shell command to run on failure</li>
<li><code>claude</code> - Claude command to run on failure</li>
<li><code>fail_workflow</code> - Whether to fail the entire workflow (default: <code>false</code>)</li>
<li><code>max_attempts</code> - Maximum retry attempts for the original command (default: <code>1</code>)</li>
<li><code>max_retries</code> - Alternative name for <code>max_attempts</code> (both are supported for backward compatibility)</li>
</ul>
<p><strong>Notes:</strong></p>
<ul>
<li>When <code>max_attempts &gt; 1</code>, Prodigy automatically retries the original command after running the failure handler (the deprecated <code>retry_original</code> flag is no longer needed)</li>
<li>Retry behavior is now controlled by the <code>max_attempts</code>/<code>max_retries</code> value, not a separate flag</li>
<li>You can specify both <code>shell</code> and <code>claude</code> commands - they will execute in sequence</li>
<li>By default, having a handler means the workflow continues even if the step fails</li>
</ul>
<blockquote>
<p><strong>Migration from <code>retry_original</code>:</strong> Previously you used <code>retry_original: true</code> with <code>max_retries: 3</code>. Now just use <code>max_attempts: 3</code> (retry is implicit when &gt; 1). Both <code>max_attempts</code> and <code>max_retries</code> are supported as aliases for backward compatibility.</p>
</blockquote>
<h3 id="detailed-handler-configuration"><a class="header" href="#detailed-handler-configuration">Detailed Handler Configuration</a></h3>
<p>For complex error handling scenarios with multiple commands and fine-grained control:</p>
<pre><code class="language-yaml">- shell: "deploy-production"
  on_failure:
    strategy: recovery        # Options: recovery, fallback, cleanup, custom
    timeout: 300             # Handler timeout in seconds
    handler_failure_fatal: true  # Fail workflow if handler fails
    fail_workflow: false     # Don't fail workflow if step fails
    capture:                 # Capture handler output to variables
      error_log: "handler_output"
      rollback_status: "rollback_result"
    commands:
      - shell: "rollback-deployment"
        continue_on_error: true
      - claude: "/analyze-deployment-failure"
      - shell: "notify-team"
</code></pre>
<p><strong>Handler Configuration Fields:</strong></p>
<ul>
<li><code>strategy</code> - Handler strategy (recovery, fallback, cleanup, custom)</li>
<li><code>timeout</code> - Handler timeout in seconds</li>
<li><code>handler_failure_fatal</code> - Fail workflow if handler fails</li>
<li><code>fail_workflow</code> - Whether to fail the entire workflow</li>
<li><code>capture</code> - Map of variable names to capture from handler output (e.g., <code>error_log: "handler_output"</code>). Note: capture applies to the handler’s combined output, not individual commands.</li>
<li><code>commands</code> - List of handler commands to execute</li>
</ul>
<p><strong>Handler Strategies:</strong></p>
<ul>
<li><code>recovery</code> - Try to fix the problem and retry (default)</li>
<li><code>fallback</code> - Use an alternative approach</li>
<li><code>cleanup</code> - Clean up resources</li>
<li><code>custom</code> - Custom handler logic</li>
</ul>
<p><strong>Handler Command Fields:</strong></p>
<ul>
<li><code>shell</code> or <code>claude</code> - The command to execute</li>
<li><code>continue_on_error</code> - Continue to next handler command even if this fails</li>
</ul>
<h3 id="success-handling"><a class="header" href="#success-handling">Success Handling</a></h3>
<p>Execute commands when a step succeeds. The <code>on_success</code> field accepts a full WorkflowStep configuration with all available fields.</p>
<p><strong>Simple Form:</strong></p>
<pre><code class="language-yaml">- shell: "deploy-staging"
  on_success:
    shell: "notify-success"
    claude: "/update-deployment-docs"
</code></pre>
<p><strong>Advanced Form with Full WorkflowStep Configuration:</strong></p>
<pre><code class="language-yaml">- shell: "build-production"
  on_success:
    claude: "/update-build-metrics"
    timeout: 60              # Success handler timeout
    capture: "metrics"       # Capture output to variable
    working_dir: "dist"      # Run in specific directory
    when: "${build.target} == 'release'"  # Conditional execution
</code></pre>
<p><strong>Note:</strong> The <code>on_success</code> handler supports all WorkflowStep fields including <code>timeout</code>, <code>capture</code>, <code>working_dir</code>, <code>when</code>, and nested <code>on_failure</code> handlers.</p>
<p><strong>Common Use Cases:</strong></p>
<p>Success handlers are useful for post-processing actions that should only occur when a step completes successfully:</p>
<ul>
<li><strong>Notifications:</strong> Send success notifications to teams via Slack, email, or other channels</li>
<li><strong>Metrics Updates:</strong> Update deployment metrics, dashboard statistics, or monitoring systems</li>
<li><strong>Downstream Workflows:</strong> Trigger dependent workflows or pipelines</li>
<li><strong>Artifact Archiving:</strong> Archive build artifacts, logs, or generated files for later use</li>
<li><strong>External System Updates:</strong> Update issue trackers, deployment records, or configuration management systems</li>
</ul>
<p>The handler receives access to step outputs via the <code>capture</code> field, allowing you to process results or pass data to subsequent steps.</p>
<h3 id="commit-requirements"><a class="header" href="#commit-requirements">Commit Requirements</a></h3>
<p>Specify whether a workflow step must create a git commit:</p>
<pre><code class="language-yaml">- claude: "/implement-feature"
  commit_required: true   # Fail if no commit is made
</code></pre>
<p>This is useful for ensuring that Claude commands that are expected to make code changes actually do so.</p>
<hr />
<h2 id="workflow-level-error-policy-mapreduce"><a class="header" href="#workflow-level-error-policy-mapreduce">Workflow-Level Error Policy (MapReduce)</a></h2>
<p>For MapReduce workflows, you can configure workflow-level error policies that control how the entire job responds to failures. This is separate from command-level error handling and only applies to MapReduce mode.</p>
<h3 id="basic-configuration"><a class="header" href="#basic-configuration">Basic Configuration</a></h3>
<pre><code class="language-yaml">name: process-items
mode: mapreduce

error_policy:
  # What to do when a work item fails
  on_item_failure: dlq      # Options: dlq, retry, skip, stop, custom:&lt;handler_name&gt;

  # Continue processing after failures
  continue_on_failure: true

  # Stop after this many failures
  max_failures: 10

  # Stop if failure rate exceeds threshold (0.0 to 1.0)
  failure_threshold: 0.2    # Stop if 20% of items fail

  # How to report errors
  error_collection: aggregate  # Options: aggregate, immediate, batched
</code></pre>
<p><strong>Item Failure Actions:</strong></p>
<ul>
<li><code>dlq</code> - Send failed items to Dead Letter Queue for later retry (default)</li>
<li><code>retry</code> - Retry the item immediately with backoff (if retry_config is set)</li>
<li><code>skip</code> - Skip the failed item and continue</li>
<li><code>stop</code> - Stop the entire workflow on first failure</li>
<li><code>custom:&lt;name&gt;</code> - Use a custom failure handler (not yet implemented)</li>
</ul>
<p><strong>Error Collection Strategies:</strong></p>
<ul>
<li><code>aggregate</code> - Collect all errors and report at the end (default)</li>
<li><code>immediate</code> - Report errors as they occur</li>
<li><code>batched: {size: N}</code> - Report errors in batches of N items</li>
</ul>
<h3 id="circuit-breaker"><a class="header" href="#circuit-breaker">Circuit Breaker</a></h3>
<p>Prevent cascading failures by opening a circuit after consecutive failures:</p>
<pre><code class="language-yaml">error_policy:
  circuit_breaker:
    failure_threshold: 5      # Open circuit after 5 consecutive failures
    success_threshold: 2      # Close circuit after 2 successes
    timeout: 30s             # Duration in humantime format (e.g., 30s, 1m, 500ms)
    half_open_requests: 3    # Test requests in half-open state
</code></pre>
<p><strong>Note:</strong> The <code>timeout</code> field uses humantime format supporting <code>1s</code>, <code>100ms</code>, <code>2m</code>, <code>30s</code> for duration parsing.</p>
<h4 id="circuit-breaker-states"><a class="header" href="#circuit-breaker-states">Circuit Breaker States</a></h4>
<p>The circuit breaker operates in three states to protect against cascading failures:</p>
<ol>
<li>
<p><strong>Closed (Normal Operation)</strong></p>
<ul>
<li>All requests are processed normally</li>
<li>Failures are tracked; consecutive failures increment the failure counter</li>
<li>Transitions to <strong>Open</strong> after <code>failure_threshold</code> consecutive failures</li>
</ul>
</li>
<li>
<p><strong>Open (Rejecting Requests)</strong></p>
<ul>
<li>All requests are immediately rejected without attempting execution</li>
<li>Prevents further load on a failing dependency</li>
<li>Transitions to <strong>HalfOpen</strong> after <code>timeout</code> duration expires</li>
</ul>
</li>
<li>
<p><strong>HalfOpen (Testing Recovery)</strong></p>
<ul>
<li>Allows a limited number of test requests (<code>half_open_requests</code>) to verify recovery</li>
<li>If test requests succeed (reaching <code>success_threshold</code>), transitions back to <strong>Closed</strong></li>
<li>If any test request fails, transitions back to <strong>Open</strong> and resets the timeout</li>
</ul>
</li>
</ol>
<p><strong>State Transition Flow:</strong></p>
<pre><code>Closed → Open (after failure_threshold consecutive failures)
Open → HalfOpen (after timeout expires)
HalfOpen → Closed (after success_threshold successes)
HalfOpen → Open (if any half_open_request fails)
</code></pre>
<h3 id="retry-configuration-with-backoff"><a class="header" href="#retry-configuration-with-backoff">Retry Configuration with Backoff</a></h3>
<p>Configure automatic retry behavior for failed items:</p>
<pre><code class="language-yaml">error_policy:
  on_item_failure: retry
  retry_config:
    max_attempts: 3
    backoff:
      type: exponential
      initial: 1s            # Initial delay (duration format)
      multiplier: 2          # Double delay each retry
</code></pre>
<p><strong>Backoff Strategy Options:</strong></p>
<pre><code class="language-yaml"># Fixed delay between retries
# Always waits the same duration
backoff:
  type: fixed
  delay: 1s

# Linear increase in delay
# Calculates: delay = initial + (retry_count * increment)
# Example with initial=1s, increment=500ms:
#   Retry 1: 1s + (1 * 500ms) = 1.5s
#   Retry 2: 1s + (2 * 500ms) = 2s
#   Retry 3: 1s + (3 * 500ms) = 2.5s
backoff:
  type: linear
  initial: 1s
  increment: 500ms

# Exponential backoff (recommended)
# Calculates: delay = initial * (multiplier ^ retry_count)
# Example: 1s, 2s, 4s, 8s...
backoff:
  type: exponential
  initial: 1s
  multiplier: 2

# Fibonacci sequence delays
# Calculates: delay = initial * fibonacci(retry_count)
# Example: 1s, 1s, 2s, 3s, 5s...
backoff:
  type: fibonacci
  initial: 1s
</code></pre>
<p><strong>Important:</strong> All duration values use humantime format (e.g., <code>1s</code>, <code>100ms</code>, <code>2m</code>, <code>30s</code>), not milliseconds.</p>
<blockquote>
<p><strong>Note on Serialization:</strong> BackoffStrategy accepts Duration values in humantime format (<code>1s</code>, <code>100ms</code>, <code>2m</code>) for convenience. Only CircuitBreakerConfig’s <code>timeout</code> field explicitly requires humantime format via serde attribute - other Duration fields use standard serialization but accept humantime strings.</p>
</blockquote>
<h3 id="error-metrics"><a class="header" href="#error-metrics">Error Metrics</a></h3>
<p>Prodigy automatically tracks error metrics for MapReduce jobs using the <code>ErrorMetrics</code> structure:</p>
<p><strong>Available Fields:</strong></p>
<ul>
<li><code>total_items</code> - Total number of work items processed</li>
<li><code>successful</code> - Number of items that completed successfully</li>
<li><code>failed</code> - Number of items that failed</li>
<li><code>skipped</code> - Number of items that were skipped</li>
<li><code>failure_rate</code> - Percentage of failures (0.0 to 1.0)</li>
<li><code>error_types</code> - Map of error types to their frequency counts</li>
<li><code>failure_patterns</code> - Detected recurring error patterns with suggested remediation</li>
</ul>
<p><strong>Accessing Metrics:</strong></p>
<p>Access metrics during execution or after completion to understand job health:</p>
<pre><code class="language-yaml"># In your reduce phase
reduce:
  - shell: "echo 'Processed ${map.successful}/${map.total} items'"
  - shell: "echo 'Failure rate: ${map.failure_rate}'"
</code></pre>
<p>You can also access metrics programmatically via the Prodigy API or through CLI commands like <code>prodigy events</code> to view detailed error statistics.</p>
<p><strong>Pattern Detection:</strong></p>
<p>Prodigy automatically detects recurring error patterns when an error type occurs 3 or more times. The <code>failure_patterns</code> field includes suggested remediation actions:</p>
<ul>
<li><strong>Timeout errors</strong> → “Consider increasing agent_timeout_secs”</li>
<li><strong>Network errors</strong> → “Check network connectivity and retry configuration”</li>
<li><strong>Permission errors</strong> → “Verify file permissions and access rights”</li>
</ul>
<p><strong>Note:</strong> Other error types receive a generic suggestion: “Review error logs for more details.”</p>
<p>These suggestions help diagnose and resolve systemic issues in MapReduce jobs.</p>
<hr />
<h2 id="dead-letter-queue-dlq-1"><a class="header" href="#dead-letter-queue-dlq-1">Dead Letter Queue (DLQ)</a></h2>
<p>The Dead Letter Queue stores failed work items from MapReduce jobs for later retry or analysis. This is only available for MapReduce workflows, not regular workflows.</p>
<h3 id="sending-items-to-dlq"><a class="header" href="#sending-items-to-dlq">Sending Items to DLQ</a></h3>
<p>Configure your MapReduce workflow to use DLQ:</p>
<pre><code class="language-yaml">mode: mapreduce
error_policy:
  on_item_failure: dlq
</code></pre>
<p>Failed items are automatically sent to the DLQ with:</p>
<ul>
<li>Original work item data</li>
<li>Failure reason and error message</li>
<li>Timestamp of failure</li>
<li>Attempt history</li>
</ul>
<h3 id="retrying-failed-items"><a class="header" href="#retrying-failed-items">Retrying Failed Items</a></h3>
<p>Use the CLI to retry failed items:</p>
<pre><code class="language-bash"># Retry all failed items for a job
prodigy dlq retry &lt;job_id&gt;

# Retry with custom parallelism (default: 5)
prodigy dlq retry &lt;job_id&gt; --max-parallel 10

# Dry run to see what would be retried
prodigy dlq retry &lt;job_id&gt; --dry-run
</code></pre>
<p><strong>DLQ Retry Features:</strong></p>
<ul>
<li>Streams items to avoid memory issues with large queues</li>
<li>Respects original workflow’s max_parallel setting (unless overridden)</li>
<li>Preserves correlation IDs for tracking</li>
<li>Updates DLQ state (removes successful, keeps failed)</li>
<li>Supports interruption and resumption</li>
<li>Shared across worktrees for centralized failure tracking</li>
</ul>
<h3 id="dlq-storage-1"><a class="header" href="#dlq-storage-1">DLQ Storage</a></h3>
<p>DLQ data is stored in:</p>
<pre><code>~/.prodigy/dlq/{repo_name}/{job_id}/
</code></pre>
<p>This centralized storage allows multiple worktrees to share the same DLQ.</p>
<hr />
<h2 id="best-practices-7"><a class="header" href="#best-practices-7">Best Practices</a></h2>
<h3 id="choosing-the-right-error-handling-level"><a class="header" href="#choosing-the-right-error-handling-level">Choosing the Right Error Handling Level</a></h3>
<p>Understanding when to use command-level versus workflow-level error handling is crucial for building robust workflows.</p>
<div class="table-wrapper"><table><thead><tr><th><strong>Aspect</strong></th><th><strong>Command-Level (<code>on_failure</code>)</strong></th><th><strong>Workflow-Level (<code>error_policy</code>)</strong></th></tr></thead><tbody>
<tr><td><strong>Scope</strong></td><td>Single workflow step</td><td>Entire MapReduce job</td></tr>
<tr><td><strong>Availability</strong></td><td>All workflow modes</td><td>MapReduce mode only</td></tr>
<tr><td><strong>Use Case</strong></td><td>Step-specific recovery logic</td><td>Consistent handling across all items</td></tr>
<tr><td><strong>Retry Control</strong></td><td>Per-command retry with <code>max_attempts</code></td><td>Per-item retry with backoff strategies</td></tr>
<tr><td><strong>Failure Action</strong></td><td>Custom handler commands</td><td>DLQ, retry, skip, or stop</td></tr>
<tr><td><strong>Circuit Breaker</strong></td><td>Not available</td><td>Available with configurable thresholds</td></tr>
<tr><td><strong>Best For</strong></td><td>Targeted recovery, cleanup, notifications</td><td>Batch processing, rate limiting, cascading failure prevention</td></tr>
</tbody></table>
</div>
<h3 id="when-to-use-command-level-error-handling"><a class="header" href="#when-to-use-command-level-error-handling">When to Use Command-Level Error Handling</a></h3>
<ul>
<li><strong>Recovery:</strong> Use <code>on_failure</code> to fix issues and retry (e.g., clearing cache before reinstalling)</li>
<li><strong>Cleanup:</strong> Use <code>strategy: cleanup</code> to clean up resources after failures</li>
<li><strong>Fallback:</strong> Use <code>strategy: fallback</code> for alternative approaches</li>
<li><strong>Notifications:</strong> Use handler commands to notify teams of failures</li>
<li><strong>Step-Specific Logic:</strong> When different steps need different error handling strategies</li>
</ul>
<h3 id="when-to-use-workflow-level-error-policy"><a class="header" href="#when-to-use-workflow-level-error-policy">When to Use Workflow-Level Error Policy</a></h3>
<ul>
<li><strong>MapReduce jobs:</strong> Use error_policy for consistent failure handling across all work items</li>
<li><strong>Failure thresholds:</strong> Use max_failures or failure_threshold to prevent runaway jobs</li>
<li><strong>Circuit breakers:</strong> Use when external dependencies might fail cascading</li>
<li><strong>DLQ:</strong> Use for large batch jobs where you want to retry failures separately</li>
<li><strong>Rate Limiting:</strong> Use backoff strategies to avoid overwhelming external services</li>
<li><strong>Batch Processing:</strong> When processing hundreds or thousands of items with similar error patterns</li>
</ul>
<h3 id="error-information-available"><a class="header" href="#error-information-available">Error Information Available</a></h3>
<p>When a command fails, you can access error information in handler commands:</p>
<pre><code class="language-yaml">- shell: "risky-command"
  on_failure:
    claude: "/analyze-error ${shell.output}"
</code></pre>
<p>The <code>${shell.output}</code> variable contains the command’s stdout/stderr output.</p>
<h3 id="common-patterns-2"><a class="header" href="#common-patterns-2">Common Patterns</a></h3>
<p><strong>Cleanup and Retry:</strong></p>
<pre><code class="language-yaml">- shell: "npm install"
  on_failure:
    - "npm cache clean --force"
    - "rm -rf node_modules"
    - "npm install"
</code></pre>
<p><strong>Conditional Recovery:</strong></p>
<pre><code class="language-yaml">- shell: "cargo test"
  on_failure:
    claude: "/fix-failing-tests"
  max_attempts: 3
  fail_workflow: false
</code></pre>
<p><strong>Critical Step with Notification:</strong></p>
<pre><code class="language-yaml">- shell: "deploy-production"
  on_failure:
    commands:
      - shell: "rollback-deployment"
      - shell: "notify-team 'Deployment failed'"
    fail_workflow: true   # Still fail workflow after cleanup
</code></pre>
<p><strong>Combined Error Handling Strategies (MapReduce):</strong></p>
<p>For complex MapReduce workflows, combine multiple error handling features:</p>
<pre><code class="language-yaml"># Process API endpoints with comprehensive error handling
mode: mapreduce
error_policy:
  on_item_failure: retry          # Try immediate retry first
  continue_on_failure: true       # Don't stop entire job
  max_failures: 50                # Stop if too many failures
  failure_threshold: 0.15         # Stop if 15% failure rate

  # Retry with exponential backoff
  retry_config:
    max_attempts: 3
    backoff:
      type: exponential
      initial: 2s
      multiplier: 2

  # Protect against cascading failures
  circuit_breaker:
    failure_threshold: 10
    success_threshold: 3
    timeout: 60s
    half_open_requests: 5

  error_collection: batched
  batch_size: 10                  # Report errors in batches

map:
  agent_template:
    - claude: "/process-endpoint ${item.path}"
      on_failure:
        # Item-level recovery before workflow-level retry
        claude: "/diagnose-api-error ${shell.output}"
        max_attempts: 2
</code></pre>
<p>This configuration provides multiple layers of protection:</p>
<ol>
<li>Item-level error handlers for immediate recovery attempts</li>
<li>Automatic retry with exponential backoff for transient failures</li>
<li>Circuit breaker to prevent overwhelming failing dependencies</li>
<li>Failure thresholds to stop runaway jobs early</li>
<li>Batched error reporting to reduce noise</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mapreduce-worktree-architecture"><a class="header" href="#mapreduce-worktree-architecture">MapReduce Worktree Architecture</a></h1>
<p>MapReduce workflows in Prodigy use an isolated git worktree architecture that ensures the main repository remains untouched during workflow execution. This chapter explains the worktree hierarchy, branch naming conventions, merge flows, and debugging strategies.</p>
<h2 id="overview-5"><a class="header" href="#overview-5">Overview</a></h2>
<p>When you run a MapReduce workflow, Prodigy creates a hierarchical worktree structure:</p>
<pre><code>Main Repository (untouched during execution)
    ↓
Parent Worktree (session-mapreduce-{id})
    ├── Setup Phase → Executes here
    ├── Reduce Phase → Executes here
    └── Map Phase → Each agent in child worktree
        ├── Child Worktree (mapreduce-agent-{id})
        ├── Child Worktree (mapreduce-agent-{id})
        └── Child Worktree (mapreduce-agent-{id})
</code></pre>
<p>This architecture provides complete isolation, allowing parallel agents to work independently while preserving a clean main repository.</p>
<h2 id="worktree-hierarchy"><a class="header" href="#worktree-hierarchy">Worktree Hierarchy</a></h2>
<h3 id="parent-worktree"><a class="header" href="#parent-worktree">Parent Worktree</a></h3>
<p>Created at the start of MapReduce workflow execution:</p>
<p><strong>Location</strong>: <code>~/.prodigy/worktrees/{project}/session-mapreduce-{timestamp}</code> (or anonymous worktree path if session_id not specified)</p>
<p><strong>Purpose</strong>:</p>
<ul>
<li>Isolates all workflow execution from main repository</li>
<li>Hosts setup phase execution</li>
<li>Hosts reduce phase execution</li>
<li>Serves as merge target for agent results</li>
</ul>
<p><strong>Branch</strong>: <code>prodigy-session-mapreduce-{timestamp}</code></p>
<p><strong>Note</strong>: MapReduce coordinators typically create named session worktrees, but individual agents may use anonymous worktrees from the pool if no session context is provided.</p>
<h3 id="child-worktrees"><a class="header" href="#child-worktrees">Child Worktrees</a></h3>
<p>Created for each map agent:</p>
<p><strong>Location</strong>: <code>~/.prodigy/worktrees/{project}/mapreduce-agent-{agent_id}</code></p>
<p><strong>Purpose</strong>:</p>
<ul>
<li>Complete isolation per agent</li>
<li>Independent failure handling</li>
<li>Parallel execution safety</li>
</ul>
<p><strong>Branch</strong>: <code>prodigy-agent-{session_id}-{item_id}</code> (branched from parent worktree)</p>
<p><strong>Note</strong>: The <code>agent_id</code> in the location path encodes the work item information. Agent worktrees are created dynamically as map agents execute.</p>
<h2 id="branch-naming-conventions"><a class="header" href="#branch-naming-conventions">Branch Naming Conventions</a></h2>
<p>Prodigy uses consistent branch naming to track worktree relationships:</p>
<h3 id="parent-worktree-branch"><a class="header" href="#parent-worktree-branch">Parent Worktree Branch</a></h3>
<p>Format: <code>prodigy-session-mapreduce-YYYYMMDD_HHMMSS</code></p>
<p>Example: <code>prodigy-session-mapreduce-20250112_143052</code></p>
<h3 id="agent-worktree-branch"><a class="header" href="#agent-worktree-branch">Agent Worktree Branch</a></h3>
<p>Format: <code>prodigy-agent-{session_id}-{item_id}</code></p>
<p>Example: <code>prodigy-agent-session-abc123-xyz456</code></p>
<p><strong>Components</strong>:</p>
<ul>
<li><code>session_id</code>: MapReduce agent session identifier</li>
<li><code>item_id</code>: Work item identifier from the map phase</li>
</ul>
<h2 id="merge-flow"><a class="header" href="#merge-flow">Merge Flow</a></h2>
<p>MapReduce workflows involve multiple merge operations to aggregate results:</p>
<h3 id="1-agent-merge-child--parent"><a class="header" href="#1-agent-merge-child--parent">1. Agent Merge (Child → Parent)</a></h3>
<p>When an agent completes successfully:</p>
<pre><code>Child Worktree (agent branch)
    ↓ merge
Parent Worktree (session branch)
</code></pre>
<p><strong>Process</strong>:</p>
<ol>
<li>Agent completes all commands successfully</li>
<li>Agent commits changes to its branch</li>
<li>Merge coordinator adds agent to merge queue</li>
<li>Sequential merge into parent worktree branch</li>
<li>Child worktree cleanup</li>
</ol>
<h3 id="2-mapreduce-to-parent-merge"><a class="header" href="#2-mapreduce-to-parent-merge">2. MapReduce to Parent Merge</a></h3>
<p>After all map agents complete and reduce phase finishes:</p>
<pre><code>Parent Worktree (session branch)
    ↓ merge
Main Repository (original branch)
</code></pre>
<p><strong>Process</strong>:</p>
<ol>
<li>All agents merged into parent worktree</li>
<li>Reduce phase executes in parent worktree</li>
<li>User confirms merge to main repository</li>
<li>Sequential merge with conflict detection</li>
<li>Parent worktree cleanup</li>
</ol>
<h3 id="merge-strategies"><a class="header" href="#merge-strategies">Merge Strategies</a></h3>
<p><strong>Fast-Forward When Possible</strong>: If no divergence, use fast-forward merge</p>
<p><strong>Three-Way Merge</strong>: When branches have diverged, perform three-way merge</p>
<p><strong>Conflict Handling</strong>: Stop and report conflicts for manual resolution</p>
<h2 id="agent-merge-details"><a class="header" href="#agent-merge-details">Agent Merge Details</a></h2>
<h3 id="merge-queue"><a class="header" href="#merge-queue">Merge Queue</a></h3>
<p>Agents are added to a merge queue as they complete:</p>
<p><strong>Queue Architecture</strong>: Merge queue is managed in-memory by a background worker task. Merge requests are processed sequentially via an unbounded channel, eliminating MERGE_HEAD race conditions. Queue state is not persisted - merge operations are atomic.</p>
<p><strong>Queue Processing</strong>: Queue processes <code>MergeRequest</code> objects containing:</p>
<ul>
<li><code>agent_id</code>: Unique agent identifier</li>
<li><code>branch_name</code>: Agent’s git branch to merge</li>
<li><code>item_id</code>: Work item identifier for correlation</li>
<li><code>env</code>: Execution environment context (variables, secrets)</li>
</ul>
<p>Merge requests are processed FIFO with automatic conflict detection.</p>
<h3 id="sequential-merge-processing"><a class="header" href="#sequential-merge-processing">Sequential Merge Processing</a></h3>
<p>Merges are processed sequentially to prevent conflicts:</p>
<ol>
<li>Lock merge queue</li>
<li>Take next agent from pending queue</li>
<li>Perform merge into parent worktree</li>
<li>Update queue (move to merged or failed)</li>
<li>Release lock</li>
</ol>
<h3 id="automatic-conflict-resolution"><a class="header" href="#automatic-conflict-resolution">Automatic Conflict Resolution</a></h3>
<p>If a standard git merge fails with conflicts, the merge queue automatically invokes Claude using the <code>/prodigy-merge-worktree</code> command to resolve conflicts intelligently:</p>
<p><strong>Conflict Resolution Flow</strong>:</p>
<ol>
<li>Standard git merge attempted</li>
<li>If conflicts detected, invoke Claude with <code>/prodigy-merge-worktree {branch_name}</code></li>
<li>Claude analyzes conflicts and attempts resolution</li>
<li>If Claude succeeds, merge completes automatically</li>
<li>If Claude fails, agent is marked as failed and added to DLQ</li>
</ol>
<p><strong>Benefits</strong>:</p>
<ul>
<li>Reduces manual merge conflict resolution overhead</li>
<li>Handles common conflict patterns automatically</li>
<li>Preserves full context for debugging via Claude logs</li>
<li>Falls back gracefully to DLQ for complex conflicts</li>
</ul>
<p>This automatic conflict resolution is especially useful when multiple agents modify overlapping code areas.</p>
<h2 id="parent-to-master-merge"><a class="header" href="#parent-to-master-merge">Parent to Master Merge</a></h2>
<h3 id="merge-confirmation"><a class="header" href="#merge-confirmation">Merge Confirmation</a></h3>
<p>After reduce phase completes, Prodigy prompts for merge confirmation:</p>
<pre><code>✓ MapReduce workflow completed successfully

Merge session-mapreduce-20250112_143052 to master? [y/N]
</code></pre>
<h3 id="custom-merge-workflows"><a class="header" href="#custom-merge-workflows">Custom Merge Workflows</a></h3>
<p>Configure custom merge validation:</p>
<pre><code class="language-yaml">merge:
  - shell: "git fetch origin"
  - shell: "cargo test"
  - shell: "cargo clippy"
  - claude: "/prodigy-merge-worktree ${merge.source_branch}"
</code></pre>
<h3 id="merge-variables-1"><a class="header" href="#merge-variables-1">Merge Variables</a></h3>
<p>Available during merge workflows:</p>
<ul>
<li><code>${merge.worktree}</code> - Worktree name</li>
<li><code>${merge.source_branch}</code> - Session branch name</li>
<li><code>${merge.target_branch}</code> - Main repository branch (usually master/main)</li>
<li><code>${merge.session_id}</code> - Session ID for correlation</li>
</ul>
<h2 id="debugging-mapreduce-worktrees"><a class="header" href="#debugging-mapreduce-worktrees">Debugging MapReduce Worktrees</a></h2>
<h3 id="inspecting-worktree-state"><a class="header" href="#inspecting-worktree-state">Inspecting Worktree State</a></h3>
<pre><code class="language-bash"># List all worktrees
git worktree list

# View worktree details
cd ~/.prodigy/worktrees/{project}/session-mapreduce-*
git status
git log

# View agent worktree
cd ~/.prodigy/worktrees/{project}/agent-*
git log --oneline
</code></pre>
<h3 id="common-debugging-scenarios"><a class="header" href="#common-debugging-scenarios">Common Debugging Scenarios</a></h3>
<p><strong>Agent Failed to Merge:</strong></p>
<ol>
<li>Check DLQ for failure details: <code>prodigy dlq show {job_id}</code></li>
<li>Inspect failed agent worktree: <code>cd ~/.prodigy/worktrees/{project}/mapreduce-agent-*</code></li>
<li>Review agent changes: <code>git diff master</code></li>
<li>Check for conflicts: <code>git status</code></li>
<li>Review Claude merge logs if conflict resolution was attempted</li>
</ol>
<p><strong>Parent Worktree Not Merging:</strong></p>
<ol>
<li>Check parent worktree: <code>cd ~/.prodigy/worktrees/{project}/session-mapreduce-*</code></li>
<li>Verify all agents merged: <code>git log --oneline</code></li>
<li>Check for uncommitted changes: <code>git status</code></li>
<li>Review merge history: <code>git log --graph --oneline --all</code></li>
</ol>
<h3 id="merge-conflict-resolution"><a class="header" href="#merge-conflict-resolution">Merge Conflict Resolution</a></h3>
<p>If merge conflicts occur:</p>
<pre><code class="language-bash"># Navigate to parent worktree
cd ~/.prodigy/worktrees/{project}/session-mapreduce-*

# View conflicts
git status

# Resolve manually
vim &lt;conflicted-file&gt;

# Complete merge
git add &lt;conflicted-file&gt;
git commit
</code></pre>
<h2 id="verification-commands"><a class="header" href="#verification-commands">Verification Commands</a></h2>
<h3 id="verify-main-repository-is-clean"><a class="header" href="#verify-main-repository-is-clean">Verify Main Repository is Clean</a></h3>
<pre><code class="language-bash"># Main repository should have no changes from MapReduce execution
git status
# Expected: nothing to commit, working tree clean
</code></pre>
<h3 id="verify-worktree-isolation"><a class="header" href="#verify-worktree-isolation">Verify Worktree Isolation</a></h3>
<pre><code class="language-bash"># Check that parent worktree has changes
cd ~/.prodigy/worktrees/{project}/session-mapreduce-*
git status
git log --oneline

# Main repository should still be clean
cd /path/to/main/repo
git status
</code></pre>
<h3 id="verify-agent-merges"><a class="header" href="#verify-agent-merges">Verify Agent Merges</a></h3>
<pre><code class="language-bash"># Check for merge events
prodigy events {job_id}

# Verify merged agents in parent worktree
cd ~/.prodigy/worktrees/{project}/session-mapreduce-*
git log --oneline | grep "Merge"
</code></pre>
<h2 id="best-practices-8"><a class="header" href="#best-practices-8">Best Practices</a></h2>
<h3 id="worktree-management"><a class="header" href="#worktree-management">Worktree Management</a></h3>
<ul>
<li><strong>Cleanup</strong>: Remove old worktrees after successful merge: <code>prodigy worktree clean</code></li>
<li><strong>Monitoring</strong>: Check worktree disk usage periodically</li>
<li><strong>Inspection</strong>: Review worktrees before deleting to verify results</li>
</ul>
<h3 id="merge-workflows-1"><a class="header" href="#merge-workflows-1">Merge Workflows</a></h3>
<ul>
<li><strong>Test Before Merge</strong>: Run tests in merge workflow to catch issues</li>
<li><strong>Sync Upstream</strong>: Fetch and merge origin/main before merging to main</li>
<li><strong>Conflict Prevention</strong>: Keep MapReduce jobs focused to minimize conflicts</li>
</ul>
<h3 id="debugging"><a class="header" href="#debugging">Debugging</a></h3>
<ul>
<li><strong>Preserve Worktrees</strong>: Don’t delete worktrees until debugging is complete</li>
<li><strong>Event Logs</strong>: Review event logs for merge failures: <code>prodigy events {job_id}</code></li>
<li><strong>DLQ Review</strong>: Check failed items that might indicate merge issues</li>
</ul>
<h2 id="troubleshooting-5"><a class="header" href="#troubleshooting-5">Troubleshooting</a></h2>
<h3 id="worktree-creation-fails"><a class="header" href="#worktree-creation-fails">Worktree Creation Fails</a></h3>
<p><strong>Issue</strong>: Cannot create parent or child worktree
<strong>Solution</strong>: Check disk space, verify git repository is valid, ensure no existing worktree with same name</p>
<h3 id="agent-merge-fails"><a class="header" href="#agent-merge-fails">Agent Merge Fails</a></h3>
<p><strong>Issue</strong>: Agent results fail to merge into parent
<strong>Solution</strong>: Check merge queue, inspect agent worktree for conflicts, review agent changes</p>
<h3 id="parent-merge-conflicts"><a class="header" href="#parent-merge-conflicts">Parent Merge Conflicts</a></h3>
<p><strong>Issue</strong>: Merging parent worktree to main causes conflicts
<strong>Solution</strong>: Resolve conflicts manually, consider rebasing parent worktree on latest main</p>
<h3 id="orphaned-worktrees"><a class="header" href="#orphaned-worktrees">Orphaned Worktrees</a></h3>
<p><strong>Issue</strong>: Worktrees remain after workflow completion
<strong>Solution</strong>: Use <code>prodigy worktree clean</code> to remove old worktrees, or manually remove with <code>git worktree remove</code></p>
<h2 id="see-also-2"><a class="header" href="#see-also-2">See Also</a></h2>
<ul>
<li><a href="mapreduce.html">MapReduce Workflows</a> - MapReduce workflow basics</li>
<li><a href="error-handling.html">Error Handling</a> - Handling merge failures</li>
<li><a href="troubleshooting.html">Troubleshooting</a> - General troubleshooting guide</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="automated-documentation-with-mdbook"><a class="header" href="#automated-documentation-with-mdbook">Automated Documentation with mdBook</a></h1>
<p>This guide shows you how to set up automated, always-up-to-date documentation for any project using Prodigy’s book workflow system. This same system maintains the documentation you’re reading right now.</p>
<h2 id="overview-6"><a class="header" href="#overview-6">Overview</a></h2>
<p>The book workflow system:</p>
<ul>
<li><strong>Analyzes your codebase</strong> to build a feature inventory</li>
<li><strong>Detects documentation drift</strong> by comparing docs to implementation</li>
<li><strong>Updates documentation</strong> automatically using Claude</li>
<li><strong>Maintains consistency</strong> across all chapters</li>
<li><strong>Runs on any project</strong> - just configure and go</li>
</ul>
<p>The generalized commands work for any codebase: Rust, Python, JavaScript, etc.</p>
<h2 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h2>
<ol>
<li>
<p><strong>Install Prodigy</strong>:</p>
<pre><code class="language-bash">cargo install prodigy
</code></pre>
</li>
<li>
<p><strong>Install mdBook</strong>:</p>
<pre><code class="language-bash">cargo install mdbook
</code></pre>
</li>
<li>
<p><strong>Claude Code CLI</strong> with valid API credentials</p>
</li>
<li>
<p><strong>Git</strong> - Version control system (git 2.25+ recommended) and an initialized git repository for your project</p>
<pre><code class="language-bash"># Verify git is installed
git --version

# Initialize a repository if needed
git init
</code></pre>
</li>
</ol>
<h2 id="quick-start-30-minutes"><a class="header" href="#quick-start-30-minutes">Quick Start (30 Minutes)</a></h2>
<h3 id="step-1-initialize-prodigy-commands"><a class="header" href="#step-1-initialize-prodigy-commands">Step 1: Initialize Prodigy Commands</a></h3>
<p>In your project directory:</p>
<pre><code class="language-bash"># Initialize Prodigy and install book documentation commands
prodigy init
</code></pre>
<p>This creates <code>.claude/commands/</code> with the generalized book commands:</p>
<ul>
<li><code>/prodigy-analyze-features-for-book</code> - Analyze codebase for feature inventory</li>
<li><code>/prodigy-analyze-book-chapter-drift</code> - Detect documentation drift per chapter</li>
<li><code>/prodigy-fix-book-drift</code> - Update chapters to fix drift</li>
<li><code>/prodigy-fix-book-build-errors</code> - Fix mdBook build errors</li>
</ul>
<h3 id="step-2-initialize-mdbook-structure"><a class="header" href="#step-2-initialize-mdbook-structure">Step 2: Initialize mdBook Structure</a></h3>
<pre><code class="language-bash"># Create book directory structure
mdbook init book --title "Your Project Documentation"

# Create workflow and config directories
mkdir -p workflows/data
mkdir -p .myproject  # Or .config, whatever you prefer
</code></pre>
<h3 id="step-3-create-project-configuration"><a class="header" href="#step-3-create-project-configuration">Step 3: Create Project Configuration</a></h3>
<p>Create <code>.myproject/book-config.json</code> (adjust paths and analysis targets for your project):</p>
<pre><code class="language-json">{
  "project_name": "YourProject",
  "project_type": "cli_tool",
  "book_dir": "book",
  "book_src": "book/src",
  "book_build_dir": "book/book",
  "analysis_targets": [
    {
      "area": "cli_commands",
      "source_files": ["src/cli/", "src/commands/"],
      "feature_categories": ["commands", "arguments", "options"]
    },
    {
      "area": "core_features",
      "source_files": ["src/lib.rs", "src/core/"],
      "feature_categories": ["api", "public_functions", "exports"]
    },
    {
      "area": "configuration",
      "source_files": ["src/config/"],
      "feature_categories": ["config_options", "defaults", "validation"]
    }
  ],
  "chapter_file": "workflows/data/chapters.json",
  "custom_analysis": {
    "include_examples": true,
    "include_best_practices": true,
    "include_troubleshooting": true
  }
}
</code></pre>
<p><strong>Key Fields to Customize</strong>:</p>
<ul>
<li><code>project_name</code>: Your project’s name (used in prompts)</li>
<li><code>project_type</code>: <code>cli_tool</code>, <code>library</code>, <code>web_service</code>, etc.</li>
<li><code>analysis_targets</code>: Areas of code to analyze for documentation
<ul>
<li><code>area</code>: Logical grouping name</li>
<li><code>source_files</code>: Paths to analyze (relative to project root)</li>
<li><code>feature_categories</code>: Types of features to extract</li>
</ul>
</li>
</ul>
<h3 id="step-4-define-chapter-structure"><a class="header" href="#step-4-define-chapter-structure">Step 4: Define Chapter Structure</a></h3>
<p>Create <code>workflows/data/chapters.json</code>:</p>
<pre><code class="language-json">{
  "chapters": [
    {
      "id": "getting-started",
      "title": "Getting Started",
      "file": "book/src/getting-started.md",
      "topics": ["Installation", "Quick start", "First steps"],
      "validation": "Check installation instructions and basic usage"
    },
    {
      "id": "user-guide",
      "title": "User Guide",
      "file": "book/src/user-guide.md",
      "topics": ["Core features", "Common workflows", "Examples"],
      "validation": "Verify all main features are documented"
    },
    {
      "id": "configuration",
      "title": "Configuration",
      "file": "book/src/configuration.md",
      "topics": ["Config files", "Options", "Defaults"],
      "validation": "Check config options match implementation"
    },
    {
      "id": "troubleshooting",
      "title": "Troubleshooting",
      "file": "book/src/troubleshooting.md",
      "topics": ["Common issues", "Debug mode", "FAQ"],
      "validation": "Ensure common issues are covered"
    }
  ]
}
</code></pre>
<p><strong>Chapter Definition Fields</strong>:</p>
<ul>
<li><code>id</code>: Unique identifier for the chapter</li>
<li><code>title</code>: Display title in the book</li>
<li><code>file</code>: Path to markdown file (relative to project root)</li>
<li><code>topics</code>: What should be covered in this chapter</li>
<li><code>validation</code>: What Claude should check for accuracy</li>
</ul>
<h3 id="step-5-create-book-configuration"><a class="header" href="#step-5-create-book-configuration">Step 5: Create Book Configuration</a></h3>
<p>Edit <code>book/book.toml</code>:</p>
<pre><code class="language-toml">[book]
title = "Your Project Documentation"
authors = ["Your Team"]
description = "Comprehensive guide to Your Project"
src = "src"
language = "en"

[build]
build-dir = "book"
create-missing = false

[output.html]
default-theme = "rust"
preferred-dark-theme = "navy"
smart-punctuation = true
git-repository-url = "https://github.com/yourorg/yourproject"
git-repository-icon = "fa-github"
edit-url-template = "https://github.com/yourorg/yourproject/edit/main/book/{path}"

[output.html.search]
enable = true
limit-results = 30
use-boolean-and = true
boost-title = 2

[output.html.fold]
enable = true
level = 1

[output.html.playground]
editable = false
copyable = true
line-numbers = true
</code></pre>
<h3 id="step-6-create-chapter-files"><a class="header" href="#step-6-create-chapter-files">Step 6: Create Chapter Files</a></h3>
<p>Create placeholder files for each chapter:</p>
<pre><code class="language-bash"># Create initial chapters with basic structure
cat &gt; book/src/getting-started.md &lt;&lt;EOF
# Getting Started

This chapter covers installation and initial setup.

## Installation

TODO: Add installation instructions

## Quick Start

TODO: Add quick start guide
EOF

# Repeat for other chapters...
</code></pre>
<p>Update <code>book/src/SUMMARY.md</code>:</p>
<pre><code class="language-markdown"># Summary

[Introduction](intro.md)

# User Guide

- [Getting Started](getting-started.md)
- [User Guide](user-guide.md)
- [Configuration](configuration.md)

# Reference

- [Troubleshooting](troubleshooting.md)
</code></pre>
<h3 id="step-7-create-the-workflow"><a class="header" href="#step-7-create-the-workflow">Step 7: Create the Workflow</a></h3>
<p>Create <code>workflows/book-docs-drift.yml</code>:</p>
<pre><code class="language-yaml">name: book-docs-drift-detection
mode: mapreduce

env:
  PROJECT_NAME: "YourProject"
  PROJECT_CONFIG: ".myproject/book-config.json"
  FEATURES_PATH: ".myproject/book-analysis/features.json"
  ANALYSIS_DIR: ".myproject/book-analysis"

setup:
  - shell: "mkdir -p ${ANALYSIS_DIR}"

  # Analyze codebase and build feature inventory
  - claude: "/prodigy-analyze-features-for-book --project $PROJECT_NAME --config $PROJECT_CONFIG"

map:
  input: "workflows/data/chapters.json"
  json_path: "$.chapters[*]"

  agent_template:
    # Step 1: Analyze the chapter for drift
    - claude: "/prodigy-analyze-book-chapter-drift --project $PROJECT_NAME --json '${item}' --features $FEATURES_PATH"
      commit_required: true

    # Step 2: Fix the drift in this chapter
    - claude: "/prodigy-fix-chapter-drift --project $PROJECT_NAME --chapter-id ${item.id}"
      commit_required: true

  max_parallel: 3
  agent_timeout_secs: 900  # 15-minute timeout per agent

reduce:
  # Rebuild the book to ensure all chapters compile together
  - shell: "(cd book &amp;&amp; mdbook build)"
    on_failure:
      # Only needed if there are build errors (broken links, etc)
      claude: "/prodigy-fix-book-build-errors --project $PROJECT_NAME"
      commit_required: true

error_policy:
  on_item_failure: dlq
  continue_on_failure: true
  max_failures: 2
  error_collection: aggregate

# Configuration parameters:
# - max_parallel: 3 chapters processed concurrently (can be a number or environment variable like $MAX_WORKERS)
# - agent_timeout_secs: 900 sets a 15-minute timeout per agent (900 seconds = 15 minutes)
#   Timeout configuration supports multiple formats:
#   * Numeric values in seconds (900)
#   * String format with units ("15m", "1h")
#   * Environment variables ($AGENT_TIMEOUT)
#   This prevents any single chapter from hanging the entire workflow
#   Adjust this value based on your expected chapter processing time
#
# Error Policy fields:
# - on_item_failure: dlq - Failed chapters are sent to the Dead Letter Queue for manual review and retry
# - continue_on_failure: true - Workflow continues processing other chapters even if some fail
# - max_failures: 2 - Stop the entire workflow if more than 2 chapters fail (prevents cascading failures)
# - error_collection: aggregate - Collect all errors and report them together at the end

merge:
  commands:
    # Step 1: Clean up temporary analysis files
    - shell: "rm -rf ${ANALYSIS_DIR}"
    # The '|| true' prevents the merge phase from failing if there are no changes to commit
    # (e.g., if cleanup didn't modify any tracked files). This is a safety pattern for optional cleanup steps.
    - shell: "git add -A &amp;&amp; git commit -m 'chore: remove temporary book analysis files for ${PROJECT_NAME}' || true"

    # Step 2: Validate book builds successfully
    - shell: "(cd book &amp;&amp; mdbook build)"

    # Step 3: Fetch latest changes and merge master into worktree
    - shell: "git fetch origin"
    - claude: "/prodigy-merge-master --project ${PROJECT_NAME}"

    # Step 4: Merge worktree back to master
    - claude: "/prodigy-merge-worktree ${merge.source_branch}"
</code></pre>
<p><strong>Workflow Sections</strong>:</p>
<ul>
<li><strong>env</strong>: Environment variables for project-specific configuration</li>
<li><strong>setup</strong>: Initialize analysis directory and build feature inventory</li>
<li><strong>map</strong>: Process each chapter in parallel to detect drift</li>
<li><strong>reduce</strong>: Aggregate results and update documentation</li>
<li><strong>merge</strong>: Cleanup and merge changes back to main branch</li>
</ul>
<p><strong>Key Variables</strong>:</p>
<ul>
<li><code>PROJECT_NAME</code>: Used in prompts and context</li>
<li><code>PROJECT_CONFIG</code>: Path to your book-config.json</li>
<li><code>FEATURES_PATH</code>: Where feature inventory is stored</li>
</ul>
<p><strong>Advanced Map Options</strong>:</p>
<p>Beyond the basic configuration shown above, the map phase supports advanced item processing options:</p>
<ul>
<li><strong><code>filter</code></strong>: Expression to filter items before processing (e.g., <code>'item.priority &gt; 5'</code>) - useful for processing only high-priority items</li>
<li><strong><code>sort_by</code></strong>: Sort items by field with direction (e.g., <code>'item.priority DESC'</code>) - control processing order</li>
<li><strong><code>max_items</code></strong>: Limit number of items to process (useful for testing or partial updates)</li>
<li><strong><code>offset</code></strong>: Skip first N items (pagination support for large item sets)</li>
<li><strong><code>distinct</code></strong>: Deduplicate items by field (ensure uniqueness when item source might have duplicates)</li>
</ul>
<p>Example with advanced options:</p>
<pre><code class="language-yaml">map:
  input: "workflows/data/chapters.json"
  json_path: "$.chapters[*]"
  filter: "item.priority &gt;= 5"  # Only process high-priority chapters
  sort_by: "item.priority DESC"  # Process highest priority first
  max_items: 10                  # Limit to first 10 items

  agent_template:
    - claude: "/prodigy-analyze-book-chapter-drift --project $PROJECT_NAME --json '${item}' --features $FEATURES_PATH"
      commit_required: true
</code></pre>
<p>Note: This workflow uses the modern direct array syntax for <code>agent_template</code> and <code>reduce</code>. The nested <code>{commands: [...]}</code> format is deprecated and will generate warnings. Always prefer the direct array syntax shown above.</p>
<h3 id="step-8-run-the-workflow"><a class="header" href="#step-8-run-the-workflow">Step 8: Run the Workflow</a></h3>
<pre><code class="language-bash"># Run the documentation workflow
prodigy run workflows/book-docs-drift.yml

# The workflow will:
# 1. Setup: Analyze your codebase for features (creates feature inventory)
# 2. Map: For each chapter in parallel:
#    a. Analyze chapter for drift (creates drift report)
#    b. Fix the chapter based on drift report
# 3. Reduce: Build the complete book to verify all chapters work together
# 4. Merge: Clean up temp files and merge changes back to main branch
</code></pre>
<h2 id="understanding-the-workflow"><a class="header" href="#understanding-the-workflow">Understanding the Workflow</a></h2>
<blockquote>
<p><strong>Note</strong>: All workflow phases execute in isolated git worktrees using a parent/child architecture. A single parent worktree hosts setup, reduce, and merge phases, while each map agent runs in a child worktree branched from the parent. Agents automatically merge back to the parent upon completion. The parent is merged to the original branch only with user confirmation at the end. This isolation ensures the main repository remains untouched during execution (Spec 127).</p>
</blockquote>
<h3 id="phase-1-setup---feature-analysis"><a class="header" href="#phase-1-setup---feature-analysis">Phase 1: Setup - Feature Analysis</a></h3>
<p>The setup phase analyzes your codebase and creates a feature inventory:</p>
<pre><code class="language-yaml">setup:
  - shell: "mkdir -p .myproject/book-analysis"
  - claude: "/prodigy-analyze-features-for-book --project $PROJECT_NAME --config $PROJECT_CONFIG"
</code></pre>
<p>This generates <code>.myproject/book-analysis/features.json</code>:</p>
<pre><code class="language-json">{
  "cli_commands": [
    {
      "name": "run",
      "description": "Execute a workflow",
      "arguments": ["workflow_file"],
      "options": ["--resume", "--dry-run"]
    }
  ],
  "api_functions": [
    {
      "name": "execute_workflow",
      "signature": "fn execute_workflow(config: Config) -&gt; Result&lt;()&gt;",
      "purpose": "Main entry point for workflow execution"
    }
  ]
}
</code></pre>
<p><strong>Advanced Setup Options:</strong></p>
<p>The setup phase optionally supports:</p>
<ul>
<li><strong><code>timeout</code></strong>: Phase-level timeout (number in seconds, string with units like “15m”, or environment variable like $SETUP_TIMEOUT)</li>
<li><strong><code>capture_outputs</code></strong>: Capture setup results into variables for use in later phases</li>
</ul>
<p>These are useful for complex setup operations that need explicit time bounds or when setup results need to be referenced in map/reduce phases. For example:</p>
<pre><code class="language-yaml">setup:
  timeout: "10m"  # 10-minute timeout for entire setup phase
  - shell: "mkdir -p .myproject/book-analysis"
  - claude: "/prodigy-analyze-features-for-book --project $PROJECT_NAME --config $PROJECT_CONFIG"
</code></pre>
<h3 id="phase-2-map---chapter-drift-detection-and-fixing"><a class="header" href="#phase-2-map---chapter-drift-detection-and-fixing">Phase 2: Map - Chapter Drift Detection and Fixing</a></h3>
<p><strong>Execution Model</strong>: The map phase processes chapters with controlled parallelism (max_parallel: 3 chapters at a time). For each chapter, two steps execute sequentially in the same isolated agent worktree:</p>
<ol>
<li><strong>Analyze</strong> - Detect drift and create a drift report</li>
<li><strong>Fix</strong> - Read the drift report and update the chapter</li>
</ol>
<p>Each agent runs in its own isolated git worktree (child worktree branched from the parent worktree), allowing multiple chapters to be processed concurrently without interference. When an agent completes successfully, its changes are automatically merged back to the parent worktree using a fast-forward merge. This enables the reduce phase to access all agent results in the parent worktree. This sequential execution within each agent ensures the drift report from step 1 is available to step 2. Meanwhile, multiple chapters are processed in parallel across different agent worktrees.</p>
<pre><code class="language-yaml">map:
  input: "workflows/data/chapters.json"
  json_path: "$.chapters[*]"

  agent_template:
    # Step 1: Analyze the chapter for drift
    - claude: "/prodigy-analyze-book-chapter-drift --project $PROJECT_NAME --json '${item}' --features $FEATURES_PATH"
      commit_required: true

    # Step 2: Fix the drift in this chapter
    - claude: "/prodigy-fix-chapter-drift --project $PROJECT_NAME --chapter-id ${item.id}"
      commit_required: true
</code></pre>
<p><strong>Detailed Breakdown</strong>:</p>
<p><strong>Step 1: Analyze</strong> - For each chapter, Claude:</p>
<ol>
<li>Reads the current chapter content</li>
<li>Compares it to the feature inventory</li>
<li>Identifies missing, outdated, or incorrect information</li>
<li>Creates a drift report (<code>.prodigy/book-analysis/drift-{chapter-id}.json</code>)</li>
</ol>
<p><strong>Step 2: Fix</strong> - Then immediately for the same chapter, Claude:</p>
<ol>
<li>Reads the drift report created in step 1</li>
<li>Updates the chapter file to fix all identified issues</li>
<li>Commits the fixes to the worktree</li>
</ol>
<p>Both steps run sequentially for each chapter, and chapters are processed in parallel.</p>
<p><strong>Why commit_required: true is Critical</strong></p>
<p>Each map agent runs in its own isolated git worktree. The <code>commit_required: true</code> flag ensures the drift report is committed to git in that worktree. This is critical because without the commit, the drift report file created by step 1 would not be accessible to step 2, even though they run sequentially in the same agent worktree. The commit also enables the reduce phase to access drift reports from all agents, as agent worktrees merge back to the parent worktree after completion.</p>
<h3 id="phase-3-reduce---validate-book-build"><a class="header" href="#phase-3-reduce---validate-book-build">Phase 3: Reduce - Validate Book Build</a></h3>
<p>The reduce phase validates that all updated chapters build successfully together:</p>
<pre><code class="language-yaml">reduce:
  # Rebuild the book to ensure all chapters compile together
  - shell: "(cd book &amp;&amp; mdbook build)"
    on_failure:
      # Only needed if there are build errors (broken links, etc)
      claude: "/prodigy-fix-book-build-errors --project $PROJECT_NAME"
      commit_required: true
</code></pre>
<p>Since chapter fixes happen in the map phase, the reduce phase focuses on:</p>
<ol>
<li>Building the complete book with all updated chapters</li>
<li>Detecting any build errors (broken cross-references, invalid links, etc.)</li>
<li>Fixing build errors if they occur (via Claude command)</li>
</ol>
<p>This ensures that all chapters work together correctly after parallel updates.</p>
<p><strong>Optional Reduce Timeout:</strong></p>
<p>The reduce phase optionally supports <code>timeout_secs</code> to limit total reduce execution time. This is useful for workflows where reduce operations might take unpredictably long (e.g., generating large reports). Example:</p>
<pre><code class="language-yaml">reduce:
  timeout_secs: 600  # 10-minute timeout for reduce phase
  - shell: "(cd book &amp;&amp; mdbook build)"
</code></pre>
<h2 id="automatic-gap-detection"><a class="header" href="#automatic-gap-detection">Automatic Gap Detection</a></h2>
<p>The book workflow system includes automatic gap detection to identify missing or incomplete documentation. This ensures your documentation coverage is comprehensive and catches areas that need attention.</p>
<h3 id="how-it-works"><a class="header" href="#how-it-works">How It Works</a></h3>
<p>Gap detection runs as part of the documentation workflow and analyzes your documentation against your codebase to identify:</p>
<ol>
<li><strong>Missing Documentation</strong>: Features, APIs, or commands in the code that aren’t documented</li>
<li><strong>Incomplete Coverage</strong>: Documentation sections that exist but don’t cover all aspects</li>
<li><strong>Structural Issues</strong>: Missing chapters or sections that should exist based on your project structure</li>
</ol>
<p>The gap detection process:</p>
<ol>
<li><strong>Feature Inventory Analysis</strong>: The setup phase creates a complete inventory of your codebase features</li>
<li><strong>Documentation Coverage Analysis</strong>: Each chapter is analyzed to determine what features it documents</li>
<li><strong>Gap Identification</strong>: Missing features are identified by comparing documentation coverage to the feature inventory</li>
<li><strong>Prioritization</strong>: Gaps are assigned severity levels (critical, high, medium, low) based on importance</li>
<li><strong>Reporting</strong>: Gaps are saved to <code>.prodigy/book-analysis/gaps-report.json</code> with details for each issue</li>
</ol>
<h3 id="gap-severity-levels"><a class="header" href="#gap-severity-levels">Gap Severity Levels</a></h3>
<p>Gaps are categorized by severity to help prioritize documentation work:</p>
<ul>
<li>
<p><strong>Critical</strong>: Core functionality or main features that are completely undocumented</p>
<ul>
<li>Example: Primary CLI commands with no usage documentation</li>
<li>Example: Public API functions that are exported but not documented</li>
</ul>
</li>
<li>
<p><strong>High</strong>: Important features or commonly-used functionality with missing documentation</p>
<ul>
<li>Example: Configuration options that affect behavior but aren’t documented</li>
<li>Example: Command-line flags that are widely used but lack examples</li>
</ul>
</li>
<li>
<p><strong>Medium</strong>: Secondary features or less critical areas with incomplete coverage</p>
<ul>
<li>Example: Advanced features that are documented but lack detailed examples</li>
<li>Example: Edge cases or error handling that isn’t fully explained</li>
</ul>
</li>
<li>
<p><strong>Low</strong>: Minor gaps or enhancements that would improve documentation quality</p>
<ul>
<li>Example: Missing troubleshooting tips for uncommon issues</li>
<li>Example: Additional examples that would be helpful but aren’t essential</li>
</ul>
</li>
</ul>
<h3 id="gap-report-format"><a class="header" href="#gap-report-format">Gap Report Format</a></h3>
<p>The gap detection system generates a detailed report at <code>.prodigy/book-analysis/gaps-report.json</code>:</p>
<pre><code class="language-json">{
  "timestamp": "2025-01-15T10:30:00Z",
  "project_name": "YourProject",
  "total_gaps": 5,
  "gaps_by_severity": {
    "critical": 1,
    "high": 2,
    "medium": 1,
    "low": 1
  },
  "gaps": [
    {
      "id": "missing-cli-command-docs",
      "severity": "critical",
      "type": "missing_documentation",
      "description": "CLI command 'process' is not documented",
      "location": "book/src/user-guide.md",
      "affected_feature": {
        "name": "process",
        "type": "cli_command",
        "source": "src/cli/commands/process.rs"
      },
      "suggested_fix": "Add section documenting the 'process' command with usage examples and options",
      "detected_at": "2025-01-15T10:30:00Z"
    }
  ]
}
</code></pre>
<h3 id="customization"><a class="header" href="#customization">Customization</a></h3>
<p>You can customize gap detection behavior in your <code>book-config.json</code>:</p>
<pre><code class="language-json">{
  "project_name": "YourProject",
  "gap_detection": {
    "enabled": true,
    "severity_rules": {
      "undocumented_public_api": "critical",
      "undocumented_cli_command": "critical",
      "missing_examples": "medium",
      "incomplete_troubleshooting": "low"
    },
    "ignore_patterns": [
      "internal_*",
      "test_helpers",
      "deprecated_*"
    ],
    "required_sections": [
      "Installation",
      "Quick Start",
      "Configuration",
      "Troubleshooting"
    ]
  }
}
</code></pre>
<p><strong>Configuration Options</strong>:</p>
<ul>
<li><code>enabled</code>: Toggle gap detection on/off (default: true)</li>
<li><code>severity_rules</code>: Custom rules for assigning severity levels to different gap types</li>
<li><code>ignore_patterns</code>: Feature name patterns to exclude from gap detection</li>
<li><code>required_sections</code>: Section names that must exist in documentation</li>
</ul>
<h3 id="manual-review-recommendations"><a class="header" href="#manual-review-recommendations">Manual Review Recommendations</a></h3>
<p>While gap detection is automatic, manual review is recommended for:</p>
<ol>
<li><strong>Critical and High Severity Gaps</strong>: Review these immediately as they indicate missing core documentation</li>
<li><strong>New Features</strong>: When adding new features to your codebase, check the gap report to ensure they’re documented</li>
<li><strong>After Major Refactoring</strong>: Restructuring code may create new gaps or invalidate existing documentation</li>
<li><strong>Before Releases</strong>: Run gap detection before major releases to ensure complete documentation coverage</li>
<li><strong>Severity Accuracy</strong>: Verify that automatically assigned severity levels match your project’s priorities</li>
</ol>
<p><strong>Best Practices for Manual Review</strong>:</p>
<ul>
<li>Run gap detection regularly (weekly or after significant code changes)</li>
<li>Address critical gaps before merging feature branches</li>
<li>Use gap reports to plan documentation work in sprints</li>
<li>Keep the gaps report file in version control to track progress</li>
<li>Review ignored patterns periodically to ensure they’re still relevant</li>
</ul>
<p><strong>Analyzing the Gaps Report:</strong></p>
<p>Use these commands to query and analyze the gaps report:</p>
<pre><code class="language-bash"># View all detected gaps
cat .prodigy/book-analysis/gaps-report.json | jq '.gaps'

# Filter to show only critical gaps
cat .prodigy/book-analysis/gaps-report.json | jq '.gaps[] | select(.severity == "critical")'

# Count gaps by severity level
cat .prodigy/book-analysis/gaps-report.json | jq '.gaps_by_severity'

# Get gap details for a specific chapter
cat .prodigy/book-analysis/gaps-report.json | jq '.gaps[] | select(.location | contains("user-guide.md"))'

# List all affected features
cat .prodigy/book-analysis/gaps-report.json | jq '.gaps[].affected_feature.name'
</code></pre>
<h3 id="integration-with-drift-detection"><a class="header" href="#integration-with-drift-detection">Integration with Drift Detection</a></h3>
<p>Gap detection complements drift detection in the book workflow:</p>
<ul>
<li><strong>Drift Detection</strong>: Identifies documentation that’s outdated or incorrect compared to current implementation</li>
<li><strong>Gap Detection</strong>: Identifies missing documentation that should exist but doesn’t</li>
</ul>
<p>Together, these systems ensure your documentation is both accurate and complete:</p>
<ol>
<li><strong>Drift Detection</strong> keeps existing documentation synchronized with code changes</li>
<li><strong>Gap Detection</strong> identifies areas where documentation is missing entirely</li>
<li>Both run as part of the same workflow for comprehensive documentation quality</li>
</ol>
<h3 id="phase-4-merge---integration"><a class="header" href="#phase-4-merge---integration">Phase 4: Merge - Integration</a></h3>
<p>The merge phase cleans up and integrates changes:</p>
<pre><code class="language-yaml">merge:
  commands:
    # Step 1: Clean up temporary analysis files
    - shell: "rm -rf ${ANALYSIS_DIR}"
    # The '|| true' prevents the merge phase from failing if there are no changes to commit
    # (e.g., if cleanup didn't modify any tracked files). This is a safety pattern for optional cleanup steps.
    - shell: "git add -A &amp;&amp; git commit -m 'chore: remove temporary book analysis files for ${PROJECT_NAME}' || true"

    # Step 2: Validate book builds successfully
    - shell: "(cd book &amp;&amp; mdbook build)"

    # Step 3: Fetch latest changes and merge master into worktree
    - shell: "git fetch origin"
    - claude: "/prodigy-merge-master --project ${PROJECT_NAME}"

    # Step 4: Merge worktree back to master
    - claude: "/prodigy-merge-worktree ${merge.source_branch}"
</code></pre>
<h2 id="github-actions-integration"><a class="header" href="#github-actions-integration">GitHub Actions Integration</a></h2>
<h3 id="automated-documentation-deployment"><a class="header" href="#automated-documentation-deployment">Automated Documentation Deployment</a></h3>
<p>Create <code>.github/workflows/deploy-docs.yml</code>:</p>
<pre><code class="language-yaml">name: Deploy Documentation

on:
  push:
    branches: [main, master]
    paths:
      - 'book/**'
      - '.github/workflows/deploy-docs.yml'
  pull_request:
    branches: [main, master]
    paths:
      - 'book/**'
      - '.github/workflows/deploy-docs.yml'

jobs:
  build-deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: write  # Required to push to gh-pages branch
    steps:
      - name: Checkout repository
        uses: actions/checkout@v5

      - name: Setup mdBook
        uses: peaceiris/actions-mdbook@v2
        with:
          mdbook-version: 'latest'

      - name: Build book
        run: mdbook build book

      - name: Deploy to GitHub Pages
        if: github.event_name == 'push' &amp;&amp; (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master')
        uses: peaceiris/actions-gh-pages@v4
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./book/book
</code></pre>
<h3 id="periodic-documentation-updates"><a class="header" href="#periodic-documentation-updates">Periodic Documentation Updates</a></h3>
<blockquote>
<p><strong>Note</strong>: Automated documentation updates in CI/CD are not yet fully supported. Claude Code CLI installation and authentication in GitHub Actions is still in development.</p>
<p>For now, run the book workflow manually:</p>
<pre><code class="language-bash">prodigy run workflows/book-docs-drift.yml
</code></pre>
<p>When CI support is added, Prodigy’s json_log_location tracking (Spec 121) will enable debugging Claude commands in CI by capturing detailed JSON logs for each command execution. This will make it easy to troubleshoot documentation updates that fail in CI environments.</p>
<p>Watch the Prodigy and Claude Code documentation for updates on CI integration.</p>
</blockquote>
<h3 id="enable-github-pages"><a class="header" href="#enable-github-pages">Enable GitHub Pages</a></h3>
<ol>
<li>Go to repository Settings → Pages</li>
<li>Source: Deploy from a branch</li>
<li>Branch: <code>gh-pages</code> / <code>root</code></li>
<li>Save</li>
</ol>
<p>Your documentation will be available at: <code>https://yourorg.github.io/yourproject</code></p>
<h2 id="customization-examples"><a class="header" href="#customization-examples">Customization Examples</a></h2>
<h3 id="for-cli-tools"><a class="header" href="#for-cli-tools">For CLI Tools</a></h3>
<p>Focus on commands and usage:</p>
<pre><code class="language-json">{
  "analysis_targets": [
    {
      "area": "cli_commands",
      "source_files": ["src/cli/", "src/commands/"],
      "feature_categories": ["commands", "subcommands", "options", "arguments"]
    },
    {
      "area": "configuration",
      "source_files": ["src/config/"],
      "feature_categories": ["config_file", "environment_vars", "flags"]
    }
  ]
}
</code></pre>
<p>Chapter structure:</p>
<ul>
<li>Installation</li>
<li>Quick Start</li>
<li>Commands Reference</li>
<li>Configuration</li>
<li>Examples</li>
<li>Troubleshooting</li>
</ul>
<h3 id="for-libraries"><a class="header" href="#for-libraries">For Libraries</a></h3>
<p>Focus on API and usage patterns:</p>
<pre><code class="language-json">{
  "analysis_targets": [
    {
      "area": "public_api",
      "source_files": ["src/lib.rs", "src/api/"],
      "feature_categories": ["functions", "types", "traits", "macros"]
    },
    {
      "area": "examples",
      "source_files": ["examples/"],
      "feature_categories": ["use_cases", "patterns", "integrations"]
    }
  ]
}
</code></pre>
<p>Chapter structure:</p>
<ul>
<li>Getting Started</li>
<li>API Reference</li>
<li>Core Concepts</li>
<li>Advanced Usage</li>
<li>Examples</li>
<li>Migration Guides</li>
</ul>
<h3 id="for-web-services"><a class="header" href="#for-web-services">For Web Services</a></h3>
<p>Focus on endpoints and integration:</p>
<pre><code class="language-json">{
  "analysis_targets": [
    {
      "area": "api_endpoints",
      "source_files": ["src/routes/", "src/handlers/"],
      "feature_categories": ["endpoints", "methods", "parameters", "responses"]
    },
    {
      "area": "authentication",
      "source_files": ["src/auth/"],
      "feature_categories": ["auth_methods", "tokens", "permissions"]
    },
    {
      "area": "deployment",
      "source_files": ["deploy/", "docker/"],
      "feature_categories": ["docker", "kubernetes", "configuration"]
    }
  ]
}
</code></pre>
<p>Chapter structure:</p>
<ul>
<li>Overview</li>
<li>Authentication</li>
<li>API Reference</li>
<li>Integration Guide</li>
<li>Deployment</li>
<li>Monitoring</li>
</ul>
<h2 id="best-practices-9"><a class="header" href="#best-practices-9">Best Practices</a></h2>
<h3 id="1-start-with-minimal-chapters"><a class="header" href="#1-start-with-minimal-chapters">1. Start with Minimal Chapters</a></h3>
<p>Don’t try to document everything at once:</p>
<pre><code class="language-json">{
  "chapters": [
    {"id": "intro", "title": "Introduction", ...},
    {"id": "quickstart", "title": "Quick Start", ...},
    {"id": "reference", "title": "Reference", ...}
  ]
}
</code></pre>
<p>Add more chapters as your project grows.</p>
<h3 id="2-focus-analysis-targets"><a class="header" href="#2-focus-analysis-targets">2. Focus Analysis Targets</a></h3>
<p>Be specific about what to analyze:</p>
<pre><code class="language-json">{
  "area": "cli_commands",
  "source_files": ["src/cli/commands/"],  // Specific path
  "feature_categories": ["commands", "options"]  // Specific categories
}
</code></pre>
<p>Overly broad targets create unfocused documentation.</p>
<h3 id="3-provide-chapter-context"><a class="header" href="#3-provide-chapter-context">3. Provide Chapter Context</a></h3>
<p>Give Claude clear guidance on what each chapter should cover:</p>
<pre><code class="language-json">{
  "id": "advanced",
  "title": "Advanced Features",
  "topics": ["Custom plugins", "Scripting", "Automation"],
  "validation": "Check that plugin API and scripting examples are up-to-date"
}
</code></pre>
<h3 id="4-review-initial-output"><a class="header" href="#4-review-initial-output">4. Review Initial Output</a></h3>
<p>The first workflow run will:</p>
<ul>
<li>Identify what’s missing</li>
<li>Add current implementation details</li>
<li>Create a baseline</li>
</ul>
<p>Review and refine before committing.</p>
<h3 id="5-run-regularly"><a class="header" href="#5-run-regularly">5. Run Regularly</a></h3>
<p>Documentation drift happens constantly:</p>
<pre><code class="language-bash"># Run monthly or after major features
prodigy run workflows/book-docs-drift.yml

# Or set up GitHub Actions for automation
</code></pre>
<h3 id="6-use-validation-topics"><a class="header" href="#6-use-validation-topics">6. Use Validation Topics</a></h3>
<p>Specify what Claude should validate:</p>
<pre><code class="language-json">{
  "validation": "Check that all CLI commands in src/cli/commands/ are documented with current options and examples"
}
</code></pre>
<p>This ensures focused, accurate updates.</p>
<h2 id="troubleshooting-6"><a class="header" href="#troubleshooting-6">Troubleshooting</a></h2>
<h3 id="issue-feature-analysis-produces-empty-results"><a class="header" href="#issue-feature-analysis-produces-empty-results">Issue: Feature Analysis Produces Empty Results</a></h3>
<p><strong>Cause</strong>: Analysis targets don’t match your code structure</p>
<p><strong>Solution</strong>: Check that <code>source_files</code> paths exist:</p>
<pre><code class="language-bash">ls -la src/cli/  # Verify paths in analysis_targets
</code></pre>
<p>Adjust paths in <code>book-config.json</code> to match your actual structure.</p>
<h3 id="issue-chapters-not-updating"><a class="header" href="#issue-chapters-not-updating">Issue: Chapters Not Updating</a></h3>
<p><strong>Cause</strong>: Chapter files don’t exist or paths are wrong</p>
<p><strong>Solution</strong>: Verify chapter files exist:</p>
<pre><code class="language-bash"># Check all chapters listed in chapters.json exist
cat workflows/data/chapters.json | jq -r '.chapters[].file' | xargs ls -la
</code></pre>
<h3 id="issue-mdbook-build-fails"><a class="header" href="#issue-mdbook-build-fails">Issue: mdBook Build Fails</a></h3>
<p><strong>Cause</strong>: SUMMARY.md doesn’t match chapter files</p>
<p><strong>Solution</strong>: Ensure all chapters in <code>SUMMARY.md</code> have corresponding files:</p>
<pre><code class="language-bash">cd book &amp;&amp; mdbook build
</code></pre>
<p>Fix any missing files or broken links.</p>
<h3 id="issue-workflow-takes-too-long"><a class="header" href="#issue-workflow-takes-too-long">Issue: Workflow Takes Too Long</a></h3>
<p><strong>Cause</strong>: Too many chapters or overly broad analysis</p>
<p><strong>Solution</strong>:</p>
<ol>
<li>Reduce <code>max_parallel</code> in map phase (default: 3)</li>
<li>Split large chapters into smaller ones</li>
<li>Narrow <code>analysis_targets</code> to essential code paths</li>
</ol>
<h3 id="issue-documentation-quality-issues"><a class="header" href="#issue-documentation-quality-issues">Issue: Documentation Quality Issues</a></h3>
<p><strong>Cause</strong>: Insufficient initial content or unclear validation</p>
<p><strong>Solution</strong>:</p>
<ol>
<li>Create better chapter outlines before running workflow</li>
<li>Add more specific <code>validation</code> criteria in chapters.json</li>
<li>Review and manually refine after first run</li>
</ol>
<h3 id="issue-some-chapters-failed-to-update"><a class="header" href="#issue-some-chapters-failed-to-update">Issue: Some Chapters Failed to Update</a></h3>
<p><strong>Cause</strong>: Chapter processing timeout, Claude error, or validation failure</p>
<p><strong>Solution</strong>: Use the Dead Letter Queue (DLQ) to retry failed chapters:</p>
<pre><code class="language-bash"># View failed chapters
prodigy dlq show &lt;job_id&gt;

# Retry all failed chapters
prodigy dlq retry &lt;job_id&gt;

# Retry with custom parallelism
prodigy dlq retry &lt;job_id&gt; --max-parallel 2

# Dry run to see what would be retried
prodigy dlq retry &lt;job_id&gt; --dry-run
</code></pre>
<p>The DLQ preserves all context from the original failure, making it safe to retry after fixing any underlying issues. Failed items in the DLQ include the <code>json_log_location</code> field pointing to detailed Claude execution logs. Use this to debug exactly what went wrong during chapter processing.</p>
<h2 id="advanced-configuration-1"><a class="header" href="#advanced-configuration-1">Advanced Configuration</a></h2>
<h3 id="custom-analysis-functions"><a class="header" href="#custom-analysis-functions">Custom Analysis Functions</a></h3>
<p>You can extend the analysis by providing custom analysis functions in your config:</p>
<pre><code class="language-json">{
  "custom_analysis": {
    "include_examples": true,
    "include_best_practices": true,
    "include_troubleshooting": true,
    "analyze_dependencies": true,
    "extract_code_comments": true,
    "include_performance_notes": true
  }
}
</code></pre>
<h3 id="multi-language-projects"><a class="header" href="#multi-language-projects">Multi-Language Projects</a></h3>
<p>For projects with multiple languages:</p>
<pre><code class="language-json">{
  "analysis_targets": [
    {
      "area": "rust_backend",
      "source_files": ["src/"],
      "feature_categories": ["api", "services"],
      "language": "rust"
    },
    {
      "area": "typescript_frontend",
      "source_files": ["web/src/"],
      "feature_categories": ["components", "hooks"],
      "language": "typescript"
    }
  ]
}
</code></pre>
<h3 id="chapter-dependencies"><a class="header" href="#chapter-dependencies">Chapter Dependencies</a></h3>
<p>Some chapters may depend on others:</p>
<pre><code class="language-json">{
  "chapters": [
    {
      "id": "basics",
      "title": "Basic Usage",
      "dependencies": []
    },
    {
      "id": "advanced",
      "title": "Advanced Usage",
      "dependencies": ["basics"],
      "validation": "Ensure examples build on concepts from Basic Usage chapter"
    }
  ]
}
</code></pre>
<h2 id="real-world-example-prodigys-own-documentation"><a class="header" href="#real-world-example-prodigys-own-documentation">Real-World Example: Prodigy’s Own Documentation</a></h2>
<p>This documentation you’re reading is maintained by the same workflow described here. You can examine the configuration:</p>
<p><strong>Configuration</strong>: <code>.prodigy/book-config.json</code></p>
<pre><code class="language-json">{
  "project_name": "Prodigy",
  "project_type": "cli_tool",
  "analysis_targets": [
    {
      "area": "workflow_execution",
      "source_files": ["src/workflow/", "src/orchestrator/"],
      "feature_categories": ["workflow_types", "execution_modes", "lifecycle"]
    },
    {
      "area": "mapreduce",
      "source_files": ["src/mapreduce/"],
      "feature_categories": ["map_phase", "reduce_phase", "parallelism"]
    }
  ]
}
</code></pre>
<p><strong>Chapters</strong>: <code>workflows/data/prodigy-chapters.json</code></p>
<pre><code class="language-json">{
  "chapters": [
    {
      "id": "workflow-basics",
      "title": "Workflow Basics",
      "file": "book/src/workflow-basics.md",
      "topics": ["Standard workflows", "Basic structure"],
      "validation": "Check workflow syntax matches current implementation"
    }
  ]
}
</code></pre>
<p><strong>Workflow</strong>: <code>workflows/book-docs-drift.yml</code></p>
<p>Study these files for a complete working example.</p>
<h2 id="next-steps-2"><a class="header" href="#next-steps-2">Next Steps</a></h2>
<ol>
<li><strong>Set up the basics</strong>: Follow the Quick Start to get a minimal book running</li>
<li><strong>Customize for your project</strong>: Adjust analysis targets and chapters</li>
<li><strong>Run the workflow</strong>: Generate your first automated update</li>
<li><strong>Refine iteratively</strong>: Review output and improve configuration</li>
<li><strong>Automate</strong>: Set up GitHub Actions for continuous documentation</li>
<li><strong>Extend</strong>: Add more chapters as your project grows</li>
</ol>
<h2 id="benefits"><a class="header" href="#benefits">Benefits</a></h2>
<p>This approach provides:</p>
<ul>
<li>✅ <strong>Always up-to-date documentation</strong> - Runs automatically to detect drift</li>
<li>✅ <strong>Consistent quality</strong> - Same analysis across all chapters</li>
<li>✅ <strong>Reduced maintenance</strong> - Less manual documentation work</li>
<li>✅ <strong>Accurate examples</strong> - Extracted from actual code</li>
<li>✅ <strong>Version control</strong> - All changes tracked in git</li>
<li>✅ <strong>Easy to customize</strong> - Configuration-based, works for any project</li>
</ul>
<p>The same commands that maintain Prodigy’s documentation can maintain yours.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="examples-1"><a class="header" href="#examples-1">Examples</a></h1>
<h2 id="example-1-simple-build-and-test"><a class="header" href="#example-1-simple-build-and-test">Example 1: Simple Build and Test</a></h2>
<pre><code class="language-yaml">- shell: "cargo build"
- shell: "cargo test"
  on_failure:
    claude: "/fix-failing-tests"
- shell: "cargo clippy"
</code></pre>
<hr />
<h2 id="example-2-coverage-improvement-with-goal-seeking"><a class="header" href="#example-2-coverage-improvement-with-goal-seeking">Example 2: Coverage Improvement with Goal Seeking</a></h2>
<pre><code class="language-yaml">- goal_seek:
    goal: "Achieve 80% test coverage"
    claude: "/improve-coverage"  # Can also use 'shell' for shell commands
    validate: |
      coverage=$(cargo tarpaulin | grep 'Coverage' | sed 's/.*: \([0-9.]*\)%.*/\1/')
      echo "score: ${coverage%.*}"
    threshold: 80
    max_attempts: 5
</code></pre>
<p><strong>Note:</strong> The <code>goal_seek</code> command will automatically commit changes made by the Claude command. Commit behavior is controlled by the command execution, not by the <code>goal_seek</code> configuration.</p>
<hr />
<h2 id="example-3-foreach-iteration"><a class="header" href="#example-3-foreach-iteration">Example 3: Foreach Iteration</a></h2>
<pre><code class="language-yaml"># Test multiple configurations in sequence
- foreach:
    - rust-version: "1.70"
      profile: debug
    - rust-version: "1.71"
      profile: release
    - rust-version: "stable"
      profile: release
  do:
    - shell: "rustup install ${foreach.item.rust-version}"
    - shell: "cargo +${foreach.item.rust-version} build --profile ${foreach.item.profile}"
    - shell: "cargo +${foreach.item.rust-version} test"

# Parallel foreach with error handling
- foreach:
    - "web-service"
    - "api-gateway"
    - "worker-service"
  parallel: 3  # Options: false (sequential), true (default parallelism), or number (specific count)
  continue_on_error: true
  do:
    - shell: "cd services/${foreach.item} &amp;&amp; cargo build"
    - shell: "cd services/${foreach.item} &amp;&amp; cargo test"
      on_failure:
        claude: "/fix-service-tests ${foreach.item}"
</code></pre>
<hr />
<h2 id="example-4-parallel-code-review"><a class="header" href="#example-4-parallel-code-review">Example 4: Parallel Code Review</a></h2>
<pre><code class="language-yaml">name: parallel-code-review
mode: mapreduce

setup:
  - shell: "find src -name '*.rs' &gt; files.txt"
  - shell: "jq -R -s -c 'split(\"\n\") | map(select(length &gt; 0) | {path: .})' files.txt &gt; items.json"

map:
  input: items.json
  json_path: "$[*]"  # Process all items in root array
  agent_template:
    - claude: "/review-file ${item.path}"
      id: "review"
      capture_output: "review_result"
      capture_format: "json"  # Formats: string, json, lines, number, boolean - see Example 5
    - shell: "cargo check ${item.path}"
  max_parallel: 5

reduce:
  - claude: "/summarize-reviews ${map.results}"
</code></pre>
<p><strong>Note:</strong> JSONPath <code>"$[*]"</code> matches all items in the root array. Since the setup phase creates an array of <code>{path: ...}</code> objects, each map agent receives an <code>item</code> object with <code>item.path</code> available for use in commands.</p>
<hr />
<h2 id="example-5-conditional-deployment"><a class="header" href="#example-5-conditional-deployment">Example 5: Conditional Deployment</a></h2>
<pre><code class="language-yaml">- shell: "cargo test --quiet &amp;&amp; echo true || echo false"
  id: "test"
  capture_output: "test_result"  # Canonical field name (alias: 'capture')
  capture_format: "boolean"  # Supported formats explained below
  timeout: 300  # Timeout in seconds (5 minutes)

- shell: "cargo build --release"
  when: "${test_result} == true"

- shell: "docker build -t myapp ."
  when: "${test_result} == true"
  on_success:
    shell: "docker push myapp:latest"
</code></pre>
<p><strong>Note:</strong> <code>capture_format</code> options:</p>
<ul>
<li><code>string</code> - Raw text output (default)</li>
<li><code>json</code> - Parse output as JSON object</li>
<li><code>lines</code> - Split output into array of lines</li>
<li><code>number</code> - Parse output as numeric value</li>
<li><code>boolean</code> - Parse as true/false based on exit code or output text</li>
</ul>
<p><strong>Advanced capture options:</strong></p>
<pre><code class="language-yaml"># Capture specific streams (stdout, stderr, exit_code, success, duration)
- shell: "cargo build 2&gt;&amp;1"
  capture_output: "build_output"
  capture_streams: "stdout,stderr,exit_code"  # Capture multiple streams

# Access captured values
- shell: "echo 'Exit code was ${build_output.exit_code}'"
</code></pre>
<hr />
<h2 id="example-6-multi-step-validation"><a class="header" href="#example-6-multi-step-validation">Example 6: Multi-Step Validation</a></h2>
<pre><code class="language-yaml">- claude: "/implement-feature auth"
  commit_required: true
  validate:
    commands:
      - shell: "cargo test auth"
      - shell: "cargo clippy -- -D warnings"
      - claude: "/validate-implementation --output validation.json"
    result_file: "validation.json"
    threshold: 90
    on_incomplete:
      claude: "/complete-gaps ${validation.gaps}"
      commit_required: true
      max_attempts: 2
</code></pre>
<hr />
<h2 id="example-7-environment-aware-workflow"><a class="header" href="#example-7-environment-aware-workflow">Example 7: Environment-Aware Workflow</a></h2>
<pre><code class="language-yaml"># Global environment variables
env:
  NODE_ENV: production
  API_URL: https://api.production.com

# Secrets (automatically masked in logs)
secrets:
  # Simple format - just the secret value
  API_KEY: "${SECRET_API_KEY}"

  # OR provider format - fetch from external secret store
  # API_KEY:
  #   provider: env
  #   key: SECRET_API_KEY
  #   version: latest  # optional

# Environment profiles for different contexts
# Note: Variables go directly under the profile name, not nested under 'env'
profiles:
  production:
    API_URL: https://api.production.com
    LOG_LEVEL: error

  staging:
    API_URL: https://api.staging.com
    LOG_LEVEL: warn

# Load additional variables from .env files
# Note: Paths are relative to workflow file location
env_files:
  - .env
  - .env.production

# Workflow steps (no 'commands' wrapper in simple format)
- shell: "cargo build --release"

# Use environment variables in commands
- shell: "echo 'Deploying to ${NODE_ENV} at ${API_URL}'"

# Override environment for specific step using env field
- shell: "./deploy.sh"
  env:
    LOG_LEVEL: debug
</code></pre>
<p><strong>Note:</strong> Profiles are activated using the <code>--profile &lt;name&gt;</code> CLI flag when running workflows. For example:</p>
<pre><code class="language-bash"># Use production profile
prodigy run workflow.yml --profile production

# Use staging profile
prodigy run workflow.yml --profile staging
</code></pre>
<p>Variables go directly under the profile name (not nested under ‘env’) because profiles use flattened serialization.</p>
<hr />
<h2 id="example-8-complex-mapreduce-with-error-handling"><a class="header" href="#example-8-complex-mapreduce-with-error-handling">Example 8: Complex MapReduce with Error Handling</a></h2>
<pre><code class="language-yaml">name: tech-debt-elimination
mode: mapreduce

setup:
  - shell: "debtmap analyze . --output debt.json"

map:
  input: debt.json
  json_path: "$.items[*]"
  filter: "item.severity == 'critical'"
  sort_by: "item.priority DESC"
  max_items: 20
  max_parallel: 5

  agent_template:
    - claude: "/fix-debt-item '${item.description}'"
      commit_required: true
    - shell: "cargo test"
      on_failure:
        claude: "/debug-and-fix"

reduce:
  - shell: "debtmap analyze . --output debt-after.json"
  - claude: "/compare-debt-reports --before debt.json --after debt-after.json"

error_policy:
  on_item_failure: dlq  # Default: dlq (failed items to Dead Letter Queue)
  continue_on_failure: true  # Default: true (continue despite failures)
  max_failures: 5  # Optional: stop after N failures
  failure_threshold: 0.3  # Optional: stop if &gt;30% fail
  error_collection: aggregate  # Default: aggregate (Options: aggregate, immediate, batched:{size})
</code></pre>
<p><strong>Note:</strong> The entire <code>error_policy</code> block is optional with sensible defaults. If not specified, failed items go to the Dead Letter Queue (<code>on_item_failure: dlq</code>), workflow continues despite failures (<code>continue_on_failure: true</code>), and errors are aggregated at the end (<code>error_collection: aggregate</code>). Use <code>max_failures</code> or <code>failure_threshold</code> to fail fast if too many items fail.</p>
<hr />
<h2 id="example-9-generating-configuration-files"><a class="header" href="#example-9-generating-configuration-files">Example 9: Generating Configuration Files</a></h2>
<pre><code class="language-yaml"># Generate a JSON configuration file
- write_file:
    path: "config/deployment.json"
    format: json  # Options: text, json, yaml
    create_dirs: true  # Create parent directories if they don't exist
    content:
      environment: production
      api_url: "${API_URL}"
      features:
        - auth
        - analytics
        - notifications
      timeout: 30

# Generate a YAML configuration file
- write_file:
    path: "config/services.yml"
    format: yaml
    content:
      services:
        web:
          image: "myapp:latest"
          ports:
            - "8080:8080"
        database:
          image: "postgres:15"
          environment:
            POSTGRES_DB: "${DB_NAME}"

# Generate a plain text report
- write_file:
    path: "reports/summary.txt"
    format: text
    mode: "0644"  # File permissions (optional)
    content: |
      Deployment Summary
      ==================
      Environment: ${NODE_ENV}
      API URL: ${API_URL}
      Timestamp: $(date)
</code></pre>
<hr />
<h2 id="example-10-advanced-features"><a class="header" href="#example-10-advanced-features">Example 10: Advanced Features</a></h2>
<pre><code class="language-yaml"># Nested error handling with retry configuration
- shell: "cargo build --release"
  on_failure:
    shell: "cargo clean"
    on_success:
      shell: "cargo build --release"
      max_attempts: 2
  on_success:
    shell: "cargo test --release"

# Complex conditional execution with max_attempts
- shell: "cargo test"
  id: "test"
  capture_output: "test_output"

- claude: "/fix-tests"
  when: "${test_output} contains 'FAILED'"
  max_attempts: 3

# Conditional deployment based on test results
- shell: "cargo build --release"
  when: "${test.exit_code} == 0"

# Multi-condition logic
- shell: "./deploy.sh"
  when: "${test_output} contains 'passed' and ${build_output} contains 'Finished'"
</code></pre>
<p><strong>Note:</strong> Advanced features currently supported:</p>
<ul>
<li><strong>Nested handlers</strong>: Chain <code>on_failure</code> and <code>on_success</code> handlers for complex error recovery</li>
<li><strong>Max attempts</strong>: Combine with conditional execution for automatic retry logic</li>
<li><strong>Conditional execution</strong>: Use <code>when</code> clauses with captured output or variables</li>
<li><strong>Complex conditionals</strong>: Combine multiple conditions with <code>and</code>/<code>or</code> operators</li>
<li><strong>Working directory</strong>: Per-command directory control using <code>working_dir</code> field in step environment</li>
</ul>
<p><strong>Example of working_dir usage:</strong></p>
<pre><code class="language-yaml"># Run command in specific directory
- shell: "cargo test"
  working_dir: "subproject/"  # Execute in subproject/ directory

# Or set for multiple commands
- shell: "npm install"
  env:
    NODE_ENV: production
  working_dir: "frontend/"
</code></pre>
<p><strong>Future capabilities</strong> (not yet implemented, but planned):</p>
<ul>
<li><strong>Git context variables</strong>: Access <code>files_modified</code>, <code>files_added</code> from git operations</li>
<li><strong>Pattern filtering</strong>: Filter file lists with <code>:*.rs</code> syntax</li>
<li><strong>Format modifiers</strong>: Advanced output transformation with <code>:json</code>, <code>:lines</code>, <code>:csv</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="troubleshooting-7"><a class="header" href="#troubleshooting-7">Troubleshooting</a></h1>
<h2 id="common-issues"><a class="header" href="#common-issues">Common Issues</a></h2>
<h3 id="1-variables-not-interpolating"><a class="header" href="#1-variables-not-interpolating">1. Variables not interpolating</a></h3>
<p><strong>Symptom:</strong> Variables appear as literal <code>${variable_name}</code> in output instead of their values.</p>
<p><strong>Causes:</strong></p>
<ul>
<li>Incorrect syntax (missing <code>${}</code> wrapper)</li>
<li>Variable not defined or not in scope</li>
<li>Typo in variable name</li>
</ul>
<p><strong>Solutions:</strong></p>
<ul>
<li>Ensure proper <code>${}</code> syntax: <code>${workflow.name}</code>, not <code>$workflow.name</code></li>
<li>Check variable is defined before use</li>
<li>Verify variable is available in current context (e.g., <code>${item.*}</code> only available in map phase)</li>
<li>Use echo to debug: <code>- shell: "echo 'Variable value: ${my_var}'"</code></li>
</ul>
<hr />
<h3 id="2-capture-not-working"><a class="header" href="#2-capture-not-working">2. Capture not working</a></h3>
<p><strong>Symptom:</strong> Captured variables are empty or contain unexpected data.</p>
<p><strong>Causes:</strong></p>
<ul>
<li>Incorrect <code>capture_format</code> for output type</li>
<li>Command output not in expected format</li>
<li>Missing or incorrect <code>capture_streams</code> configuration</li>
</ul>
<p><strong>Solutions:</strong></p>
<ul>
<li>Match <code>capture_format</code> to output type and how it transforms output:
<ul>
<li><code>string</code> - Captures raw text output as-is</li>
<li><code>number</code> - Parses output as numeric value (int or float)</li>
<li><code>json</code> - Parses JSON and allows JSONPath queries on the result</li>
<li><code>lines</code> - Splits multi-line output into an array</li>
<li><code>boolean</code> - Evaluates to true/false based on success status</li>
</ul>
</li>
<li>Test command output manually first</li>
<li>Capture all streams for debugging:
<pre><code class="language-yaml">- shell: "cargo test 2&gt;&amp;1"
  capture: "test_output"
  capture_streams:
    stdout: true      # Optional, default false - Capture standard output
    stderr: true      # Optional, default false - Capture error output
    exit_code: true   # Optional, default false - Capture exit code
    success: true     # Optional, default false - Capture success boolean
    duration: true    # Optional, default false - Capture execution duration
</code></pre>
</li>
<li><strong>Note:</strong> All capture_streams fields are optional and default to false. Only specify the streams you need to capture</li>
</ul>
<hr />
<h3 id="3-validation-failing"><a class="header" href="#3-validation-failing">3. Validation failing</a></h3>
<p><strong>Symptom:</strong> Goal-seeking or validation commands fail to recognize completion.</p>
<p><strong>Causes:</strong></p>
<ul>
<li>Validate command not outputting <code>score: N</code> format</li>
<li>Threshold too high</li>
<li>Score calculation incorrect</li>
<li>Validation command not configured correctly</li>
</ul>
<p><strong>Solutions:</strong></p>
<ul>
<li>Ensure validate command outputs exactly <code>score: N</code> (where N is 0-100)</li>
<li>Validation is part of goal_seek commands with these fields:
<ul>
<li><code>validate</code> - Command that outputs score</li>
<li><code>threshold</code> - Minimum score to consider success (0-100)</li>
<li><code>max_iterations</code> - Maximum attempts before giving up</li>
<li><code>on_incomplete</code> - Commands to run when score below threshold</li>
</ul>
</li>
<li>Test validate command independently</li>
<li>Lower threshold temporarily for debugging</li>
<li>Example correct format:
<pre><code class="language-yaml">- goal_seek:
    validate: |
      result=$(run-checks.sh | grep 'Percentage' | sed 's/.*: \([0-9]*\)%.*/\1/')
      echo "score: $result"
    threshold: 80
    max_iterations: 5
    on_incomplete:
      - claude: "/fix-issues"
</code></pre>
</li>
</ul>
<hr />
<h3 id="4-mapreduce-items-not-found"><a class="header" href="#4-mapreduce-items-not-found">4. MapReduce items not found</a></h3>
<p><strong>Symptom:</strong> Map phase finds zero items or wrong items.</p>
<p><strong>Causes:</strong></p>
<ul>
<li>Incorrect JSONPath expression</li>
<li>Input file format doesn’t match expectations</li>
<li>Input file not generated in setup phase</li>
<li>JSONPath syntax errors</li>
</ul>
<p><strong>Solutions:</strong></p>
<ul>
<li>
<p>Test JSONPath expression with actual data.</p>
<p><strong>⚠️ IMPORTANT: jq uses its own filter syntax, NOT JSONPath!</strong></p>
<pre><code class="language-bash"># jq uses its own syntax (not JSONPath)
jq '.items[]' items.json

# For filtering with jq
jq '.items[] | select(.score &gt;= 5)' items.json

# To test actual JSONPath expressions, use an online JSONPath tester
# or a tool that supports JSONPath directly
</code></pre>
</li>
<li>
<p>Verify input file exists and contains expected structure</p>
</li>
<li>
<p>Check setup phase completed successfully</p>
</li>
<li>
<p>Use simpler JSONPath first: <code>$[*]</code> to get all items</p>
</li>
<li>
<p>Common JSONPath mistakes:</p>
<ul>
<li>Wrong bracket syntax: Use <code>$.items[*]</code> not <code>$.items[]</code></li>
<li>Missing root <code>$</code>: Always start with <code>$</code></li>
<li>Incorrect filter syntax: <code>$[?(@.score &gt;= 5)]</code> for filtering</li>
<li>Nested paths: <code>$.data.items[*].field</code> for deep structures</li>
</ul>
</li>
</ul>
<hr />
<h3 id="5-timeout-errors"><a class="header" href="#5-timeout-errors">5. Timeout errors</a></h3>
<p><strong>Symptom:</strong> Commands or workflows fail with timeout errors.</p>
<p><strong>Causes:</strong></p>
<ul>
<li>Commands take longer than expected</li>
<li>Default timeout too short</li>
<li>Infinite loops or hanging processes</li>
</ul>
<p><strong>Solutions:</strong></p>
<ul>
<li>Increase timeout values using duration strings:
<pre><code class="language-yaml">- shell: "slow-command.sh"
  timeout: "600s"  # or "10m" - uses humantime duration format
</code></pre>
</li>
<li>For MapReduce, increase agent timeout (note: this uses seconds as a number):
<pre><code class="language-yaml">map:
  agent_timeout_secs: 600  # Takes a number (seconds) not a duration string
</code></pre>
</li>
<li><strong>Note:</strong> <code>agent_timeout_secs</code> takes a number (seconds) while most other timeout fields use duration strings like “10m”</li>
<li>Debug hanging commands by running them manually</li>
<li>Add logging to identify slow steps</li>
</ul>
<hr />
<h3 id="6-environment-variables-not-set"><a class="header" href="#6-environment-variables-not-set">6. Environment variables not set</a></h3>
<p><strong>Symptom:</strong> Commands fail because required environment variables are missing.</p>
<p><strong>Causes:</strong></p>
<ul>
<li>Environment not inherited from parent process</li>
<li>Typo in variable name</li>
<li>Profile not activated</li>
<li>Secret not loaded</li>
</ul>
<p><strong>Solutions:</strong></p>
<ul>
<li>Ensure <code>inherit: true</code> in workflow config (default)</li>
<li>Verify profile activation:
<pre><code class="language-yaml">active_profile: "development"
</code></pre>
</li>
<li>Check secrets are properly configured:
<pre><code class="language-yaml">secrets:
  API_KEY: "${env:SECRET_API_KEY}"
</code></pre>
</li>
<li>Debug with: <code>- shell: "env | grep VARIABLE_NAME"</code></li>
</ul>
<hr />
<h3 id="7-merge-workflow-not-running"><a class="header" href="#7-merge-workflow-not-running">7. Merge workflow not running</a></h3>
<p><strong>Symptom:</strong> Custom merge commands not executed when merging worktree.</p>
<p><strong>Causes:</strong></p>
<ul>
<li>Merge block not properly formatted</li>
<li>Syntax error in merge commands</li>
<li>Merge workflow timeout too short</li>
</ul>
<p><strong>Solutions:</strong></p>
<ul>
<li>
<p>Both merge formats are valid - choose based on needs:</p>
<p><strong>Simplified format (direct list of commands):</strong></p>
<pre><code class="language-yaml">merge:
  - shell: "git fetch origin"
  - claude: "/merge-worktree ${merge.source_branch}"
</code></pre>
<p><strong>Full format (with timeout configuration):</strong></p>
<pre><code class="language-yaml">merge:
  commands:
    - shell: "slow-merge-validation.sh"
  timeout: 600  # Timeout in seconds (plain number, not a duration string)
</code></pre>
</li>
<li>
<p>Use the full format when you need to set a custom timeout</p>
</li>
<li>
<p><strong>Note:</strong> Unlike most other timeout fields in Prodigy which use duration strings (“10m”, “600s”), the merge.timeout field takes a plain number representing seconds</p>
</li>
<li>
<p>Check logs for merge execution errors</p>
</li>
</ul>
<hr />
<h3 id="8-commands-failing-without-error-handler"><a class="header" href="#8-commands-failing-without-error-handler">8. Commands failing without error handler</a></h3>
<p><strong>Symptom:</strong> Command fails and workflow stops immediately without recovery.</p>
<p><strong>Causes:</strong></p>
<ul>
<li>No <code>on_failure</code> handler configured</li>
<li>Error not being caught by handler</li>
<li>Handler itself failing</li>
</ul>
<p><strong>Solutions:</strong></p>
<ul>
<li>Add <code>on_failure</code> handler to commands that might fail:
<pre><code class="language-yaml">- shell: "risky-command.sh"
  on_failure:
    - shell: "echo 'Command failed, attempting recovery'"
    - claude: "/fix-issue"
</code></pre>
</li>
<li>Commands without <code>on_failure</code> will stop the workflow on first error</li>
<li>Check that your handler commands don’t also fail</li>
<li>Use shell exit codes to control failure: <code>command || exit 0</code> to ignore failures</li>
</ul>
<hr />
<h3 id="9-error-policy-configuration-issues"><a class="header" href="#9-error-policy-configuration-issues">9. Error policy configuration issues</a></h3>
<p><strong>Symptom:</strong> Retry, backoff, or circuit breaker not working as expected.</p>
<p><strong>Causes:</strong></p>
<ul>
<li>Incorrect Duration format for timeouts</li>
<li>Wrong BackoffStrategy enum variant</li>
<li>Invalid retry_config structure</li>
</ul>
<p><strong>Solutions:</strong></p>
<ul>
<li>Use Duration strings for all timeout values:
<pre><code class="language-yaml">error_policy:
  retry_config:
    max_attempts: 3
    initial_delay: "1s"    # Not 1000
    max_delay: "30s"       # Use duration strings
  circuit_breaker:
    timeout: "60s"         # Not 60
    failure_threshold: 5
</code></pre>
</li>
<li><strong>Backoff Strategy:</strong> Prodigy uses exponential backoff with base 2.0 by default. While you cannot change the backoff algorithm itself (it’s always exponential), you CAN control the backoff behavior through the retry_config parameters:
<ul>
<li><code>max_attempts</code> - Number of retry attempts before giving up</li>
<li><code>initial_delay</code> - Starting delay between retries (e.g., “1s”)</li>
<li><code>max_delay</code> - Maximum delay between retries (e.g., “30s”)</li>
<li>The delay doubles with each retry (exponential backoff with base 2.0) up to max_delay</li>
<li>Example: With <code>initial_delay: "1s"</code> and <code>max_delay: "30s"</code>, retries occur at 1s, 2s, 4s, 8s, 16s, 30s, 30s…</li>
<li>These parameters shape the exponential backoff curve to match your needs - adjust <code>initial_delay</code> to control how aggressively retries start, and <code>max_delay</code> to cap the maximum wait time</li>
</ul>
</li>
<li>Circuit breaker requires both timeout and failure_threshold</li>
</ul>
<hr />
<h3 id="10-claude-output-visibility-issues"><a class="header" href="#10-claude-output-visibility-issues">10. Claude output visibility issues</a></h3>
<p><strong>Symptom:</strong> Can’t see Claude’s streaming output, or seeing too much output when you don’t need it.</p>
<p><strong>Causes:</strong></p>
<ul>
<li>Default verbosity level hides Claude streaming output</li>
<li>Running in CI/CD where streaming output clutters logs</li>
<li>Need to debug Claude interactions but not seeing details</li>
</ul>
<p><strong>Solutions:</strong></p>
<ul>
<li><strong>To see Claude streaming output</strong>, use the <code>-v</code> flag:
<pre><code class="language-bash"># Shows Claude streaming JSON output
prodigy run workflow.yml -v
</code></pre>
</li>
<li><strong>To force streaming output</strong>, set environment variable:
<pre><code class="language-bash"># Forces streaming regardless of verbosity
PRODIGY_CLAUDE_CONSOLE_OUTPUT=true prodigy run workflow.yml
</code></pre>
</li>
<li><strong>To disable streaming in CI/CD</strong>, set environment variable:
<pre><code class="language-bash"># Disables streaming for cleaner logs
PRODIGY_CLAUDE_STREAMING=false prodigy run workflow.yml
</code></pre>
</li>
<li><strong>For more detailed logs</strong>, increase verbosity:
<pre><code class="language-bash">prodigy run workflow.yml -vv   # Debug logs
prodigy run workflow.yml -vvv  # Trace logs
</code></pre>
</li>
</ul>
<p><strong>When to Use Each Mode:</strong></p>
<ul>
<li><strong>Use streaming (default)</strong>: For debugging Claude interactions, maintaining an audit trail, and local development. Streaming logs provide a complete record of Claude’s execution and are valuable for post-execution analysis.</li>
<li><strong>Disable streaming (<code>PRODIGY_CLAUDE_STREAMING=false</code>)</strong>: In CI/CD environments where disk space is constrained, in high-frequency workflows where log storage becomes costly, or when streaming logs aren’t needed for debugging.</li>
</ul>
<p><strong>Streaming logs are saved to:</strong> <code>~/.prodigy/logs/claude-streaming/</code> with format <code>{timestamp}-{uuid}.jsonl</code>. The log path is displayed before execution starts with a 📁 emoji for easy reference. These logs persist after execution for later analysis and debugging.</p>
<hr />
<h2 id="debug-tips"><a class="header" href="#debug-tips">Debug Tips</a></h2>
<h3 id="use-verbosity-flags-for-debugging"><a class="header" href="#use-verbosity-flags-for-debugging">Use verbosity flags for debugging</a></h3>
<p>Prodigy supports multiple verbosity levels for debugging:</p>
<pre><code class="language-bash"># Default: Clean output, no Claude streaming
prodigy run workflow.yml

# -v: Shows Claude streaming JSON output (useful for debugging Claude interactions)
prodigy run workflow.yml -v

# -vv: Adds debug-level logs
prodigy run workflow.yml -vv

# -vvv: Adds trace-level logs (very detailed)
prodigy run workflow.yml -vvv

# Force Claude streaming regardless of verbosity
PRODIGY_CLAUDE_CONSOLE_OUTPUT=true prodigy run workflow.yml
</code></pre>
<h3 id="enable-verbose-output-in-shell-commands"><a class="header" href="#enable-verbose-output-in-shell-commands">Enable verbose output in shell commands</a></h3>
<pre><code class="language-yaml">- shell: "set -x; your-command"
</code></pre>
<h3 id="inspect-variables"><a class="header" href="#inspect-variables">Inspect variables</a></h3>
<pre><code class="language-yaml">- shell: "echo 'Variable value: ${my_var}'"
- shell: "echo 'Item fields: path=${item.path} name=${item.name}'"
</code></pre>
<h3 id="capture-all-streams-for-debugging"><a class="header" href="#capture-all-streams-for-debugging">Capture all streams for debugging</a></h3>
<pre><code class="language-yaml">- shell: "cargo test 2&gt;&amp;1"
  capture: "test_output"
  capture_streams:
    stdout: true
    stderr: true
    exit_code: true
    success: true
    duration: true

# Then inspect
- shell: "echo 'Exit code: ${test_output.exit_code}'"
- shell: "echo 'Success: ${test_output.success}'"
- shell: "echo 'Duration: ${test_output.duration}s'"
</code></pre>
<h3 id="test-jsonpath-expressions"><a class="header" href="#test-jsonpath-expressions">Test JSONPath expressions</a></h3>
<pre><code class="language-bash"># Note: jq uses its own filter syntax, not JSONPath
# Use jq for quick testing with equivalent expressions

# Get all items (equivalent to JSONPath $.items[*])
jq '.items[]' items.json

# Test with filter (equivalent to JSONPath $[?(@.score &gt;= 5)])
jq '.items[] | select(.score &gt;= 5)' items.json

# For actual JSONPath testing, use an online JSONPath tester
# or a tool that supports JSONPath directly
</code></pre>
<h3 id="validate-workflow-syntax"><a class="header" href="#validate-workflow-syntax">Validate workflow syntax</a></h3>
<pre><code class="language-bash"># Workflows are validated automatically when loaded
# Check for syntax errors by attempting to run
prodigy run workflow.yml

# View the validation result file (if workflow validation completed)
cat .prodigy/validation-result.json
</code></pre>
<h3 id="access-claude-json-logs-for-debugging"><a class="header" href="#access-claude-json-logs-for-debugging">Access Claude JSON logs for debugging</a></h3>
<p>Prodigy maintains two types of Claude logs for comprehensive debugging:</p>
<ol>
<li>
<p><strong>Prodigy’s streaming logs</strong> (JSONL format): <code>~/.prodigy/logs/claude-streaming/{timestamp}-{uuid}.jsonl</code></p>
<ul>
<li>Real-time streaming output during command execution</li>
<li>One JSON object per line (JSONL format)</li>
<li>Log path displayed before execution with 📁 emoji</li>
<li>Controlled by <code>PRODIGY_CLAUDE_STREAMING</code> environment variable (default: enabled)</li>
<li>Persists after execution for post-execution analysis</li>
<li>Useful for debugging specific execution patterns and failures</li>
</ul>
</li>
<li>
<p><strong>Claude’s native session logs</strong> (JSON format): <code>~/.local/state/claude/logs/session-{id}.json</code></p>
<ul>
<li>Complete session history created by Claude Code CLI</li>
<li>Full message history and tool invocations</li>
<li>Token usage statistics and error details</li>
<li>Location displayed with verbose mode (<code>-v</code>)</li>
<li>Contains complete conversation context and API interactions</li>
</ul>
</li>
</ol>
<p><strong>When to use each log type:</strong></p>
<ul>
<li><strong>Streaming logs</strong>: Debugging Prodigy-specific issues, tracking real-time execution, analyzing workflow behavior</li>
<li><strong>Native session logs</strong>: Deep debugging of Claude’s reasoning, analyzing tool usage patterns, investigating API errors</li>
</ul>
<p><strong>Primary method - Use the <code>prodigy logs</code> command:</strong></p>
<pre><code class="language-bash"># View the most recent Claude log
prodigy logs --latest

# View with summary of activity
prodigy logs --latest --summary

# Follow the latest log in real-time
prodigy logs --latest --tail

# List recent logs
prodigy logs
</code></pre>
<p><strong>Alternative - Manual inspection:</strong></p>
<p>When using verbose mode (<code>-v</code>), Prodigy displays the location of Claude’s native session logs:</p>
<pre><code class="language-bash">prodigy run workflow.yml -v
# Output: Claude JSON log: ~/.local/state/claude/logs/session-abc123.json
</code></pre>
<p>Claude’s native logs contain:</p>
<ul>
<li>Complete message history (user messages and Claude responses)</li>
<li>All tool invocations with parameters and results</li>
<li>Token usage statistics</li>
<li>Error details and stack traces</li>
</ul>
<p><strong>Advanced analysis with jq:</strong></p>
<pre><code class="language-bash"># View complete message history
cat ~/.local/state/claude/logs/session-abc123.json | jq '.messages'

# Check tool invocations
cat ~/.local/state/claude/logs/session-abc123.json | jq '.messages[].content[] | select(.type == "tool_use")'

# Analyze token usage
cat ~/.local/state/claude/logs/session-abc123.json | jq '.usage'

# Extract error details
cat ~/.local/state/claude/logs/session-abc123.json | jq '.messages[] | select(.role == "assistant") | .content[] | select(.type == "error")'
</code></pre>
<p><strong>For MapReduce jobs</strong>, check the DLQ for json_log_location:</p>
<pre><code class="language-bash"># Get log location from DLQ
prodigy dlq show &lt;job_id&gt; | jq '.items[].failure_history[].json_log_location'

# Inspect the Claude JSON log
cat /path/from/above/session-xyz.json | jq '.messages[-3:]'
</code></pre>
<p>This is especially valuable for debugging MapReduce agent failures, as you can see exactly what Claude was doing when the agent failed.</p>
<h3 id="check-dlq-for-failed-items"><a class="header" href="#check-dlq-for-failed-items">Check DLQ for failed items</a></h3>
<pre><code class="language-bash"># List failed items
prodigy dlq list &lt;job_id&gt;

# View failure details (inspect and show are aliases - both commands work identically)
prodigy dlq inspect &lt;job_id&gt;
prodigy dlq show &lt;job_id&gt;

# Retry failed items (primary recovery operation)
prodigy dlq retry &lt;job_id&gt;

# Retry with custom parallelism
prodigy dlq retry &lt;job_id&gt; --max-parallel 10

# Dry run to see what would be retried
prodigy dlq retry &lt;job_id&gt; --dry-run

# Analyze failure patterns across items
prodigy dlq analyze &lt;job_id&gt;

# Export DLQ items to file for external analysis
prodigy dlq export output.json --job-id &lt;job_id&gt;

# Show DLQ statistics for workflow
prodigy dlq stats --workflow-id &lt;workflow_id&gt;

# Purge old DLQ items
prodigy dlq purge --older-than-days 30

# Clear processed items from DLQ
prodigy dlq clear &lt;workflow_id&gt;
</code></pre>
<h3 id="monitor-mapreduce-progress"><a class="header" href="#monitor-mapreduce-progress">Monitor MapReduce progress</a></h3>
<pre><code class="language-bash"># View events
prodigy events &lt;job_id&gt;

# Check checkpoints
prodigy checkpoints list

# View event logs directly
ls ~/.prodigy/events/

# Check session state
cat .prodigy/session_state.json
</code></pre>
<h3 id="inspect-checkpoint-files-for-job-state"><a class="header" href="#inspect-checkpoint-files-for-job-state">Inspect checkpoint files for job state</a></h3>
<p>Checkpoint files contain the complete state of a MapReduce job and are crucial for debugging:</p>
<pre><code class="language-bash"># View checkpoint to understand job state
cat ~/.prodigy/state/{repo_name}/mapreduce/jobs/{job_id}/checkpoint.json

# Check checkpoint version and completed items count
jq '.version, .completed_items | length' ~/.prodigy/state/{repo_name}/mapreduce/jobs/{job_id}/checkpoint.json

# List all pending items (items not yet processed)
jq '.pending_items' ~/.prodigy/state/{repo_name}/mapreduce/jobs/{job_id}/checkpoint.json

# View job progress summary
jq '{version, total: (.completed_items | length) + (.pending_items | length), completed: (.completed_items | length), pending: (.pending_items | length)}' ~/.prodigy/state/{repo_name}/mapreduce/jobs/{job_id}/checkpoint.json
</code></pre>
<p><strong>Key checkpoint fields:</strong></p>
<ul>
<li><code>version</code> - Checkpoint format version (for migration compatibility)</li>
<li><code>completed_items</code> - Work items that have been successfully processed</li>
<li><code>pending_items</code> - Work items still waiting to be processed</li>
<li><code>job_id</code> - Unique identifier for the MapReduce job</li>
<li><code>timestamp</code> - When the checkpoint was last saved</li>
</ul>
<hr />
<h2 id="faq"><a class="header" href="#faq">FAQ</a></h2>
<h3 id="q-why-are-my-changes-not-being-committed"><a class="header" href="#q-why-are-my-changes-not-being-committed">Q: Why are my changes not being committed?</a></h3>
<p><strong>A:</strong> Add <code>commit_required: true</code> to your command or use <code>auto_commit: true</code> for automatic commits when changes are detected. Note: <code>auto_commit</code> can be set at the workflow level (applies to all steps) or per-step. When true, Prodigy creates commits automatically when git diff detects changes.</p>
<h3 id="q-how-do-i-retry-failed-mapreduce-items"><a class="header" href="#q-how-do-i-retry-failed-mapreduce-items">Q: How do I retry failed MapReduce items?</a></h3>
<p><strong>A:</strong> Use the DLQ retry command:</p>
<pre><code class="language-bash">prodigy dlq retry &lt;job_id&gt;
</code></pre>
<h3 id="q-can-i-use-environment-variables-in-jsonpath-expressions"><a class="header" href="#q-can-i-use-environment-variables-in-jsonpath-expressions">Q: Can I use environment variables in JSONPath expressions?</a></h3>
<p><strong>A:</strong> No, JSONPath expressions are evaluated against the input data, not the environment. Use variables in command arguments instead.</p>
<h3 id="q-how-do-i-skip-items-in-mapreduce"><a class="header" href="#q-how-do-i-skip-items-in-mapreduce">Q: How do I skip items in MapReduce?</a></h3>
<p><strong>A:</strong> Use the <code>filter</code> field:</p>
<pre><code class="language-yaml">map:
  filter: "item.score &gt;= 5"
</code></pre>
<h3 id="q-whats-the-difference-between-on_failure-and-on_incomplete"><a class="header" href="#q-whats-the-difference-between-on_failure-and-on_incomplete">Q: What’s the difference between <code>on_failure</code> and <code>on_incomplete</code>?</a></h3>
<p><strong>A:</strong> <code>on_failure</code> runs when a command exits with a non-zero code. <code>on_incomplete</code> is used in goal_seek commands and runs when the validation score is below the threshold.</p>
<h3 id="q-how-do-i-run-commands-in-parallel"><a class="header" href="#q-how-do-i-run-commands-in-parallel">Q: How do I run commands in parallel?</a></h3>
<p><strong>A:</strong> Use MapReduce mode with <code>max_parallel</code>:</p>
<pre><code class="language-yaml">mode: mapreduce
map:
  max_parallel: 5
</code></pre>
<h3 id="q-can-i-nest-workflows"><a class="header" href="#q-can-i-nest-workflows">Q: Can I nest workflows?</a></h3>
<p><strong>A:</strong> Not directly, but you can use <code>shell</code> commands to invoke other workflows:</p>
<pre><code class="language-yaml">- shell: "prodigy run other-workflow.yml"
</code></pre>
<h3 id="q-how-do-i-clean-up-old-worktrees"><a class="header" href="#q-how-do-i-clean-up-old-worktrees">Q: How do I clean up old worktrees?</a></h3>
<p><strong>A:</strong> Use the <code>prodigy worktree clean</code> command to remove completed worktrees:</p>
<pre><code class="language-bash"># List all worktrees
prodigy worktree ls

# Clean up completed worktrees
prodigy worktree clean

# Force cleanup of all worktrees (use with caution)
prodigy worktree clean -f
</code></pre>
<p>Old worktrees can consume disk space, so periodic cleanup is recommended. The <code>clean</code> command safely removes worktrees that are no longer in use, while <code>-f</code> forces removal of all worktrees including those that may still be active.</p>
<hr />
<h2 id="common-error-messages-1"><a class="header" href="#common-error-messages-1">Common Error Messages</a></h2>
<h3 id="mapreduceerror-types"><a class="header" href="#mapreduceerror-types">MapReduceError Types</a></h3>
<p>Prodigy uses structured errors to help diagnose issues:</p>
<p><strong>Job-level errors:</strong></p>
<ul>
<li><code>JobInitializationFailed</code> - Job failed to initialize, check configuration and permissions</li>
<li><code>JobAlreadyExists</code> - Job ID already exists, choose a different job ID or clean up old job</li>
<li><code>JobNotFound</code> - Job ID doesn’t exist, check job_id spelling or if job was cleaned up</li>
</ul>
<p><strong>Agent-level errors:</strong></p>
<ul>
<li><code>AgentFailed</code> - Individual agent execution failed, check DLQ for details</li>
<li><code>AgentTimeout</code> - Agent exceeded timeout, increase agent_timeout_secs</li>
<li><code>CommandExecutionFailed</code> - Shell or Claude command failed in agent</li>
<li><code>CommandFailed</code> - Command execution failed with non-zero exit code</li>
</ul>
<p><strong>Resource errors:</strong></p>
<ul>
<li><code>ResourceExhausted</code> - Out of disk space, memory, or other resources</li>
</ul>
<p><strong>Worktree errors:</strong></p>
<ul>
<li><code>WorktreeCreationFailed</code> - Failed to create git worktree, check disk space and git status</li>
<li><code>WorktreeMergeConflict</code> - Git merge conflict when merging agent results</li>
</ul>
<p><strong>Configuration and validation errors:</strong></p>
<ul>
<li><code>InvalidConfiguration</code> - Workflow YAML has configuration errors</li>
<li><code>InvalidJsonPath</code> - JSONPath expression syntax error, check your $.path syntax</li>
<li><code>ValidationFailed</code> - Validation check failed, review validation criteria</li>
<li><code>ShellSubstitutionFailed</code> - Variable substitution failed, check ${variable} references</li>
<li><code>EnvironmentError</code> - Environment validation failed, check required env vars</li>
</ul>
<p><strong>Checkpoint errors:</strong></p>
<ul>
<li><code>CheckpointCorrupted</code> - Checkpoint file corrupted at specific version, may need to restart job</li>
<li><code>CheckpointLoadFailed</code> - Failed to load checkpoint, check file permissions and format</li>
<li><code>CheckpointSaveFailed</code> - Failed to save checkpoint, check disk space and permissions</li>
<li><code>CheckpointPersistFailed</code> - Failed to persist checkpoint to disk, check disk space</li>
</ul>
<p><strong>I/O errors:</strong></p>
<ul>
<li><code>WorkItemLoadFailed</code> - Failed to load work items from input file, check file format and path</li>
</ul>
<p><strong>Concurrency errors:</strong></p>
<ul>
<li><code>DeadlockDetected</code> - Deadlock in job execution, reduce parallelism or check for circular dependencies</li>
<li><code>ConcurrentModification</code> - Concurrent modification of job state, retry operation</li>
</ul>
<p><strong>Other errors:</strong></p>
<ul>
<li><code>DlqError</code> - DLQ operation failed, check DLQ storage and permissions</li>
<li><code>ProcessingError</code> - General processing error, check logs for details</li>
<li><code>Timeout</code> - Operation timed out, increase timeout values</li>
<li><code>General</code> - General error for migration compatibility</li>
</ul>
<p><strong>Recovery actions:</strong></p>
<ul>
<li>Check event logs: <code>prodigy events &lt;job_id&gt;</code></li>
<li>Review DLQ: <code>prodigy dlq list &lt;job_id&gt;</code></li>
<li>View detailed state: <code>cat ~/.prodigy/state/{repo}/mapreduce/jobs/{job_id}/checkpoint.json</code></li>
<li>Check Claude JSON logs (see Debug Tips section below)</li>
</ul>
<h3 id="checkpoint-and-resume-errors"><a class="header" href="#checkpoint-and-resume-errors">Checkpoint and Resume Errors</a></h3>
<p><strong>“Checkpoint not found”</strong></p>
<ul>
<li>Cause: No checkpoint file exists for this job</li>
<li>Solution: Job may have completed or checkpoint was deleted, start fresh</li>
</ul>
<p><strong>“Failed to resume from checkpoint”</strong></p>
<ul>
<li>Cause: Checkpoint file is corrupted or format changed</li>
<li>Solution: Check checkpoint JSON syntax, may need to start over</li>
</ul>
<p><strong>“Worktree conflicts during merge”</strong></p>
<ul>
<li>Cause: Git merge conflicts when combining agent results</li>
<li>Solution: Resolve conflicts manually in worktree, then retry merge</li>
</ul>
<h3 id="variable-and-capture-errors"><a class="header" href="#variable-and-capture-errors">Variable and Capture Errors</a></h3>
<p><strong>“Variable not found: ${variable_name}”</strong></p>
<ul>
<li>Cause: Variable not defined or out of scope</li>
<li>Solution: Check variable is defined before use, verify scope (workflow vs item vs capture)</li>
</ul>
<p><strong>“Failed to parse capture output as {format}”</strong></p>
<ul>
<li>Cause: Command output doesn’t match capture_format</li>
<li>Solution: Check output manually, adjust capture_format or command output</li>
</ul>
<p><strong>“JSONPath expression failed”</strong></p>
<ul>
<li>Cause: Invalid JSONPath syntax or doesn’t match data structure</li>
<li>Solution: Test with <code>jq</code> command, simplify expression, check input data format</li>
</ul>
<hr />
<h2 id="best-practices-for-debugging"><a class="header" href="#best-practices-for-debugging">Best Practices for Debugging</a></h2>
<ol>
<li><strong>Start simple</strong>: Test commands individually before adding to workflow</li>
<li><strong>Use verbosity flags</strong>: Use <code>-v</code> to see Claude interactions, <code>-vv</code> for debug logs, <code>-vvv</code> for trace</li>
<li><strong>Use echo liberally</strong>: Debug variable values with echo statements</li>
<li><strong>Check logs and state</strong>: Review event logs (<code>~/.prodigy/events/</code>) and session state (<code>.prodigy/session_state.json</code>)</li>
<li><strong>Test incrementally</strong>: Add commands one at a time and test after each</li>
<li><strong>Validate input data</strong>: Ensure JSON files and data formats are correct before MapReduce</li>
<li><strong>Check DLQ regularly</strong>: Monitor failed items with <code>prodigy dlq list</code> and retry when appropriate</li>
<li><strong>Monitor resources</strong>: Check disk space, memory, and CPU during execution</li>
<li><strong>Version control</strong>: Commit working workflows before making changes</li>
<li><strong>Read error messages carefully</strong>: MapReduceError types indicate specific failure modes</li>
<li><strong>Ask for help</strong>: Include full error messages, workflow config, and verbosity output when seeking support</li>
</ol>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
