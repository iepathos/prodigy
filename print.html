<!DOCTYPE HTML>
<html lang="en" class="rust sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Prodigy Documentation</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="AI-powered workflow orchestration for development teams">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "rust";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('rust')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Prodigy Documentation</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/iepathos/prodigy" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>Prodigy is an AI-powered workflow orchestration tool that enables development teams to automate complex tasks using Claude AI through structured YAML workflows.</p>
<h2 id="what-is-prodigy"><a class="header" href="#what-is-prodigy">What is Prodigy?</a></h2>
<p>Prodigy combines the power of Claude AI with workflow orchestration to:</p>
<ul>
<li><strong>Automate repetitive development tasks</strong> - Code reviews, refactoring, testing</li>
<li><strong>Process work in parallel</strong> - MapReduce-style parallel execution across git worktrees</li>
<li><strong>Maintain quality</strong> - Built-in validation, error handling, and retry mechanisms</li>
<li><strong>Track changes</strong> - Full git integration with automatic commits and merge workflows</li>
</ul>
<h2 id="quick-start"><a class="header" href="#quick-start">Quick Start</a></h2>
<p>Create a simple workflow in <code>workflow.yml</code>:</p>
<pre><code class="language-yaml">- shell: "cargo build"
- shell: "cargo test"
  on_failure:
    claude: "/fix-failing-tests"
- shell: "cargo clippy"
</code></pre>
<p>Run it:</p>
<pre><code class="language-bash">prodigy run workflow.yml
</code></pre>
<h2 id="key-concepts"><a class="header" href="#key-concepts">Key Concepts</a></h2>
<ul>
<li><strong>Workflows</strong>: YAML files defining sequences of commands</li>
<li><strong>Commands</strong>: Shell commands, Claude AI invocations, or control flow</li>
<li><strong>Variables</strong>: Dynamic values captured and interpolated across steps</li>
<li><strong>MapReduce</strong>: Parallel processing across multiple git worktrees</li>
<li><strong>Validation</strong>: Automatic testing and quality checks</li>
</ul>
<h2 id="next-steps"><a class="header" href="#next-steps">Next Steps</a></h2>
<ul>
<li><a href="workflow-basics.html">Workflow Basics</a> - Learn workflow fundamentals</li>
<li><a href="commands.html">Command Types</a> - Explore available command types</li>
<li><a href="examples.html">Examples</a> - See real-world workflows</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="workflow-basics"><a class="header" href="#workflow-basics">Workflow Basics</a></h1>
<p>This chapter covers the fundamentals of creating Prodigy workflows. You’ll learn about workflow structure, basic commands, and configuration options.</p>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>Prodigy workflows are YAML files that define a sequence of commands to execute. They can be as simple as a list of shell commands or as complex as parallel MapReduce jobs.</p>
<p><strong>Two Main Workflow Types:</strong></p>
<ul>
<li><strong>Standard Workflows</strong>: Sequential command execution (covered here)</li>
<li><strong>MapReduce Workflows</strong>: Parallel processing with map/reduce phases (see <a href="workflow-basics/mapreduce.html">MapReduce chapter</a>)</li>
</ul>
<h2 id="simple-workflows"><a class="header" href="#simple-workflows">Simple Workflows</a></h2>
<p>The simplest workflow is just an array of commands:</p>
<pre><code class="language-yaml"># Simple array format - just list your commands
- shell: "echo 'Starting workflow...'"
- claude: "/prodigy-analyze"
- shell: "cargo test"
</code></pre>
<p>This executes each command sequentially. No additional configuration needed.</p>
<h2 id="additional-topics"><a class="header" href="#additional-topics">Additional Topics</a></h2>
<p>See also:</p>
<ul>
<li><a href="workflow-basics/full-workflow-structure.html">Full Workflow Structure</a></li>
<li><a href="workflow-basics/available-fields.html">Available Fields</a></li>
<li><a href="workflow-basics/command-types.html">Command Types</a></li>
<li><a href="workflow-basics/command-level-options.html">Command-Level Options</a></li>
<li><a href="workflow-basics/environment-configuration.html">Environment Configuration</a></li>
<li><a href="workflow-basics/merge-workflows.html">Merge Workflows</a></li>
<li><a href="workflow-basics/complete-example.html">Complete Example</a></li>
<li><a href="workflow-basics/next-steps.html">Next Steps</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="full-workflow-structure"><a class="header" href="#full-workflow-structure">Full Workflow Structure</a></h2>
<p>For more complex workflows, use the full format with explicit configuration:</p>
<pre><code class="language-yaml"># Full format with environment and merge configuration
commands:
  - shell: "cargo build"
  - claude: "/prodigy-test"

# Global environment variables (available to all commands)
env:
  NODE_ENV: production
  API_URL: https://api.example.com

# Secret environment variables (masked in logs)
secrets:
  API_KEY: "${env:SECRET_API_KEY}"

# Environment files to load (.env format)
env_files:
  - .env.production

# Environment profiles (switch contexts easily)
profiles:
  development:
    NODE_ENV: development
    DEBUG: "true"

# Custom merge workflow (for worktree integration)
# Simplified format (direct array of commands)
merge:
  - shell: "git fetch origin"
  - claude: "/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}"

# OR with timeout (use config object format)
merge:
  commands:
    - shell: "git fetch origin"
    - shell: "git merge origin/main"
    - shell: "cargo test"
    - claude: "/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}"
  timeout: 600  # Timeout in seconds for entire merge phase
</code></pre>
<p><strong>Source</strong>: Merge workflow structure from <code>src/config/mapreduce.rs:86-124</code></p>
<h2 id="merge-workflow-formats"><a class="header" href="#merge-workflow-formats">Merge Workflow Formats</a></h2>
<p>Prodigy supports two formats for merge workflows:</p>
<ol>
<li>
<p><strong>Direct Array Format</strong> - For simple merge operations without timeout:</p>
<pre><code class="language-yaml">merge:
  - shell: "git fetch origin"
  - claude: "/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}"
</code></pre>
</li>
<li>
<p><strong>Config Object Format</strong> - When you need to specify a timeout:</p>
<pre><code class="language-yaml">merge:
  commands:
    - shell: "git fetch origin"
    - claude: "/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}"
  timeout: 600  # Optional timeout in seconds
</code></pre>
</li>
</ol>
<p><strong>When to Use Custom Merge Workflows:</strong></p>
<p>Custom merge workflows execute when merging worktree changes back to the main branch. Use them for:</p>
<ul>
<li>Pre-merge validation and testing</li>
<li>Automatic conflict resolution</li>
<li>Running CI checks before merge</li>
<li>Cleaning up temporary files</li>
</ul>
<p>If no custom merge workflow is specified, Prodigy uses default merge behavior.</p>
<p><strong>Source</strong>: Deserializer implementation in <code>src/config/mapreduce.rs:96-124</code></p>
<h2 id="merge-context-variables"><a class="header" href="#merge-context-variables">Merge Context Variables</a></h2>
<p>The following variables are available in merge workflows:</p>
<ul>
<li><code>${merge.worktree}</code> - Name of the worktree being merged</li>
<li><code>${merge.source_branch}</code> - Source branch (worktree branch)</li>
<li><code>${merge.target_branch}</code> - Target branch (your original branch when workflow started)</li>
<li><code>${merge.session_id}</code> - Session ID for correlation and debugging</li>
</ul>
<p><strong>Example with all variables</strong>:</p>
<pre><code class="language-yaml">merge:
  - shell: |
      echo "Merging worktree: ${merge.worktree}"
      echo "From: ${merge.source_branch}"
      echo "To: ${merge.target_branch}"
      echo "Session: ${merge.session_id}"
  - claude: "/validate-merge --branch ${merge.source_branch}"
  - claude: "/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}"
</code></pre>
<p><strong>Source</strong>: Variable substitution from <code>tests/merge_workflow_integration.rs:274-290</code></p>
<h2 id="real-world-examples"><a class="header" href="#real-world-examples">Real-World Examples</a></h2>
<p><strong>Example 1: Pre-merge Validation</strong> (from <code>workflows/implement.yml:32-42</code>):</p>
<pre><code class="language-yaml">merge:
  - claude: "/prodigy-merge-master"  # Merge main into worktree first
  - claude: "/prodigy-ci"             # Run CI checks and auto-fix issues
  - claude: "/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}"
</code></pre>
<p><strong>Example 2: Cleanup and Testing</strong> (from <code>workflows/workflow-syntax-drift.yml:38-49</code>):</p>
<pre><code class="language-yaml">merge:
  - shell: "rm -rf .prodigy/syntax-analysis"
  - shell: "git add -A &amp;&amp; git commit -m 'chore: cleanup temp files' || true"
  - shell: "git fetch origin"
  - claude: "/prodigy-merge-master"
  - claude: "/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}"
</code></pre>
<p><strong>Example 3: With Environment Variables</strong> (from <code>workflows/mapreduce-env-example.yml:82-94</code>):</p>
<pre><code class="language-yaml">merge:
  commands:
    - shell: "echo Merging changes for $PROJECT_NAME"
    - claude: "/validate-merge --branch ${merge.source_branch} --project $PROJECT_NAME"
    - shell: "echo Merge completed"
  timeout: 600
</code></pre>
<h2 id="see-also"><a class="header" href="#see-also">See Also</a></h2>
<ul>
<li><a href="workflow-basics/merge-workflows.html">Merge Workflows</a> - Detailed merge workflow documentation</li>
<li><a href="workflow-basics/environment-configuration.html">Environment Configuration</a> - Environment variables and secrets</li>
<li><a href="workflow-basics/command-types.html">Command Types</a> - Available command types in workflows</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="available-fields"><a class="header" href="#available-fields">Available Fields</a></h2>
<p>Standard workflows support these top-level fields:</p>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Required</th><th>Description</th></tr></thead><tbody>
<tr><td><code>name</code></td><td>String</td><td>No</td><td>Workflow name for identification (defaults to “default”)</td></tr>
<tr><td><code>commands</code></td><td>Array<WorkflowCommand></td><td>Yes*</td><td>List of commands to execute sequentially</td></tr>
<tr><td><code>env</code></td><td>Map&lt;String, String&gt;</td><td>No</td><td>Global environment variables</td></tr>
<tr><td><code>secrets</code></td><td>Map&lt;String, SecretValue&gt;</td><td>No</td><td>Secret environment variables (masked in logs)</td></tr>
<tr><td><code>env_files</code></td><td>Array<PathBuf></td><td>No</td><td>Paths to .env files to load</td></tr>
<tr><td><code>profiles</code></td><td>Map&lt;String, EnvProfile&gt;</td><td>No</td><td>Named environment profiles for different contexts</td></tr>
<tr><td><code>merge</code></td><td>MergeWorkflow</td><td>No</td><td>Custom merge workflow for worktree integration</td></tr>
</tbody></table>
</div>
<p><strong>Source</strong>: Type definitions from <a href="workflow-basics/../../src/config/workflow.rs">src/config/workflow.rs:11-38</a></p>
<p><strong>Note:</strong> <code>commands</code> is only required in the full format. Use the simple array format for quick workflows without environment configuration. Use the full format when you need environment variables, profiles, or custom merge workflows.</p>
<h3 id="field-relationships-and-precedence"><a class="header" href="#field-relationships-and-precedence">Field Relationships and Precedence</a></h3>
<p>Understanding how fields interact is important for effective workflow configuration:</p>
<ol>
<li>
<p><strong>Environment Variable Resolution Order</strong>:</p>
<ul>
<li>Command-level <code>env</code> overrides all other sources</li>
<li>Profile-specific variables (when a profile is active) override global <code>env</code></li>
<li>Global <code>env</code> variables provide base configuration</li>
<li>Variables from <code>env_files</code> are loaded first and can be overridden</li>
</ul>
</li>
<li>
<p><strong>Secrets vs. Environment Variables</strong>:</p>
<ul>
<li><code>secrets</code> are a special type of environment variable that are masked in logs and output</li>
<li>Both <code>env</code> and <code>secrets</code> are available to all commands</li>
<li>Secrets take precedence over regular environment variables with the same name</li>
</ul>
</li>
<li>
<p><strong>Profile Activation</strong>:</p>
<ul>
<li>Profiles are activated via <code>--profile &lt;name&gt;</code> CLI flag</li>
<li>Profile variables merge with global <code>env</code>, with profile values taking precedence</li>
<li>Common use case: different configurations for dev, staging, and production</li>
</ul>
</li>
</ol>
<h3 id="format-examples"><a class="header" href="#format-examples">Format Examples</a></h3>
<p><strong>Simple Array Format</strong> (from <a href="workflow-basics/../../examples/standard-workflow.yml">examples/standard-workflow.yml</a>):</p>
<pre><code class="language-yaml">- shell: echo "Starting code analysis..."
- shell: cargo check --quiet
- shell: echo "Workflow complete"
</code></pre>
<p>Use this format when:</p>
<ul>
<li>You don’t need environment variables</li>
<li>You have a quick, straightforward sequence of commands</li>
<li>You want minimal YAML verbosity</li>
</ul>
<p><strong>Full Format with Environment</strong> (from <a href="workflow-basics/../../workflows/environment-example.yml">workflows/environment-example.yml</a>):</p>
<pre><code class="language-yaml">name: production-deploy

env:
  NODE_ENV: production
  API_URL: https://api.example.com

secrets:
  API_KEY: "${env:SECRET_API_KEY}"

env_files:
  - .env.production

profiles:
  development:
    NODE_ENV: development
    API_URL: http://localhost:3000
  staging:
    NODE_ENV: staging
    API_URL: https://staging.api.example.com

commands:
  - shell: echo "Deploying with NODE_ENV=$NODE_ENV"
  - shell: npm run build
  - shell: npm run deploy
</code></pre>
<p>Use this format when:</p>
<ul>
<li>You need environment variables</li>
<li>You have different configurations for different environments (profiles)</li>
<li>You need to mask sensitive values (secrets)</li>
<li>You want to load variables from <code>.env</code> files</li>
</ul>
<h3 id="see-also-1"><a class="header" href="#see-also-1">See Also</a></h3>
<ul>
<li><a href="workflow-basics/environment-configuration.html">Environment Configuration</a> - Detailed environment variable documentation</li>
<li><a href="workflow-basics/merge-workflows.html">Merge Workflows</a> - Custom merge workflow configuration</li>
<li><a href="workflow-basics/command-types.html">Command Types</a> - Available command types and their options</li>
<li><a href="workflow-basics/full-workflow-structure.html">Full Workflow Structure</a> - Complete workflow structure reference</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="command-types"><a class="header" href="#command-types">Command Types</a></h2>
<p>Prodigy supports several types of commands in workflows. <strong>Each command step must specify exactly one command type</strong> - they are mutually exclusive.</p>
<h3 id="shell-commands"><a class="header" href="#shell-commands">Shell Commands</a></h3>
<p>Execute shell commands during workflow execution.</p>
<p><strong>Source</strong>: src/config/command.rs:328</p>
<p><strong>Syntax</strong>:</p>
<pre><code class="language-yaml">- shell: "command to execute"
  timeout: 300                    # Optional: timeout in seconds
  capture_output: true            # Optional: capture command output
  capture_format: "json"          # Optional: json, text, lines
  capture_streams: "both"         # Optional: stdout, stderr, both
  output_file: "results.txt"      # Optional: redirect output to file
  on_failure:                     # Optional: commands to run on failure
    claude: "/debug-failure"
  on_success:                     # Optional: commands to run on success
    shell: "echo 'Success!'"
  when: "variable == 'value'"     # Optional: conditional execution
</code></pre>
<p><strong>Fields</strong>:</p>
<ul>
<li><code>shell</code> (required): The shell command to execute</li>
<li><code>timeout</code> (optional): Maximum execution time in seconds</li>
<li><code>capture_output</code> (optional): Boolean or variable name to capture output to</li>
<li><code>capture_format</code> (optional): Format for captured output (json, text, lines)</li>
<li><code>capture_streams</code> (optional): Which streams to capture (stdout, stderr, both)</li>
<li><code>output_file</code> (optional): File path to redirect output to</li>
<li><code>on_failure</code> (optional): Nested command to execute if shell command fails</li>
<li><code>on_success</code> (optional): Nested command to execute if shell command succeeds</li>
<li><code>when</code> (optional): Conditional expression - command only runs if true</li>
</ul>
<p><strong>Example</strong> (from workflows/fix-files-mapreduce.yml:36):</p>
<pre><code class="language-yaml">- shell: "cargo check --lib 2&gt;&amp;1 | grep -E '(error|warning)' | head -10 || echo 'No errors'"
  capture_output: true
</code></pre>
<p><strong>Example with on_failure</strong> (from workflows/coverage.yml:13):</p>
<pre><code class="language-yaml">- shell: "just test"
  on_failure:
    claude: "/prodigy-debug-test-failure --spec ${coverage.spec} --output ${shell.output}"
</code></pre>
<p><strong>Use Cases</strong>:</p>
<ul>
<li>Running build commands</li>
<li>Executing tests</li>
<li>File operations</li>
<li>Running analysis tools</li>
<li>Generating data files</li>
</ul>
<h3 id="claude-commands"><a class="header" href="#claude-commands">Claude Commands</a></h3>
<p>Execute Claude CLI commands via Claude Code.</p>
<p><strong>Source</strong>: src/config/command.rs:324</p>
<p><strong>Syntax</strong>:</p>
<pre><code class="language-yaml">- claude: "/command-name args"
  id: "command_id"                # Optional: identifier for referencing outputs
  commit_required: true           # Optional: expect git commit (default: false)
  timeout: 600                    # Optional: timeout in seconds
  outputs:                        # Optional: declare outputs for downstream use
    spec:
      file_pattern: "*.md"
  when: "condition"               # Optional: conditional execution
  on_failure:                     # Optional: commands to run on failure
    claude: "/fix-issue"
  on_success:                     # Optional: commands to run on success
    shell: "echo 'Done'"
</code></pre>
<p><strong>Fields</strong>:</p>
<ul>
<li><code>claude</code> (required): The Claude command string with arguments</li>
<li><code>id</code> (optional): Unique identifier for this command in the workflow</li>
<li><code>commit_required</code> (optional): Whether command is expected to create git commits (default: false)</li>
<li><code>timeout</code> (optional): Maximum execution time in seconds</li>
<li><code>outputs</code> (optional): Declare outputs that can be referenced by other commands</li>
<li><code>when</code> (optional): Conditional expression for execution</li>
<li><code>on_failure</code> (optional): Nested command to execute on failure</li>
<li><code>on_success</code> (optional): Nested command to execute on success</li>
</ul>
<p><strong>Example with outputs</strong> (from workflows/coverage.yml:3):</p>
<pre><code class="language-yaml">- claude: "/prodigy-coverage"
  id: coverage
  commit_required: true
  outputs:
    spec:
      file_pattern: "*-coverage-improvements.md"

- claude: "/prodigy-implement-spec ${coverage.spec}"
  commit_required: true
</code></pre>
<p><strong>Example in MapReduce</strong> (from workflows/fix-files-mapreduce.yml:31):</p>
<pre><code class="language-yaml">- claude: "/analyze-and-fix-file ${item.path} --complexity ${item.complexity}"
  capture_output: true
  timeout: 300
</code></pre>
<p><strong>Use Cases</strong>:</p>
<ul>
<li>Running custom Claude commands</li>
<li>Code analysis and generation</li>
<li>Implementing specifications</li>
<li>Debugging test failures</li>
<li>Code review and linting</li>
</ul>
<h3 id="goal-seek-commands"><a class="header" href="#goal-seek-commands">Goal Seek Commands</a></h3>
<p>Iteratively refine implementation until validation threshold is met.</p>
<p><strong>Source</strong>: src/cook/goal_seek/mod.rs:15-41</p>
<p><strong>Syntax</strong>:</p>
<pre><code class="language-yaml">- goal_seek:
    goal: "Human-readable goal description"
    claude: "/command-for-refinement"  # Either claude or shell required
    shell: "refinement-command"        # Either claude or shell required
    validate: "validation-command"     # Required: returns score 0-100
    threshold: 90                      # Required: success threshold (0-100)
    max_attempts: 5                    # Required: maximum refinement attempts
    timeout_seconds: 600               # Optional: timeout for entire operation
    fail_on_incomplete: true           # Optional: fail workflow if goal not met
  commit_required: true
</code></pre>
<p><strong>Fields</strong>:</p>
<ul>
<li><code>goal</code> (required): Human-readable description of what to achieve</li>
<li><code>claude</code> (optional): Claude command for refinement attempts</li>
<li><code>shell</code> (optional): Shell command for refinement attempts (use claude OR shell)</li>
<li><code>validate</code> (required): Command that returns validation score (0-100)</li>
<li><code>threshold</code> (required): Minimum score required for success (0-100)</li>
<li><code>max_attempts</code> (required): Maximum number of refinement attempts</li>
<li><code>timeout_seconds</code> (optional): Timeout for the entire goal-seeking operation</li>
<li><code>fail_on_incomplete</code> (optional): Whether to fail workflow if goal is not met</li>
</ul>
<p><strong>Validation Output Format</strong>:
The validate command must output a line containing <code>score: &lt;number&gt;</code> where number is 0-100:</p>
<pre><code>score: 85
</code></pre>
<p><strong>Result Types</strong>:</p>
<ul>
<li><code>Complete</code>: Goal achieved (score &gt;= threshold)</li>
<li><code>Incomplete</code>: Maximum attempts reached without achieving threshold</li>
<li><code>Failed</code>: Error during execution</li>
</ul>
<p><strong>Example</strong> (from workflows/implement-goal.yml:8):</p>
<pre><code class="language-yaml">- goal_seek:
    goal: "Implement specification: $ARG"
    claude: "/prodigy-implement-spec $ARG"
    validate: "git diff --stat | grep -q '.*\\.rs' &amp;&amp; echo 'score: 100' || echo 'score: 0'"
    threshold: 90
    max_attempts: 3
    timeout_seconds: 600
  commit_required: true
</code></pre>
<p><strong>Example with shell refinement</strong> (from workflows/goal-seeking-examples.yml:7):</p>
<pre><code class="language-yaml">- goal_seek:
    goal: "Achieve 90% test coverage"
    shell: "cargo tarpaulin --out Lcov"
    validate: "cargo tarpaulin --print-summary 2&gt;/dev/null | grep 'Coverage' | sed 's/.*Coverage=\\([0-9]*\\).*/score: \\1/'"
    threshold: 90
    max_attempts: 5
    timeout_seconds: 300
    fail_on_incomplete: true
  commit_required: true
</code></pre>
<p><strong>Use Cases</strong>:</p>
<ul>
<li>Iteratively improving code quality</li>
<li>Achieving test coverage thresholds</li>
<li>Performance optimization</li>
<li>Fixing linting issues</li>
<li>Implementing specifications completely</li>
</ul>
<h3 id="foreach-commands"><a class="header" href="#foreach-commands">Foreach Commands</a></h3>
<p>Iterate over a list of items, executing commands for each item in parallel or sequentially.</p>
<p><strong>Source</strong>: src/config/command.rs:344, src/config/command.rs:188-211</p>
<p><strong>Syntax</strong>:</p>
<pre><code class="language-yaml">- foreach:
    items: ["item1", "item2"]       # Static list
    # OR
    items: "shell-command"          # Command that produces items
    do:                             # Commands to execute per item
      - claude: "/process ${item}"
      - shell: "test ${item}"
    parallel: 5                     # Optional: number of parallel workers
    continue_on_error: true         # Optional: continue if item fails
    max_items: 100                  # Optional: limit number of items
</code></pre>
<p><strong>Input Types</strong>:</p>
<ol>
<li><strong>Static List</strong>: Array of strings directly in YAML</li>
<li><strong>Command Output</strong>: Shell command whose output produces item list</li>
</ol>
<p><strong>Fields</strong>:</p>
<ul>
<li><code>items</code> (required): Either a static list or a command to generate items</li>
<li><code>do</code> (required): List of commands to execute for each item</li>
<li><code>parallel</code> (optional): Number of parallel workers (boolean or count)
<ul>
<li><code>true</code>: Default parallel count</li>
<li><code>false</code>: Sequential execution</li>
<li>Number: Specific parallel count</li>
</ul>
</li>
<li><code>continue_on_error</code> (optional): Continue processing other items if one fails (default: false)</li>
<li><code>max_items</code> (optional): Maximum number of items to process</li>
</ul>
<p><strong>Variable Access</strong>:
Inside <code>do</code> commands, use <code>${item}</code> to reference the current item.</p>
<p><strong>Example</strong> (from workflows/fix-files-mapreduce.yml - conceptual):</p>
<pre><code class="language-yaml">- foreach:
    items: "find src -name '*.rs' -type f"
    do:
      - claude: "/analyze-file ${item}"
      - shell: "rustfmt ${item}"
    parallel: 4
    continue_on_error: true
</code></pre>
<p><strong>Use Cases</strong>:</p>
<ul>
<li>Processing multiple files</li>
<li>Running operations on a list of identifiers</li>
<li>Batch operations with parallelism</li>
<li>Conditional processing of items</li>
</ul>
<h3 id="write-file-commands"><a class="header" href="#write-file-commands">Write File Commands</a></h3>
<p>Write content to files with format validation and directory creation.</p>
<p><strong>Source</strong>: src/config/command.rs:348, src/config/command.rs:279-317</p>
<p><strong>Syntax</strong>:</p>
<pre><code class="language-yaml">- write_file:
    path: "output/file.json"        # Required: file path (supports variables)
    content: "${map.results}"       # Required: content to write (supports variables)
    format: json                    # Optional: text, json, yaml (default: text)
    mode: "0644"                    # Optional: file permissions (default: 0644)
    create_dirs: true               # Optional: create parent directories (default: false)
</code></pre>
<p><strong>Fields</strong>:</p>
<ul>
<li><code>path</code> (required): File path to write to (supports variable interpolation)</li>
<li><code>content</code> (required): Content to write (supports variable interpolation)</li>
<li><code>format</code> (optional): Output format - text, json, or yaml (default: text)
<ul>
<li><code>text</code>: Write content as-is</li>
<li><code>json</code>: Validate JSON and pretty-print</li>
<li><code>yaml</code>: Validate YAML and format</li>
</ul>
</li>
<li><code>mode</code> (optional): File permissions in octal format (default: “0644”)</li>
<li><code>create_dirs</code> (optional): Create parent directories if they don’t exist (default: false)</li>
</ul>
<p><strong>Format Types</strong> (src/config/command.rs:301-313):</p>
<ul>
<li><code>Text</code>: No processing, write content directly</li>
<li><code>Json</code>: Validate JSON syntax and pretty-print</li>
<li><code>Yaml</code>: Validate YAML syntax and format</li>
</ul>
<p><strong>Example</strong> (from workflows/debtmap-reduce.yml:105):</p>
<pre><code class="language-yaml">- write_file:
    path: ".prodigy/map-results.json"
    content: "${map.results}"
    format: json
    create_dirs: true
</code></pre>
<p><strong>Use Cases</strong>:</p>
<ul>
<li>Saving workflow results to files</li>
<li>Generating configuration files</li>
<li>Writing JSON/YAML data with validation</li>
<li>Creating reports and logs</li>
<li>Persisting intermediate data</li>
</ul>
<h3 id="validation-commands"><a class="header" href="#validation-commands">Validation Commands</a></h3>
<p>Validate implementation completeness using shell or Claude commands with threshold-based checking.</p>
<p><strong>Source</strong>: src/cook/workflow/validation.rs:12-50</p>
<p><strong>Syntax</strong>:</p>
<pre><code class="language-yaml">- validate:
    shell: "validation-command"         # Either shell or claude or commands
    claude: "/validate-command"         # Either shell or claude or commands
    commands:                           # Or array of commands for multi-step validation
      - shell: "step1"
      - claude: "/step2"
    expected_schema: {...}              # Optional: JSON schema for validation output
    threshold: 100                      # Optional: completion threshold % (default: 100)
    timeout: 300                        # Optional: timeout in seconds
    result_file: "validation.json"      # Optional: where validation results are written
    on_incomplete:                      # Optional: commands to run if validation fails
      claude: "/fix-gaps ${validation.gaps}"
      max_attempts: 3
</code></pre>
<p><strong>Fields</strong>:</p>
<ul>
<li><code>shell</code> (optional): Shell command for validation (deprecated, use <code>commands</code>)</li>
<li><code>claude</code> (optional): Claude command for validation</li>
<li><code>commands</code> (optional): Array of commands for multi-step validation</li>
<li><code>expected_schema</code> (optional): JSON schema that validation output must match</li>
<li><code>threshold</code> (optional): Minimum completion percentage required (default: 100)</li>
<li><code>timeout</code> (optional): Timeout for validation command in seconds</li>
<li><code>result_file</code> (optional): File where validation results are written</li>
<li><code>on_incomplete</code> (optional): Configuration for handling incomplete implementations
<ul>
<li>Nested command to execute if validation fails</li>
<li><code>max_attempts</code>: Maximum retry attempts</li>
</ul>
</li>
</ul>
<p><strong>Validation Output Format</strong>:
Validation commands should output JSON matching this structure:</p>
<pre><code class="language-json">{
  "complete": true,
  "score": 100,
  "gaps": [],
  "incomplete_specs": []
}
</code></pre>
<p><strong>Example with result_file</strong> (from workflows/implement.yml:8):</p>
<pre><code class="language-yaml">validate:
  claude: "/prodigy-validate-spec $ARG --output .prodigy/validation-result.json"
  result_file: ".prodigy/validation-result.json"
  threshold: 100
  on_incomplete:
    claude: "/prodigy-complete-spec $ARG --gaps ${validation.gaps}"
</code></pre>
<p><strong>Example with commands array</strong> (from workflows/debtmap.yml:13):</p>
<pre><code class="language-yaml">validate:
  commands:
    - claude: "/prodigy-validate-debtmap-plan --before .prodigy/debtmap-before.json --plan .prodigy/IMPLEMENTATION_PLAN.md --output .prodigy/plan-validation.json"
  result_file: ".prodigy/plan-validation.json"
  threshold: 75
  on_incomplete:
    claude: "/fix-plan-gaps --gaps ${validation.gaps}"
    max_attempts: 3
</code></pre>
<p><strong>Use Cases</strong>:</p>
<ul>
<li>Verifying specification completeness</li>
<li>Checking implementation against requirements</li>
<li>Validating test coverage</li>
<li>Ensuring documentation quality</li>
<li>Multi-step validation pipelines</li>
</ul>
<h3 id="common-fields"><a class="header" href="#common-fields">Common Fields</a></h3>
<p>Several fields are available across all command types:</p>
<p><strong>Source</strong>: src/config/command.rs:320-401</p>
<ul>
<li><code>id</code> (string): Unique identifier for referencing command outputs</li>
<li><code>timeout</code> (number): Maximum execution time in seconds</li>
<li><code>when</code> (string): Conditional execution expression</li>
<li><code>on_success</code> (command): Command to run on successful execution</li>
<li><code>on_failure</code> (command): Command to run on failed execution</li>
<li><code>capture_output</code> (boolean or string): Capture command output (true/false or variable name)</li>
<li><code>capture_format</code> (string): Format for captured output (json, text, lines)</li>
<li><code>capture_streams</code> (string): Which streams to capture (stdout, stderr, both)</li>
<li><code>output_file</code> (string): File to redirect output to</li>
</ul>
<h3 id="command-exclusivity"><a class="header" href="#command-exclusivity">Command Exclusivity</a></h3>
<p>Each workflow step must specify <strong>exactly one</strong> command type. You cannot combine multiple command types in a single step:</p>
<p><strong>Valid</strong>:</p>
<pre><code class="language-yaml">- shell: "cargo test"        # ✓ Only shell command
- claude: "/lint"            # ✓ Only Claude command
- goal_seek:                 # ✓ Only goal_seek command
    goal: "Fix tests"
    validate: "..."
</code></pre>
<p><strong>Invalid</strong>:</p>
<pre><code class="language-yaml">- shell: "cargo test"        # ✗ Cannot combine shell and claude
  claude: "/lint"

- goal_seek: {...}           # ✗ Cannot combine goal_seek and foreach
  foreach: {...}
</code></pre>
<p><strong>Enforcement</strong>: src/config/command.rs:465-476</p>
<h3 id="deprecated-test-command"><a class="header" href="#deprecated-test-command">Deprecated: Test Command</a></h3>
<p>The <code>test:</code> command syntax is deprecated. Use <code>shell:</code> with <code>on_failure:</code> instead.</p>
<p><strong>Source</strong>: src/config/command.rs:446-463</p>
<p><strong>Old (deprecated)</strong>:</p>
<pre><code class="language-yaml">- test:
    command: "cargo test"
    on_failure:
      claude: "/debug-test"
</code></pre>
<p><strong>New (recommended)</strong>:</p>
<pre><code class="language-yaml">- shell: "cargo test"
  on_failure:
    claude: "/debug-test"
</code></pre>
<h2 id="see-also-2"><a class="header" href="#see-also-2">See Also</a></h2>
<ul>
<li><a href="workflow-basics/available-fields.html">Available Fields</a> - Complete reference of all command fields</li>
<li><a href="workflow-basics/../advanced/goal-seeking-operations.html">Goal Seeking Operations</a> - Advanced goal-seeking patterns</li>
<li><a href="workflow-basics/../advanced/parallel-iteration-with-foreach.html">Parallel Iteration with Foreach</a> - Foreach best practices</li>
<li><a href="workflow-basics/../advanced/implementation-validation.html">Implementation Validation</a> - Validation strategies</li>
<li><a href="workflow-basics/../variables/index.html">Variables</a> - Variable interpolation in commands</li>
<li><a href="workflow-basics/../error-handling.html">Error Handling</a> - Error handling strategies</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="command-level-options"><a class="header" href="#command-level-options">Command-Level Options</a></h2>
<p>All command types (<code>claude:</code>, <code>shell:</code>, <code>goal_seek:</code>, <code>foreach:</code>) support additional fields for advanced control and orchestration. These options enable timeout management, output capture, error handling, conditional execution, and more.</p>
<h2 id="core-options"><a class="header" href="#core-options">Core Options</a></h2>
<h3 id="timeout"><a class="header" href="#timeout">timeout</a></h3>
<p>Sets a maximum execution time for the command (in seconds). If the command exceeds this duration, it will be terminated.</p>
<p><strong>Type</strong>: <code>Option&lt;u64&gt;</code> (optional, no default timeout)</p>
<p><strong>Source</strong>: <a href="workflow-basics/../../src/config/command.rs">src/config/command.rs:383</a></p>
<pre><code class="language-yaml">commands:
  # 5 minute timeout for test suite
  - shell: "npm test"
    timeout: 300

  # 10 minute timeout for Claude implementation
  - claude: "/implement feature"
    timeout: 600

  # No timeout (runs until completion)
  - shell: "cargo build --release"
</code></pre>
<p><strong>Real-world examples</strong>:</p>
<ul>
<li><a href="workflow-basics/../../workflows/debtmap-reduce.yml">workflows/debtmap-reduce.yml:6</a> - 15 minute timeout for coverage generation</li>
<li><a href="workflow-basics/../../workflows/complex-build-pipeline.yml">workflows/complex-build-pipeline.yml:23</a> - 10 minute timeout for benchmarks</li>
<li><a href="workflow-basics/../../workflows/documentation-drift.yml">workflows/documentation-drift.yml:48</a> - 5 minute timeout for doc tests</li>
</ul>
<h3 id="id"><a class="header" href="#id">id</a></h3>
<p>Assigns an identifier to the command for referencing its outputs in subsequent commands via workflow variables.</p>
<p><strong>Type</strong>: <code>Option&lt;String&gt;</code> (optional)</p>
<p><strong>Source</strong>: <a href="workflow-basics/../../src/config/command.rs">src/config/command.rs:351-352</a></p>
<pre><code class="language-yaml">commands:
  - shell: "git rev-parse --short HEAD"
    id: "get_commit"
    capture_output: "commit_hash"

  # Use the captured output
  - shell: "echo 'Building commit ${commit_hash}'"
</code></pre>
<h3 id="commit_required"><a class="header" href="#commit_required">commit_required</a></h3>
<p>Specifies whether the command is expected to create git commits. Prodigy tracks commits for workflow provenance and rollback.</p>
<p><strong>Type</strong>: <code>bool</code> (default: <code>false</code>)</p>
<p><strong>Source</strong>: <a href="workflow-basics/../../src/config/command.rs">src/config/command.rs:354-356</a></p>
<pre><code class="language-yaml">commands:
  # Claude commands that modify code should commit
  - claude: "/prodigy-coverage"
    commit_required: true

  # Test commands typically don't commit
  - shell: "cargo test"
    commit_required: false

  # Linting fixes may commit changes
  - claude: "/prodigy-lint"
    commit_required: true
</code></pre>
<p><strong>Real-world examples</strong>:</p>
<ul>
<li><a href="workflow-basics/../../workflows/coverage.yml">workflows/coverage.yml:5,11</a> - Coverage and implementation commands</li>
<li><a href="workflow-basics/../../workflows/documentation-drift.yml">workflows/documentation-drift.yml:19,23,27</a> - Documentation update commands</li>
<li><a href="workflow-basics/../../workflows/implement-with-tests.yml">workflows/implement-with-tests.yml:27,31</a> - Test vs implementation distinction</li>
</ul>
<h2 id="output-capture-options"><a class="header" href="#output-capture-options">Output Capture Options</a></h2>
<h3 id="capture_output"><a class="header" href="#capture_output">capture_output</a></h3>
<p>Captures command output and stores it in a workflow variable. Supports both boolean mode (captures to default variable) and variable name mode (captures to named variable).</p>
<p><strong>Type</strong>: <code>Option&lt;CaptureOutputConfig&gt;</code> where <code>CaptureOutputConfig</code> is:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum CaptureOutputConfig {
    Boolean(bool),      // Simple capture (true/false)
    Variable(String),   // Capture to named variable
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Source</strong>: <a href="workflow-basics/../../src/config/command.rs">src/config/command.rs:367-368, 403-411</a></p>
<pre><code class="language-yaml">commands:
  # Capture to default variable name (shell.output)
  - shell: "echo 'Starting analysis...'"
    capture_output: true

  # Capture to custom variable name
  - shell: "ls -la | wc -l"
    capture_output: "file_count"

  # Use the captured variable
  - shell: "echo 'Found ${file_count} files'"

  # Disable capture explicitly
  - shell: "cargo build"
    capture_output: false
</code></pre>
<p><strong>Real-world examples</strong>:</p>
<ul>
<li><a href="workflow-basics/../../examples/capture-output-custom-vars.yml">examples/capture-output-custom-vars.yml:10-48</a> - Custom variable names</li>
<li><a href="workflow-basics/../../workflows/implement-with-tests.yml">workflows/implement-with-tests.yml:26,55</a> - Test output capture</li>
<li><a href="workflow-basics/../../workflows/complex-build-pipeline.yml">workflows/complex-build-pipeline.yml:17,24</a> - Build diagnostics</li>
</ul>
<h3 id="capture_format"><a class="header" href="#capture_format">capture_format</a></h3>
<p>Specifies how to parse captured output. Determines the data type and structure of the captured variable.</p>
<p><strong>Type</strong>: <code>Option&lt;String&gt;</code> with values: <code>string</code> (default), <code>json</code>, <code>lines</code>, <code>number</code>, <code>boolean</code></p>
<p><strong>Source</strong>: <a href="workflow-basics/../../src/config/command.rs">src/config/command.rs:391-392</a>, <a href="workflow-basics/../../src/cook/workflow/variables.rs">src/cook/workflow/variables.rs:250-265</a></p>
<p><strong>Supported formats</strong>:</p>
<ul>
<li><code>string</code> - Raw string output (default)</li>
<li><code>json</code> - Parse as JSON object/array</li>
<li><code>lines</code> - Split into array of lines</li>
<li><code>number</code> - Parse as numeric value</li>
<li><code>boolean</code> - Parse as boolean (<code>true</code>/<code>false</code>)</li>
</ul>
<pre><code class="language-yaml">commands:
  # Capture as JSON object
  - shell: "cat package.json"
    capture_output: "package_info"
    capture_format: "json"

  # Access JSON fields
  - shell: "echo 'Package: ${package_info.name} v${package_info.version}'"

  # Capture as boolean
  - shell: "cargo test --quiet &amp;&amp; echo true || echo false"
    capture_output: "tests_passed"
    capture_format: "boolean"

  # Capture as number
  - shell: "find src -name '*.rs' | wc -l"
    capture_output: "file_count"
    capture_format: "number"

  # Capture as array of lines
  - shell: "git diff --name-only"
    capture_output: "changed_files"
    capture_format: "lines"
</code></pre>
<p><strong>Real-world examples</strong>:</p>
<ul>
<li><a href="workflow-basics/../../examples/capture-json-processing.yml">examples/capture-json-processing.yml:9-54</a> - JSON metadata extraction</li>
<li><a href="workflow-basics/../../examples/capture-conditional-flow.yml">examples/capture-conditional-flow.yml:9-37</a> - Boolean and number formats</li>
<li><a href="workflow-basics/../../examples/capture-parallel-analysis.yml">examples/capture-parallel-analysis.yml:9-101</a> - Multi-format capture pipeline</li>
</ul>
<h3 id="capture_streams"><a class="header" href="#capture_streams">capture_streams</a></h3>
<p>Controls which output streams to capture from command execution. By default, only stdout is captured.</p>
<p><strong>Type</strong>: <code>Option&lt;String&gt;</code> or structured configuration</p>
<p><strong>Source</strong>: <a href="workflow-basics/../../src/config/command.rs">src/config/command.rs:394-396</a>, <a href="workflow-basics/../../src/cook/workflow/variables.rs">src/cook/workflow/variables.rs:267-292</a></p>
<p><strong>Captured fields</strong>:</p>
<ul>
<li><code>stdout</code> - Standard output (default: <code>true</code>)</li>
<li><code>stderr</code> - Standard error (default: <code>false</code>)</li>
<li><code>exit_code</code> - Process exit code (default: <code>true</code>)</li>
<li><code>success</code> - Whether command succeeded (default: <code>true</code>)</li>
<li><code>duration</code> - Execution duration (default: <code>true</code>)</li>
</ul>
<pre><code class="language-yaml">commands:
  # Capture all streams and metadata
  - shell: "cargo build --release"
    capture_output: "build_result"
    capture_streams:
      stdout: true
      stderr: true
      exit_code: true
      success: true
      duration: true

  # Access captured fields
  - shell: |
      echo "Build Success: ${build_result.success}"
      echo "Exit Code: ${build_result.exit_code}"
      echo "Duration: ${build_result.duration}s"
</code></pre>
<p><strong>Real-world examples</strong>:</p>
<ul>
<li><a href="workflow-basics/../../examples/capture-conditional-flow.yml">examples/capture-conditional-flow.yml:44-51</a> - Multi-stream capture with conditionals</li>
</ul>
<h3 id="output_file"><a class="header" href="#output_file">output_file</a></h3>
<p>Redirects command output to a file. This option is defined in the type definition but not yet widely used in the codebase.</p>
<p><strong>Type</strong>: <code>Option&lt;String&gt;</code> (file path)</p>
<p><strong>Source</strong>: <a href="workflow-basics/../../src/config/command.rs">src/config/command.rs:398-400</a></p>
<pre><code class="language-yaml">commands:
  - shell: "cargo test --verbose"
    output_file: "test-results.txt"

  - shell: "cargo doc --no-deps"
    output_file: "docs/api-output.log"
</code></pre>
<p><strong>Note</strong>: This feature is defined but no production examples currently exist. Consider contributing examples if you use this option.</p>
<h2 id="error-handling"><a class="header" href="#error-handling">Error Handling</a></h2>
<h3 id="on_failure"><a class="header" href="#on_failure">on_failure</a></h3>
<p>Specifies commands to execute when the main command fails. Supports automatic retry with configurable attempts and workflow failure control.</p>
<p><strong>Type</strong>: <code>Option&lt;TestDebugConfig&gt;</code> with fields:</p>
<ul>
<li><code>claude: String</code> - Claude command to run on failure</li>
<li><code>max_attempts: u32</code> - Maximum retry attempts (default: 3)</li>
<li><code>fail_workflow: bool</code> - Whether to fail workflow after max attempts (default: <code>false</code>)</li>
<li><code>commit_required: bool</code> - Whether debug command should commit (default: <code>true</code>)</li>
</ul>
<p><strong>Source</strong>: <a href="workflow-basics/../../src/config/command.rs">src/config/command.rs:370-372, 166-183</a></p>
<pre><code class="language-yaml">commands:
  # Retry tests with automated fixing
  - shell: "cargo test"
    on_failure:
      claude: "/prodigy-debug-test-failure --output ${shell.output}"
      max_attempts: 3
      fail_workflow: false

  # Critical check that must pass
  - shell: "just fmt-check &amp;&amp; just lint"
    on_failure:
      claude: "/prodigy-lint ${shell.output}"
      max_attempts: 5
      fail_workflow: true

  # Doc test failures with commit requirement
  - shell: "cargo test --doc"
    on_failure:
      claude: "/prodigy-fix-doc-tests --output ${shell.output}"
      max_attempts: 2
      fail_workflow: false
      commit_required: true
</code></pre>
<p><strong>Real-world examples</strong>:</p>
<ul>
<li><a href="workflow-basics/../../workflows/coverage-with-test-debug.yml">workflows/coverage-with-test-debug.yml:13-23</a> - Test debugging with retries</li>
<li><a href="workflow-basics/../../workflows/debtmap-reduce.yml">workflows/debtmap-reduce.yml:58-70</a> - Critical quality gates</li>
<li><a href="workflow-basics/../../workflows/documentation-drift.yml">workflows/documentation-drift.yml:47-53</a> - Doc test recovery</li>
</ul>
<h3 id="on_success"><a class="header" href="#on_success">on_success</a></h3>
<p>Executes another command when the main command succeeds. Enables chaining of dependent operations.</p>
<p><strong>Type</strong>: <code>Option&lt;Box&lt;WorkflowStepCommand&gt;&gt;</code> (nested command)</p>
<p><strong>Source</strong>: <a href="workflow-basics/../../src/config/command.rs">src/config/command.rs:374-376</a></p>
<pre><code class="language-yaml">commands:
  # Chain successful operations
  - shell: "cargo check"
    on_success:
      shell: "cargo build --release"
      on_success:
        shell: "cargo test --release"

  # Nested success/failure handlers
  - shell: "cargo test"
    capture_output: "test_output"
    on_failure:
      claude: "/prodigy-debug-test-failures '${test_output}'"
      commit_required: true
      on_success:
        # After fixing, verify tests pass
        shell: "cargo test"
        on_failure:
          claude: "/prodigy-fix-test-failures '${shell.output}' --deep-analysis"

  # Success notification
  - shell: "cargo test --release"
    capture_output: "final_results"
    on_success:
      shell: "echo '✅ All tests passing!'"
</code></pre>
<p><strong>Real-world examples</strong>:</p>
<ul>
<li><a href="workflow-basics/../../workflows/implement-with-tests.yml">workflows/implement-with-tests.yml:28-40,61-63</a> - Nested test-fix-verify loops</li>
<li><a href="workflow-basics/../../workflows/complex-build-pipeline.yml">workflows/complex-build-pipeline.yml:7-13</a> - Build pipeline chaining</li>
</ul>
<h2 id="conditional-execution"><a class="header" href="#conditional-execution">Conditional Execution</a></h2>
<h3 id="when"><a class="header" href="#when">when</a></h3>
<p>Evaluates a boolean expression to determine whether to execute the command. Supports variable interpolation and boolean logic.</p>
<p><strong>Type</strong>: <code>Option&lt;String&gt;</code> (boolean expression)</p>
<p><strong>Source</strong>: <a href="workflow-basics/../../src/config/command.rs">src/config/command.rs:387-388</a></p>
<pre><code class="language-yaml">commands:
  # Capture test status
  - shell: "cargo test --quiet &amp;&amp; echo true || echo false"
    capture_output: "tests_passed"
    capture_format: "boolean"

  # Conditional execution based on test results
  - shell: "echo 'Running coverage analysis...'"
    when: "${tests_passed}"

  # Multiple conditions
  - shell: "cargo build --release"
    when: "${tests_passed} &amp;&amp; ${lint_passed}"
    capture_output: "build_output"

  # Conditional deployment
  - shell: |
      if [ "${tests_passed}" = "true" ] &amp;&amp; [ "${build_output.success}" = "true" ]; then
        echo "✅ Deployment ready!"
      fi
    when: "${tests_passed}"
</code></pre>
<p><strong>Real-world examples</strong>:</p>
<ul>
<li><a href="workflow-basics/../../examples/capture-conditional-flow.yml">examples/capture-conditional-flow.yml:20-51</a> - Multi-stage conditional pipeline</li>
</ul>
<h2 id="advanced-options"><a class="header" href="#advanced-options">Advanced Options</a></h2>
<h3 id="validate"><a class="header" href="#validate">validate</a></h3>
<p>Configures implementation completeness validation with automatic gap detection and filling.</p>
<p><strong>Type</strong>: <code>Option&lt;ValidationConfig&gt;</code> (validation configuration)</p>
<p><strong>Source</strong>: <a href="workflow-basics/../../src/config/command.rs">src/config/command.rs:378-380</a></p>
<pre><code class="language-yaml">commands:
  - claude: "/implement-feature"
    validate:
      force_refresh: false
      max_cache_age: 300
</code></pre>
<p><strong>Note</strong>: See validation documentation for comprehensive coverage of this feature.</p>
<h3 id="analysis"><a class="header" href="#analysis">analysis</a></h3>
<p>Specifies per-step analysis requirements for code quality and coverage tracking.</p>
<p><strong>Type</strong>: <code>Option&lt;AnalysisConfig&gt;</code> with fields:</p>
<ul>
<li><code>force_refresh: bool</code> - Force fresh analysis even if cached (default: <code>false</code>)</li>
<li><code>max_cache_age: u64</code> - Maximum cache age in seconds (default: 300)</li>
</ul>
<p><strong>Source</strong>: <a href="workflow-basics/../../src/config/command.rs">src/config/command.rs:116-126</a></p>
<pre><code class="language-yaml">commands:
  - claude: "/implement-feature"
    analysis:
      force_refresh: true
      max_cache_age: 600
</code></pre>
<h3 id="env"><a class="header" href="#env">env</a></h3>
<p>Sets command-specific environment variables that override workflow-level and system environment variables.</p>
<p><strong>Type</strong>: <code>HashMap&lt;String, String&gt;</code> (key-value pairs)</p>
<p><strong>Source</strong>: <a href="workflow-basics/../../src/config/command.rs">src/config/command.rs:143-145</a></p>
<pre><code class="language-yaml">commands:
  - shell: "cargo build"
    env:
      RUST_BACKTRACE: "1"
      CARGO_INCREMENTAL: "0"

  - claude: "/analyze-code"
    env:
      DEBUG_MODE: "true"
      LOG_LEVEL: "trace"
</code></pre>
<h2 id="option-combinations"><a class="header" href="#option-combinations">Option Combinations</a></h2>
<h3 id="test-fix-verify-pattern"><a class="header" href="#test-fix-verify-pattern">Test-Fix-Verify Pattern</a></h3>
<p>Combines output capture, error handling, and conditionals for robust test workflows:</p>
<pre><code class="language-yaml">commands:
  # Initial test run
  - shell: "cargo test"
    capture_output: "test_output"
    commit_required: false
    on_failure:
      # Automated fix on failure
      claude: "/prodigy-debug-test-failures '${test_output}'"
      commit_required: true
      max_attempts: 3
      on_success:
        # Verify fix worked
        shell: "cargo test"
        commit_required: false
</code></pre>
<h3 id="conditional-pipeline-pattern"><a class="header" href="#conditional-pipeline-pattern">Conditional Pipeline Pattern</a></h3>
<p>Uses capture formats and conditionals for decision-making workflows:</p>
<pre><code class="language-yaml">commands:
  # Capture test status as boolean
  - shell: "cargo test --quiet &amp;&amp; echo true || echo false"
    capture_output: "tests_passed"
    capture_format: "boolean"

  # Capture coverage as number
  - shell: "cargo tarpaulin --output-dir coverage | grep -oP '\\d+\\.\\d+(?=%)' | head -1"
    capture_output: "coverage_percent"
    capture_format: "number"
    when: "${tests_passed}"

  # Deploy only if quality gates pass
  - shell: "echo 'Deploying to production...'"
    when: "${tests_passed} &amp;&amp; ${coverage_percent} &gt;= 80"
</code></pre>
<h3 id="parallel-capture-pattern"><a class="header" href="#parallel-capture-pattern">Parallel Capture Pattern</a></h3>
<p>Captures multiple metrics in parallel for aggregation:</p>
<pre><code class="language-yaml">commands:
  # Capture metadata
  - shell: "find src -name '*.rs' | wc -l"
    capture_output: "total_files"
    capture_format: "number"

  # Build summary JSON
  - shell: |
      echo '{
        "repository": "'$(basename $(pwd))'",
        "total_files": ${total_files},
        "timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'"
      }'
    capture_output: "metadata"
    capture_format: "json"
</code></pre>
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<h3 id="timeout-guidelines"><a class="header" href="#timeout-guidelines">Timeout Guidelines</a></h3>
<ul>
<li><strong>Short commands</strong> (&lt; 1 min): No timeout needed</li>
<li><strong>Medium commands</strong> (1-5 min): Set <code>timeout: 300</code> (5 minutes)</li>
<li><strong>Long commands</strong> (5-10 min): Set <code>timeout: 600</code> (10 minutes)</li>
<li><strong>Very long commands</strong> (&gt; 10 min): Set explicit timeout or ensure proper progress reporting</li>
</ul>
<h3 id="output-capture-strategy"><a class="header" href="#output-capture-strategy">Output Capture Strategy</a></h3>
<ol>
<li>
<p><strong>Choose the right format</strong>:</p>
<ul>
<li><code>string</code> - Human-readable output, logs</li>
<li><code>json</code> - Structured data, API responses</li>
<li><code>lines</code> - File lists, multi-line output</li>
<li><code>number</code> - Counts, metrics, percentages</li>
<li><code>boolean</code> - Status checks, conditions</li>
</ul>
</li>
<li>
<p><strong>Name variables descriptively</strong>:</p>
<ul>
<li>Good: <code>test_output</code>, <code>coverage_percent</code>, <code>changed_files</code></li>
<li>Bad: <code>out</code>, <code>result</code>, <code>temp</code></li>
</ul>
</li>
<li>
<p><strong>Capture only what you need</strong>:</p>
<ul>
<li>Default <code>capture_streams</code> is sufficient for most cases</li>
<li>Only capture <code>stderr</code> when you need error diagnostics</li>
<li>Use <code>capture_output: false</code> for commands with no useful output</li>
</ul>
</li>
</ol>
<h3 id="error-handling-strategy"><a class="header" href="#error-handling-strategy">Error Handling Strategy</a></h3>
<ol>
<li>
<p><strong>Set appropriate <code>max_attempts</code></strong>:</p>
<ul>
<li>Deterministic failures: 1-2 attempts</li>
<li>Flaky tests: 3 attempts</li>
<li>Complex fixes: 5 attempts</li>
</ul>
</li>
<li>
<p><strong>Use <code>fail_workflow</code> judiciously</strong>:</p>
<ul>
<li><code>true</code> for critical quality gates (security, correctness)</li>
<li><code>false</code> for optional improvements (coverage, docs)</li>
</ul>
</li>
<li>
<p><strong>Nest <code>on_success</code> for verification</strong>:</p>
<ul>
<li>After automated fixes, verify they worked</li>
<li>Chain dependent operations</li>
</ul>
</li>
</ol>
<h3 id="conditional-execution-guidelines"><a class="header" href="#conditional-execution-guidelines">Conditional Execution Guidelines</a></h3>
<ol>
<li>
<p><strong>Capture boolean status explicitly</strong>:</p>
<pre><code class="language-yaml">- shell: "cargo test &amp;&amp; echo true || echo false"
  capture_output: "tests_passed"
  capture_format: "boolean"
</code></pre>
</li>
<li>
<p><strong>Use descriptive condition expressions</strong>:</p>
<ul>
<li>Good: <code>when: "${tests_passed} &amp;&amp; ${coverage_sufficient}"</code></li>
<li>Bad: <code>when: "${x} &amp;&amp; ${y}"</code></li>
</ul>
</li>
<li>
<p><strong>Provide fallback values</strong>:</p>
<pre><code class="language-yaml">when: "${build_output.success|default:false}"
</code></pre>
</li>
</ol>
<h2 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h2>
<h3 id="command-times-out"><a class="header" href="#command-times-out">Command Times Out</a></h3>
<p><strong>Problem</strong>: Command exceeds timeout and is terminated</p>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Increase <code>timeout</code> value</li>
<li>Optimize command performance</li>
<li>Split into multiple smaller commands</li>
<li>Remove timeout for commands with unpredictable duration</li>
</ul>
<h3 id="capture-format-mismatch"><a class="header" href="#capture-format-mismatch">Capture Format Mismatch</a></h3>
<p><strong>Problem</strong>: Error parsing captured output with specified format</p>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Verify command output matches expected format (use <code>echo</code> to inspect)</li>
<li>Add format validation to command output</li>
<li>Use <code>capture_format: "string"</code> as fallback</li>
<li>Check for extra whitespace or unexpected characters</li>
</ul>
<h3 id="on_failure-not-triggering"><a class="header" href="#on_failure-not-triggering">on_failure Not Triggering</a></h3>
<p><strong>Problem</strong>: Failure handler doesn’t execute when command fails</p>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Verify command actually returns non-zero exit code</li>
<li>Check <code>max_attempts</code> hasn’t been exceeded</li>
<li>Ensure <code>on_failure</code> syntax is correct (nested under command)</li>
<li>Review workflow logs for execution details</li>
</ul>
<h3 id="variable-not-available"><a class="header" href="#variable-not-available">Variable Not Available</a></h3>
<p><strong>Problem</strong>: <code>${variable}</code> not found or empty in subsequent command</p>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Verify <code>capture_output</code> is set with correct variable name</li>
<li>Check command actually produces output (not empty)</li>
<li>Ensure <code>capture_format</code> matches output type</li>
<li>Use default values: <code>${var|default:fallback}</code></li>
</ul>
<h2 id="see-also-3"><a class="header" href="#see-also-3">See Also</a></h2>
<ul>
<li><a href="workflow-basics/environment-variables.html">Environment Variables</a> - Workflow-level environment configuration</li>
<li><a href="workflow-basics/../../advanced/timeout-configuration.html">../../advanced/timeout-configuration.md</a> - Advanced timeout strategies</li>
<li><a href="workflow-basics/../../advanced/parallel-iteration-with-foreach.html">../../advanced/parallel-iteration-with-foreach.md</a> - Foreach command with parallel options</li>
<li><a href="workflow-basics/../../examples.html">../../examples.md</a> - Complete workflow examples</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="environment-configuration"><a class="header" href="#environment-configuration">Environment Configuration</a></h2>
<p>Environment variables can be configured at multiple levels in Prodigy workflows, providing flexible control over workflow execution across different environments and contexts.</p>
<p><strong>Source</strong>: Configuration structures defined in <a href="workflow-basics/../../../src/cook/environment/config.rs">src/cook/environment/config.rs:11-144</a>, workflow configuration in <a href="workflow-basics/../../../src/config/workflow.rs">src/config/workflow.rs:12-39</a>.</p>
<h3 id="overview-1"><a class="header" href="#overview-1">Overview</a></h3>
<p>Prodigy supports comprehensive environment configuration including:</p>
<ul>
<li>Global environment variables with static, dynamic, and conditional values</li>
<li>Secret management with automatic log masking</li>
<li>Environment profiles for different deployment contexts</li>
<li>Environment files (.env format)</li>
<li>Step-level environment overrides</li>
<li>Variable interpolation in commands</li>
</ul>
<p>This subsection provides a quick introduction to environment configuration. For comprehensive coverage, see the <a href="workflow-basics/../environment/index.html">Environment chapter</a>.</p>
<h3 id="global-environment-variables"><a class="header" href="#global-environment-variables">Global Environment Variables</a></h3>
<p>Define environment variables in the <code>env:</code> block at the workflow root. Variables can be static strings, dynamically computed from commands, or conditionally set based on expressions.</p>
<p><strong>Source</strong>: <a href="workflow-basics/../../../workflows/environment-example.yml">workflows/environment-example.yml:4-18</a></p>
<pre><code class="language-yaml">env:
  # Static environment variables
  NODE_ENV: production
  API_URL: https://api.example.com

  # Dynamic environment variable (computed from command)
  WORKERS:
    command: "nproc 2&gt;/dev/null || echo 4"
    cache: true

  # Conditional environment variable
  DEPLOY_ENV:
    condition: "${branch} == 'main'"
    when_true: "production"
    when_false: "staging"
</code></pre>
<p><strong>Type Definitions</strong> (<a href="workflow-basics/../../../src/cook/environment/config.rs">src/cook/environment/config.rs:38-81</a>):</p>
<ul>
<li><code>EnvValue::Static(String)</code> - Static string value</li>
<li><code>EnvValue::Dynamic(DynamicEnv)</code> - Computed from command with optional caching</li>
<li><code>EnvValue::Conditional(ConditionalEnv)</code> - Based on condition expression</li>
</ul>
<h3 id="environment-files"><a class="header" href="#environment-files">Environment Files</a></h3>
<p>Load environment variables from .env format files using the <code>env_files:</code> field.</p>
<p><strong>Source</strong>: <a href="workflow-basics/../../../workflows/environment-example.yml">workflows/environment-example.yml:25-27</a>, <a href="workflow-basics/../../../src/cook/environment/config.rs">src/cook/environment/config.rs:22-23</a></p>
<pre><code class="language-yaml">env_files:
  - .env.production
  - .env.local
</code></pre>
<p>Variables from env files are merged with global env variables, with explicit env: block taking precedence.</p>
<p>For detailed information about .env file format and loading behavior, see <a href="workflow-basics/../environment/environment-files.html">Environment Files</a>.</p>
<h3 id="secrets-management"><a class="header" href="#secrets-management">Secrets Management</a></h3>
<p>Secret environment variables are automatically masked in all log output, preventing accidental credential leaks.</p>
<p><strong>Source</strong>: <a href="workflow-basics/../../../workflows/mapreduce-env-example.yml">workflows/mapreduce-env-example.yml:23-26</a>, <a href="workflow-basics/../../../src/cook/environment/config.rs">src/cook/environment/config.rs:86-112</a></p>
<pre><code class="language-yaml">secrets:
  # Simple secret reference
  API_KEY: "${env:SECRET_API_KEY}"

  # Provider-based secret
  API_TOKEN:
    provider: env
    key: "GITHUB_TOKEN"
</code></pre>
<p><strong>Secret Providers</strong> (<a href="workflow-basics/../../../src/cook/environment/config.rs">src/cook/environment/config.rs:101-112</a>):</p>
<ul>
<li><code>env</code> - Environment variable</li>
<li><code>file</code> - File-based secret</li>
<li><code>vault</code> - HashiCorp Vault</li>
<li><code>aws</code> - AWS Secrets Manager</li>
<li>Custom providers</li>
</ul>
<p>For comprehensive secrets management documentation, see <a href="workflow-basics/../environment/secrets-management.html">Secrets Management</a>.</p>
<h3 id="environment-profiles"><a class="header" href="#environment-profiles">Environment Profiles</a></h3>
<p>Profiles enable different environment configurations for various deployment contexts (development, staging, production).</p>
<p><strong>Source</strong>: <a href="workflow-basics/../../../workflows/environment-example.yml">workflows/environment-example.yml:30-39</a>, <a href="workflow-basics/../../../src/cook/environment/config.rs">src/cook/environment/config.rs:116-124</a></p>
<pre><code class="language-yaml">profiles:
  development:
    description: "Development environment with debug enabled"
    NODE_ENV: development
    API_URL: http://localhost:3000
    DEBUG: "true"

  production:
    description: "Production environment"
    NODE_ENV: production
    API_URL: https://api.example.com
    DEBUG: "false"
</code></pre>
<p><strong>Activate a profile</strong>:</p>
<pre><code class="language-bash"># Via command line flag
prodigy run workflow.yml --profile development

# Via environment variable
export PRODIGY_PROFILE=production
prodigy run workflow.yml
</code></pre>
<p>For detailed profile configuration and best practices, see <a href="workflow-basics/../environment/environment-profiles.html">Environment Profiles</a>.</p>
<h3 id="step-level-environment-overrides"><a class="header" href="#step-level-environment-overrides">Step-Level Environment Overrides</a></h3>
<p>Commands can define their own environment variables that override global and profile settings.</p>
<p><strong>Source</strong>: <a href="workflow-basics/../../../workflows/environment-example.yml">workflows/environment-example.yml:47-60</a>, <a href="workflow-basics/../../../src/cook/environment/config.rs">src/cook/environment/config.rs:126-144</a></p>
<pre><code class="language-yaml">commands:
  - name: "Build frontend"
    shell: "echo 'Building with NODE_ENV='$NODE_ENV"
    env:
      BUILD_TARGET: production
      OPTIMIZE: "true"
    working_dir: ./frontend

  - name: "Run tests"
    shell: "pytest"
    env:
      PYTHONPATH: "./src:./tests"
      TEST_ENV: "true"
    working_dir: ./backend
    temporary: true  # Environment restored after this step
</code></pre>
<p><strong>Step Environment Fields</strong> (<a href="workflow-basics/../../../src/cook/environment/config.rs">src/cook/environment/config.rs:128-144</a>):</p>
<ul>
<li><code>env: HashMap&lt;String, String&gt;</code> - Step-specific variables</li>
<li><code>working_dir: Option&lt;PathBuf&gt;</code> - Working directory for this step</li>
<li><code>clear_env: bool</code> - Clear parent environment before applying step env</li>
<li><code>temporary: bool</code> - Restore environment after step completes</li>
</ul>
<p>For detailed step-level configuration, see <a href="workflow-basics/../environment/per-command-environment-overrides.html">Per-Command Environment Overrides</a>.</p>
<h3 id="variable-interpolation"><a class="header" href="#variable-interpolation">Variable Interpolation</a></h3>
<p>Environment variables can be referenced in commands using two syntaxes: <code>$VAR</code> or <code>${VAR}</code>.</p>
<p><strong>Source</strong>: <a href="workflow-basics/../../../workflows/mapreduce-env-example.yml">workflows/mapreduce-env-example.yml:44-80</a></p>
<pre><code class="language-yaml">env:
  PROJECT_NAME: "my-project"
  OUTPUT_DIR: "output"

commands:
  # Both syntaxes work
  - shell: "echo Starting $PROJECT_NAME"
  - shell: "echo Output directory: ${OUTPUT_DIR}"

  # Use ${} for complex expressions
  - shell: "cp summary.json ${OUTPUT_DIR}/${PROJECT_NAME}-report.json"
</code></pre>
<p>Variable interpolation is available in:</p>
<ul>
<li>Shell commands</li>
<li>Claude commands</li>
<li>File paths</li>
<li>MapReduce configuration (max_parallel, timeout, etc.)</li>
</ul>
<p>For comprehensive variable interpolation documentation, see <a href="workflow-basics/../variables/available-variables.html">Variable Interpolation</a>.</p>
<h3 id="environment-precedence"><a class="header" href="#environment-precedence">Environment Precedence</a></h3>
<p>When the same variable is defined at multiple levels, precedence is:</p>
<ol>
<li><strong>Step-level env</strong> (highest priority)</li>
<li><strong>Profile env</strong></li>
<li><strong>Global env</strong></li>
<li><strong>System environment</strong> (lowest priority)</li>
</ol>
<p><strong>Source</strong>: Precedence logic in <a href="workflow-basics/../../../src/cook/environment/builder.rs">src/cook/environment/builder.rs:48-53</a>, tests in <a href="workflow-basics/../../../tests/environment_workflow_test.rs">tests/environment_workflow_test.rs:62-132</a></p>
<p>For detailed precedence rules and examples, see <a href="workflow-basics/../environment/environment-precedence.html">Environment Precedence</a>.</p>
<h3 id="mapreduce-environment-variables"><a class="header" href="#mapreduce-environment-variables">MapReduce Environment Variables</a></h3>
<p>Environment variables are available across all MapReduce workflow phases: setup, map, reduce, and merge.</p>
<p><strong>Source</strong>: <a href="workflow-basics/../../../workflows/mapreduce-env-example.yml">workflows/mapreduce-env-example.yml:1-95</a>, <a href="workflow-basics/../../../src/config/mapreduce.rs">src/config/mapreduce.rs:24-38</a></p>
<pre><code class="language-yaml">name: mapreduce-workflow
mode: mapreduce

env:
  PROJECT_NAME: "my-project"
  MAX_RETRIES: "3"

setup:
  - shell: "echo Starting $PROJECT_NAME"

map:
  agent_template:
    - claude: "/process --project $PROJECT_NAME --retries $MAX_RETRIES"

reduce:
  - shell: "echo Completed $PROJECT_NAME workflow"

merge:
  commands:
    - shell: "echo Merging $PROJECT_NAME changes"
</code></pre>
<p>For comprehensive MapReduce environment documentation, see <a href="workflow-basics/../environment/mapreduce-environment-variables.html">MapReduce Environment Variables</a>.</p>
<h3 id="complete-example"><a class="header" href="#complete-example">Complete Example</a></h3>
<p>This example demonstrates multiple environment features working together.</p>
<p><strong>Source</strong>: Complete workflow in <a href="workflow-basics/../../../workflows/environment-example.yml">workflows/environment-example.yml:1-70</a></p>
<pre><code class="language-yaml"># Global environment with static, dynamic, and conditional values
env:
  NODE_ENV: production
  API_URL: https://api.example.com

  WORKERS:
    command: "nproc 2&gt;/dev/null || echo 4"
    cache: true

  DEPLOY_ENV:
    condition: "${branch} == 'main'"
    when_true: "production"
    when_false: "staging"

# Secrets (masked in logs)
secrets:
  API_KEY: "${env:SECRET_API_KEY}"

# Environment files
env_files:
  - .env.production

# Profiles
profiles:
  development:
    NODE_ENV: development
    API_URL: http://localhost:3000
    DEBUG: "true"

# Commands with step-level overrides
commands:
  - name: "Build"
    shell: "npm run build"
    env:
      BUILD_TARGET: production
    working_dir: ./frontend

  - name: "Deploy"
    shell: "echo Deploying to $DEPLOY_ENV"
</code></pre>
<h3 id="best-practices-1"><a class="header" href="#best-practices-1">Best Practices</a></h3>
<ol>
<li><strong>Use profiles for environments</strong>: Define development, staging, and production profiles rather than duplicating workflows</li>
<li><strong>Mark secrets appropriately</strong>: Always use the <code>secrets:</code> block for sensitive data</li>
<li><strong>Prefer dynamic values</strong>: Use dynamic env for values that may change (e.g., CPU count, git branch)</li>
<li><strong>Use step-level env sparingly</strong>: Only override at step level when necessary</li>
<li><strong>Document environment requirements</strong>: Use comments to document required system environment variables</li>
</ol>
<p>For comprehensive best practices, see <a href="workflow-basics/../environment/best-practices.html">Environment Best Practices</a>.</p>
<h3 id="further-reading"><a class="header" href="#further-reading">Further Reading</a></h3>
<ul>
<li><a href="workflow-basics/../environment/index.html">Environment chapter</a> - Comprehensive environment configuration guide</li>
<li><a href="workflow-basics/../variables/available-variables.html">Variable Interpolation</a> - Complete variable reference</li>
<li><a href="workflow-basics/../environment/mapreduce-environment-variables.html">MapReduce Environment Variables</a> - Environment in distributed workflows</li>
<li><a href="workflow-basics/../environment/secrets-management.html">Secrets Management</a> - Secure credential handling</li>
<li><a href="workflow-basics/../configuration/configuration-precedence-rules.html">Configuration Precedence</a> - How configuration values are resolved</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="merge-workflows"><a class="header" href="#merge-workflows">Merge Workflows</a></h2>
<p>Merge workflows execute when merging worktree changes back to the main branch. This feature enables custom validation, testing, and conflict resolution before integrating changes.</p>
<p><strong>When to use merge workflows:</strong></p>
<ul>
<li>Run tests before merging</li>
<li>Validate code quality</li>
<li>Handle merge conflicts automatically</li>
<li>Sync with upstream changes</li>
</ul>
<h3 id="configuration-formats"><a class="header" href="#configuration-formats">Configuration Formats</a></h3>
<p>Merge workflows support two configuration formats (src/config/mapreduce.rs:96-123):</p>
<p><strong>1. Simplified Format</strong> (direct array of commands, no timeout support):</p>
<pre><code class="language-yaml">merge:
  - shell: "git fetch origin"
  - shell: "cargo test"
  - claude: "/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}"
</code></pre>
<p><strong>2. Full Format</strong> (config object with commands array and optional timeout):</p>
<pre><code class="language-yaml">merge:
  commands:
    - shell: "git fetch origin"
    - shell: "git merge origin/main"
    - shell: "cargo test"
    - claude: "/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}"
  timeout: 600  # Optional: timeout for entire merge phase (seconds)
</code></pre>
<p>Use the simplified format for quick merge workflows without timeouts. Use the full format when you need timeout control for long-running merge operations.</p>
<p><strong>Important</strong>: Always pass both <code>${merge.source_branch}</code> and <code>${merge.target_branch}</code> to the <code>/prodigy-merge-worktree</code> command. This ensures the merge targets the branch you were on when you started the workflow, not a hardcoded main/master branch.</p>
<h3 id="available-merge-variables"><a class="header" href="#available-merge-variables">Available Merge Variables</a></h3>
<p>The following variables are available exclusively within merge workflow commands. Variable interpolation happens before command execution, and these variables are NOT available in setup/map/reduce phases:</p>
<ul>
<li><code>${merge.worktree}</code> - Worktree name (e.g., “prodigy-session-abc123”)</li>
<li><code>${merge.source_branch}</code> - Source branch (worktree branch)</li>
<li><code>${merge.target_branch}</code> - Target branch (the branch you were on when workflow started)</li>
<li><code>${merge.session_id}</code> - Session ID for correlation</li>
</ul>
<p>Merge workflows also have access to all workflow <a href="workflow-basics/environment-configuration.html">environment variables</a> defined in the env block, including profile-specific values and secrets.</p>
<h3 id="claude-merge-streaming"><a class="header" href="#claude-merge-streaming">Claude Merge Streaming</a></h3>
<p>Claude commands in merge workflows respect verbosity settings (src/worktree/merge_orchestrator.rs:521-534):</p>
<ul>
<li>Use <code>-v</code> flag for real-time streaming output</li>
<li>Set <code>PRODIGY_CLAUDE_CONSOLE_OUTPUT=true</code> to force streaming regardless of verbosity</li>
<li>Default behavior shows clean minimal output</li>
</ul>
<p>This provides full visibility into Claude’s merge operations and tool invocations.</p>
<h3 id="real-world-examples-1"><a class="header" href="#real-world-examples-1">Real-World Examples</a></h3>
<p><strong>Pre-merge CI validation</strong> (workflows/implement.yml:33-41):</p>
<pre><code class="language-yaml">merge:
  - claude: "/prodigy-merge-master"  # Merge main into worktree first
  - claude: "/prodigy-ci"  # Run all CI checks
  - claude: "/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}"
</code></pre>
<p><strong>Environment-aware merge</strong> (workflows/mapreduce-env-example.yml:83-93):</p>
<pre><code class="language-yaml">merge:
  commands:
    - shell: "echo Merging changes for $PROJECT_NAME"
    - shell: "echo Debug mode was: $DEBUG_MODE"
    - claude: "/validate-merge --branch ${merge.source_branch} --project $PROJECT_NAME"
</code></pre>
<p><strong>Documentation workflow with cleanup</strong> (workflows/book-docs-drift.yml:93-100):</p>
<pre><code class="language-yaml">merge:
  commands:
    - shell: "git fetch origin"
    - claude: "/prodigy-merge-master --project ${PROJECT_NAME}"
    - claude: "/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}"
</code></pre>
<h3 id="best-practices-2"><a class="header" href="#best-practices-2">Best Practices</a></h3>
<ol>
<li><strong>Always use both branch parameters</strong>: Pass both <code>${merge.source_branch}</code> and <code>${merge.target_branch}</code> to <code>/prodigy-merge-worktree</code> to ensure correct merge target</li>
<li><strong>Run tests before final merge</strong>: Catch integration issues early in the merge workflow</li>
<li><strong>Sync with upstream first</strong>: Use <code>git fetch</code> to get latest changes before merging</li>
<li><strong>Use timeouts for long operations</strong>: Apply the timeout field when merge operations might hang</li>
<li><strong>Handle conflicts proactively</strong>: Consider adding conflict resolution commands for complex merges (e.g., <code>/prodigy-merge-master</code> before the final merge)</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h2 id="complete-example-1"><a class="header" href="#complete-example-1">Complete Example</a></h2>
<p>Here’s a complete workflow demonstrating Prodigy’s core features in a single file. This example combines environment configuration, workflow commands, and custom merge behavior.</p>
<p><strong>Source</strong>: Based on <a href="workflow-basics/../../../workflows/implement.yml">workflows/implement.yml</a></p>
<pre><code class="language-yaml"># Environment configuration
env:
  RUST_BACKTRACE: 1              # Standard environment variable

env_files:
  - .env                         # Load variables from .env file (dotenv format)

profiles:
  ci:                            # Activate with: prodigy run workflow.yml --profile ci
    CI: "true"
    VERBOSE: "true"

# Workflow commands
commands:
  - shell: "cargo fmt --check"
  - shell: "cargo clippy -- -D warnings"
  - shell: "cargo test --all"
  - claude: "/prodigy-lint"

# Custom merge workflow (simplified format)
merge:
  - shell: "cargo test"          # Validate before merging
  - claude: "/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}"
</code></pre>
<h3 id="key-features-demonstrated"><a class="header" href="#key-features-demonstrated">Key Features Demonstrated</a></h3>
<ol>
<li>
<p><strong>Environment Variables</strong> (lines 2-4): Define variables available to all commands</p>
<ul>
<li>See <a href="workflow-basics/../environment/index.html">Environment Variables</a> for details</li>
</ul>
</li>
<li>
<p><strong>Environment Files</strong> (lines 6-7): Load variables from <code>.env</code> files in dotenv format</p>
<ul>
<li>See <a href="workflow-basics/../environment/environment-files.html">Environment Files</a> for syntax</li>
</ul>
</li>
<li>
<p><strong>Profiles</strong> (lines 9-12): Define environment sets activated via <code>--profile</code> flag</p>
<ul>
<li>Example: <code>prodigy run workflow.yml --profile ci</code></li>
<li>See <a href="workflow-basics/../environment/environment-profiles.html">Environment Profiles</a> for advanced usage</li>
</ul>
</li>
<li>
<p><strong>Workflow Commands</strong> (lines 15-19): Execute shell and Claude commands sequentially</p>
<ul>
<li>See <a href="workflow-basics/../commands.html">Command Types</a> for all command types</li>
</ul>
</li>
<li>
<p><strong>Custom Merge Workflow</strong> (lines 22-24): Customize the merge-back process</p>
<ul>
<li><strong>Important</strong>: Always include both <code>${merge.source_branch}</code> and <code>${merge.target_branch}</code> parameters</li>
<li>The simplified array format is shown here (supported by <a href="workflow-basics/../../../src/config/mapreduce.rs:96-124">MergeWorkflow</a>)</li>
<li>See next section for the full configuration format</li>
</ul>
</li>
</ol>
<h3 id="alternative-merge-format-with-timeout"><a class="header" href="#alternative-merge-format-with-timeout">Alternative Merge Format (with timeout)</a></h3>
<p>The merge block also supports a configuration format with timeout:</p>
<pre><code class="language-yaml">merge:
  commands:
    - shell: "cargo test"
    - claude: "/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}"
  timeout: 600                   # Optional: timeout in seconds (10 minutes)
</code></pre>
<p><strong>Source</strong>: <a href="workflow-basics/../../../src/config/mapreduce.rs">src/config/mapreduce.rs:86-124</a></p>
<h3 id="real-world-example"><a class="header" href="#real-world-example">Real-World Example</a></h3>
<p>For a production-grade workflow with validation and error handling, see the implementation workflow:</p>
<p><strong>File</strong>: <code>workflows/implement.yml</code> (lines 32-41)</p>
<pre><code class="language-yaml">merge:
  # Step 1: Merge master into worktree
  - claude: "/prodigy-merge-master"

  # Step 2: Run CI checks and fix any issues
  - claude: "/prodigy-ci"

  # Step 3: Merge worktree back to original branch
  - claude: "/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}"
</code></pre>
<p>This demonstrates best practices:</p>
<ul>
<li>Sync with upstream before merging back</li>
<li>Validate changes with CI checks</li>
<li>Use proper merge parameters</li>
</ul>
<h3 id="next-steps-1"><a class="header" href="#next-steps-1">Next Steps</a></h3>
<ul>
<li><a href="workflow-basics/../error-handling.html">Error Handling</a> - Add <code>on_failure</code> handlers for robust workflows</li>
<li><a href="workflow-basics/../variables/available-variables.html">Variable Interpolation</a> - Use dynamic values in commands</li>
<li><a href="workflow-basics/../advanced/index.html">Advanced Features</a> - Explore goal-seeking, validation, and more</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="next-steps-2"><a class="header" href="#next-steps-2">Next Steps</a></h2>
<p>Now that you understand basic workflows, explore these topics:</p>
<h3 id="within-workflow-basics"><a class="header" href="#within-workflow-basics">Within Workflow Basics</a></h3>
<p>Continue learning about workflow fundamentals:</p>
<ul>
<li>
<p><strong><a href="workflow-basics/command-types.html">Command Types</a></strong> - Learn about all available command types (claude, shell, goal_seek, foreach) and when to use each one</p>
</li>
<li>
<p><strong><a href="workflow-basics/command-level-options.html">Command-Level Options</a></strong> - Advanced command configuration including timeout management, output capture, error handling, and conditional execution</p>
</li>
<li>
<p><strong><a href="workflow-basics/full-workflow-structure.html">Full Workflow Structure</a></strong> - Complete workflow format with explicit configuration for complex automation pipelines</p>
</li>
<li>
<p><strong><a href="workflow-basics/environment-configuration.html">Environment Configuration</a></strong> - Configure environment variables at multiple levels (workflow-wide, step-specific, profiles)</p>
</li>
</ul>
<h3 id="advanced-topics"><a class="header" href="#advanced-topics">Advanced Topics</a></h3>
<p>Ready to level up? Explore these advanced features:</p>
<ul>
<li>
<p><strong><a href="workflow-basics/../advanced/index.html">Advanced Features</a></strong> - Conditional execution, parallel processing with foreach, goal-seeking operations with validation, and complex control flow</p>
</li>
<li>
<p><strong><a href="workflow-basics/../environment/index.html">Environment Configuration</a></strong> - Comprehensive guide to environment variables, secrets management, profiles, and per-command overrides</p>
</li>
<li>
<p><strong><a href="workflow-basics/../error-handling.html">Error Handling</a></strong> - Workflow-level and command-level error handling strategies for building resilient automation</p>
</li>
<li>
<p><strong><a href="workflow-basics/../mapreduce/index.html">MapReduce Workflows</a></strong> - Massive parallel processing with setup/map/reduce phases for large-scale tasks like codebase transformations</p>
</li>
<li>
<p><strong><a href="workflow-basics/../variables/index.html">Variable Interpolation</a></strong> - Capture command outputs, reference work items, access map results, and build dynamic workflows with variables</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mapreduce-workflows"><a class="header" href="#mapreduce-workflows">MapReduce Workflows</a></h1>
<h2 id="quick-start-1"><a class="header" href="#quick-start-1">Quick Start</a></h2>
<p>Want to get started with MapReduce? Here’s a minimal working example:</p>
<pre><code class="language-yaml">name: my-first-mapreduce
mode: mapreduce

# Generate work items
setup:
  - shell: "echo '[{\"id\": 1, \"name\": \"task-1\"}, {\"id\": 2, \"name\": \"task-2\"}]' &gt; items.json"

# Process items in parallel
map:
  input: "items.json"
  json_path: "$[*]"
  agent_template:
    - shell: "echo Processing ${item.name}"
  max_parallel: 5

# Aggregate results
reduce:
  - shell: "echo Completed ${map.successful}/${map.total} items"
</code></pre>
<p>Run it:</p>
<pre><code class="language-bash">prodigy run workflow.yml
</code></pre>
<p>That’s it! Now let’s explore the full capabilities.</p>
<h2 id="complete-structure"><a class="header" href="#complete-structure">Complete Structure</a></h2>
<pre><code class="language-yaml">name: parallel-processing
mode: mapreduce

# Optional setup phase
setup:
  - shell: "generate-work-items.sh"
  - shell: "debtmap analyze . --output items.json"

# Map phase: Process items in parallel
map:
  # Input source (JSON file or command)
  input: "items.json"

  # JSONPath expression to extract items
  json_path: "$.items[*]"

  # Agent template (commands run for each item)
  # Modern syntax: Commands directly under agent_template
  agent_template:
    - claude: "/process '${item}'"
    - shell: "test ${item.path}"
      on_failure:
        claude: "/fix-issue '${item}'"

  # DEPRECATED: Nested 'commands' syntax (still supported)
  # agent_template:
  #   commands:
  #     - claude: "/process '${item}'"

  # Maximum parallel agents (can use environment variables)
  max_parallel: 10  # or max_parallel: "$MAX_WORKERS"

  # Optional: Filter items
  filter: "item.score &gt;= 5"

  # Optional: Sort items
  sort_by: "item.priority DESC"

  # Optional: Limit number of items
  max_items: 100

  # Optional: Skip items
  offset: 10

  # Optional: Deduplicate by field
  distinct: "item.id"

  # Optional: Agent timeout in seconds
  agent_timeout_secs: 300

  # Optional: Advanced timeout configuration (alternative to agent_timeout_secs)
  # timeout_config:
  #   default: "5m"
  #   per_command: "2m"

# Reduce phase: Aggregate results
# Modern syntax: Commands directly under reduce
reduce:
  - claude: "/summarize ${map.results}"
  - shell: "echo 'Processed ${map.successful}/${map.total} items'"

# DEPRECATED: Nested 'commands' syntax (still supported)
# reduce:
#   commands:
#     - claude: "/summarize ${map.results}"

# Optional: Custom merge workflow (supports two formats)
merge:
  # Simple array format
  - shell: "git fetch origin"
  - claude: "/merge-worktree ${merge.source_branch}"
  - shell: "cargo test"

# OR full format with timeout
# merge:
#   commands:
#     - shell: "git fetch origin"
#     - claude: "/merge-worktree ${merge.source_branch}"
#   timeout: 600  # Timeout in seconds

# Error handling policy
error_policy:
  on_item_failure: dlq  # dlq, retry, skip, stop, or custom handler name
  continue_on_failure: true
  max_failures: 5
  failure_threshold: 0.2  # 20% failure rate
  error_collection: aggregate  # aggregate, immediate, or batched:N

  # Circuit breaker configuration
  circuit_breaker:
    failure_threshold: 5      # Open circuit after N failures
    success_threshold: 2      # Close circuit after N successes
    timeout: "60s"           # Duration before attempting half-open (humantime format: "1s", "1m", "5m")
    half_open_requests: 3    # Test requests in half-open state

  # Retry configuration with backoff
  retry_config:
    max_attempts: 3
    # BackoffStrategy is an enum - use one variant:
    backoff:
      exponential:
        initial: "1s"
        multiplier: 2.0

# Convenience fields (alternative to nested error_policy)
# These top-level fields map to error_policy for simpler syntax
on_item_failure: dlq
continue_on_failure: true
max_failures: 5
</code></pre>
<h2 id="additional-topics-1"><a class="header" href="#additional-topics-1">Additional Topics</a></h2>
<p>See also:</p>
<ul>
<li><a href="mapreduce/environment-variables-in-configuration.html">Environment Variables in Configuration</a></li>
<li><a href="mapreduce/backoff-strategies.html">Backoff Strategies</a></li>
<li><a href="mapreduce/error-collection-strategies.html">Error Collection Strategies</a></li>
<li><a href="mapreduce/setup-phase-advanced.html">Setup Phase (Advanced)</a></li>
<li><a href="mapreduce/global-storage-architecture.html">Global Storage Architecture</a></li>
<li><a href="mapreduce/event-tracking.html">Event Tracking</a></li>
<li><a href="mapreduce/checkpoint-and-resume.html">Checkpoint and Resume</a></li>
<li><a href="mapreduce/dead-letter-queue-dlq.html">Dead Letter Queue (DLQ)</a></li>
<li><a href="mapreduce/common-pitfalls.html">Common Pitfalls</a></li>
<li><a href="mapreduce/troubleshooting.html">Troubleshooting</a></li>
<li><a href="mapreduce/performance-tuning.html">Performance Tuning</a></li>
<li><a href="mapreduce/real-world-use-cases.html">Real-World Use Cases</a></li>
<li><a href="mapreduce/workflow-format-comparison.html">Workflow Format Comparison</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="environment-variables-in-configuration"><a class="header" href="#environment-variables-in-configuration">Environment Variables in Configuration</a></h2>
<p>MapReduce workflows support comprehensive environment variable configuration, enabling parameterized workflows with secrets management, multi-environment deployment, and secure credential handling.</p>
<h3 id="configuration-fields"><a class="header" href="#configuration-fields">Configuration Fields</a></h3>
<p>MapReduce workflows support four types of environment configuration:</p>
<p><strong>1. Basic Environment Variables (<code>env</code>)</strong></p>
<p>Define static environment variables available throughout the workflow:</p>
<pre><code class="language-yaml">env:
  MAX_WORKERS: "10"
  AGENT_TIMEOUT: "300"
  PROJECT_NAME: "my-project"
</code></pre>
<p><strong>Source</strong>: <code>src/config/workflow.rs:11-39</code> - WorkflowConfig struct with <code>env: HashMap&lt;String, String&gt;</code> field</p>
<p><strong>2. Secrets Configuration</strong></p>
<p>Secrets are defined in a dedicated <code>secrets:</code> block (separate from <code>env:</code>). Secret values are automatically masked in logs, output, events, and checkpoints.</p>
<p><strong>Source</strong>: <code>src/config/workflow.rs:24-26</code> - WorkflowConfig with <code>secrets: HashMap&lt;String, SecretValue&gt;</code> field</p>
<p><strong>Simple Secret Syntax (Recommended for most cases):</strong></p>
<pre><code class="language-yaml">secrets:
  # Reference to environment variable
  API_KEY: "${env:SECRET_API_KEY}"

  # Direct secret value (not recommended - use env refs instead)
  DB_PASSWORD: "my-secret-password"
</code></pre>
<p><strong>Provider-Based Secret Syntax:</strong></p>
<pre><code class="language-yaml">secrets:
  API_TOKEN:
    provider: env
    key: "GITHUB_TOKEN"

  AWS_SECRET:
    provider: aws
    key: "prod/api/credentials"
    version: "v1"  # Optional version
</code></pre>
<p><strong>Source</strong>: <code>src/cook/environment/config.rs:86-96</code> - SecretValue enum with Simple and Provider variants</p>
<p><strong>Supported Secret Providers</strong> (defined in <code>src/cook/environment/config.rs:99-112</code>):</p>
<ul>
<li><code>env</code> - Environment variable reference (fully supported)</li>
<li><code>file</code> - File-based secrets (fully supported)</li>
<li><code>vault</code> - HashiCorp Vault integration (planned)</li>
<li><code>aws</code> - AWS Secrets Manager (planned)</li>
<li><code>custom</code> - Custom provider support (planned)</li>
</ul>
<p><strong>Real-World Examples</strong>:</p>
<p>From <code>workflows/mapreduce-env-example.yml:22-26</code>:</p>
<pre><code class="language-yaml">secrets:
  API_TOKEN:
    provider: env
    key: "GITHUB_TOKEN"
</code></pre>
<p>From <code>workflows/environment-example.yml:20-23</code>:</p>
<pre><code class="language-yaml">secrets:
  API_KEY: "${env:SECRET_API_KEY}"
</code></pre>
<p><strong>3. Environment Files (<code>env_files</code>)</strong></p>
<p>Load environment variables from <code>.env</code> files:</p>
<pre><code class="language-yaml">env_files:
  - ".env"
  - ".env.local"
</code></pre>
<p><strong>Source</strong>: <code>src/config/workflow.rs:11-39</code> - WorkflowConfig with <code>env_files: Vec&lt;PathBuf&gt;</code> field</p>
<p>Files are loaded in order, with later files overriding earlier ones. Standard <code>.env</code> format: <code>KEY=value</code></p>
<p><strong>4. Profiles (<code>profiles</code>)</strong></p>
<p>Environment-specific configurations for different deployment targets:</p>
<pre><code class="language-yaml">profiles:
  dev:
    API_URL: "http://localhost:3000"
    TIMEOUT: "60"
    MAX_WORKERS: "5"

  prod:
    API_URL: "https://api.prod.com"
    TIMEOUT: "30"
    MAX_WORKERS: "20"
</code></pre>
<p><strong>Source</strong>: <code>src/cook/environment/config.rs:115-124</code> - EnvProfile struct</p>
<p>Activate a profile with:</p>
<pre><code class="language-bash">prodigy run workflow.yml --profile prod
</code></pre>
<p><strong>Profile Structure</strong>: Each profile contains a HashMap of environment variables and an optional description field for documentation.</p>
<h3 id="variable-interpolation-1"><a class="header" href="#variable-interpolation-1">Variable Interpolation</a></h3>
<p>Environment variables can be referenced using two syntaxes:</p>
<ul>
<li><code>$VAR</code> - Simple variable reference (shell-style)</li>
<li><code>${VAR}</code> - Bracketed reference for clarity and complex expressions</li>
</ul>
<p><strong>Source</strong>: Verified in <code>tests/mapreduce_env_execution_test.rs:157-189</code> - both syntaxes are fully supported</p>
<p>Use <code>${VAR}</code> when:</p>
<ul>
<li>Variable name is followed by alphanumeric characters</li>
<li>Embedding in strings or paths</li>
<li>Preference for explicit syntax</li>
</ul>
<p><strong>Interpolation in Environment Values</strong>:</p>
<p>Environment variables can reference other environment variables or system environment variables within their values:</p>
<pre><code class="language-yaml">env:
  BASE_URL: "https://api.example.com"
  API_ENDPOINT: "${BASE_URL}/v1/data"
  FULL_PATH: "${PROJECT_DIR}/output/${REPORT_FORMAT}"
</code></pre>
<p><strong>Source</strong>: <code>src/cook/environment/config.rs:39-60</code> - EnvValue supports Static, Dynamic, and Conditional variants</p>
<p><strong>Real-World Composition Example</strong>:</p>
<p>From <code>workflows/mapreduce-env-example.yml:78</code>:</p>
<pre><code class="language-yaml">env:
  PROJECT_NAME: "my-project"
  OUTPUT_DIR: "output"
  REPORT_FORMAT: "json"

reduce:
  # Composing multiple env vars in a single path
  - shell: "cp summary.$REPORT_FORMAT $OUTPUT_DIR/${PROJECT_NAME}-summary.$REPORT_FORMAT"
</code></pre>
<p>This demonstrates environment variable composition across different workflow variables to build complex paths dynamically.</p>
<p><strong>Supported Fields:</strong></p>
<ul>
<li><code>max_parallel</code> - Control parallelism dynamically</li>
<li><code>agent_timeout_secs</code> - Adjust timeouts per environment</li>
<li><code>setup.timeout</code> - Configure setup phase timeouts</li>
<li><code>merge.timeout</code> - Control merge operation timeouts</li>
<li>Any string field in your workflow (commands, paths, etc.)</li>
</ul>
<p><strong>Example from tests</strong> (<code>tests/mapreduce_env_execution_test.rs:10-52</code>):</p>
<pre><code class="language-yaml">env:
  MAX_PARALLEL: "5"
  TIMEOUT_SECONDS: "900"

map:
  max_parallel: ${MAX_PARALLEL}
  agent_timeout_secs: ${TIMEOUT_SECONDS}
</code></pre>
<h3 id="environment-precedence-1"><a class="header" href="#environment-precedence-1">Environment Precedence</a></h3>
<p>When the same variable is defined in multiple places, Prodigy uses the following precedence order (highest to lowest):</p>
<ol>
<li><strong>Command-line environment variables</strong> - Set when invoking prodigy</li>
<li><strong>Step-level environment</strong> - Variables defined in individual command <code>env:</code> blocks</li>
<li><strong>Active profile values</strong> - Variables from the selected profile</li>
<li><strong>Global env block</strong> - Base workflow environment variables</li>
<li><strong>Environment files</strong> - Variables loaded from .env files (later files override earlier)</li>
<li><strong>System environment</strong> - Parent process environment (if <code>inherit: true</code>)</li>
</ol>
<p><strong>Source</strong>: <code>src/cook/environment/config.rs:11-36</code> - EnvironmentConfig structure and <code>tests/environment_workflow_test.rs:225-256</code> - precedence verification</p>
<p><strong>Example</strong>:</p>
<pre><code class="language-bash"># Command-line overrides profile, which overrides base env
MAX_WORKERS=10 prodigy run workflow.yml --profile prod
</code></pre>
<h3 id="complete-example-2"><a class="header" href="#complete-example-2">Complete Example</a></h3>
<p>Here’s a complete MapReduce workflow demonstrating all environment features:</p>
<pre><code class="language-yaml">name: configurable-mapreduce
mode: mapreduce

# Basic environment variables
env:
  PROJECT_NAME: "data-pipeline"
  VERSION: "1.0.0"
  OUTPUT_DIR: "output"

# Secrets (masked in logs)
secrets:
  API_KEY: "${env:SECRET_API_KEY}"

# Environment files
env_files:
  - ".env"

# Multi-environment profiles
profiles:
  dev:
    MAX_WORKERS: "5"
    API_URL: "http://localhost:3000"
    DEBUG_MODE: "true"

  prod:
    MAX_WORKERS: "20"
    API_URL: "https://api.prod.com"
    DEBUG_MODE: "false"

setup:
  timeout: 300
  commands:
    - shell: "echo Processing $PROJECT_NAME v$VERSION"
    - shell: "mkdir -p $OUTPUT_DIR"
    - shell: "curl -H 'Authorization: Bearer ${API_KEY}' $API_URL/init"

map:
  input: "items.json"
  json_path: "$[*]"
  max_parallel: ${MAX_WORKERS}
  agent_template:
    - claude: "/process ${item} --project $PROJECT_NAME"
    - shell: "curl -H 'Authorization: Bearer ${API_KEY}' $API_URL/items"

reduce:
  - claude: "/summarize ${map.results} --project $PROJECT_NAME"
  - shell: "cp summary.json $OUTPUT_DIR/${PROJECT_NAME}-summary.json"
</code></pre>
<p><strong>Source</strong>: Based on <code>workflows/mapreduce-env-example.yml:1-94</code> with structure validated against <code>src/config/mapreduce.rs:15-82</code></p>
<h3 id="running-with-different-configurations"><a class="header" href="#running-with-different-configurations">Running with Different Configurations</a></h3>
<pre><code class="language-bash"># Development environment
prodigy run workflow.yml --profile dev

# Production environment
prodigy run workflow.yml --profile prod

# Override specific variables (command-line takes precedence)
MAX_WORKERS=10 prodigy run workflow.yml --profile prod

# Local development with .env file
echo "MAX_WORKERS=3" &gt; .env
prodigy run workflow.yml
</code></pre>
<p><strong>Precedence Note</strong>: Command-line environment variables override profile settings, which override the base env block.</p>
<p><strong>Source</strong>: Precedence behavior verified in <code>src/cook/environment/manager.rs:43-52</code> and <code>tests/mapreduce_env_execution_test.rs:225-256</code></p>
<h3 id="step-level-environment-overrides-1"><a class="header" href="#step-level-environment-overrides-1">Step-Level Environment Overrides</a></h3>
<p>Individual commands can override environment variables:</p>
<pre><code class="language-yaml">map:
  agent_template:
    - shell: "process-item.sh"
      env:
        CUSTOM_VAR: "value"
        PATH: "/custom/bin:${PATH}"
</code></pre>
<p><strong>Source</strong>: <code>src/cook/environment/config.rs:127-144</code> - StepEnvironment struct with <code>env</code>, <code>working_dir</code>, <code>clear_env</code>, and <code>temporary</code> fields</p>
<p>Step-level variables inherit from global <code>env</code> and active <code>profiles</code>, with step-level values taking precedence.</p>
<p><strong>Advanced Step Environment Features</strong> (<code>tests/environment_workflow_test.rs:42-70</code>):</p>
<ul>
<li><code>clear_env: true</code> - Start with clean environment except step-specific vars</li>
<li><code>temporary: true</code> - Restore environment after step execution</li>
<li><code>working_dir</code> - Set working directory for the step</li>
</ul>
<h3 id="best-practices-3"><a class="header" href="#best-practices-3">Best Practices</a></h3>
<p><strong>Secrets vs Environment Variables:</strong></p>
<ul>
<li>Use the dedicated <code>secrets:</code> block for sensitive data (API keys, passwords, tokens)</li>
<li>Use plain <code>env:</code> block for non-sensitive configuration (timeouts, URLs, feature flags)</li>
<li>Secrets are automatically masked in all logs and outputs</li>
<li><strong>Recommended</strong>: Use simple secret syntax with environment variable references: <code>API_KEY: "${env:SECRET_API_KEY}"</code></li>
<li><strong>Advanced</strong>: Use provider-based syntax for complex scenarios: <code>API_TOKEN: { provider: env, key: "GITHUB_TOKEN" }</code></li>
</ul>
<p><strong>Security Considerations:</strong></p>
<ul>
<li>Never commit secrets to version control</li>
<li>Use environment variable references like <code>${env:SECRET_VAR}</code> to inject secrets at runtime</li>
<li>Leverage <code>.env.local</code> files (git-ignored) for local development</li>
<li>Provider-based secrets for Vault and AWS Secrets Manager are defined in the type system but not yet fully implemented</li>
<li>File-based secrets are fully supported for reading secrets from files</li>
</ul>
<p><strong>Profile Usage:</strong></p>
<ul>
<li>Use profiles for multi-environment deployment (dev, staging, prod)</li>
<li>Define sensible defaults in the base <code>env</code> block</li>
<li>Override only environment-specific values in profiles</li>
<li>Activate profiles explicitly to avoid accidental production deployments</li>
<li>Include <code>description</code> field in profiles for documentation</li>
</ul>
<p><strong>Environment Files:</strong></p>
<ul>
<li>Use <code>env_files</code> for local development configuration</li>
<li>Order matters: later files override earlier ones</li>
<li>Combine with profiles for flexible local/remote workflows</li>
<li>Add <code>.env.local</code> to <code>.gitignore</code></li>
</ul>
<h3 id="debugging-environment-variables"><a class="header" href="#debugging-environment-variables">Debugging Environment Variables</a></h3>
<p>When running workflows with verbose output (<code>-vv</code> flag), Prodigy shows environment variable values with sensitive data masked:</p>
<p><strong>Source</strong>: <code>examples/test-sensitive-masking.yml:1-22</code> - demonstrates automatic masking</p>
<pre><code class="language-bash"># Run with debug output to see environment resolution
prodigy run workflow.yml -vv --profile dev

# Output will show:
# - PASSWORD as ***REDACTED***
# - API_KEY as ***REDACTED***
# - Normal variables displayed in full
</code></pre>
<h3 id="cross-references"><a class="header" href="#cross-references">Cross-References</a></h3>
<p>For comprehensive environment variable documentation, see:</p>
<ul>
<li><a href="mapreduce/../variables/index.html">Variables Chapter</a> - Complete guide to environment variables and interpolation</li>
<li><a href="mapreduce/../environment/index.html">Environment Configuration</a> - Detailed environment management patterns</li>
<li><a href="mapreduce/../environment/mapreduce-environment-variables.html">MapReduce Environment Variables</a> - MapReduce-specific environment features</li>
<li><a href="mapreduce/../environment/secrets-management.html">Secrets Management</a> - Security best practices for secrets</li>
<li><a href="mapreduce/../environment/environment-profiles.html">Environment Profiles</a> - Multi-environment deployment patterns</li>
<li><a href="mapreduce/./setup-phase-advanced.html">Setup Phase</a> - Using environment variables in setup commands</li>
<li><a href="mapreduce/./index.html">MapReduce Index</a> - Main MapReduce workflow documentation</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="backoff-strategies"><a class="header" href="#backoff-strategies">Backoff Strategies</a></h2>
<p>Backoff strategies control the delay between retry attempts when a MapReduce agent fails. Prodigy supports multiple backoff algorithms to handle different failure patterns and workload characteristics.</p>
<p><strong>Source</strong>: <code>BackoffStrategy</code> enum in <a href="mapreduce/../../src/cook/retry_v2.rs">src/cook/retry_v2.rs:73-90</a></p>
<h3 id="overview-2"><a class="header" href="#overview-2">Overview</a></h3>
<p>The <code>retry_config.backoff</code> field configures which backoff algorithm to use. All strategies work with the following <code>RetryConfig</code> fields:</p>
<ul>
<li><strong>initial_delay</strong>: Base delay for calculations (default: <code>1s</code>)</li>
<li><strong>max_delay</strong>: Maximum cap on any calculated delay (default: <code>30s</code>)</li>
<li><strong>jitter</strong>: Add randomness to prevent thundering herd (default: <code>false</code>)</li>
</ul>
<p><strong>Default Strategy</strong>: If no backoff is specified, Prodigy uses <strong>Exponential backoff with base 2.0</strong>.</p>
<p>Source: <a href="mapreduce/../../src/cook/retry_v2.rs">src/cook/retry_v2.rs:92-97</a></p>
<h3 id="available-strategies"><a class="header" href="#available-strategies">Available Strategies</a></h3>
<h4 id="fixed-backoff"><a class="header" href="#fixed-backoff">Fixed Backoff</a></h4>
<p>Uses a constant delay between all retry attempts.</p>
<p><strong>Simple form</strong> (uses <code>initial_delay</code>):</p>
<pre><code class="language-yaml">retry_config:
  backoff: fixed
  initial_delay: 2s
</code></pre>
<p><strong>Explicit form</strong> (error_policy implementation):</p>
<pre><code class="language-yaml">retry_config:
  backoff:
    fixed:
      delay: 5s
</code></pre>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Retry 1: 2s delay</li>
<li>Retry 2: 2s delay</li>
<li>Retry 3: 2s delay</li>
</ul>
<p><strong>Use cases</strong>:</p>
<ul>
<li>Fast retries for transient errors</li>
<li>Quick recovery for flaky tests</li>
<li>Network requests with predictable timeouts</li>
</ul>
<p><strong>Source</strong>: <a href="mapreduce/../../src/cook/retry_v2.rs">src/cook/retry_v2.rs:75</a> (retry_v2), <a href="mapreduce/../../src/cook/workflow/error_policy.rs">src/cook/workflow/error_policy.rs:110</a> (error_policy)</p>
<p><strong>Test</strong>: <a href="mapreduce/../../src/cook/retry_v2.rs">src/cook/retry_v2.rs:583-594</a></p>
<h4 id="linear-backoff"><a class="header" href="#linear-backoff">Linear Backoff</a></h4>
<p>Increases delay linearly on each retry: <code>initial_delay + (attempt * increment)</code>.</p>
<p><strong>retry_v2 form</strong>:</p>
<pre><code class="language-yaml">retry_config:
  backoff:
    linear:
      increment: 2s
  initial_delay: 1s
</code></pre>
<p><strong>error_policy form</strong>:</p>
<pre><code class="language-yaml">retry_config:
  backoff:
    linear:
      initial: 1s
      increment: 2s
</code></pre>
<p><strong>Behavior</strong> (retry_v2 example):</p>
<ul>
<li>Retry 1: 1s (initial_delay)</li>
<li>Retry 2: 3s (1s + 2s)</li>
<li>Retry 3: 5s (1s + 4s)</li>
</ul>
<p><strong>Use cases</strong>:</p>
<ul>
<li>Database connection retries</li>
<li>Resource allocation failures</li>
<li>Predictable backpressure</li>
</ul>
<p><strong>Source</strong>: <a href="mapreduce/../../src/cook/retry_v2.rs">src/cook/retry_v2.rs:77-80</a> (retry_v2), <a href="mapreduce/../../src/cook/workflow/error_policy.rs">src/cook/workflow/error_policy.rs:112-115</a> (error_policy)</p>
<p><strong>Test</strong>: <a href="mapreduce/../../src/cook/retry_v2.rs">src/cook/retry_v2.rs:597-610</a></p>
<h4 id="exponential-backoff-default"><a class="header" href="#exponential-backoff-default">Exponential Backoff (Default)</a></h4>
<p>Increases delay exponentially: <code>initial_delay * (base ^ attempt)</code>.</p>
<p><strong>retry_v2 form</strong> (uses <code>base</code>):</p>
<pre><code class="language-yaml">retry_config:
  backoff:
    exponential:
      base: 2.0
  initial_delay: 1s
  max_delay: 100s
</code></pre>
<p><strong>error_policy form</strong> (uses <code>multiplier</code>):</p>
<pre><code class="language-yaml">retry_config:
  backoff:
    exponential:
      initial: 1s
      multiplier: 2.0
</code></pre>
<p><strong>Behavior</strong> (base 2.0):</p>
<ul>
<li>Retry 1: 1s</li>
<li>Retry 2: 2s (1s * 2^1)</li>
<li>Retry 3: 4s (1s * 2^2)</li>
<li>Retry 4: 8s (1s * 2^3)</li>
<li>Retry 5: 16s (capped at max_delay if exceeded)</li>
</ul>
<p><strong>Use cases</strong>:</p>
<ul>
<li>API rate limiting</li>
<li>Network failures</li>
<li>Most general-purpose retry scenarios</li>
<li><strong>Recommended for MapReduce workloads</strong></li>
</ul>
<p><strong>Source</strong>: <a href="mapreduce/../../src/cook/retry_v2.rs">src/cook/retry_v2.rs:82-85</a> (retry_v2), <a href="mapreduce/../../src/cook/workflow/error_policy.rs">src/cook/workflow/error_policy.rs:117</a> (error_policy)</p>
<p><strong>Test</strong>: <a href="mapreduce/../../src/cook/retry_v2.rs">src/cook/retry_v2.rs:613-626</a></p>
<h4 id="fibonacci-backoff"><a class="header" href="#fibonacci-backoff">Fibonacci Backoff</a></h4>
<p>Uses the Fibonacci sequence (0, 1, 1, 2, 3, 5, 8, 13…) multiplied by <code>initial_delay</code>.</p>
<p><strong>retry_v2 form</strong>:</p>
<pre><code class="language-yaml">retry_config:
  backoff: fibonacci
  initial_delay: 1s
</code></pre>
<p><strong>error_policy form</strong>:</p>
<pre><code class="language-yaml">retry_config:
  backoff:
    fibonacci:
      initial: 1s
</code></pre>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Retry 1: 1s (fib(1) = 1)</li>
<li>Retry 2: 1s (fib(2) = 1)</li>
<li>Retry 3: 2s (fib(3) = 2)</li>
<li>Retry 4: 3s (fib(4) = 3)</li>
<li>Retry 5: 5s (fib(5) = 5)</li>
<li>Retry 6: 8s (fib(6) = 8)</li>
</ul>
<p><strong>Use cases</strong>:</p>
<ul>
<li><strong>Distributed systems</strong> (natural backpressure)</li>
<li><strong>Parallel MapReduce jobs</strong> (reduces retry storms)</li>
<li>Gradual recovery from cascading failures</li>
</ul>
<p><strong>Implementation</strong>: The Fibonacci sequence is calculated using iterative approach to avoid stack overflow for large attempt numbers.</p>
<p><strong>Source</strong>: <a href="mapreduce/../../src/cook/retry_v2.rs">src/cook/retry_v2.rs:87</a>, <a href="mapreduce/../../src/cook/retry_v2.rs">src/cook/retry_v2.rs:424-440</a> (fibonacci function)</p>
<p><strong>Test</strong>: <a href="mapreduce/../../src/cook/retry_v2.rs">src/cook/retry_v2.rs:629-642</a></p>
<h4 id="custom-backoff-retry_v2-only"><a class="header" href="#custom-backoff-retry_v2-only">Custom Backoff (retry_v2 only)</a></h4>
<p>Specify an explicit list of delays for each retry attempt.</p>
<p><strong>Note</strong>: Only available in the retry_v2 implementation, not in error_policy.</p>
<pre><code class="language-yaml">retry_config:
  backoff:
    custom:
      delays:
        - 500ms
        - 1s
        - 2s
        - 5s
        - 10s
</code></pre>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Retry 1: 500ms (delays[0])</li>
<li>Retry 2: 1s (delays[1])</li>
<li>Retry 3: 2s (delays[2])</li>
<li>Retry 4: 5s (delays[3])</li>
<li>Retry 5: 10s (delays[4])</li>
<li>Retry 6+: max_delay (if attempt exceeds array length)</li>
</ul>
<p><strong>Use cases</strong>:</p>
<ul>
<li>Fine-tuned retry patterns</li>
<li>Known failure recovery timelines</li>
<li>Testing specific delay sequences</li>
</ul>
<p><strong>Edge cases</strong>:</p>
<ul>
<li>Empty <code>delays</code> array: Uses <code>max_delay</code> for all attempts (fallback behavior)</li>
<li>Attempt exceeds array length: Uses <code>max_delay</code> (fallback behavior)</li>
</ul>
<p>This ensures predictable behavior when custom delays are exhausted or misconfigured, preventing undefined delay calculations.</p>
<p><strong>Source</strong>: <a href="mapreduce/../../src/cook/retry_v2.rs">src/cook/retry_v2.rs:88-89</a>, <a href="mapreduce/../../src/cook/retry_v2.rs">src/cook/retry_v2.rs:298-301</a> (calculation logic)</p>
<h3 id="mapreduce-integration"><a class="header" href="#mapreduce-integration">MapReduce Integration</a></h3>
<p>Backoff strategies are configured in the <code>retry_config</code> block, which can be part of:</p>
<ol>
<li><strong>WorkflowErrorPolicy</strong> at workflow level (applies to all items)</li>
<li><strong>TimeoutConfig</strong> in map phase (more specific control)</li>
</ol>
<p><strong>Complete MapReduce Example</strong>:</p>
<pre><code class="language-yaml">name: resilient-mapreduce-job
mode: mapreduce

map:
  input: "items.json"
  json_path: "$.items[*]"

  # Workflow-level error policy
  error_policy:
    retry_config:
      max_attempts: 5
      backoff:
        fibonacci:
          initial: 1s
      jitter: true
      jitter_factor: 0.3
    continue_on_failure: true

  agent_template:
    - claude: "/process-item '${item.id}'"
    - shell: "validate ${item.output}"
      on_failure:
        claude: "/fix-validation-error"

  max_parallel: 10
</code></pre>
<p><strong>Source</strong>: <a href="mapreduce/../../src/cook/workflow/error_policy.rs">src/cook/workflow/error_policy.rs:160</a> (retry_config field in WorkflowErrorPolicy)</p>
<h3 id="retryconfig-fields"><a class="header" href="#retryconfig-fields">RetryConfig Fields</a></h3>
<p>The <code>RetryConfig</code> structure controls all retry behavior:</p>
<pre><code class="language-yaml">retry_config:
  attempts: 3              # Maximum retry attempts (default: 3)
  backoff: exponential     # Strategy (default: exponential with base 2.0)
  initial_delay: 1s        # Base delay (default: 1s)
  max_delay: 30s           # Delay cap (default: 30s)
  jitter: true             # Add randomness (default: false)
  jitter_factor: 0.3       # Jitter amount 0.0-1.0 (default: 0.3)
</code></pre>
<p><strong>How fields interact</strong>:</p>
<ol>
<li>Backoff strategy calculates base delay using <code>initial_delay</code></li>
<li>Calculated delay is capped by <code>max_delay</code></li>
<li>If <code>jitter: true</code>, random offset is added: <code>delay ± (delay * jitter_factor)</code></li>
<li>Final delay is applied before next retry attempt</li>
</ol>
<p><strong>Source</strong>: <a href="mapreduce/../../src/cook/retry_v2.rs">src/cook/retry_v2.rs:16-52</a></p>
<p><strong>Defaults</strong>: <a href="mapreduce/../../src/cook/retry_v2.rs">src/cook/retry_v2.rs:443-461</a></p>
<h3 id="implementation-variants"><a class="header" href="#implementation-variants">Implementation Variants</a></h3>
<p>Prodigy has <strong>two BackoffStrategy implementations</strong>:</p>
<h4 id="retry_v2rs-recommended"><a class="header" href="#retry_v2rs-recommended">retry_v2.rs (Recommended)</a></h4>
<ul>
<li><strong>Location</strong>: <a href="mapreduce/../../src/cook/retry_v2.rs">src/cook/retry_v2.rs:73-90</a></li>
<li><strong>Features</strong>: Includes <code>Custom</code> backoff strategy</li>
<li><strong>Exponential parameter</strong>: <code>base</code> (default 2.0)</li>
<li><strong>Simple syntax</strong>: Most strategies use <code>backoff: strategy_name</code> form</li>
<li><strong>More comprehensive</strong>: Full RetryConfig with jitter, retry_budget, error matchers</li>
</ul>
<h4 id="error_policyrs-legacy"><a class="header" href="#error_policyrs-legacy">error_policy.rs (Legacy)</a></h4>
<ul>
<li><strong>Location</strong>: <a href="mapreduce/../../src/cook/workflow/error_policy.rs">src/cook/workflow/error_policy.rs:108-120</a></li>
<li><strong>Features</strong>: No <code>Custom</code> strategy</li>
<li><strong>Exponential parameter</strong>: <code>multiplier</code> instead of <code>base</code></li>
<li><strong>Explicit syntax</strong>: All strategies require nested configuration with <code>initial</code> delay</li>
<li><strong>Simpler</strong>: Minimal RetryConfig with max_attempts and backoff only</li>
</ul>
<p><strong>Which to use</strong>: Both are valid, but retry_v2 provides more features and better defaults. In MapReduce workflows, the error_policy implementation is used at the workflow level.</p>
<h3 id="best-practices-4"><a class="header" href="#best-practices-4">Best Practices</a></h3>
<p><strong>Choosing a Strategy</strong>:</p>
<div class="table-wrapper"><table><thead><tr><th>Scenario</th><th>Recommended Strategy</th><th>Reasoning</th></tr></thead><tbody>
<tr><td>Parallel MapReduce jobs</td><td>Fibonacci</td><td>Reduces retry storms, natural backpressure</td></tr>
<tr><td>API rate limiting</td><td>Exponential</td><td>Quickly backs off from rate limits</td></tr>
<tr><td>Database retries</td><td>Linear</td><td>Predictable recovery for connection pools</td></tr>
<tr><td>Fast transient errors</td><td>Fixed</td><td>Quick recovery without overloading</td></tr>
<tr><td>Custom requirements</td><td>Custom (retry_v2)</td><td>Fine-grained control for known patterns</td></tr>
</tbody></table>
</div>
<p><strong>Jitter Usage</strong>:</p>
<ul>
<li><strong>Enable jitter</strong> for MapReduce workloads to prevent thundering herd</li>
<li><strong>Disable jitter</strong> for testing and debugging (predictable delays)</li>
</ul>
<p><strong>Delay Configuration</strong>:</p>
<ul>
<li>Set <code>initial_delay</code> based on typical failure recovery time</li>
<li>Set <code>max_delay</code> to prevent excessively long waits</li>
<li>For MapReduce: Consider agent timeout interaction (delays count toward timeout)</li>
</ul>
<p><strong>Timeout Interaction</strong>:
Backoff delays count toward the agent’s overall timeout. If you configure:</p>
<ul>
<li>Agent timeout: 300s (5 minutes)</li>
<li>Max retries: 5</li>
<li>Fibonacci backoff with initial_delay: 10s</li>
</ul>
<p>Total delay could be: 10s + 10s + 20s + 30s + 50s = 120s of retries, leaving 180s for actual work.</p>
<h3 id="troubleshooting-1"><a class="header" href="#troubleshooting-1">Troubleshooting</a></h3>
<p><strong>Delays too long</strong>:</p>
<ul>
<li>Reduce <code>initial_delay</code> or <code>base</code>/<code>multiplier</code></li>
<li>Lower <code>max_delay</code> cap</li>
<li>Consider switching from Exponential to Linear or Fibonacci</li>
</ul>
<p><strong>Delays too short</strong>:</p>
<ul>
<li>Increase <code>initial_delay</code></li>
<li>Increase <code>base</code>/<code>multiplier</code> for Exponential</li>
<li>Switch from Fixed to Linear or Exponential</li>
</ul>
<p><strong>Max delay being ignored</strong>:</p>
<ul>
<li>Verify <code>max_delay</code> is set correctly in RetryConfig</li>
<li>Check that backoff calculation doesn’t skip max_delay check</li>
<li>All strategies respect max_delay cap (see source for verification)</li>
</ul>
<p><strong>Custom delays edge cases</strong>:</p>
<ul>
<li>Empty <code>delays</code> array: Defaults to <code>max_delay</code> for all retries</li>
<li>Attempt &gt; array length: Uses <code>max_delay</code> (not initial_delay)</li>
<li>Invalid duration format: Fails YAML parsing (use humantime format: <code>1s</code>, <code>500ms</code>, etc.)</li>
</ul>
<p><strong>Humantime format errors</strong>:
Valid formats: <code>1s</code>, <code>500ms</code>, <code>2m</code>, <code>1h30m</code>, <code>100ms</code>
Invalid formats: <code>1 second</code>, <code>500</code>, <code>2 minutes</code></p>
<h3 id="see-also-4"><a class="header" href="#see-also-4">See Also</a></h3>
<ul>
<li><a href="mapreduce/../retry-configuration/basic-retry-configuration.html">Basic Retry Configuration</a> - Full RetryConfig documentation</li>
<li><a href="mapreduce/./dead-letter-queue-dlq.html">Dead Letter Queue (DLQ)</a> - What happens when retries are exhausted</li>
<li><a href="mapreduce/./checkpoint-and-resume.html">Checkpoint and Resume</a> - How retry state is preserved for resume</li>
<li><a href="mapreduce/../advanced/timeout-configuration.html">Timeout Configuration</a> - Interaction between backoff and timeouts</li>
<li><a href="mapreduce/./error-collection-strategies.html">Error Collection Strategies</a> - Overall error handling in MapReduce</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="error-collection-strategies"><a class="header" href="#error-collection-strategies">Error Collection Strategies</a></h2>
<p>The <code>error_collection</code> field controls how errors are reported during workflow execution.</p>
<h3 id="syntax-flexibility"><a class="header" href="#syntax-flexibility">Syntax Flexibility</a></h3>
<p>Error collection can be configured in two ways for backward compatibility:</p>
<p><strong>Top-level convenience syntax</strong> (recommended for simple workflows):</p>
<pre><code class="language-yaml">name: my-workflow
mode: mapreduce

error_collection: aggregate  # Top-level field

map:
  # ... map configuration
</code></pre>
<p><strong>Nested under error_policy block</strong> (recommended when using other error policy features):</p>
<pre><code class="language-yaml">name: my-workflow
mode: mapreduce

error_policy:
  error_collection: aggregate
  continue_on_failure: true
  max_failures: 10
  # ... other error policy fields

map:
  # ... map configuration
</code></pre>
<p>Both syntaxes are fully supported. Use the top-level syntax for simplicity, or the nested syntax when configuring multiple error policy fields together.</p>
<h3 id="available-strategies-1"><a class="header" href="#available-strategies-1">Available Strategies</a></h3>
<p><strong>Aggregate (default)</strong>:</p>
<pre><code class="language-yaml">error_collection: aggregate
</code></pre>
<ul>
<li>Collects all errors in memory and reports at workflow end</li>
<li>Errors are stored as they occur but not logged</li>
<li>Full error list displayed when workflow completes</li>
<li>Use for: Final summary reports, batch processing where individual failures don’t need immediate attention</li>
<li>Trade-off: Low noise, but you won’t see errors until completion</li>
</ul>
<p><strong>Immediate</strong>:</p>
<pre><code class="language-yaml">error_collection: immediate
</code></pre>
<ul>
<li>Logs each error as soon as it happens via <code>warn!</code> level logging</li>
<li>No error collection in memory</li>
<li>Errors visible in real-time during execution</li>
<li>Use for: Debugging, development, real-time monitoring</li>
<li>Trade-off: More verbose output, but immediate visibility</li>
</ul>
<p><strong>Batched</strong>:</p>
<pre><code class="language-yaml">error_collection: batched:10
</code></pre>
<ul>
<li>Collects errors in memory until batch size is reached</li>
<li>When N errors collected, logs the entire batch via <code>warn!</code> level logging and automatically clears the buffer</li>
<li>Use for: Progress updates without spam, monitoring long-running jobs</li>
<li>Trade-off: Balance between noise and visibility (e.g., <code>batched:10</code> reports every 10 failures)</li>
<li><strong>Implementation</strong>: Buffer is cleared using <code>drain(..)</code> after each batch is logged (src/cook/workflow/error_policy.rs:593)</li>
</ul>
<h3 id="complete-example-3"><a class="header" href="#complete-example-3">Complete Example</a></h3>
<p>Combining error collection with other error policy features:</p>
<pre><code class="language-yaml">name: data-processing
mode: mapreduce

error_policy:
  # Report errors in batches of 5
  error_collection: batched:5

  # Send failed items to DLQ instead of failing workflow
  on_item_failure: dlq

  # Continue processing even if items fail
  continue_on_failure: true

  # Stop if failure rate exceeds 30%
  failure_threshold: 0.3

map:
  input: "items.json"
  json_path: "$.items[*]"
  agent_template:
    - claude: "/process '${item}'"
</code></pre>
<p><strong>Note</strong>: If <code>error_collection</code> is not specified, the default behavior is <code>aggregate</code>.</p>
<p>See also: <a href="mapreduce/../error-handling.html">Error Handling</a>, <a href="mapreduce/dead-letter-queue-dlq.html">Dead Letter Queue</a></p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="setup-phase-advanced"><a class="header" href="#setup-phase-advanced">Setup Phase (Advanced)</a></h2>
<p>The setup phase runs once before the map phase begins, executing in the parent worktree. It’s used to initialize the environment, generate work items, download data, or prepare configuration.</p>
<h3 id="execution-context"><a class="header" href="#execution-context">Execution Context</a></h3>
<p>The setup phase:</p>
<ul>
<li><strong>Runs once</strong> before map phase begins</li>
<li><strong>Executes in parent worktree</strong>, providing isolation from main repository (not the main repository itself)</li>
<li><strong>Creates checkpoint</strong> after successful completion, stored at <code>~/.prodigy/state/{repo_name}/mapreduce/jobs/{job_id}/setup-checkpoint.json</code></li>
<li><strong>Preserves setup outputs, environment state, and captured variables</strong> for resume capability</li>
<li><strong>Outputs available</strong> to map and reduce phases via captured variables</li>
</ul>
<p>This isolation ensures the main repository remains untouched while setup operations prepare the environment for parallel processing. All setup operations execute in the parent worktree, providing safety for initialization tasks.</p>
<h3 id="common-use-cases"><a class="header" href="#common-use-cases">Common Use Cases</a></h3>
<p>The setup phase is typically used for:</p>
<ul>
<li><strong>Generate work items</strong> - Create JSON arrays of items to process in parallel</li>
<li><strong>Initialize environment</strong> - Install dependencies, configure tools, set up databases</li>
<li><strong>Download data</strong> - Fetch datasets, clone repositories, pull artifacts</li>
<li><strong>Prepare configuration</strong> - Generate configs, resolve templates, validate settings</li>
</ul>
<h3 id="configuration-formats-1"><a class="header" href="#configuration-formats-1">Configuration Formats</a></h3>
<p>The setup phase supports two formats: simple array OR full configuration object.</p>
<pre><code class="language-yaml"># Simple array format
setup:
  - shell: "prepare-data.sh"
  - shell: "analyze-codebase.sh"

# Full configuration format with timeout and capture
setup:
  commands:
    - shell: "prepare-data.sh"
    - shell: "analyze-codebase.sh"

  # Timeout for entire setup phase
  # Accepts numeric seconds (300) or string with env var ("${SETUP_TIMEOUT}")
  timeout: 300  # or timeout: "${SETUP_TIMEOUT}"

  # Capture outputs from setup commands
  capture_outputs:
    # Simple format (shorthand - captures stdout with defaults)
    # Use for basic stdout capture without JSON extraction
    file_count: 0  # Capture stdout from command at index 0

    # Detailed CaptureConfig format
    # Use for JSON extraction, size limits, or custom sources
    analysis_result:
      command_index: 1
      source: stdout           # stdout, stderr, both, combined (see below)
      json_path: "$.result"    # Extract JSON field
      pattern: "^Result: (.+)$" # Optional: Extract using regex capture group
      max_size: 1048576        # Max bytes (1MB)
      default: "{}"            # Fallback if extraction fails
      multiline: preserve      # preserve, join, first_line, last_line, array (see below)
</code></pre>
<p><strong>Setup Phase Fields:</strong></p>
<ul>
<li><code>commands</code> - Array of commands to execute (or use simple array format at top level)</li>
<li><code>timeout</code> - Timeout for entire setup phase in seconds (numeric or environment variable)</li>
<li><code>capture_outputs</code> - Map of variable names to command outputs (Simple or Detailed format)</li>
</ul>
<p><strong>CaptureConfig Fields (Detailed Format):</strong></p>
<ul>
<li><code>command_index</code> - Which command’s output to capture (0-based index)</li>
<li><code>source</code> - Which output stream to capture (see Capture Source Options below)</li>
<li><code>pattern</code> - Optional regex pattern with capture group to extract specific text</li>
<li><code>json_path</code> - Optional JSON path to extract field from JSON output (e.g., “$.result.value”)</li>
<li><code>max_size</code> - Maximum bytes to capture (default: 1MB)</li>
<li><code>default</code> - Fallback value if capture/extraction fails</li>
<li><code>multiline</code> - How to handle multi-line output (see Multiline Options below)</li>
</ul>
<h3 id="capture-source-options"><a class="header" href="#capture-source-options">Capture Source Options</a></h3>
<p>The <code>source</code> field controls which command output streams to capture (src/cook/execution/variable_capture.rs:96-103):</p>
<ul>
<li><strong><code>stdout</code></strong> (default) - Capture standard output only</li>
<li><strong><code>stderr</code></strong> - Capture standard error only</li>
<li><strong><code>both</code></strong> - Capture both streams with labels: <code>"stdout:\n{}\nstderr:\n{}"</code></li>
<li><strong><code>combined</code></strong> - Capture both streams concatenated without labels (interleaved order)</li>
</ul>
<p><strong>Use Cases:</strong></p>
<ul>
<li><code>stdout</code> - For normal command output (logs, data, results)</li>
<li><code>stderr</code> - For error messages or diagnostic output</li>
<li><code>both</code> - When you need to see both streams separately with clear labels</li>
<li><code>combined</code> - When you need chronological order of all output (e.g., debugging)</li>
</ul>
<p><strong>Example from tests</strong> (src/cook/execution/variable_capture_test.rs:210):</p>
<pre><code class="language-yaml">error_log:
  command_index: 0
  source: stderr    # Capture error messages
  default: "No errors"
</code></pre>
<h3 id="multiline-options"><a class="header" href="#multiline-options">Multiline Options</a></h3>
<p>The <code>multiline</code> field controls how multi-line output is processed (src/cook/execution/variable_capture.rs:114-129):</p>
<div class="table-wrapper"><table><thead><tr><th>Option</th><th>Behavior</th><th>Use Case</th></tr></thead><tbody>
<tr><td><code>preserve</code> (default)</td><td>Keep all lines with newlines</td><td>Full output needed</td></tr>
<tr><td><code>join</code></td><td>Join lines with spaces</td><td>Create single-line summary</td></tr>
<tr><td><code>first_line</code></td><td>Take only first line</td><td>Extract header or title</td></tr>
<tr><td><code>last_line</code></td><td>Take only last line</td><td>Get final result or status</td></tr>
<tr><td><code>array</code></td><td>Return as JSON array of lines</td><td>Process each line separately</td></tr>
</tbody></table>
</div>
<p><strong>Examples from tests</strong> (src/cook/execution/variable_capture_test.rs:103-145):</p>
<pre><code class="language-yaml"># Extract first line (e.g., version number)
version:
  command_index: 0
  source: stdout
  multiline: first_line

# Extract last line (e.g., final status)
status:
  command_index: 1
  source: stdout
  multiline: last_line

# Join all lines (e.g., single-line summary)
summary:
  command_index: 2
  source: stdout
  multiline: join

# Array format (e.g., list of files)
files:
  command_index: 3
  source: stdout
  multiline: array
</code></pre>
<h3 id="pattern-extraction"><a class="header" href="#pattern-extraction">Pattern Extraction</a></h3>
<p>Use the <code>pattern</code> field to extract specific text using regex capture groups (src/cook/execution/variable_capture.rs:29):</p>
<p><strong>Example from tests</strong> (src/cook/execution/variable_capture_test.rs:35):</p>
<pre><code class="language-yaml">capture_outputs:
  VERSION:
    command_index: 0
    source: stdout
    pattern: "version (\\d+\\.\\d+\\.\\d+)"  # Extract semantic version
    default: "0.0.0"
</code></pre>
<p>The pattern uses regex capture groups. The first capture group <code>(...)</code> is extracted as the variable value.</p>
<h3 id="best-practices-5"><a class="header" href="#best-practices-5">Best Practices</a></h3>
<p><strong>Idempotent Operations:</strong></p>
<ul>
<li>Design setup commands to be safe to run multiple times</li>
<li>Use conditional checks before creating resources</li>
<li>Clean up stale artifacts before generating new ones</li>
</ul>
<p><strong>Timeout Sizing:</strong></p>
<ul>
<li>Set generous timeouts for network operations (downloads, API calls)</li>
<li>Use environment variables (<code>${SETUP_TIMEOUT}</code>) for flexibility across environments</li>
<li>Consider total time for all setup commands, not individual commands</li>
</ul>
<p><strong>Output Capture Patterns:</strong></p>
<ul>
<li>Use simple format (<code>command_index: number</code>) for basic text capture</li>
<li>Use detailed <code>CaptureConfig</code> when extracting JSON fields or limiting size</li>
<li>Always provide <code>default</code> value for robust error handling</li>
</ul>
<p><strong>Setup Command Failures:</strong></p>
<ul>
<li>If a setup command fails, the entire workflow stops (src/cook/execution/mapreduce/phases/setup.rs:39-43)</li>
<li>Design setup commands to fail fast with clear error messages</li>
<li>Use <code>default</code> values in <code>capture_outputs</code> to handle optional data gracefully</li>
<li>Test setup commands independently before workflow execution</li>
</ul>
<h3 id="see-also-5"><a class="header" href="#see-also-5">See Also</a></h3>
<ul>
<li><a href="mapreduce/./environment-variables-in-configuration.html">Environment Variables in Configuration</a> - Using env vars in timeout and commands</li>
<li><a href="mapreduce/./checkpoint-and-resume.html">Checkpoint and Resume</a> - How setup checkpoints enable resume</li>
<li><a href="mapreduce/./dead-letter-queue-dlq.html">Dead Letter Queue (DLQ)</a> - Handling setup phase failures</li>
<li><a href="mapreduce/./global-storage-architecture.html">Global Storage Architecture</a> - Understanding checkpoint storage locations</li>
<li><a href="mapreduce/./index.html">MapReduce Overview</a> - Complete workflow format and phase documentation</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="global-storage-architecture"><a class="header" href="#global-storage-architecture">Global Storage Architecture</a></h2>
<p>MapReduce workflows use a global storage architecture located in <code>~/.prodigy/</code> (not <code>.prodigy/</code> in your project directory). This architecture replaced the legacy local storage system and is now the default for all Prodigy workflows.</p>
<h3 id="benefits"><a class="header" href="#benefits">Benefits</a></h3>
<p>Global storage provides several key advantages for MapReduce workflows:</p>
<ul>
<li>
<p><strong>Cross-worktree event aggregation</strong>: When multiple worktrees process the same MapReduce job, all agents write to <code>~/.prodigy/events/{repo_name}/{job_id}/</code>, enabling unified monitoring across all parallel agents without manual log aggregation.</p>
</li>
<li>
<p><strong>Persistent state management</strong>: Job checkpoints and state files stored in <code>~/.prodigy/state/</code> survive worktree cleanup. You can delete agent worktrees after completion while preserving full job history and resume capability.</p>
</li>
<li>
<p><strong>Centralized monitoring</strong>: All job data is accessible from a single location (<code>~/.prodigy/</code>), making it easy to track multiple concurrent jobs, review historical executions, and debug failures across different worktrees and repositories.</p>
</li>
<li>
<p><strong>Efficient storage</strong>: Shared event logs, DLQ data, and checkpoints are deduplicated across worktrees, reducing disk usage when multiple agents process the same job.</p>
</li>
</ul>
<h3 id="directory-structure"><a class="header" href="#directory-structure">Directory Structure</a></h3>
<p>The global storage directory is organized by repository name to isolate data between different projects:</p>
<pre><code>~/.prodigy/
├── events/                    # Event logs for all MapReduce jobs
│   └── {repo_name}/          # Events for specific repository
│       └── {job_id}/         # Events for specific job
│           └── events-{timestamp}.jsonl
├── dlq/                      # Dead Letter Queue for failed work items
│   └── {repo_name}/          # DLQ for specific repository
│       └── {job_id}/         # Failed items for specific job
│           └── items/        # Individual failed item files
├── state/                    # State and checkpoints for jobs
│   └── {repo_name}/          # State for specific repository
│       ├── mapreduce/        # MapReduce job state
│       │   └── jobs/         # Individual job directories
│       │       └── {job_id}/ # Job-specific state and checkpoints
│       ├── checkpoints/      # Legacy checkpoint storage
│       └── mappings/         # Session-to-job ID mappings
├── worktrees/                # Git worktrees for sessions
│   └── {repo_name}/          # Worktrees for specific repository
│       └── session-{id}/     # Session-specific worktree
├── sessions/                 # Unified session tracking
│   └── {session-id}.json    # Session metadata and status
├── resume_locks/             # Concurrent resume protection
│   └── {job_id}.lock        # Lock file for job resume
└── logs/                     # Claude execution logs
    └── {repo_name}/          # Logs for specific repository
        └── {timestamp}/      # Log files by timestamp
</code></pre>
<h3 id="storage-components"><a class="header" href="#storage-components">Storage Components</a></h3>
<h4 id="events"><a class="header" href="#events">Events</a></h4>
<p>Event logs capture the complete lifecycle of MapReduce jobs in JSONL format (newline-delimited JSON):</p>
<ul>
<li><strong>Path pattern</strong>: <code>~/.prodigy/events/{repo_name}/{job_id}/events-{timestamp}.jsonl</code></li>
<li><strong>Content</strong>: Agent lifecycle events (started, completed, failed), work item processing status, checkpoint saves, Claude messages, and error details</li>
<li><strong>Usage</strong>: Real-time monitoring, debugging agent failures, auditing job execution</li>
<li><strong>Cross-reference</strong>: See <a href="mapreduce/./event-tracking.html">Event Tracking</a> for detailed event types and usage</li>
</ul>
<h4 id="dlq-dead-letter-queue"><a class="header" href="#dlq-dead-letter-queue">DLQ (Dead Letter Queue)</a></h4>
<p>Failed work items are stored with full context for retry and debugging:</p>
<ul>
<li><strong>Path pattern</strong>: <code>~/.prodigy/dlq/{repo_name}/{job_id}/items/{item-id}.json</code></li>
<li><strong>Content</strong>: Original work item data, failure reason, retry count, timestamps, error context</li>
<li><strong>Usage</strong>: Review failed items, retry with <code>prodigy dlq retry</code>, analyze failure patterns</li>
<li><strong>Cross-reference</strong>: See <a href="mapreduce/./dead-letter-queue-dlq.html">Dead Letter Queue</a> for DLQ operations and retry strategies</li>
</ul>
<h4 id="state"><a class="header" href="#state">State</a></h4>
<p>Job state and checkpoints enable resume and recovery:</p>
<ul>
<li><strong>Path pattern</strong>: <code>~/.prodigy/state/{repo_name}/mapreduce/jobs/{job_id}/</code></li>
<li><strong>Content</strong>: Setup checkpoints, map phase progress, reduce phase state, job metadata</li>
<li><strong>Additional storage</strong>: Session-job ID mappings stored in <code>state/{repo_name}/mappings/</code> for bidirectional resume lookup (session ID ↔ job ID)</li>
<li><strong>Usage</strong>: Resume interrupted jobs, track execution progress, recover from failures</li>
<li><strong>Cross-reference</strong>: See <a href="mapreduce/./checkpoint-and-resume.html">Checkpoint and Resume</a> for checkpoint structure and resume behavior</li>
</ul>
<p>The state directory includes session-job ID mappings (src/storage/session_job_mapping.rs) that enable resume operations to work with either session IDs or job IDs, providing flexibility in workflow recovery.</p>
<h4 id="worktrees"><a class="header" href="#worktrees">Worktrees</a></h4>
<p>Isolated git worktrees for parallel execution:</p>
<ul>
<li><strong>Path pattern</strong>: <code>~/.prodigy/worktrees/{repo_name}/session-{id}/</code></li>
<li><strong>Content</strong>: Git worktree for session/agent execution, temporary files, execution artifacts</li>
<li><strong>Usage</strong>: Isolated execution environment, parallel agent processing, clean separation from main repo</li>
<li><strong>Cross-reference</strong>: See <a href="mapreduce/../mapreduce-worktree-architecture.html">MapReduce Worktree Architecture</a> for worktree isolation details</li>
</ul>
<h4 id="sessions"><a class="header" href="#sessions">Sessions</a></h4>
<p>Unified session tracking for all workflow executions (src/unified_session/):</p>
<ul>
<li><strong>Path pattern</strong>: <code>~/.prodigy/sessions/{session-id}.json</code></li>
<li><strong>Content</strong>: Session status (Running, Paused, Completed, Failed), timing data, workflow metadata, checkpoint references, execution progress</li>
<li><strong>Session types</strong>: Workflow sessions and MapReduce sessions with phase-specific data</li>
<li><strong>Usage</strong>: Track active sessions, resume interrupted workflows, monitor execution time, correlate with job IDs</li>
<li><strong>Integration</strong>: Works with session-job mappings for flexible resume operations</li>
</ul>
<p>The unified session system provides a single source of truth for all workflow executions, whether standard workflows or MapReduce jobs. Each session file contains complete metadata about the execution state, progress, and timing information.</p>
<h4 id="resume-locks"><a class="header" href="#resume-locks">Resume Locks</a></h4>
<p>Concurrent resume protection to prevent conflicts:</p>
<ul>
<li><strong>Path pattern</strong>: <code>~/.prodigy/resume_locks/{job_id}.lock</code></li>
<li><strong>Content</strong>: Process ID, hostname, acquisition timestamp, job/session ID</li>
<li><strong>Usage</strong>: Prevent multiple resume processes on same job, automatic stale lock cleanup</li>
</ul>
<h3 id="repository-isolation"><a class="header" href="#repository-isolation">Repository Isolation</a></h3>
<p>Storage is automatically organized by repository name (extracted from your project path) to enable multiple projects to use global storage without conflicts.</p>
<p>The repository name is determined using the <code>extract_repo_name()</code> function (src/storage/mod.rs:42-59), which:</p>
<ol>
<li>Canonicalizes the project path to resolve symlinks</li>
<li>Extracts the final path component as the repository name</li>
<li>Example: <code>/path/to/my-project</code> → <code>my-project</code></li>
</ol>
<p>Key isolation features:</p>
<ul>
<li>All storage paths include <code>{repo_name}</code> to isolate data between repositories</li>
<li>You can work on multiple Prodigy projects simultaneously without storage collisions</li>
<li>Each repository has independent events, DLQ, state, and worktrees</li>
<li>Repository names are consistent even when accessing via symlinks (due to canonicalization)</li>
</ul>
<h3 id="configuration"><a class="header" href="#configuration">Configuration</a></h3>
<h4 id="prodigy_home-environment-variable"><a class="header" href="#prodigy_home-environment-variable">PRODIGY_HOME Environment Variable</a></h4>
<p>The default global storage location (<code>~/.prodigy/</code>) can be overridden using the <code>PRODIGY_HOME</code> environment variable:</p>
<pre><code class="language-bash"># Use custom storage location
export PRODIGY_HOME=/mnt/fast-storage/prodigy
prodigy run workflow.yml

# Useful for testing with isolated storage
export PRODIGY_HOME=/tmp/prodigy-test
prodigy run test-workflow.yml
</code></pre>
<p>This is particularly useful for:</p>
<ul>
<li><strong>Testing</strong>: Isolate test runs from production data</li>
<li><strong>Custom deployments</strong>: Use specific storage locations (network mounts, SSDs, etc.)</li>
<li><strong>Multi-user systems</strong>: Separate storage per user or team</li>
<li><strong>CI/CD</strong>: Use temporary storage that’s cleaned up after runs</li>
</ul>
<h3 id="examples"><a class="header" href="#examples">Examples</a></h3>
<h4 id="inspecting-global-storage"><a class="header" href="#inspecting-global-storage">Inspecting Global Storage</a></h4>
<p>Check disk usage of global storage:</p>
<pre><code class="language-bash">du -sh ~/.prodigy/*
# Output:
# 150M    /Users/you/.prodigy/events
# 25M     /Users/you/.prodigy/dlq
# 80M     /Users/you/.prodigy/state
# 200M    /Users/you/.prodigy/worktrees
</code></pre>
<p>Find all data for a specific job:</p>
<pre><code class="language-bash"># Find events
ls ~/.prodigy/events/my-repo/mapreduce-20250111_120000/

# Check for DLQ items
ls ~/.prodigy/dlq/my-repo/mapreduce-20250111_120000/items/

# View checkpoints
ls ~/.prodigy/state/my-repo/mapreduce/jobs/mapreduce-20250111_120000/
</code></pre>
<p>List all repositories using global storage:</p>
<pre><code class="language-bash">ls ~/.prodigy/events/
# Output:
# my-project/
# another-repo/
# test-project/
</code></pre>
<h4 id="storage-maintenance"><a class="header" href="#storage-maintenance">Storage Maintenance</a></h4>
<p>Clean up old job data:</p>
<pre><code class="language-bash"># Remove old event logs (older than 30 days)
find ~/.prodigy/events -type d -mtime +30 -exec rm -rf {} +

# Clear processed DLQ items for a workflow
prodigy dlq clear &lt;workflow_id&gt;

# Purge old DLQ items (older than N days)
prodigy dlq purge --older-than-days 30

# Remove orphaned worktrees after failed cleanup
prodigy worktree clean-orphaned &lt;job_id&gt;
</code></pre>
<p>Monitor storage and sessions:</p>
<pre><code class="language-bash"># List active sessions and their status
prodigy sessions list

# Show details about a specific session
prodigy sessions show &lt;session_id&gt;

# View DLQ statistics for a workflow
prodigy dlq stats --workflow-id &lt;workflow_id&gt;

# Analyze failure patterns in DLQ
prodigy dlq analyze --job-id &lt;job_id&gt;

# Check resume locks (detect stuck jobs)
ls ~/.prodigy/resume_locks/

# List all repositories using global storage
ls ~/.prodigy/events/
</code></pre>
<p><strong>Available DLQ Commands</strong> (src/cli/args.rs:578-675):</p>
<ul>
<li><code>list</code> - List items in the Dead Letter Queue</li>
<li><code>inspect</code> - Inspect a specific DLQ item</li>
<li><code>analyze</code> - Analyze failure patterns</li>
<li><code>export</code> - Export DLQ items to a file</li>
<li><code>purge</code> - Purge old items from the DLQ</li>
<li><code>retry</code> - Retry failed items</li>
<li><code>stats</code> - Show DLQ statistics</li>
<li><code>clear</code> - Clear processed items from DLQ</li>
</ul>
<p><strong>Note</strong>: The <code>health_check()</code> method exists in the GlobalStorage implementation (src/storage/global.rs:115) but is not currently exposed as a CLI command. It’s used internally for programmatic health verification.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="event-tracking"><a class="header" href="#event-tracking">Event Tracking</a></h2>
<p>All MapReduce execution events are logged to <code>~/.prodigy/events/{repo_name}/{job_id}/</code> for debugging and monitoring. Prodigy provides comprehensive event tracking with correlation IDs, metadata enrichment, buffering, and powerful CLI tools for querying and analysis.</p>
<h3 id="event-types"><a class="header" href="#event-types">Event Types</a></h3>
<p>Prodigy tracks 24 event types across different categories:</p>
<h4 id="job-lifecycle-events"><a class="header" href="#job-lifecycle-events">Job Lifecycle Events</a></h4>
<ul>
<li><code>JobStarted</code> - Job begins with config and total items</li>
<li><code>JobCompleted</code> - Job finishes with success/failure counts and duration</li>
<li><code>JobFailed</code> - Job fails with error and partial results count</li>
<li><code>JobPaused</code> - Job paused with checkpoint version</li>
<li><code>JobResumed</code> - Job resumed from checkpoint with pending items</li>
</ul>
<h4 id="agent-lifecycle-events"><a class="header" href="#agent-lifecycle-events">Agent Lifecycle Events</a></h4>
<ul>
<li><code>AgentStarted</code> - Agent begins processing work item (includes item_id, worktree, and attempt number)</li>
<li><code>AgentProgress</code> - Agent reports progress percentage and current step</li>
<li><code>AgentCompleted</code> - Agent finishes successfully with job_id, agent_id, duration, commits (Vec<String>), and optional json_log_location</li>
<li><code>AgentFailed</code> - Agent fails with error and retry eligibility</li>
<li><code>AgentRetrying</code> - Agent retries with attempt number and backoff delay in milliseconds</li>
</ul>
<h4 id="checkpoint-events"><a class="header" href="#checkpoint-events">Checkpoint Events</a></h4>
<ul>
<li><code>CheckpointCreated</code> - Checkpoint saved with version and completed agent count</li>
<li><code>CheckpointLoaded</code> - Checkpoint loaded for resume</li>
<li><code>CheckpointFailed</code> - Checkpoint operation failed</li>
</ul>
<h4 id="worktree-events"><a class="header" href="#worktree-events">Worktree Events</a></h4>
<ul>
<li><code>WorktreeCreated</code> - Git worktree created for agent (includes branch name)</li>
<li><code>WorktreeMerged</code> - Agent changes merged to target branch</li>
<li><code>WorktreeCleaned</code> - Worktree removed after agent completion</li>
</ul>
<h4 id="performance-monitoring-events"><a class="header" href="#performance-monitoring-events">Performance Monitoring Events</a></h4>
<ul>
<li><code>QueueDepthChanged</code> - Work queue status (pending/active/completed counts)</li>
<li><code>MemoryPressure</code> - Resource usage monitoring (used vs limit in MB)</li>
</ul>
<h4 id="dead-letter-queue-events"><a class="header" href="#dead-letter-queue-events">Dead Letter Queue Events</a></h4>
<ul>
<li><code>DLQItemAdded</code> - Failed item added to DLQ with error signature</li>
<li><code>DLQItemRemoved</code> - Item removed from DLQ (successful retry)</li>
<li><code>DLQItemsReprocessed</code> - Batch reprocessing of DLQ items</li>
<li><code>DLQItemsEvicted</code> - Old DLQ items evicted per retention policy</li>
<li><code>DLQAnalysisGenerated</code> - Error pattern analysis completed</li>
</ul>
<h4 id="claude-observability-events"><a class="header" href="#claude-observability-events">Claude Observability Events</a></h4>
<ul>
<li><code>ClaudeToolInvoked</code> - Claude tool use with name, ID, and parameters</li>
<li><code>ClaudeTokenUsage</code> - Token consumption (input/output/cache tokens)</li>
<li><code>ClaudeSessionStarted</code> - Claude session initialized with model and available tools</li>
<li><code>ClaudeMessage</code> - Claude message with content and JSON log location</li>
</ul>
<h3 id="event-record-structure"><a class="header" href="#event-record-structure">Event Record Structure</a></h3>
<p>Each event is wrapped in an <code>EventRecord</code> with rich metadata:</p>
<pre><code class="language-json">{
  "id": "550e8400-e29b-41d4-a716-446655440000",
  "timestamp": "2024-01-01T12:00:00Z",
  "correlation_id": "workflow-abc123",
  "event": {
    "event_type": "agent_completed",
    "job_id": "mapreduce-xyz789",
    "agent_id": "agent-1",
    "duration": { "secs": 45, "nanos": 0 },
    "commits": ["a1b2c3d"],
    "json_log_location": "/home/user/.local/state/claude/logs/session-xyz.json"
  },
  "metadata": {
    "host": "worker-01",
    "pid": 12345,
    "thread": "tokio-runtime-worker"
  }
}
</code></pre>
<p><strong>Note</strong>: The <code>event_type</code> field is serialized using serde’s tagged enum format (<code>#[serde(tag = "event_type")]</code>), which means it appears as a sibling field alongside other event-specific fields within the <code>event</code> object.</p>
<p><strong>Fields:</strong></p>
<ul>
<li><code>id</code> - Unique UUID for this event</li>
<li><code>timestamp</code> - When the event occurred (UTC)</li>
<li><code>correlation_id</code> - Links related events across agents and phases</li>
<li><code>event</code> - The actual event data (varies by type)</li>
<li><code>metadata</code> - Runtime context (host, process ID, thread). This field is extensible and supports custom fields via the <code>log_with_metadata</code> method for application-specific tracking needs.</li>
</ul>
<p><strong>Source</strong>: EventRecord definition in src/cook/execution/events/event_logger.rs:17-25</p>
<h3 id="event-storage"><a class="header" href="#event-storage">Event Storage</a></h3>
<p><strong>Location:</strong>
<code>~/.prodigy/events/{repo_name}/{job_id}/events-{timestamp}.jsonl</code></p>
<p><strong>Format:</strong>
Events are stored in JSONL (JSON Lines) format with one event per line.</p>
<p><strong>Buffering:</strong></p>
<ul>
<li>Events are buffered in memory before being written to disk</li>
<li><strong>Default buffer size</strong>: 1000 events</li>
<li><strong>Default flush interval</strong>: 5 seconds</li>
<li>Background flush task runs automatically</li>
<li>Buffer size and flush interval are configurable</li>
</ul>
<p><strong>Source</strong>: EventLogger configuration in src/cook/execution/events/event_logger.rs:44-45</p>
<p><strong>File Rotation:</strong></p>
<ul>
<li>Log files automatically rotate at 100MB (configurable via <code>with_rotation()</code>)</li>
<li>Rotated files are moved with timestamp suffix: <code>events-{timestamp}.jsonl.rotated</code></li>
<li>Compression support is implemented but currently moves files without actual compression (marked as TODO in implementation)</li>
<li>Cross-worktree event aggregation for parallel jobs</li>
</ul>
<p><strong>Source</strong>: Rotation implementation in src/cook/execution/events/event_writer.rs:rotation_size field and rotate_if_needed() method</p>
<h3 id="correlation-ids"><a class="header" href="#correlation-ids">Correlation IDs</a></h3>
<p>Each workflow run has a unique <code>correlation_id</code> that links all related events:</p>
<pre><code class="language-bash"># All events from the same workflow share correlation_id
# Makes it easy to trace execution flow across agents
</code></pre>
<p>Use correlation IDs to:</p>
<ul>
<li>Trace work item through multiple retries</li>
<li>Link agent execution to parent job</li>
<li>Track checkpoint saves and resumes</li>
<li>Debug cross-agent dependencies</li>
</ul>
<h3 id="viewing-events-with-cli"><a class="header" href="#viewing-events-with-cli">Viewing Events with CLI</a></h3>
<p><strong>Note on Event File Paths</strong>: The CLI commands use <code>.prodigy/events/mapreduce_events.jsonl</code> as the default path for backward compatibility. However, MapReduce workflows using global storage write events to <code>~/.prodigy/events/{repo_name}/{job_id}/events-{timestamp}.jsonl</code>. Use the <code>--file</code> flag to specify the global storage path when querying events from MapReduce jobs.</p>
<p><strong>Source</strong>: CLI default paths in src/cli/events/mod.rs (all subcommands use <code>default_value = ".prodigy/events/mapreduce_events.jsonl"</code>)</p>
<h4 id="list-events"><a class="header" href="#list-events">List Events</a></h4>
<pre><code class="language-bash"># List all events for a job (local storage)
prodigy events ls --job-id &lt;job_id&gt;

# List events from global storage
prodigy events ls --job-id &lt;job_id&gt; --file ~/.prodigy/events/{repo_name}/{job_id}/events-{timestamp}.jsonl

# Filter by event type
prodigy events ls --job-id &lt;job_id&gt; --event-type agent_completed

# Filter by agent
prodigy events ls --job-id &lt;job_id&gt; --agent-id agent-1

# Recent events only (last N minutes)
prodigy events ls --job-id &lt;job_id&gt; --since 30

# Limit results
prodigy events ls --job-id &lt;job_id&gt; --limit 50
</code></pre>
<h4 id="event-statistics"><a class="header" href="#event-statistics">Event Statistics</a></h4>
<pre><code class="language-bash"># Show statistics grouped by event type
prodigy events stats

# Group by job ID
prodigy events stats --group-by job_id

# Group by agent ID
prodigy events stats --group-by agent_id
</code></pre>
<h4 id="search-events"><a class="header" href="#search-events">Search Events</a></h4>
<pre><code class="language-bash"># Search by pattern (regex supported)
prodigy events search "error|failed"

# Search in specific fields only
prodigy events search "timeout" --fields error,description
</code></pre>
<h4 id="follow-events-live"><a class="header" href="#follow-events-live">Follow Events Live</a></h4>
<pre><code class="language-bash"># Stream events in real-time (tail -f style)
prodigy events follow --job-id &lt;job_id&gt;

# Filter while following
prodigy events follow --job-id &lt;job_id&gt; --event-type agent_failed
</code></pre>
<h4 id="clean-old-events"><a class="header" href="#clean-old-events">Clean Old Events</a></h4>
<pre><code class="language-bash"># Preview cleanup (dry run)
prodigy events clean --older-than 30d --dry-run

# Keep only recent events
prodigy events clean --max-events 10000

# Size-based retention
prodigy events clean --max-size 100MB

# Archive instead of delete
prodigy events clean --older-than 7d --archive --archive-path /backup/events
</code></pre>
<p><strong>Note</strong>: Cleanup operations require user confirmation unless running in automation mode (<code>PRODIGY_AUTOMATION=true</code>) or using <code>--dry-run</code> to preview changes.</p>
<p><strong>Source</strong>: Cleanup confirmation logic in src/cli/events/mod.rs:591-627</p>
<h3 id="debugging-with-events"><a class="header" href="#debugging-with-events">Debugging with Events</a></h3>
<p><strong>Common debugging scenarios:</strong></p>
<p><strong>Track failed agent:</strong></p>
<pre><code class="language-bash"># Find all events for failed agent
prodigy events ls --job-id &lt;job_id&gt; --agent-id &lt;agent_id&gt;

# Check Claude JSON log from AgentCompleted event
cat &lt;json_log_location&gt;
</code></pre>
<p><strong>Analyze performance:</strong></p>
<pre><code class="language-bash"># Monitor queue depth changes
prodigy events ls --event-type queue_depth_changed

# Check memory pressure events
prodigy events ls --event-type memory_pressure
</code></pre>
<p><strong>Debug checkpoint issues:</strong></p>
<pre><code class="language-bash"># Find checkpoint events
prodigy events ls --event-type checkpoint_created
prodigy events ls --event-type checkpoint_failed
</code></pre>
<p><strong>Review DLQ patterns:</strong></p>
<pre><code class="language-bash"># See DLQ additions
prodigy events ls --event-type dlq_item_added

# Check error pattern analysis
prodigy events ls --event-type dlq_analysis_generated
</code></pre>
<p><strong>Trace specific workflow run:</strong></p>
<pre><code class="language-bash"># Filter events by correlation_id to trace entire workflow execution
prodigy events search "&lt;correlation_id&gt;"

# Find correlation_id from recent job
prodigy events ls --job-id &lt;job_id&gt; --limit 1
</code></pre>
<h3 id="troubleshooting-2"><a class="header" href="#troubleshooting-2">Troubleshooting</a></h3>
<p><strong>Events not being written:</strong></p>
<ul>
<li>Check event file permissions in <code>~/.prodigy/events/{repo_name}/{job_id}/</code></li>
<li>Verify directory exists and is writable</li>
<li>Check disk space availability</li>
<li>Review buffer configuration if events are delayed</li>
</ul>
<p><strong>Missing events:</strong></p>
<ul>
<li>Events may be buffered (default: 5 second flush interval)</li>
<li>Check if logger was properly shut down (ensures buffer flush)</li>
<li>Verify event type filter isn’t excluding events</li>
</ul>
<h3 id="cross-references-1"><a class="header" href="#cross-references-1">Cross-References</a></h3>
<ul>
<li>See <a href="mapreduce/checkpoint-and-resume.html">Checkpoint and Resume</a> for checkpoint events</li>
<li>See <a href="mapreduce/dead-letter-queue-dlq.html">Dead Letter Queue (DLQ)</a> for DLQ event details</li>
<li>See <a href="mapreduce/../retry-configuration/retry-metrics-and-observability.html">Retry Metrics and Observability</a> for retry-specific monitoring</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="checkpoint-and-resume"><a class="header" href="#checkpoint-and-resume">Checkpoint and Resume</a></h2>
<p>Prodigy provides comprehensive checkpoint and resume capabilities for MapReduce workflows, ensuring work can be recovered from any point of failure. Checkpoints are automatically created during workflow execution, preserving all state needed to continue from where you left off. This enables resilient workflows that can survive interruptions, crashes, or planned pauses without losing progress.</p>
<h3 id="checkpoint-behavior"><a class="header" href="#checkpoint-behavior">Checkpoint Behavior</a></h3>
<p>Checkpoints are automatically created at strategic points during workflow execution:</p>
<p><strong>Setup Phase Checkpointing</strong>:</p>
<ul>
<li>Checkpoint created after successful setup completion</li>
<li>Preserves setup output, generated artifacts, and environment state</li>
<li>Stored as <code>setup-checkpoint.json</code></li>
<li>Resume restarts setup from beginning (idempotent operations recommended)</li>
</ul>
<p><strong>Map Phase Checkpointing</strong>:</p>
<ul>
<li>Checkpoints created after processing configurable number of work items</li>
<li>Tracks completed, in-progress, and pending work items</li>
<li>Stores agent results and failure details for recovery</li>
<li>Resume continues from last successful checkpoint</li>
<li>In-progress items are moved back to pending on resume</li>
<li>Stored as <code>map-checkpoint-{timestamp}.json</code></li>
</ul>
<p><strong>Reduce Phase Checkpointing</strong>:</p>
<ul>
<li>Checkpoint created after each reduce command execution</li>
<li>Tracks completed steps, step results, variables, and map results</li>
<li>Enables resume from any point in reduce phase execution</li>
<li>Resume continues from last completed step</li>
<li>Stored as <code>reduce-checkpoint-v1-{timestamp}.json</code></li>
</ul>
<h3 id="checkpoint-interval-configuration"><a class="header" href="#checkpoint-interval-configuration">Checkpoint Interval Configuration</a></h3>
<p>Prodigy controls when checkpoints are created through configurable intervals. The checkpoint strategy differs between workflow types:</p>
<p><strong>Standard Workflow Checkpoints</strong> (src/cook/workflow/checkpoint.rs:20):</p>
<ul>
<li><strong>Default interval</strong>: 60 seconds between checkpoints</li>
<li><strong>Configurable</strong>: Use <code>.with_interval(Duration)</code> in checkpoint manager builder</li>
<li><strong>Decision logic</strong>: Compares elapsed time since last checkpoint</li>
</ul>
<p><strong>MapReduce Checkpoints</strong> (src/cook/execution/mapreduce/checkpoint/types.rs:242-264):</p>
<ul>
<li><strong>Default <code>interval_items</code></strong>: 100 work items per checkpoint</li>
<li><strong>Default <code>interval_duration</code></strong>: 300 seconds (5 minutes) per checkpoint</li>
<li><strong>Dual triggers</strong>: Checkpoint created when either item count OR duration threshold is reached</li>
<li><strong>Retention policy</strong>:
<ul>
<li><code>max_checkpoints</code>: Keep 10 most recent checkpoints</li>
<li><code>max_age</code>: Retain checkpoints for 7 days (604,800 seconds)</li>
<li><code>keep_final</code>: Always preserve final checkpoint</li>
</ul>
</li>
</ul>
<p><strong>Example MapReduce checkpoint configuration</strong>:</p>
<pre><code class="language-yaml">name: my-workflow
mode: mapreduce

checkpoint:
  interval_items: 50      # Checkpoint every 50 items (default: 100)
  interval_duration: 600  # Checkpoint every 10 minutes (default: 300s)
  max_checkpoints: 15     # Keep 15 recent checkpoints (default: 10)
  max_age: 1209600        # Keep for 14 days (default: 604,800s / 7 days)
</code></pre>
<p>These intervals balance between checkpoint overhead and recovery granularity. More frequent checkpoints enable finer-grained resume but increase I/O overhead.</p>
<h3 id="resume-commands"><a class="header" href="#resume-commands">Resume Commands</a></h3>
<p>MapReduce jobs can be resumed using either session IDs or job IDs:</p>
<pre><code class="language-bash"># Resume using session ID
prodigy resume session-mapreduce-1234567890

# Resume using job ID
prodigy resume-job mapreduce-1234567890

# Unified resume command (auto-detects ID type)
prodigy resume mapreduce-1234567890
</code></pre>
<p><strong>Session-Job Mapping</strong>:</p>
<p>The <code>SessionJobMapping</code> structure provides bidirectional mapping between session and job identifiers (src/storage/session_job_mapping.rs:14-26):</p>
<ul>
<li><strong>Storage location</strong>: <code>~/.prodigy/state/{repo_name}/mappings/</code></li>
<li><strong>Mapping fields</strong>:
<ul>
<li><code>session_id</code>: Unique identifier for the workflow session</li>
<li><code>job_id</code>: MapReduce job identifier</li>
<li><code>workflow_name</code>: Name of the workflow for easier identification</li>
<li><code>created_at</code>: Timestamp when the mapping was created</li>
</ul>
</li>
<li><strong>Created</strong>: Automatically when MapReduce workflow starts</li>
<li><strong>Purpose</strong>: Enables resume with either session ID or job ID</li>
</ul>
<p>This mapping allows you to resume a workflow using whichever identifier is more convenient or available in your context.</p>
<h3 id="state-preservation"><a class="header" href="#state-preservation">State Preservation</a></h3>
<p>All critical state is preserved across resume operations:</p>
<p><strong>Variables and Context</strong>:</p>
<ul>
<li>Workflow variables preserved across resume</li>
<li>Captured outputs from setup and reduce phases</li>
<li>Environment variables maintained</li>
<li>Map results available to reduce phase after resume</li>
</ul>
<p><strong>Work Item State</strong>:</p>
<ul>
<li><strong>Completed items</strong>: Preserved with full results</li>
<li><strong>In-progress items</strong>: Moved back to pending on resume</li>
<li><strong>Failed items</strong>: Tracked with retry counts and error details</li>
<li><strong>Pending items</strong>: Continue processing from where left off</li>
</ul>
<p><strong>Agent State</strong>:</p>
<ul>
<li>Active agent information preserved</li>
<li>Resource allocation tracked</li>
<li>Worktree paths recorded for cleanup</li>
</ul>
<h3 id="resume-strategies"><a class="header" href="#resume-strategies">Resume Strategies</a></h3>
<p>Based on checkpoint state and phase, different resume strategies apply:</p>
<ul>
<li><strong>Setup Phase</strong>: Restart setup from beginning (idempotent operations recommended)</li>
<li><strong>Map Phase</strong>: Continue from last checkpoint, re-process in-progress items</li>
<li><strong>Reduce Phase</strong>: Continue from last completed step</li>
<li><strong>Validate and Continue</strong>: Verify checkpoint integrity before resuming</li>
</ul>
<h3 id="storage-structure"><a class="header" href="#storage-structure">Storage Structure</a></h3>
<p>Checkpoints are stored in a structured directory hierarchy:</p>
<pre><code>~/.prodigy/state/{repo_name}/mapreduce/jobs/{job_id}/
├── setup-checkpoint.json           # Setup phase results
├── map-checkpoint-{timestamp}.json  # Map phase progress
├── reduce-checkpoint-v1-{timestamp}.json  # Reduce phase progress
└── job-state.json                  # Overall job state
</code></pre>
<h3 id="checkpoint-file-structure"><a class="header" href="#checkpoint-file-structure">Checkpoint File Structure</a></h3>
<p>Checkpoint files contain JSON-serialized state for recovery. Here’s what each checkpoint type stores:</p>
<p><strong>MapReduce Checkpoint</strong> (src/cook/execution/mapreduce/checkpoint/types.rs:48-64):</p>
<pre><code class="language-json">{
  "metadata": {
    "checkpoint_id": "ckpt-1704556800",
    "version": 1,
    "phase": "Map",
    "created_at": "2025-01-11T12:00:00Z",
    "items_processed": 150,
    "items_total": 500
  },
  "work_items": {
    "pending": ["item-151", "item-152", "..."],
    "in_progress": [],
    "completed": ["item-1", "item-2", "..."],
    "failed": []
  },
  "agent_state": {
    "active_agents": [],
    "agent_assignments": {},
    "agent_results": {
      "item-1": {"status": "success", "commits": ["abc123"]},
      "item-2": {"status": "success", "commits": ["def456"]}
    },
    "resource_allocation": {
      "max_parallel": 10,
      "current_agents": 0
    }
  },
  "variables": {
    "workflow_vars": {"PROJECT_NAME": "prodigy"},
    "captured_vars": {},
    "environment_vars": {},
    "item_vars": {}
  },
  "error_state": {
    "error_count": 0,
    "dlq_items": [],
    "error_threshold_reached": false
  },
  "reason": "Interval",
  "checksum": "sha256:abc123..."
}
</code></pre>
<p><strong>Workflow Checkpoint</strong> (src/cook/workflow/checkpoint.rs:26-57):</p>
<pre><code class="language-json">{
  "workflow_id": "workflow-1704556800",
  "version": 1,
  "execution_state": {
    "current_step_index": 3,
    "status": "Running",
    "started_at": "2025-01-11T12:00:00Z",
    "updated_at": "2025-01-11T12:05:00Z"
  },
  "completed_steps": [
    {
      "step_index": 0,
      "command": "shell: cargo build",
      "status": "Success",
      "duration": {"secs": 45, "nanos": 0},
      "completed_at": "2025-01-11T12:01:00Z"
    }
  ],
  "variable_state": {
    "BUILD_OUTPUT": "/target/release/prodigy",
    "VERSION": "1.0.0"
  },
  "workflow_hash": "sha256:def789..."
}
</code></pre>
<p><strong>Key Fields</strong>:</p>
<ul>
<li><strong>metadata/execution_state</strong>: Current progress and timestamps</li>
<li><strong>work_items</strong>: Work item status tracking (MapReduce only)</li>
<li><strong>agent_state</strong>: Agent results and resource allocation (MapReduce only)</li>
<li><strong>variables/variable_state</strong>: Preserved workflow variables for resume</li>
<li><strong>completed_steps</strong>: Audit trail of successfully completed steps</li>
<li><strong>checksum/workflow_hash</strong>: Integrity verification</li>
</ul>
<p>These structures enable Prodigy to reconstruct exact execution state during resume operations.</p>
<h3 id="concurrent-resume-protection"><a class="header" href="#concurrent-resume-protection">Concurrent Resume Protection</a></h3>
<p>Prodigy prevents multiple resume processes from running on the same session/job simultaneously using an RAII-based locking mechanism:</p>
<p><strong>Lock Behavior</strong>:</p>
<ul>
<li>Resume automatically acquires exclusive lock before starting</li>
<li>Lock creation is atomic - fails if another process holds the lock</li>
<li>Lock automatically released when resume completes or fails (RAII pattern)</li>
<li>Stale locks (from crashed processes) are automatically detected and cleaned up</li>
</ul>
<p><strong>Lock Metadata</strong>:
Lock files contain:</p>
<ul>
<li>Process ID (PID) of the holding process</li>
<li>Hostname where the process is running</li>
<li>Timestamp when lock was acquired</li>
<li>Job/session ID being locked</li>
</ul>
<p><strong>Lock Storage</strong>:</p>
<pre><code>~/.prodigy/resume_locks/
├── session-abc123.lock
├── mapreduce-xyz789.lock
└── ...
</code></pre>
<p><strong>Error Messages</strong>:
If a resume is blocked by an active lock:</p>
<pre><code class="language-bash">$ prodigy resume &lt;job_id&gt;
Error: Resume already in progress for job &lt;job_id&gt;
Lock held by: PID 12345 on hostname (acquired 2025-01-11 10:30:00 UTC)
Please wait for the other process to complete, or use --force to override.
</code></pre>
<p><strong>Stale Lock Detection</strong>:</p>
<ul>
<li>Platform-specific process existence check (Unix: <code>kill -0</code>, Windows: <code>tasklist</code>)</li>
<li>If holding process is no longer running, lock is automatically removed</li>
<li>New resume attempt succeeds after stale lock cleanup</li>
</ul>
<p><strong>Safety Guarantees</strong>:</p>
<ul>
<li><strong>Data Corruption Prevention</strong>: Only one process can modify job state at a time</li>
<li><strong>No Duplicate Work</strong>: Work items cannot be processed by multiple agents concurrently</li>
<li><strong>Consistent State</strong>: Checkpoint updates are serialized</li>
<li><strong>Automatic Cleanup</strong>: RAII pattern ensures locks are released even on errors</li>
<li><strong>Cross-Host Safety</strong>: Hostname in lock prevents conflicts across machines</li>
</ul>
<h3 id="example-resume-workflow"><a class="header" href="#example-resume-workflow">Example Resume Workflow</a></h3>
<p>Here’s a typical workflow for resuming an interrupted MapReduce job:</p>
<ol>
<li><strong>Workflow interrupted</strong> during reduce phase (e.g., laptop closed, terminal killed)</li>
<li><strong>Find job ID</strong> with <code>prodigy sessions list</code> or <code>prodigy resume-job list</code></li>
<li><strong>Resume execution</strong> using <code>prodigy resume &lt;session-or-job-id&gt;</code></li>
<li><strong>Prodigy loads checkpoint</strong> from <code>~/.prodigy/state/{repo_name}/mapreduce/jobs/{job_id}/</code></li>
<li><strong>Reconstructs execution state</strong> with all variables, work items, and progress</li>
<li><strong>Continues from last completed step</strong> in reduce phase (or re-processes in-progress map items)</li>
</ol>
<h3 id="best-practices-6"><a class="header" href="#best-practices-6">Best Practices</a></h3>
<p><strong>Designing Resumable Workflows</strong>:</p>
<ul>
<li>Make setup commands idempotent (safe to run multiple times)</li>
<li>Avoid side effects that can’t be safely repeated</li>
<li>Use descriptive work item IDs for easier debugging</li>
<li>Test resume behavior by intentionally interrupting workflows</li>
</ul>
<p><strong>Troubleshooting Resume Issues</strong>:</p>
<ul>
<li>Check checkpoint files exist: <code>ls ~/.prodigy/state/{repo_name}/mapreduce/jobs/{job_id}/</code></li>
<li>List available sessions: <code>prodigy sessions list</code> (shows session IDs, job IDs, and status)</li>
<li>List available checkpoints: <code>prodigy checkpoints list</code></li>
<li>Show detailed checkpoint info: <code>prodigy checkpoints show &lt;job_id&gt;</code></li>
<li>Validate checkpoint integrity: <code>prodigy checkpoints validate &lt;checkpoint_id&gt;</code></li>
<li>Review event logs: <code>prodigy events &lt;job_id&gt;</code></li>
<li>Check for stale locks: <code>ls ~/.prodigy/resume_locks/</code></li>
<li>Clean old checkpoints: <code>prodigy checkpoints clean --all</code></li>
</ul>
<h3 id="troubleshooting-stale-locks"><a class="header" href="#troubleshooting-stale-locks">Troubleshooting Stale Locks</a></h3>
<p>Resume locks can occasionally become “stale” if a resume process crashes or is forcefully terminated before releasing its lock. Prodigy includes automatic detection and cleanup, but here’s how to troubleshoot persistent lock issues:</p>
<p><strong>Identifying Stale Locks</strong>:</p>
<p>When a resume is blocked by a lock, you’ll see this error message:</p>
<pre><code class="language-bash">$ prodigy resume mapreduce-1234567890
Error: Resume already in progress for job mapreduce-1234567890
Lock held by: PID 12345 on hostname (acquired 2025-01-11 10:30:00 UTC)
Please wait for the other process to complete, or use --force to override.
</code></pre>
<p><strong>Automatic Cleanup</strong> (src/cook/execution/resume_lock.rs:96-120):</p>
<p>Prodigy automatically detects stale locks using platform-specific process existence checks:</p>
<ul>
<li><strong>Unix/Linux/macOS</strong>: Uses <code>kill -0 &lt;PID&gt;</code> to check if process is running</li>
<li><strong>Windows</strong>: Uses <code>tasklist</code> to verify process existence</li>
<li><strong>Cross-host detection</strong>: Compares hostname in lock file to current system</li>
</ul>
<p>If the holding process is no longer running, the lock is automatically removed and resume proceeds.</p>
<p><strong>Manual Verification Steps</strong>:</p>
<ol>
<li>
<p><strong>Check if the process is actually running</strong>:</p>
<pre><code class="language-bash"># Extract PID from error message (e.g., 12345)
ps aux | grep 12345
</code></pre>
</li>
<li>
<p><strong>Examine the lock file</strong>:</p>
<pre><code class="language-bash">cat ~/.prodigy/resume_locks/mapreduce-1234567890.lock
</code></pre>
<p>Lock file contains:</p>
<pre><code class="language-json">{
  "job_id": "mapreduce-1234567890",
  "process_id": 12345,
  "hostname": "my-laptop",
  "acquired_at": "2025-01-11T10:30:00Z"
}
</code></pre>
</li>
<li>
<p><strong>Verify process on the same host</strong>:</p>
<pre><code class="language-bash"># If hostname matches your current system
ps -p 12345

# If process doesn't exist, the lock is stale
# Retry resume - it will auto-clean the stale lock
</code></pre>
</li>
<li>
<p><strong>Cross-host lock scenario</strong>:</p>
<ul>
<li>If <code>hostname</code> in lock file differs from your current system, the process may be running elsewhere</li>
<li>Verify the other system is actually running the resume</li>
<li>If other system crashed or is offline, the lock is stale</li>
</ul>
</li>
</ol>
<p><strong>Manual Lock Removal</strong> (last resort):</p>
<p>If automatic cleanup fails (rare), you can manually remove the lock:</p>
<pre><code class="language-bash"># ⚠️ Only do this if you're certain the process is dead!
rm ~/.prodigy/resume_locks/mapreduce-1234567890.lock

# Then retry resume
prodigy resume mapreduce-1234567890
</code></pre>
<p><strong>Prevention Tips</strong>:</p>
<ul>
<li>Always let resume processes complete naturally (don’t use <code>kill -9</code>)</li>
<li>Use <code>Ctrl+C</code> for graceful interruption (triggers RAII cleanup)</li>
<li>The RAII pattern ensures locks are released even on panic or early exit</li>
<li>Stale locks are rare in normal operation</li>
</ul>
<p><strong>When Automatic Cleanup Fails</strong>:</p>
<p>Test coverage (tests/concurrent_resume_test.rs:77-105) validates stale lock detection. If you encounter persistent stale locks:</p>
<ol>
<li>Verify the lock file has valid JSON structure</li>
<li>Check file permissions on <code>~/.prodigy/resume_locks/</code></li>
<li>Confirm platform-specific process check is working (<code>kill -0</code> on Unix, <code>tasklist</code> on Windows)</li>
<li>Report issue with lock file contents and platform details</li>
</ol>
<p><strong>Available checkpoint commands</strong> (src/cli/args.rs:363-413):</p>
<ul>
<li><code>prodigy checkpoints list</code> - List all available checkpoints</li>
<li><code>prodigy checkpoints show &lt;job_id&gt;</code> - Show detailed checkpoint information</li>
<li><code>prodigy checkpoints validate &lt;checkpoint_id&gt;</code> - Verify checkpoint integrity</li>
<li><code>prodigy checkpoints clean</code> - Delete checkpoints for completed workflows</li>
</ul>
<h3 id="see-also-6"><a class="header" href="#see-also-6">See Also</a></h3>
<ul>
<li><a href="mapreduce/./dead-letter-queue-dlq.html">Dead Letter Queue (DLQ)</a> - Managing failed work items during resume operations</li>
<li><a href="mapreduce/./event-tracking.html">Event Tracking</a> - Understanding events logged during checkpoint creation and resume</li>
<li><a href="mapreduce/./global-storage-architecture.html">Global Storage Architecture</a> - Storage locations for checkpoints and state files</li>
<li><a href="mapreduce/./troubleshooting.html">Troubleshooting</a> - General MapReduce workflow debugging techniques</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="dead-letter-queue-dlq"><a class="header" href="#dead-letter-queue-dlq">Dead Letter Queue (DLQ)</a></h2>
<p>The Dead Letter Queue (DLQ) captures persistently failing work items for analysis and retry while allowing MapReduce jobs to continue processing other items. Instead of blocking the entire workflow when individual items fail, the DLQ provides fault tolerance and enables debugging of failure patterns.</p>
<h3 id="overview-3"><a class="header" href="#overview-3">Overview</a></h3>
<p>When a map agent fails to process a work item after exhausting retry attempts, the item is automatically sent to the DLQ. This allows the MapReduce job to complete successfully while preserving all failure information for later investigation and reprocessing.</p>
<p>The DLQ integrates with MapReduce through the <code>on_item_failure</code> policy, which defaults to <code>dlq</code> for MapReduce workflows. Alternative policies include <code>retry</code> (immediate retry), <code>skip</code> (ignore failures), <code>stop</code> (halt workflow), and <code>custom</code> (user-defined handling).</p>
<h3 id="storage-structure-1"><a class="header" href="#storage-structure-1">Storage Structure</a></h3>
<p>DLQ data is stored in the global Prodigy directory using this structure:</p>
<pre><code>~/.prodigy/dlq/{repo_name}/{job_id}/mapreduce/dlq/{job_id}/
├── items/                  # Individual item files
│   ├── item-123.json      # DeadLetteredItem for item-123
│   ├── item-456.json      # DeadLetteredItem for item-456
│   └── ...
└── index.json             # DLQ index metadata
</code></pre>
<p>For example:</p>
<pre><code>~/.prodigy/dlq/prodigy/mapreduce-1234567890/mapreduce/dlq/mapreduce-1234567890/
├── items/
│   └── item-123.json
└── index.json
</code></pre>
<p><strong>Storage Implementation</strong>:</p>
<ul>
<li><code>GlobalStorage</code> provides the base path: <code>~/.prodigy/dlq/{repo_name}/{job_id}</code> (src/storage/global.rs:47)</li>
<li><code>DLQStorage</code> adds subdirectories: <code>mapreduce/dlq/{job_id}</code> for backward compatibility (src/cook/execution/dlq.rs:529)</li>
<li>Each failed item is stored as a separate JSON file in the <code>items/</code> directory (src/cook/execution/dlq.rs:536)</li>
<li>File naming: <code>{item_id}.json</code> (e.g., <code>item-123.json</code>)</li>
<li><code>index.json</code> maintains a list of all item IDs and metadata for fast lookups (src/cook/execution/dlq.rs:608-637)</li>
<li>Items are loaded into memory on-demand for operations</li>
</ul>
<p>This global storage architecture enables:</p>
<ul>
<li><strong>Cross-worktree access</strong>: Multiple worktrees working on the same job share DLQ data</li>
<li><strong>Persistent state</strong>: DLQ survives worktree cleanup</li>
<li><strong>Centralized monitoring</strong>: All failures accessible from a single location</li>
<li><strong>Scalability</strong>: Individual item files prevent loading entire DLQ into memory</li>
</ul>
<h3 id="dlq-item-structure"><a class="header" href="#dlq-item-structure">DLQ Item Structure</a></h3>
<p>Each failed item in the DLQ is stored as a <code>DeadLetteredItem</code> with comprehensive failure information:</p>
<pre><code class="language-json">{
  "item_id": "item-123",
  "item_data": { "file": "src/main.rs", "priority": 5 },
  "first_attempt": "2025-01-11T10:25:00Z",
  "last_attempt": "2025-01-11T10:30:00Z",
  "failure_count": 3,
  "failure_history": [
    {
      "attempt_number": 1,
      "timestamp": "2025-01-11T10:30:00Z",
      "error_type": { "CommandFailed": { "exit_code": 101 } },
      "error_message": "cargo test failed with exit code 101",
      "stack_trace": "thread 'main' panicked at src/main.rs:42...",
      "agent_id": "agent-1",
      "step_failed": "shell: cargo test",
      "duration_ms": 45000,
      "json_log_location": "/Users/user/.local/state/claude/logs/session-abc123.json"
    }
  ],
  "error_signature": "CommandFailed::cargo test failed with exit",
  "reprocess_eligible": true,
  "manual_review_required": false,
  "worktree_artifacts": {
    "worktree_path": "/Users/user/.prodigy/worktrees/prodigy/agent-1",
    "branch_name": "agent-1",
    "uncommitted_changes": "src/main.rs modified",
    "error_logs": "error.log contents..."
  }
}
</code></pre>
<h4 id="key-fields"><a class="header" href="#key-fields">Key Fields</a></h4>
<p><strong>Source</strong>: <code>DeadLetteredItem</code> struct (src/cook/execution/dlq.rs:32-43)</p>
<ul>
<li><code>item_id</code>: Unique identifier for the work item</li>
<li><code>item_data</code>: Original work item data from input JSON</li>
<li><code>first_attempt</code>: Timestamp of first failure attempt (DateTime<Utc>)</li>
<li><code>last_attempt</code>: Timestamp of most recent failure attempt (DateTime<Utc>)</li>
<li><code>failure_count</code>: Number of failed attempts (u32)</li>
<li><code>failure_history</code>: Array of <code>FailureDetail</code> objects capturing each attempt</li>
<li><code>error_signature</code>: Simplified error pattern for grouping similar failures</li>
<li><code>reprocess_eligible</code>: Whether item can be retried automatically</li>
<li><code>manual_review_required</code>: Whether item needs human intervention</li>
<li><code>worktree_artifacts</code>: Captured state from failed agent’s worktree (Optional)</li>
</ul>
<h3 id="failure-detail-fields"><a class="header" href="#failure-detail-fields">Failure Detail Fields</a></h3>
<p>Each attempt in <code>failure_history</code> is a <code>FailureDetail</code> object (src/cook/execution/dlq.rs:46-59):</p>
<pre><code class="language-json">{
  "attempt_number": 1,
  "timestamp": "2025-01-11T10:30:00Z",
  "error_type": { "CommandFailed": { "exit_code": 101 } },
  "error_message": "cargo test failed with exit code 101",
  "stack_trace": "thread 'main' panicked at src/main.rs:42...",
  "agent_id": "agent-1",
  "step_failed": "claude: /process '${item}'",
  "duration_ms": 45000,
  "json_log_location": "/path/to/claude-session.json"
}
</code></pre>
<p><strong>Field Details</strong>:</p>
<ul>
<li><code>attempt_number</code>: Sequential attempt counter (u32)</li>
<li><code>timestamp</code>: When this attempt occurred (DateTime<Utc>)</li>
<li><code>error_type</code>: Error classification (see Error Types below)</li>
<li><code>error_message</code>: Human-readable error description</li>
<li><code>stack_trace</code>: Optional detailed stack trace</li>
<li><code>agent_id</code>: ID of the agent that failed</li>
<li><code>step_failed</code>: Command string that failed (e.g., “shell: cargo test”)</li>
<li><code>duration_ms</code>: How long the attempt ran before failing (u64)</li>
<li><code>json_log_location</code>: Path to Claude JSON log for debugging (Optional, see <a href="mapreduce/../troubleshooting/best-practices-for-debugging.html">Best Practices for Debugging</a>)</li>
</ul>
<h4 id="error-types"><a class="header" href="#error-types">Error Types</a></h4>
<p>The <code>error_type</code> field uses the <code>ErrorType</code> enum (src/cook/execution/dlq.rs:62-71):</p>
<ul>
<li><code>Timeout</code>: Agent execution exceeded timeout</li>
<li><code>CommandFailed { exit_code: i32 }</code>: Shell or Claude command returned non-zero exit code (includes exit code for diagnostics)</li>
<li><code>WorktreeError</code>: Git worktree operation failed</li>
<li><code>MergeConflict</code>: Merge back to parent worktree failed</li>
<li><code>ValidationFailed</code>: Validation command failed</li>
<li><code>ResourceExhausted</code>: System resources (memory, disk) exhausted</li>
<li><code>Unknown</code>: Unclassified error</li>
</ul>
<p><strong>Example from tests</strong> (tests/dlq_agent_integration_test.rs:69):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>error_type: ErrorType::CommandFailed { exit_code: 1 }
<span class="boring">}</span></code></pre></pre>
<h3 id="worktree-artifacts"><a class="header" href="#worktree-artifacts">Worktree Artifacts</a></h3>
<p>The <code>WorktreeArtifacts</code> structure captures the agent’s execution environment (src/cook/execution/dlq.rs:74-80):</p>
<ul>
<li><code>worktree_path</code>: Path to agent’s isolated worktree (PathBuf)</li>
<li><code>branch_name</code>: Git branch created for agent (String)</li>
<li><code>uncommitted_changes</code>: Files modified but not committed (Option<String>)</li>
<li><code>error_logs</code>: Captured error output (Option<String>)</li>
</ul>
<p><strong>Note</strong>: Both <code>uncommitted_changes</code> and <code>error_logs</code> are stored as optional strings, not arrays. If present, they contain descriptive text about the changes/logs rather than file lists.</p>
<p>These artifacts are preserved for debugging and can be accessed after failure.</p>
<h2 id="dlq-commands"><a class="header" href="#dlq-commands">DLQ Commands</a></h2>
<blockquote>
<p><strong>⚠️ PLANNED FEATURE</strong>: The DLQ CLI commands are currently implemented as stubs that only print status messages. Full functionality is planned for a future release.</p>
<p><strong>Current Status</strong> (as of implementation):</p>
<ul>
<li>Command definitions exist in src/cli/args.rs:577-677</li>
<li>Stub implementations in src/cli/commands/dlq.rs:1-74</li>
<li>Commands will print confirmation messages but do not execute actual operations</li>
</ul>
<p>See <a href="mapreduce/dead-letter-queue-dlq.html#integration-with-mapreduce">DLQ Integration</a> for current workflow-level DLQ functionality.</p>
</blockquote>
<p>Prodigy provides comprehensive CLI commands for managing the DLQ. The following sections describe the planned command interface.</p>
<h3 id="list-command"><a class="header" href="#list-command">List Command</a></h3>
<p>View DLQ items across jobs:</p>
<pre><code class="language-bash"># List all DLQ items
prodigy dlq list

# List items for specific job
prodigy dlq list --job-id mapreduce-1234567890

# List only retry-eligible items
prodigy dlq list --eligible

# Limit results
prodigy dlq list --limit 10
</code></pre>
<h3 id="inspect-command"><a class="header" href="#inspect-command">Inspect Command</a></h3>
<p>Examine detailed information for a specific item:</p>
<pre><code class="language-bash"># Inspect item by ID
prodigy dlq inspect item-123

# Inspect item in specific job
prodigy dlq inspect item-123 --job-id mapreduce-1234567890
</code></pre>
<p>Output includes:</p>
<ul>
<li>Full item data</li>
<li>Complete failure history with all attempts</li>
<li>Error messages and stack traces</li>
<li>JSON log locations for debugging</li>
<li>Worktree artifacts</li>
</ul>
<h3 id="analyze-command"><a class="header" href="#analyze-command">Analyze Command</a></h3>
<p>Analyze failure patterns across items:</p>
<pre><code class="language-bash"># Analyze all failures
prodigy dlq analyze

# Analyze specific job
prodigy dlq analyze --job-id mapreduce-1234567890

# Export analysis results
prodigy dlq analyze --export analysis.json
</code></pre>
<p>Analysis output includes:</p>
<ul>
<li><code>pattern_groups</code>: Failures grouped by error type and message similarity</li>
<li><code>error_distribution</code>: Histogram of error types</li>
<li><code>temporal_distribution</code>: Failures over time</li>
</ul>
<h3 id="retry-command"><a class="header" href="#retry-command">Retry Command</a></h3>
<blockquote>
<p><strong>⚠️ STUB</strong>: This command is not yet implemented. Description below shows planned interface.</p>
</blockquote>
<p>Reprocess failed items (src/cli/args.rs:640-654):</p>
<pre><code class="language-bash"># Retry all failed items for a workflow
prodigy dlq retry &lt;workflow_id&gt;

# Control parallelism (default: 10)
prodigy dlq retry &lt;workflow_id&gt; --parallel 10

# Filter items to retry
prodigy dlq retry &lt;workflow_id&gt; --filter 'item.priority &gt;= 5'

# Limit retry attempts per item (default: 3)
prodigy dlq retry &lt;workflow_id&gt; --max-retries 2

# Force retry even if not eligible
prodigy dlq retry &lt;workflow_id&gt; --force
</code></pre>
<p><strong>Command Parameters</strong>:</p>
<ul>
<li><code>&lt;workflow_id&gt;</code>: The workflow/job identifier to retry items from (e.g., “mapreduce-1234567890”)</li>
<li><code>--parallel</code>: Number of concurrent retry workers (default: 10, src/cli/args.rs:653)</li>
<li><code>--max-retries</code>: Maximum retry attempts per item (default: 3, src/cli/args.rs:648)</li>
<li><code>--filter</code>: Expression to filter which items to retry (default: all eligible items, src/cli/args.rs:644)</li>
<li><code>--force</code>: Force retry of items marked as not eligible (default: false, src/cli/args.rs:650)</li>
</ul>
<h4 id="retry-behavior"><a class="header" href="#retry-behavior">Retry Behavior</a></h4>
<p>The retry functionality is designed to handle large-scale DLQ reprocessing:</p>
<ul>
<li><strong>Streaming support</strong>: Items are processed incrementally to avoid memory issues with large DLQs</li>
<li><strong>Parallel execution</strong>: Uses <code>--parallel</code> flag (default: 10) to control concurrent workers</li>
<li><strong>State updates</strong>:
<ul>
<li>Successful items are removed from DLQ</li>
<li>Failed items remain with updated <code>failure_history</code></li>
<li><code>failure_count</code> is incremented</li>
</ul>
</li>
<li><strong>Correlation tracking</strong>: Maintains original item IDs and correlation metadata</li>
<li><strong>Interruption safe</strong>: Supports stopping and resuming retry operations</li>
</ul>
<p><strong>Implementation Note</strong>: The DLQ reprocessing logic uses the <code>DeadLetterQueue::reprocess</code> method (src/cook/execution/dlq.rs:202-233) which filters for <code>reprocess_eligible</code> items before attempting retry.</p>
<h3 id="export-command"><a class="header" href="#export-command">Export Command</a></h3>
<p>Export DLQ items for external analysis:</p>
<pre><code class="language-bash"># Export to JSON (default)
prodigy dlq export dlq-items.json

# Export to CSV
prodigy dlq export dlq-items.csv --format csv

# Export specific job
prodigy dlq export output.json --job-id mapreduce-1234567890
</code></pre>
<h3 id="stats-command"><a class="header" href="#stats-command">Stats Command</a></h3>
<p>View DLQ statistics:</p>
<pre><code class="language-bash"># Overall DLQ statistics
prodigy dlq stats

# Stats for specific workflow
prodigy dlq stats --workflow-id my-workflow
</code></pre>
<p>Output includes:</p>
<ul>
<li>Total items in DLQ</li>
<li>Items by error type</li>
<li>Average retry count</li>
<li>Retry-eligible vs manual review required</li>
<li>Temporal distribution</li>
</ul>
<h3 id="clear-command"><a class="header" href="#clear-command">Clear Command</a></h3>
<p>Remove items from DLQ:</p>
<pre><code class="language-bash"># Clear all items for a workflow (prompts for confirmation)
prodigy dlq clear my-workflow

# Skip confirmation prompt
prodigy dlq clear my-workflow --yes
</code></pre>
<h3 id="purge-command"><a class="header" href="#purge-command">Purge Command</a></h3>
<p>Clean up old DLQ items:</p>
<pre><code class="language-bash"># Purge items older than 30 days
prodigy dlq purge --older-than-days 30

# Purge for specific job
prodigy dlq purge --older-than-days 30 --job-id mapreduce-1234567890

# Skip confirmation
prodigy dlq purge --older-than-days 30 --yes
</code></pre>
<h2 id="debugging-with-dlq"><a class="header" href="#debugging-with-dlq">Debugging with DLQ</a></h2>
<h3 id="accessing-claude-json-logs"><a class="header" href="#accessing-claude-json-logs">Accessing Claude JSON Logs</a></h3>
<p>Each <code>FailureDetail</code> includes a <code>json_log_location</code> field pointing to the Claude Code JSON log for that execution. This log contains:</p>
<ul>
<li>Complete conversation history</li>
<li>All tool invocations and results</li>
<li>Error details and stack traces</li>
<li>Token usage statistics</li>
</ul>
<pre><code class="language-bash"># View JSON log from DLQ item
cat $(prodigy dlq inspect item-123 | jq -r '.failure_history[0].json_log_location')

# Pretty-print with jq
cat /path/to/session.json | jq '.'
</code></pre>
<p>For more details on Claude JSON logs, see <a href="mapreduce/../troubleshooting/best-practices-for-debugging.html">Best Practices for Debugging</a> and <a href="mapreduce/../retry-configuration/retry-metrics-and-observability.html">Retry Metrics and Observability</a>.</p>
<h3 id="common-debugging-workflow"><a class="header" href="#common-debugging-workflow">Common Debugging Workflow</a></h3>
<ol>
<li>
<p><strong>List failed items</strong>:</p>
<pre><code class="language-bash">prodigy dlq list --job-id mapreduce-1234567890
</code></pre>
</li>
<li>
<p><strong>Inspect specific failure</strong>:</p>
<pre><code class="language-bash">prodigy dlq inspect item-123
</code></pre>
</li>
<li>
<p><strong>Examine Claude logs</strong>:</p>
<pre><code class="language-bash">cat /path/to/claude-session.json | jq '.messages[-3:]'
</code></pre>
</li>
<li>
<p><strong>Analyze failure patterns</strong>:</p>
<pre><code class="language-bash">prodigy dlq analyze --job-id mapreduce-1234567890
</code></pre>
</li>
<li>
<p><strong>Fix underlying issue</strong> (code bug, config error, etc.)</p>
</li>
<li>
<p><strong>Retry failed items</strong>:</p>
<pre><code class="language-bash">prodigy dlq retry mapreduce-1234567890
</code></pre>
</li>
</ol>
<h2 id="integration-with-mapreduce"><a class="header" href="#integration-with-mapreduce">Integration with MapReduce</a></h2>
<p>The DLQ is tightly integrated with MapReduce workflows through the <code>on_item_failure</code> policy:</p>
<pre><code class="language-yaml">name: my-workflow
mode: mapreduce

map:
  input: "items.json"
  json_path: "$.items[*]"

  # Default policy: send failures to DLQ
  on_item_failure: dlq

  agent_template:
    - claude: "/process '${item}'"
</code></pre>
<h3 id="available-policies"><a class="header" href="#available-policies">Available Policies</a></h3>
<ul>
<li><strong><code>dlq</code></strong> (default): Failed items sent to DLQ, job continues</li>
<li><strong><code>retry</code></strong>: Immediate retry with exponential backoff</li>
<li><strong><code>skip</code></strong>: Ignore failures, mark as skipped, continue</li>
<li><strong><code>stop</code></strong>: Halt entire workflow on first failure</li>
<li><strong><code>custom</code></strong>: User-defined failure handler</li>
</ul>
<h3 id="failure-flow"><a class="header" href="#failure-flow">Failure Flow</a></h3>
<pre><code>Work Item Processing
       ↓
   Command Failed
       ↓
  Retry Attempts (if configured)
       ↓
   Still Failing?
       ↓
on_item_failure: dlq
       ↓
Create DeadLetteredItem
       ↓
Save to ~/.prodigy/dlq/{repo}/{job_id}/mapreduce/dlq/{job_id}/items/{item_id}.json
       ↓
Continue Processing Other Items
</code></pre>
<h2 id="best-practices-7"><a class="header" href="#best-practices-7">Best Practices</a></h2>
<h3 id="when-to-retry-vs-manual-fix"><a class="header" href="#when-to-retry-vs-manual-fix">When to Retry vs Manual Fix</a></h3>
<p><strong>Automatic Retry</strong> (via <code>prodigy dlq retry</code>):</p>
<ul>
<li>Transient failures (network timeouts, resource contention)</li>
<li>Flaky tests or intermittent issues</li>
<li>Items that may succeed with more resources (<code>--max-parallel 1</code>)</li>
</ul>
<p><strong>Manual Fix</strong> (code changes, then retry):</p>
<ul>
<li>Logic errors in processing code</li>
<li>Invalid assumptions about item data</li>
<li>Missing dependencies or configuration</li>
<li>Systematic failures affecting multiple items</li>
</ul>
<h3 id="dlq-management"><a class="header" href="#dlq-management">DLQ Management</a></h3>
<ol>
<li><strong>Monitor regularly</strong>: Use <code>prodigy dlq stats</code> to track failure rates</li>
<li><strong>Analyze patterns</strong>: Run <code>prodigy dlq analyze</code> to identify systematic issues</li>
<li><strong>Clean up old items</strong>: Periodically run <code>prodigy dlq purge</code> to remove resolved failures</li>
<li><strong>Set review flags</strong>: Mark <code>manual_review_required: true</code> for items needing human investigation</li>
</ol>
<h3 id="performance-considerations"><a class="header" href="#performance-considerations">Performance Considerations</a></h3>
<ul>
<li><strong>Large DLQs</strong>: Retry command uses streaming to handle thousands of items efficiently</li>
<li><strong>Parallelism</strong>: Tune <code>--max-parallel</code> based on failure type (CPU-bound vs I/O-bound)</li>
<li><strong>Filtering</strong>: Use <code>--filter</code> to target specific subsets of failures</li>
</ul>
<h2 id="advanced-dlq-storage-internals"><a class="header" href="#advanced-dlq-storage-internals">Advanced: DLQ Storage Internals</a></h2>
<p>For advanced debugging or direct file access, understanding the DLQ storage structure can be helpful.</p>
<h3 id="index-file-structure"><a class="header" href="#index-file-structure">Index File Structure</a></h3>
<p>The <code>index.json</code> file provides metadata about the DLQ (src/cook/execution/dlq.rs:608-637):</p>
<pre><code class="language-json">{
  "job_id": "mapreduce-1234567890",
  "item_count": 3,
  "item_ids": ["item-123", "item-456", "item-789"],
  "updated_at": "2025-01-11T10:30:00Z"
}
</code></pre>
<p>This index is automatically updated when items are added or removed from the DLQ.</p>
<h3 id="direct-file-access"><a class="header" href="#direct-file-access">Direct File Access</a></h3>
<p>To inspect DLQ items directly:</p>
<pre><code class="language-bash"># List all DLQ items for a job
ls ~/.prodigy/dlq/prodigy/mapreduce-1234567890/mapreduce/dlq/mapreduce-1234567890/items/

# View a specific item
cat ~/.prodigy/dlq/prodigy/mapreduce-1234567890/mapreduce/dlq/mapreduce-1234567890/items/item-123.json | jq

# Count total items
jq '.item_count' ~/.prodigy/dlq/prodigy/mapreduce-1234567890/mapreduce/dlq/mapreduce-1234567890/index.json
</code></pre>
<p><strong>Note</strong>: Direct file access is provided for debugging. Always use the Prodigy CLI commands for production operations to ensure data consistency.</p>
<h2 id="cross-references-2"><a class="header" href="#cross-references-2">Cross-References</a></h2>
<ul>
<li><a href="mapreduce/./checkpoint-and-resume.html">Checkpoint and Resume</a>: DLQ state preserved in checkpoints</li>
<li><a href="mapreduce/./event-tracking.html">Event Tracking</a>: DLQ operations emit trackable events</li>
<li><a href="mapreduce/../error-handling.html">Error Handling</a>: Broader error handling strategies</li>
<li><a href="mapreduce/../mapreduce-worktree-architecture.html">Worktree Architecture</a>: Agent isolation and artifact preservation</li>
<li><a href="mapreduce/../troubleshooting/best-practices-for-debugging.html">Best Practices for Debugging</a>: Using JSON logs and DLQ for debugging failed items</li>
<li><a href="mapreduce/../retry-configuration/retry-metrics-and-observability.html">Retry Metrics and Observability</a>: Monitoring retry behavior and failures</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="workflow-format-comparison"><a class="header" href="#workflow-format-comparison">Workflow Format Comparison</a></h2>
<p>Prodigy supports multiple workflow format styles to balance simplicity for quick tasks with power for production workflows. This section explains the differences and helps you choose the right format.</p>
<h3 id="standard-workflow-formats"><a class="header" href="#standard-workflow-formats">Standard Workflow Formats</a></h3>
<p>Standard workflows (non-MapReduce) can be written in two formats:</p>
<h4 id="simple-array-format"><a class="header" href="#simple-array-format">Simple Array Format</a></h4>
<p>For quick workflows, use a simple array of commands:</p>
<pre><code class="language-yaml"># Simple array format - minimal syntax
- claude: "/prodigy-coverage"
  commit_required: true

- shell: "just test"
  on_failure:
    claude: "/prodigy-debug-test-failure"
</code></pre>
<p><strong>Use this format when:</strong></p>
<ul>
<li>Creating quick automation scripts</li>
<li>No environment variables or profiles needed</li>
<li>Workflow is self-contained and straightforward</li>
</ul>
<h4 id="full-configuration-format"><a class="header" href="#full-configuration-format">Full Configuration Format</a></h4>
<p>For production workflows, use the full configuration format with metadata:</p>
<pre><code class="language-yaml"># Full config format - includes metadata and environment
name: mapreduce-env-example
mode: mapreduce

env:
  PROJECT_NAME: "example-project"
  OUTPUT_DIR: "output"
  DEBUG_MODE: "false"

# Secrets are a separate top-level configuration
# They support two formats: Simple and Provider-based
secrets:
  # Simple format - directly references environment variable
  SIMPLE_SECRET: "ENV_VAR_NAME"

  # Provider format - explicit provider configuration
  API_TOKEN:
    provider: env
    key: "GITHUB_TOKEN"

  # File-based secret provider
  FILE_SECRET:
    provider: file
    key: "/path/to/secret/file"

profiles:
  development:
    DEBUG_MODE: "true"
  production:
    DEBUG_MODE: "false"

# Commands go here
setup:
  - shell: "echo Starting $PROJECT_NAME"
</code></pre>
<p><strong>Use this format when:</strong></p>
<ul>
<li>Deploying to multiple environments (dev, staging, prod)</li>
<li>Need environment variables or secrets</li>
<li>Workflow requires parameterization</li>
<li>Building reusable workflow templates</li>
</ul>
<p><strong>Secret Format Details:</strong></p>
<p>Secrets support two formats (defined in <code>src/cook/environment/config.rs:86-95</code>):</p>
<ol>
<li>
<p><strong>Simple format</strong> - Direct environment variable reference:</p>
<pre><code class="language-yaml">secrets:
  API_KEY: "ENV_VAR_NAME"  # Reads from $ENV_VAR_NAME
</code></pre>
</li>
<li>
<p><strong>Provider format</strong> - Explicit provider configuration:</p>
<pre><code class="language-yaml">secrets:
  API_KEY:
    provider: env          # Providers: env, file, vault, aws
    key: "GITHUB_TOKEN"    # Source key/path
    version: "v1"          # Optional version
</code></pre>
</li>
</ol>
<p><strong>Available Providers</strong> (defined in <code>src/cook/environment/config.rs:101-109</code>):</p>
<ul>
<li><code>env</code> - Environment variables (most common)</li>
<li><code>file</code> - File-based secrets</li>
<li><code>vault</code> - HashiCorp Vault integration</li>
<li><code>aws</code> - AWS Secrets Manager integration</li>
</ul>
<p>Both formats mask secret values in logs. The Simple format is convenient for environment variables, while Provider format supports advanced secret management systems like Vault and AWS Secrets Manager.</p>
<p><strong>Note</strong>: For detailed Vault and AWS provider configuration, see <a href="mapreduce/../environment/secrets-management.html">Secrets Management</a> and <a href="mapreduce/./environment-variables-in-configuration.html">Environment Variables in Configuration</a>.</p>
<p><strong>Source</strong>: Example workflow at <code>workflows/mapreduce-env-example.yml:23-26</code></p>
<h3 id="mapreduce-syntax-evolution"><a class="header" href="#mapreduce-syntax-evolution">MapReduce Syntax Evolution</a></h3>
<p>MapReduce workflows have evolved to use simpler, more concise syntax.</p>
<h4 id="preferred-syntax-current"><a class="header" href="#preferred-syntax-current">Preferred Syntax (Current)</a></h4>
<p>Commands are listed directly under <code>agent_template</code> and <code>reduce</code>:</p>
<pre><code class="language-yaml">name: parallel-debt-elimination
mode: mapreduce

setup:
  - shell: "debtmap analyze . --output debt_items.json"

map:
  input: debt_items.json
  json_path: "$.debt_items[*]"

  # Direct array syntax - preferred
  agent_template:
    - claude: "/fix-issue ${item.description}"
    - shell: "cargo test"
      on_failure:
        claude: "/debug-test"

  max_parallel: 10

# Direct array syntax for reduce - preferred
reduce:
  - claude: "/summarize-fixes ${map.results}"
  - shell: "echo Processed ${map.total} items"
</code></pre>
<p><strong>Benefits:</strong></p>
<ul>
<li>Less nesting, easier to read</li>
<li>Cleaner YAML structure</li>
<li>Follows YAML array conventions</li>
<li>Consistent with standard workflow format</li>
<li>Forward compatibility - the nested format may be removed in future versions</li>
</ul>
<h4 id="legacy-syntax-deprecated"><a class="header" href="#legacy-syntax-deprecated">Legacy Syntax (Deprecated)</a></h4>
<p>The old format nested commands under a <code>commands</code> field:</p>
<pre><code class="language-yaml"># Old syntax - deprecated but still supported
map:
  input: "work-items.json"
  json_path: "$.items[*]"

  # Nested under 'commands' - deprecated
  agent_template:
    commands:
      - shell: echo "Processing item ${item.id}"
      - shell: echo "Completed ${item.task}"

  max_parallel: 3

# Nested under 'commands' - deprecated
reduce:
  commands:
    - shell: echo "Processed ${map.total} items"
</code></pre>
<p><strong>Deprecation Notice:</strong></p>
<ul>
<li>This format is still supported for backward compatibility</li>
<li>New workflows should use the direct array syntax</li>
<li>Future versions may remove support for nested <code>commands</code></li>
<li>When using the old format, Prodigy emits a warning: “Using deprecated nested ‘commands’ syntax in agent_template. Consider using the simplified array format directly under ‘agent_template’.”</li>
</ul>
<p><strong>Source</strong>: Deprecation warnings in <code>src/config/mapreduce.rs:310, 347</code></p>
<h3 id="migration-guide"><a class="header" href="#migration-guide">Migration Guide</a></h3>
<p>To migrate from old to new syntax:</p>
<p><strong>Before (Old):</strong></p>
<pre><code class="language-yaml">agent_template:
  commands:
    - claude: "/process ${item}"
    - shell: "test ${item.path}"

reduce:
  commands:
    - claude: "/summarize ${map.results}"
</code></pre>
<p><strong>After (New):</strong></p>
<pre><code class="language-yaml">agent_template:
  - claude: "/process ${item}"
  - shell: "test ${item.path}"

reduce:
  - claude: "/summarize ${map.results}"
</code></pre>
<p><strong>Migration Steps:</strong></p>
<ol>
<li>Remove the <code>commands:</code> line from <code>agent_template</code></li>
<li>Remove the <code>commands:</code> line from <code>reduce</code></li>
<li>Unindent the command list by one level</li>
<li>Test the workflow to ensure it works correctly</li>
</ol>
<p><strong>Important Notes:</strong></p>
<ul>
<li>The workflow format is all-or-nothing - you cannot mix old and new formats within the same workflow</li>
<li>Both <code>agent_template</code> and <code>reduce</code> must use the same format (both direct array or both nested)</li>
<li>After migration, run <code>prodigy run workflow.yml --dry-run</code> to validate syntax before executing</li>
<li>If the workflow fails after migration, check for indentation errors - YAML is whitespace-sensitive</li>
</ul>
<h3 id="format-decision-tree"><a class="header" href="#format-decision-tree">Format Decision Tree</a></h3>
<p>Choose your format based on these questions:</p>
<ol>
<li>
<p><strong>Is this a MapReduce workflow?</strong></p>
<ul>
<li>Yes → Use <code>mode: mapreduce</code> with direct array syntax</li>
<li>No → Continue to question 2</li>
</ul>
</li>
<li>
<p><strong>Do you need environment variables or profiles?</strong></p>
<ul>
<li>Yes → Use full configuration format</li>
<li>No → Continue to question 3</li>
</ul>
</li>
<li>
<p><strong>Is this a quick one-off workflow?</strong></p>
<ul>
<li>Yes → Use simple array format</li>
<li>No → Use full configuration format for maintainability</li>
</ul>
</li>
</ol>
<h3 id="cross-references-3"><a class="header" href="#cross-references-3">Cross-References</a></h3>
<ul>
<li><a href="mapreduce/./setup-phase-advanced.html">Setup Phase Advanced</a> - Detailed setup phase configuration and patterns</li>
<li><a href="mapreduce/./index.html">MapReduce Overview</a> - MapReduce workflow fundamentals and phase documentation</li>
<li><a href="mapreduce/../workflow-basics/full-workflow-structure.html">Full Workflow Structure</a> - Complete workflow configuration reference</li>
<li><a href="mapreduce/./environment-variables-in-configuration.html">Environment Variables in Configuration</a> - Using variables and secrets in workflows</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="command-types-1"><a class="header" href="#command-types-1">Command Types</a></h1>
<h2 id="1-shell-commands"><a class="header" href="#1-shell-commands">1. Shell Commands</a></h2>
<pre><code class="language-yaml"># Simple shell command
- shell: "cargo test"

# With output capture
- shell: "ls -la | wc -l"
  capture_output: "file_count"

# With failure handling
- shell: "cargo clippy"
  on_failure:
    claude: "/fix-warnings ${shell.output}"

# With nested failure handlers (multi-level error recovery)
- shell: "cargo test"
  on_failure:
    claude: "/debug-test-failures ${shell.output}"
    on_failure:
      shell: "notify-team.sh 'Tests failed and debugging unsuccessful'"

# With timeout
- shell: "cargo bench"
  timeout: 600  # seconds

# With conditional execution
- shell: "cargo build --release"
  when: "${tests_passed}"

# Working directory control (use shell cd command)
- shell: "cd crates/prodigy-core &amp;&amp; cargo test"

# Environment variables (use shell syntax)
- shell: "PATH=/custom/bin:$PATH rustfmt --check src/**/*.rs"
</code></pre>
<h2 id="2-claude-commands"><a class="header" href="#2-claude-commands">2. Claude Commands</a></h2>
<pre><code class="language-yaml"># Simple Claude command
- claude: "/prodigy-analyze"

# With arguments
- claude: "/prodigy-implement-spec ${spec_file}"

# With commit requirement
- claude: "/prodigy-fix-bugs"
  commit_required: true

# With output capture
- claude: "/prodigy-generate-plan"
  capture_output: "implementation_plan"
</code></pre>
<h2 id="3-goal-seeking-commands"><a class="header" href="#3-goal-seeking-commands">3. Goal-Seeking Commands</a></h2>
<p>Iteratively refine code until a validation threshold is met.</p>
<pre><code class="language-yaml">- goal_seek:
    goal: "Achieve 90% test coverage"
    claude: "/prodigy-coverage --improve"
    validate: "cargo tarpaulin --print-summary | grep 'Coverage' | sed 's/.*Coverage=\\([0-9]*\\).*/score: \\1/'"
    threshold: 90
    max_attempts: 5
    timeout_seconds: 300
    fail_on_incomplete: true
  commit_required: true
</code></pre>
<p><strong>Fields:</strong></p>
<ul>
<li><code>goal</code>: Human-readable description</li>
<li><code>claude</code> or <code>shell</code>: Command to execute for refinement</li>
<li><code>validate</code>: Command that outputs <code>score: N</code> (0-100)</li>
<li><code>threshold</code>: Minimum score to consider complete</li>
<li><code>max_attempts</code>: Maximum refinement iterations</li>
<li><code>timeout_seconds</code>: Optional timeout per attempt</li>
<li><code>fail_on_incomplete</code>: Whether to fail workflow if threshold not met (default: true)</li>
</ul>
<p><strong>Troubleshooting:</strong></p>
<ul>
<li><strong>Threshold not met:</strong> Check that validate command outputs exactly <code>score: N</code> format (0-100)</li>
<li><strong>Not converging:</strong> Use <code>fail_on_incomplete: false</code> for optional quality gates</li>
<li><strong>Debug scores:</strong> Run workflow with verbose mode (<code>-v</code>) to see validation scores each iteration</li>
<li><strong>Max attempts reached:</strong> Increase <code>max_attempts</code> or lower <code>threshold</code> if goal is too ambitious</li>
</ul>
<h2 id="4-foreach-commands"><a class="header" href="#4-foreach-commands">4. Foreach Commands</a></h2>
<p>Iterate over a list with optional parallelism.</p>
<pre><code class="language-yaml">- foreach:
    input: "find . -name '*.rs' -type f"  # Command
    # OR
    # input: ["file1.rs", "file2.rs"]    # List

    parallel: 5  # Number of parallel executions (or true/false)

    do:
      - claude: "/analyze-file ${item}"
      - shell: "cargo check ${item}"

    continue_on_error: true
    max_items: 50
</code></pre>
<p><strong>Variables and Error Handling:</strong></p>
<ul>
<li><strong>${item}</strong>: Current item value available in loop body</li>
<li><strong>continue_on_error: true</strong> (default): Failed items don’t stop the loop</li>
<li><strong>Parallel execution caveat</strong>: Output order is not guaranteed when using <code>parallel</code></li>
<li><strong>No built-in result aggregation</strong>: Use <code>write_file</code> commands to collect results if needed</li>
</ul>
<p><strong>Example with result collection:</strong></p>
<pre><code class="language-yaml">- foreach:
    input: ["module1", "module2", "module3"]
    parallel: 3
    do:
      - shell: "cargo test --package ${item}"
      - write_file:
          path: "results/${item}.txt"
          content: "Test result: ${shell.output}"
          create_dirs: true
</code></pre>
<h2 id="5-write-file-commands"><a class="header" href="#5-write-file-commands">5. Write File Commands</a></h2>
<p>Create or overwrite files with content from variables or literals. Supports text, JSON, and YAML formats with automatic validation and formatting.</p>
<pre><code class="language-yaml"># Write plain text file
- write_file:
    path: "output/result.txt"
    content: "Build completed at ${shell.output}"
    format: text
    mode: "0644"
    create_dirs: true

# Write JSON file with validation
- write_file:
    path: "config/generated.json"
    content: |
      {
        "version": "${version}",
        "timestamp": "${timestamp}",
        "items": ${items_json}
      }
    format: json

# Write YAML file with formatting
- write_file:
    path: ".prodigy/metadata.yml"
    content: |
      workflow: ${workflow.name}
      iteration: ${workflow.iteration}
      results:
        success: ${map.successful}
        total: ${map.total}
    format: yaml
</code></pre>
<p><strong>WriteFileConfig Fields:</strong></p>
<ul>
<li><code>path</code> - File path to write (supports variable interpolation)</li>
<li><code>content</code> - Content to write (supports variable interpolation)</li>
<li><code>format</code> - Output format: <code>text</code> (default), <code>json</code>, <code>yaml</code></li>
<li><code>mode</code> - File permissions in octal (default: “0644”)</li>
<li><code>create_dirs</code> - Create parent directories if they don’t exist (default: false)</li>
</ul>
<p><strong>Format Validation:</strong></p>
<ul>
<li><code>json</code> - Validates JSON syntax and pretty-prints output</li>
<li><code>yaml</code> - Validates YAML syntax and formats output</li>
<li><code>text</code> - Writes content as-is without validation</li>
</ul>
<p><strong>Best Practices:</strong></p>
<ul>
<li><strong>Use format validation for config files</strong>: Set <code>format: json</code> or <code>format: yaml</code> when generating configuration files to catch syntax errors early</li>
<li><strong>Set appropriate permissions</strong>: Use <code>mode</code> field to control file permissions (e.g., <code>"0600"</code> for sensitive files)</li>
<li><strong>Handle nested paths</strong>: Set <code>create_dirs: true</code> when writing to paths that may not exist</li>
<li><strong>Combine with validation</strong>: Use <code>validate</code> field to ensure generated files meet requirements before proceeding</li>
<li><strong>For logs and documentation</strong>: Use <code>format: text</code> to write content as-is without validation overhead</li>
</ul>
<h2 id="6-validation-commands"><a class="header" href="#6-validation-commands">6. Validation Commands</a></h2>
<p>Validate implementation completeness with automatic retry.</p>
<blockquote>
<p><strong>Warning:</strong> The <code>command</code> field in ValidationConfig is deprecated. Use <code>shell</code> instead for shell commands or <code>claude</code> for Claude commands. The <code>command</code> field is still supported for backward compatibility but will be removed in a future version.</p>
</blockquote>
<pre><code class="language-yaml">- claude: "/implement-auth-spec"
  validate:
    shell: "debtmap validate --spec auth.md --output result.json"
    result_file: "result.json"
    threshold: 95  # Percentage completion required (default: 100.0)
    timeout: 60
    expected_schema: "validation-schema.json"  # Optional JSON schema

    # What to do if incomplete
    on_incomplete:
      claude: "/complete-implementation ${validation.gaps}"
      max_attempts: 3
      fail_workflow: true
      commit_required: true
      prompt: "Implementation incomplete. Continue?"  # Optional interactive prompt
</code></pre>
<p><strong>ValidationConfig Fields:</strong></p>
<ul>
<li><code>shell</code> or <code>claude</code> - Single validation command (use <code>shell</code>, not deprecated <code>command</code>)</li>
<li><code>commands</code> - Array of commands for multi-step validation</li>
<li><code>result_file</code> - Path to JSON file with validation results</li>
<li><code>threshold</code> - Minimum completion percentage (default: 100.0)</li>
<li><code>timeout</code> - Timeout in seconds</li>
<li><code>expected_schema</code> - JSON schema for validation output structure</li>
</ul>
<p><strong>OnIncompleteConfig Fields:</strong></p>
<ul>
<li><code>shell</code> or <code>claude</code> - Single gap-filling command</li>
<li><code>commands</code> - Array of commands for multi-step gap filling</li>
<li><code>max_attempts</code> - Maximum retry attempts (default: 2)</li>
<li><code>fail_workflow</code> - Whether to fail workflow if validation incomplete (default: true)</li>
<li><code>commit_required</code> - Whether to require commit after gap filling (default: false)</li>
<li><code>prompt</code> - Optional interactive prompt for user guidance</li>
</ul>
<p><strong>Alternative: Array format for multi-step validation</strong></p>
<pre><code class="language-yaml">- claude: "/implement-feature"
  validate:
    # When using array format, ValidationConfig uses default threshold (100.0)
    # and creates a commands array
    - shell: "run-tests.sh"
    - shell: "check-coverage.sh"
    - claude: "/validate-implementation --output validation.json"
      result_file: "validation.json"
</code></pre>
<p><strong>Alternative: Multi-step gap filling</strong></p>
<pre><code class="language-yaml">- claude: "/implement-feature"
  validate:
    shell: "validate.sh"
    result_file: "result.json"
    on_incomplete:
      commands:
        - claude: "/analyze-gaps ${validation.gaps}"
        - shell: "run-fix-script.sh"
        - claude: "/verify-fixes"
      max_attempts: 2
</code></pre>
<hr />
<h2 id="command-reference"><a class="header" href="#command-reference">Command Reference</a></h2>
<h3 id="command-fields"><a class="header" href="#command-fields">Command Fields</a></h3>
<p>All command types support these common fields:</p>
<p><strong>Source</strong>: src/config/command.rs:320-401</p>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody>
<tr><td><code>id</code></td><td>string</td><td>Unique identifier for referencing outputs</td></tr>
<tr><td><code>timeout</code></td><td>number</td><td>Command timeout in seconds</td></tr>
<tr><td><code>commit_required</code></td><td>boolean</td><td>Whether command should create a git commit</td></tr>
<tr><td><code>when</code></td><td>string</td><td>Conditional execution expression</td></tr>
<tr><td><code>capture_output</code></td><td>boolean/string</td><td>Capture command output to a variable (boolean for default “output” var, string for custom name)</td></tr>
<tr><td><code>capture_format</code></td><td>enum</td><td>Output parsing format: <code>string</code> (default), <code>number</code>, <code>json</code>, <code>lines</code>, <code>boolean</code></td></tr>
<tr><td><code>on_success</code></td><td>object</td><td>Command to run on success</td></tr>
<tr><td><code>on_failure</code></td><td>object</td><td>Error handling configuration (see OnFailure Handler section below)</td></tr>
<tr><td><code>validate</code></td><td>object</td><td>Validation configuration for implementation completeness</td></tr>
<tr><td><code>output_file</code></td><td>string</td><td>Redirect command output to a file path</td></tr>
</tbody></table>
</div>
<p><strong>Note on Field Availability</strong>: All fields in this table are available in the user-facing YAML WorkflowStepCommand format (src/config/command.rs:320-401). Additional fields like <code>working_dir</code>, <code>env</code>, <code>on_exit_code</code>, and <code>capture</code> exist in the internal WorkflowStep representation (src/cook/workflow/executor/data_structures.rs:35-157) but are NOT exposed in YAML. See the Technical Notes section below for workarounds.</p>
<h3 id="onfailure-handler-configuration"><a class="header" href="#onfailure-handler-configuration">OnFailure Handler Configuration</a></h3>
<p>The <code>on_failure</code> field accepts an <code>OnFailureConfig</code> which supports multiple configuration formats:</p>
<p><strong>Source</strong>: src/cook/workflow/on_failure.rs:67-115</p>
<p><strong>Simple Format - Single Command:</strong></p>
<pre><code class="language-yaml">- shell: "cargo clippy"
  on_failure:
    claude: "/fix-warnings ${shell.output}"
</code></pre>
<p><strong>Advanced Format - With Retry Control:</strong></p>
<pre><code class="language-yaml">- shell: "cargo test"
  on_failure:
    claude: "/debug-test-failures ${shell.output}"
    max_retries: 3              # Maximum retry attempts (default: 1)
    fail_workflow: false        # Continue workflow even after failure handling
    retry_original: true        # Retry original command after handler (default: false)
</code></pre>
<p><strong>Detailed Format - With Handler Strategy:</strong></p>
<p>For fine-grained control over failure handling behavior, use the Detailed format with <code>FailureHandlerConfig</code> (src/cook/workflow/on_failure.rs:9-49):</p>
<pre><code class="language-yaml">- shell: "cargo build"
  on_failure:
    commands:
      - claude: "/analyze-build-errors ${shell.output}"
      - shell: "cargo clean &amp;&amp; cargo build"
    strategy: recovery          # recovery, fallback, cleanup, or custom
    timeout: 300                # Handler timeout in seconds
    fail_workflow: false        # Whether to fail workflow after handling
    handler_failure_fatal: true # Whether handler failure should be fatal
</code></pre>
<p><strong>Handler Strategies</strong> (src/cook/workflow/on_failure.rs:9-22):</p>
<ul>
<li><code>recovery</code> (default) - Attempt to fix the problem and retry the original operation. Use this when failures are transient or can be resolved programmatically.</li>
<li><code>fallback</code> - Use an alternative approach when the primary method fails. Best for scenarios with multiple valid execution paths.</li>
<li><code>cleanup</code> - Perform resource cleanup before failing. Use for releasing locks, closing connections, or removing temporary files.</li>
<li><code>custom</code> - Custom handler logic for specialized error recovery patterns not covered by other strategies.</li>
</ul>
<p><strong>Real-World Examples:</strong></p>
<p>From <code>workflows/coverage.yml</code> (lines 13-17):</p>
<pre><code class="language-yaml">- shell: "just test"
  on_failure:
    claude: "/prodigy-debug-test-failure --spec ${coverage.spec} --output ${shell.output}"
    max_attempts: 3
    fail_workflow: false
</code></pre>
<p>From <code>workflows/implement.yml</code> (lines 20-30):</p>
<pre><code class="language-yaml">- shell: "just test"
  on_failure:
    claude: "/prodigy-debug-test-failure --spec $ARG --output ${shell.output}"
    max_attempts: 5
    fail_workflow: false
</code></pre>
<p><strong>Nested Failure Handlers:</strong></p>
<pre><code class="language-yaml">- shell: "cargo test"
  on_failure:
    claude: "/debug-test-failures ${shell.output}"
    on_failure:
      shell: "notify-team.sh 'Tests failed and debugging unsuccessful'"
</code></pre>
<h3 id="planned-feature-capturestreams"><a class="header" href="#planned-feature-capturestreams">Planned Feature: CaptureStreams</a></h3>
<blockquote>
<p><strong>Note:</strong> The <code>capture_streams</code> field is defined in WorkflowStepCommand (src/config/command.rs:394-396) but is <strong>reserved for future use</strong> and not yet functional in YAML workflows.</p>
<p><strong>Why it’s not available yet</strong>: The field exists as a <code>String</code> placeholder in the YAML parser, but the execution engine doesn’t yet support fine-grained stream capture control. This feature is planned to provide selective capture of stdout, stderr, exit codes, success status, and execution duration.</p>
</blockquote>
<p><strong>Current Approach:</strong> Use the <code>capture_output</code> and <code>capture_format</code> fields to control output capture, which cover most common use cases:</p>
<pre><code class="language-yaml"># Capture stdout as string (most common use case)
- shell: "cargo test"
  capture_output: "test_output"
  capture_format: "string"

# Capture exit status as boolean
- shell: "cargo test"
  capture_output: "test_passed"
  capture_format: "boolean"

# Capture and parse JSON output
- shell: "cargo metadata --format-version 1"
  capture_output: "project_info"
  capture_format: "json"
</code></pre>
<p><strong>When will this be available?</strong> This feature requires execution engine changes to support the internal <code>CaptureStreams</code> type (src/cook/workflow/executor/data_structures.rs:83). Until then, use the <code>capture_output</code> and <code>capture_format</code> fields which cover most common use cases.</p>
<h3 id="capture-format-examples"><a class="header" href="#capture-format-examples">Capture Format Examples</a></h3>
<p>The <code>capture_format</code> field controls how captured output is parsed:</p>
<pre><code class="language-yaml"># String format (default) - raw text output
- shell: "git rev-parse HEAD"
  capture: "commit_hash"
  capture_format: "string"

# Number format - parses numeric output
- shell: "wc -l &lt; file.txt"
  capture: "line_count"
  capture_format: "number"

# JSON format - parses JSON output
- shell: "cargo metadata --format-version 1"
  capture: "project_metadata"
  capture_format: "json"

# Lines format - splits output into array of lines
- shell: "git diff --name-only"
  capture: "changed_files"
  capture_format: "lines"

# Boolean format - true if command succeeds, false otherwise
- shell: "grep -q 'pattern' file.txt"
  capture: "pattern_found"
  capture_format: "boolean"
</code></pre>
<h3 id="deprecated-fields"><a class="header" href="#deprecated-fields">Deprecated Fields</a></h3>
<blockquote>
<p><strong>Warning:</strong> The following fields are deprecated but still supported for backward compatibility. They will be removed in a future version. Please migrate to the recommended alternatives.</p>
</blockquote>
<p>These fields are deprecated:</p>
<ul>
<li><code>test:</code> - <strong>Use <code>shell:</code> with <code>on_failure:</code> instead</strong></li>
<li><code>command:</code> in ValidationConfig - <strong>Use <code>shell:</code> instead</strong></li>
<li>Nested <code>commands:</code> in <code>agent_template</code> and <code>reduce</code> - <strong>Use direct array format instead</strong></li>
<li>Legacy variable aliases (<code>$ARG</code>, <code>$ARGUMENT</code>, <code>$FILE</code>, <code>$FILE_PATH</code>) - <strong>Use modern <code>${item.*}</code> syntax</strong></li>
</ul>
<p><strong>Using capture_output Field</strong></p>
<p>The <code>capture_output</code> field supports both Boolean and String variants for backward compatibility:</p>
<p><strong>Source</strong>: src/config/command.rs:366-368 (CaptureOutputConfig enum: src/config/command.rs:403-411)</p>
<pre><code class="language-yaml"># String variant - captures to named variable (recommended)
- shell: "git rev-parse HEAD"
  capture_output: "commit_hash"

# Boolean variant - captures to default "output" variable
- shell: "ls -la | wc -l"
  capture_output: true
</code></pre>
<p><strong>Migration Guide - capture_output Variants:</strong></p>
<p>The <code>capture_output</code> field accepts both boolean and string values for backward compatibility:</p>
<div class="table-wrapper"><table><thead><tr><th>Syntax</th><th>Variable Name</th><th>When to Use</th></tr></thead><tbody>
<tr><td><code>capture_output: true</code></td><td><code>${output}</code></td><td>Quick capture to default variable</td></tr>
<tr><td><code>capture_output: "myvar"</code></td><td><code>${myvar}</code></td><td>Recommended: explicit variable name</td></tr>
</tbody></table>
</div>
<p><strong>Before and After Examples:</strong></p>
<pre><code class="language-yaml"># Boolean variant - captures to ${output}
# BEFORE:
- shell: "git rev-parse HEAD"
  capture_output: true
# Access with: ${output}

# AFTER (recommended):
- shell: "git rev-parse HEAD"
  capture_output: "commit_hash"
# Access with: ${commit_hash}

# String variant - already correct
- shell: "ls -la | wc -l"
  capture_output: "file_count"
# Access with: ${file_count}
</code></pre>
<p><strong>Note</strong>: The internal WorkflowStep struct (src/cook/workflow/executor/data_structures.rs:75) uses a <code>capture</code> field internally, but this is NOT exposed in the user-facing YAML WorkflowStepCommand struct. Always use <code>capture_output</code> for capturing command output in YAML workflows.</p>
<p>You can then reference captured values using <code>${commit_hash}</code> or <code>${output}</code> in subsequent commands.</p>
<hr />
<h2 id="technical-notes"><a class="header" href="#technical-notes">Technical Notes</a></h2>
<details>
<summary>Internal Implementation Fields (for contributors)</summary>
<p>The following fields exist in the internal WorkflowStep struct (src/cook/workflow/executor/data_structures.rs:35-157) but are NOT available in the user-facing WorkflowStepCommand YAML syntax (src/config/command.rs:320-401). These are implementation details managed by Prodigy’s execution engine during workflow execution:</p>
<p><strong>Execution Control Fields (Internal Only):</strong></p>
<ul>
<li><code>handler</code> - Internal HandlerStep for execution routing</li>
<li><code>retry</code> - Internal RetryConfig for automatic retry logic</li>
<li><code>auto_commit</code> - Internal commit tracking</li>
<li><code>commit_config</code> - Internal commit configuration</li>
<li><code>step_validate</code> - Internal validation state</li>
<li><code>skip_validation</code> - Internal validation control</li>
<li><code>validation_timeout</code> - Internal validation timing</li>
<li><code>ignore_validation_failure</code> - Internal validation handling</li>
</ul>
<p><strong>Environment and Context Fields (Planned/Not Yet Exposed in YAML):</strong></p>
<ul>
<li><code>working_dir</code> - Working directory control (WorkflowStep:99, NOT in WorkflowStepCommand)</li>
<li><code>env</code> - Environment variable overrides (WorkflowStep:103, NOT in WorkflowStepCommand)</li>
<li><code>on_exit_code</code> - Exit code specific handlers (WorkflowStep:119, NOT in WorkflowStepCommand)</li>
<li><code>capture</code> - Variable name for output (WorkflowStep:75, use <code>capture_output</code> in YAML instead)</li>
</ul>
<p><strong>Why the separation?</strong> WorkflowStepCommand defines what users can write in YAML. WorkflowStep is the internal runtime representation with additional fields populated during execution. Some features exist in the execution layer but aren’t yet exposed in the YAML syntax.</p>
<p><strong>Workarounds for missing YAML fields:</strong></p>
<p>Since <code>cwd</code>, <code>env</code>, and <code>on_exit_code</code> are NOT available in WorkflowStepCommand, use these shell-level alternatives:</p>
<pre><code class="language-yaml"># Instead of cwd field (NOT available):
# ❌ - shell: "cargo test"
#      cwd: "crates/prodigy-core"

# ✓ Use shell cd command:
- shell: "cd crates/prodigy-core &amp;&amp; cargo test"

# Instead of env field (NOT available):
# ❌ - shell: "rustfmt --check src/**/*.rs"
#      env:
#        PATH: "/custom/bin:${PATH}"

# ✓ Use shell environment syntax:
- shell: "PATH=/custom/bin:$PATH rustfmt --check src/**/*.rs"

# Instead of on_exit_code field (NOT available):
# ❌ - shell: "cargo build"
#      on_exit_code:
#        1:
#          claude: "/fix-compile-errors"

# ✓ Use on_failure with conditional logic:
- shell: "cargo build"
  on_failure:
    claude: "/fix-compile-errors ${shell.output}"
</code></pre>
<p>These planned features may be exposed in future versions of Prodigy.</p>
</details>
<hr />
<h2 id="cross-references-4"><a class="header" href="#cross-references-4">Cross-References</a></h2>
<p>For more information on related topics:</p>
<ul>
<li><strong>Variable Interpolation</strong>: See the <a href="./variables/index.html#captured-variables">Variables chapter</a> for details on using captured outputs like <code>${variable_name}</code> in subsequent commands</li>
<li><strong>Environment Variables</strong>: See the <a href="./environment.html#step-level-environment-variables">Environment Variables chapter</a> for global env, secrets, and profiles</li>
<li><strong>Error Handling</strong>: See the <a href="./error-handling.html#on-failure-handlers">Error Handling chapter</a> for advanced <code>on_failure</code> strategies and retry patterns</li>
<li><strong>MapReduce Workflows</strong>: See the <a href="./mapreduce/index.html#map-phase">MapReduce chapter</a> for large-scale parallel command execution with agent templates</li>
</ul>
<p><strong>Example: Using Captured Output in Subsequent Commands</strong></p>
<pre><code class="language-yaml"># Capture build output and use it in later commands
- shell: "cargo build --release 2&gt;&amp;1"
  capture: "build_output"
  capture_format: "string"

# Use the captured output in Claude command
- claude: "/analyze-warnings '${build_output}'"
  when: "${build_output contains 'warning'}"

# Store output to file for later analysis
- write_file:
    path: "logs/build-${workflow.iteration}.log"
    content: "${build_output}"
    create_dirs: true
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="variable-interpolation-2"><a class="header" href="#variable-interpolation-2">Variable Interpolation</a></h1>
<h2 id="overview-4"><a class="header" href="#overview-4">Overview</a></h2>
<p>Prodigy provides two complementary variable systems:</p>
<ol>
<li><strong>Built-in Variables</strong>: Automatically available based on workflow context (workflow state, step info, work items, etc.)</li>
<li><strong>Custom Captured Variables</strong>: User-defined variables created via the <code>capture:</code> field in commands</li>
</ol>
<p>Both systems use the same <code>${variable.name}</code> interpolation syntax and can be freely mixed in your workflows.</p>
<h2 id="variable-availability-by-phase"><a class="header" href="#variable-availability-by-phase">Variable Availability by Phase</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Variable Category</th><th>Setup</th><th>Map</th><th>Reduce</th><th>Merge</th></tr></thead><tbody>
<tr><td>Standard Variables</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td></tr>
<tr><td>Output Variables</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td></tr>
<tr><td>Item Variables (<code>${item.*}</code>)</td><td>✗</td><td>✓</td><td>✗</td><td>✗</td></tr>
<tr><td>Map Aggregation (<code>${map.total}</code>, etc.)</td><td>✗</td><td>✗</td><td>✓</td><td>✗</td></tr>
<tr><td>Merge Variables</td><td>✗</td><td>✗</td><td>✗</td><td>✓</td></tr>
<tr><td>Custom Captured Variables</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td></tr>
</tbody></table>
</div>
<p><strong>Note</strong>: Using phase-specific variables outside their designated phase (e.g., <code>${item}</code> in reduce phase, <code>${map.results}</code> in map phase) will result in interpolation errors or empty values. Always verify variable availability matches your workflow phase.</p>
<p><strong>Reduce Phase Access to Item Data</strong>: In reduce phase, individual item variables (<code>${item.*}</code>) are not directly available, but you can access all item data through <code>${map.results}</code> which contains the aggregated results from all map agents. This allows you to process item-level information during aggregation.</p>
<h2 id="additional-topics-2"><a class="header" href="#additional-topics-2">Additional Topics</a></h2>
<p>See also:</p>
<ul>
<li><a href="variables/available-variables.html">Available Variables</a></li>
<li><a href="variables/custom-variable-capture.html">Custom Variable Capture</a></li>
<li><a href="variables/troubleshooting-variable-interpolation.html">Troubleshooting Variable Interpolation</a></li>
<li><a href="variables/see-also.html">See Also</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="available-variables"><a class="header" href="#available-variables">Available Variables</a></h2>
<p>Prodigy provides a comprehensive set of built-in variables that are automatically available based on your workflow context. All variables use the <code>${variable.name}</code> interpolation syntax.</p>
<h3 id="standard-variables"><a class="header" href="#standard-variables">Standard Variables</a></h3>
<p>These variables capture output from the most recently executed command:</p>
<div class="table-wrapper"><table><thead><tr><th>Variable</th><th>Description</th><th>Example</th></tr></thead><tbody>
<tr><td><code>${last.output}</code></td><td>Output from the last command of any type (shell, claude, handler)</td><td><code>echo ${last.output}</code></td></tr>
<tr><td><code>${last.exit_code}</code></td><td>Exit code from the last command</td><td><code>if [ ${last.exit_code} -eq 0 ]</code></td></tr>
<tr><td><code>${shell.output}</code></td><td>Output from the last shell command specifically</td><td><code>echo ${shell.output}</code></td></tr>
<tr><td><code>${claude.output}</code></td><td>Output from the last Claude command specifically</td><td><code>echo ${claude.output}</code></td></tr>
</tbody></table>
</div>
<p><strong>Note:</strong> Use <code>${last.output}</code> when you need output from any command type. Use <code>${shell.output}</code> or <code>${claude.output}</code> when you specifically want output from that command type.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-yaml">- shell: "cargo test --lib"
- shell: "echo 'Test output: ${shell.output}'"

# last.output works with any command type
- claude: "/analyze-code"
- shell: "echo 'Claude analysis: ${last.output}'"
</code></pre>
<h3 id="computed-variables"><a class="header" href="#computed-variables">Computed Variables</a></h3>
<p>Computed variables are dynamically evaluated at runtime, providing access to external data sources and generated values. These variables are prefixed with specific identifiers that trigger their evaluation.</p>
<p><strong>Source:</strong> src/cook/execution/variables.rs:100-305</p>
<div class="table-wrapper"><table><thead><tr><th>Variable Type</th><th>Syntax</th><th>Description</th><th>Cached</th><th>Example</th></tr></thead><tbody>
<tr><td><strong>Environment</strong></td><td><code>${env.VAR_NAME}</code></td><td>Read environment variable</td><td>Yes</td><td><code>${env.HOME}</code>, <code>${env.PATH}</code></td></tr>
<tr><td><strong>File Content</strong></td><td><code>${file:path/to/file}</code></td><td>Read file contents</td><td>Yes</td><td><code>${file:config.txt}</code>, <code>${file:data.json}</code></td></tr>
<tr><td><strong>Command Output</strong></td><td><code>${cmd:shell-command}</code></td><td>Execute command and capture output</td><td>Yes</td><td><code>${cmd:git rev-parse HEAD}</code>, <code>${cmd:date +%Y}</code></td></tr>
<tr><td><strong>JSON Path</strong></td><td><code>${json:path:from:source_var}</code></td><td>Extract from JSON using JSONPath</td><td>No</td><td><code>${json:$.items[0].name:from:data}</code></td></tr>
<tr><td><strong>Date Format</strong></td><td><code>${date:format}</code></td><td>Current date/time with format</td><td>No</td><td><code>${date:%Y-%m-%d}</code>, <code>${date:%H:%M:%S}</code></td></tr>
<tr><td><strong>UUID</strong></td><td><code>${uuid}</code></td><td>Generate random UUID v4</td><td>No</td><td><code>${uuid}</code> (always unique)</td></tr>
</tbody></table>
</div>
<p><strong>Available in:</strong> All phases</p>
<h4 id="environment-variables-env"><a class="header" href="#environment-variables-env">Environment Variables (<code>env.*</code>)</a></h4>
<p>Access environment variables at runtime. Useful for reading system configuration or secrets.</p>
<p><strong>Source:</strong> src/cook/execution/variables.rs:160-187</p>
<p><strong>Examples:</strong></p>
<pre><code class="language-yaml"># Read user's home directory
- shell: "echo 'Home: ${env.HOME}'"

# Use CI environment variables
- shell: "echo 'Running in ${env.CI_PROVIDER:-local}'"

# Access secrets from environment
- shell: "curl -H 'Authorization: Bearer ${env.API_TOKEN}' https://api.example.com"
</code></pre>
<p><strong>Caching:</strong> Environment variable reads are cached for performance (LRU cache, 100 entries).</p>
<h4 id="file-content-file"><a class="header" href="#file-content-file">File Content (<code>file:</code>)</a></h4>
<p>Read file contents directly into variables. Useful for configuration, templates, or data files.</p>
<p><strong>Source:</strong> src/cook/execution/variables.rs:189-216</p>
<p><strong>Examples:</strong></p>
<pre><code class="language-yaml"># Read version from file
- shell: "echo 'Version: ${file:VERSION}'"

# Use file content in command
- shell: "git commit -m '${file:.commit-message.txt}'"

# Read JSON configuration
- shell: "echo '${file:config.json}' | jq '.database.host'"
</code></pre>
<p><strong>Caching:</strong> File reads are cached (file content is expensive to read repeatedly).</p>
<p><strong>Note:</strong> File paths are relative to workflow execution directory.</p>
<h4 id="command-output-cmd"><a class="header" href="#command-output-cmd">Command Output (<code>cmd:</code>)</a></h4>
<p>Execute shell commands and capture their output as variable values. Powerful for dynamic configuration.</p>
<p><strong>Source:</strong> src/cook/execution/variables.rs:218-256</p>
<p><strong>Examples:</strong></p>
<pre><code class="language-yaml"># Get current git commit
- shell: "echo 'Building from ${cmd:git rev-parse --short HEAD}'"

# Use command output in logic
- shell: "if [ '${cmd:uname}' = 'Darwin' ]; then echo 'macOS'; fi"

# Capture timestamp
- shell: "echo 'Build started at ${cmd:date +%Y-%m-%d_%H-%M-%S}'"

# Dynamic configuration
- shell: "cargo build --jobs ${cmd:nproc}"
</code></pre>
<p><strong>Caching:</strong> Command execution results are cached (commands are expensive to execute repeatedly).</p>
<p><strong>Security Warning:</strong> Be cautious with <code>cmd:</code> variables in untrusted workflows - they execute arbitrary shell commands.</p>
<h4 id="json-path-extraction-json"><a class="header" href="#json-path-extraction-json">JSON Path Extraction (<code>json:</code>)</a></h4>
<p>Extract values from JSON data using JSONPath syntax. Useful for processing complex JSON structures.</p>
<p><strong>Source:</strong> src/cook/execution/variables.rs:350-379</p>
<p><strong>Syntax:</strong> <code>${json:path:from:source_variable}</code></p>
<p><strong>Examples:</strong></p>
<pre><code class="language-yaml"># Extract from captured variable
- shell: "curl https://api.example.com/data"
  capture_output: "api_response"
- shell: "echo 'ID: ${json:$.id:from:api_response}'"

# Extract array element
- shell: "echo 'First item: ${json:$.items[0].name:from:api_response}'"

# Extract nested field
- shell: "echo 'Author: ${json:$.metadata.author:from:config}'"
</code></pre>
<p><strong>Not Cached:</strong> JSON path extraction is fast and not cached.</p>
<p><strong>Requires:</strong> Source variable must contain valid JSON.</p>
<h4 id="date-formatting-date"><a class="header" href="#date-formatting-date">Date Formatting (<code>date:</code>)</a></h4>
<p>Generate current date/time with custom formatting using chrono format specifiers.</p>
<p><strong>Source:</strong> src/cook/execution/variables.rs:278-305</p>
<p><strong>Syntax:</strong> <code>${date:format}</code> (uses chrono format specifiers)</p>
<p><strong>Examples:</strong></p>
<pre><code class="language-yaml"># ISO 8601 date
- shell: "echo 'Report generated: ${date:%Y-%m-%d}'"

# Full timestamp
- shell: "echo 'Build time: ${date:%Y-%m-%d %H:%M:%S}'"

# Custom format
- shell: "mkdir backup-${date:%Y%m%d-%H%M%S}"

# Use in filenames
- shell: "cp logs.txt logs-${date:%Y-%m-%d}.txt"
</code></pre>
<p><strong>Common Format Specifiers:</strong></p>
<ul>
<li><code>%Y</code> - 4-digit year (2025)</li>
<li><code>%m</code> - Month (01-12)</li>
<li><code>%d</code> - Day (01-31)</li>
<li><code>%H</code> - Hour 24h (00-23)</li>
<li><code>%M</code> - Minute (00-59)</li>
<li><code>%S</code> - Second (00-59)</li>
<li><code>%F</code> - ISO 8601 date (2025-01-15)</li>
<li><code>%T</code> - ISO 8601 time (14:30:45)</li>
</ul>
<p><strong>Not Cached:</strong> Date values change over time and are not cached.</p>
<h4 id="uuid-generation-uuid"><a class="header" href="#uuid-generation-uuid">UUID Generation (<code>uuid</code>)</a></h4>
<p>Generate a random UUID version 4. Useful for unique identifiers, temporary filenames, or correlation IDs.</p>
<p><strong>Source:</strong> src/cook/execution/variables.rs:258-276</p>
<p><strong>Examples:</strong></p>
<pre><code class="language-yaml"># Generate unique identifier
- shell: "echo 'Request ID: ${uuid}'"

# Create unique temporary file
- shell: "mkdir /tmp/build-${uuid}"

# Correlation ID for tracking
- shell: "curl -H 'X-Correlation-ID: ${uuid}' https://api.example.com"

# Unique test run ID
- shell: "cargo test -- --test-id ${uuid}"
</code></pre>
<p><strong>Not Cached:</strong> Each <code>${uuid}</code> reference generates a NEW unique UUID. If you need the same UUID multiple times, capture it first:</p>
<pre><code class="language-yaml">- shell: "echo '${uuid}'"
  capture_output: "run_id"
- shell: "echo 'Run ID: ${run_id}'"  # Same UUID
- shell: "echo 'Same ID: ${run_id}'" # Still same UUID
</code></pre>
<h4 id="computed-variable-caching"><a class="header" href="#computed-variable-caching">Computed Variable Caching</a></h4>
<p>Computed variables have different caching behaviors based on their expense and volatility:</p>
<p><strong>Cached (Expensive Operations):</strong></p>
<ul>
<li><code>env.*</code> - Environment variable reads</li>
<li><code>file:*</code> - File system operations</li>
<li><code>cmd:*</code> - Shell command execution</li>
</ul>
<p><strong>Not Cached (Fast or Volatile):</strong></p>
<ul>
<li><code>json:*</code> - JSON parsing is fast</li>
<li><code>date:*</code> - Values change over time</li>
<li><code>uuid</code> - Must be unique each time</li>
</ul>
<p><strong>Cache Details:</strong></p>
<ul>
<li><strong>Type:</strong> LRU (Least Recently Used) cache</li>
<li><strong>Size:</strong> 100 entries maximum</li>
<li><strong>Scope:</strong> Per workflow execution</li>
<li><strong>Thread Safety:</strong> Async RwLock protection</li>
</ul>
<p><strong>Source:</strong> src/cook/execution/variables.rs:218-256 (caching implementation)</p>
<h3 id="workflow-context-variables"><a class="header" href="#workflow-context-variables">Workflow Context Variables</a></h3>
<p>Variables that provide information about the current workflow execution:</p>
<div class="table-wrapper"><table><thead><tr><th>Variable</th><th>Description</th><th>Example</th></tr></thead><tbody>
<tr><td><code>${workflow.name}</code></td><td>Workflow name from YAML config</td><td><code>echo "Running ${workflow.name}"</code></td></tr>
<tr><td><code>${workflow.id}</code></td><td>Unique workflow identifier</td><td><code>log-${workflow.id}.txt</code></td></tr>
<tr><td><code>${workflow.iteration}</code></td><td>Current iteration number (for loops)</td><td><code>Iteration ${workflow.iteration}</code></td></tr>
</tbody></table>
</div>
<p><strong>Available in:</strong> All phases (setup, map, reduce, merge)</p>
<h3 id="step-context-variables"><a class="header" href="#step-context-variables">Step Context Variables</a></h3>
<p>Variables providing information about the current execution step:</p>
<div class="table-wrapper"><table><thead><tr><th>Variable</th><th>Description</th><th>Example</th></tr></thead><tbody>
<tr><td><code>${step.name}</code></td><td>Step name or identifier</td><td><code>echo "Step: ${step.name}"</code></td></tr>
<tr><td><code>${step.index}</code></td><td>Zero-based step index</td><td><code>Step ${step.index} of ${total_steps}</code></td></tr>
</tbody></table>
</div>
<p><strong>Available in:</strong> All phases</p>
<h3 id="item-variables-map-phase-only"><a class="header" href="#item-variables-map-phase-only">Item Variables (Map Phase Only)</a></h3>
<p>Variables for accessing work item data during parallel processing. The <code>${item.*}</code> syntax supports <strong>arbitrary field access</strong> - you can access any field present in your JSON work items, not just the predefined ones shown below.</p>
<p><strong>Source:</strong> src/cook/workflow/variables.rs:16-23 (item variable resolution)</p>
<div class="table-wrapper"><table><thead><tr><th>Variable</th><th>Description</th><th>Example</th></tr></thead><tbody>
<tr><td><code>${item}</code></td><td>Full item object (as string)</td><td><code>echo ${item}</code></td></tr>
<tr><td><code>${item.value}</code></td><td>Item value for simple types</td><td><code>process ${item.value}</code></td></tr>
<tr><td><code>${item.path}</code></td><td>File path (for file inputs)</td><td><code>cat ${item.path}</code></td></tr>
<tr><td><code>${item.name}</code></td><td>Item display name</td><td><code>echo "Processing ${item.name}"</code></td></tr>
<tr><td><code>${item_index}</code></td><td>Zero-based item index</td><td><code>Item ${item_index}</code></td></tr>
<tr><td><code>${item_total}</code></td><td>Total number of items</td><td><code>of ${item_total}</code></td></tr>
<tr><td><code>${item.*}</code></td><td><strong>Any JSON field</strong> - Access arbitrary fields from your work items</td><td><code>${item.priority}</code>, <code>${item.custom_field}</code></td></tr>
</tbody></table>
</div>
<p><strong>Available in:</strong> Map phase only</p>
<h4 id="arbitrary-field-access"><a class="header" href="#arbitrary-field-access">Arbitrary Field Access</a></h4>
<p>The <code>${item.*}</code> syntax provides full access to any field in your JSON work items. This includes:</p>
<ul>
<li><strong>Top-level fields:</strong> <code>${item.priority}</code>, <code>${item.status}</code>, <code>${item.category}</code></li>
<li><strong>Nested fields:</strong> <code>${item.metadata.author}</code>, <code>${item.config.database.host}</code></li>
<li><strong>Array indices:</strong> <code>${item.tags[0]}</code>, <code>${item.dependencies[2].version}</code></li>
<li><strong>Mixed access:</strong> <code>${item.data.results[0].score}</code></li>
</ul>
<p><strong>Example with custom JSON structure:</strong></p>
<pre><code class="language-yaml"># Input: items.json
# [
#   {
#     "file": "src/main.rs",
#     "priority": 10,
#     "owner": "backend-team",
#     "metadata": {
#       "last_modified": "2025-01-10",
#       "reviewer": "alice"
#     },
#     "tags": ["critical", "security"]
#   }
# ]

map:
  input: "items.json"
  json_path: "$[*]"
  agent_template:
    # Access any field from your JSON structure
    - shell: "echo 'Processing ${item.file}'"
    - shell: "echo 'Priority: ${item.priority}'"
    - shell: "echo 'Owner: ${item.owner}'"
    - shell: "echo 'Reviewer: ${item.metadata.reviewer}'"
    - shell: "echo 'First tag: ${item.tags[0]}'"
    - claude: "/analyze '${item.file}' --priority ${item.priority} --owner ${item.owner}"
</code></pre>
<p><strong>Best Practice:</strong> Use descriptive field names in your JSON work items - they become your variable names.</p>
<h3 id="mapreduce-variables-reduce-phase-only"><a class="header" href="#mapreduce-variables-reduce-phase-only">MapReduce Variables (Reduce Phase Only)</a></h3>
<p>Variables for accessing aggregated results from map phase. Map results support <strong>indexed access</strong> for retrieving individual agent results and <strong>nested field access</strong> for extracting specific properties.</p>
<p><strong>Source:</strong> src/cook/execution/mapreduce/utils.rs:119-121, src/cook/execution/mapreduce/reduce_phase.rs:146</p>
<div class="table-wrapper"><table><thead><tr><th>Variable</th><th>Description</th><th>Example</th></tr></thead><tbody>
<tr><td><code>${map.total}</code></td><td>Total items in map phase</td><td><code>echo "Processed ${map.total} items"</code></td></tr>
<tr><td><code>${map.successful}</code></td><td>Successfully processed items</td><td><code>echo "${map.successful} succeeded"</code></td></tr>
<tr><td><code>${map.failed}</code></td><td>Failed items count</td><td><code>echo "${map.failed} failed"</code></td></tr>
<tr><td><code>${map.results}</code></td><td>All map results as JSON array</td><td><code>echo '${map.results}' | jq</code></td></tr>
<tr><td><code>${map.results_json}</code></td><td>Alias for <code>map.results</code> (same value)</td><td><code>echo '${map.results_json}' | jq</code></td></tr>
<tr><td><code>${map.results[index]}</code></td><td>Individual result by index (0-based)</td><td><code>${map.results[0]}</code>, <code>${map.results[5]}</code></td></tr>
<tr><td><code>${map.results[index].field}</code></td><td>Nested field access</td><td><code>${map.results[0].output}</code>, <code>${map.results[2].item_id}</code></td></tr>
<tr><td><code>${map.key}</code></td><td>Key for map output (optional)</td><td><code>${map.key}</code></td></tr>
<tr><td><code>${worker.id}</code></td><td>Worker ID for tracking</td><td><code>Worker ${worker.id}</code></td></tr>
</tbody></table>
</div>
<p><strong>Available in:</strong> Reduce phase only</p>
<h4 id="indexed-access-to-map-results"><a class="header" href="#indexed-access-to-map-results">Indexed Access to Map Results</a></h4>
<p>You can access individual agent results using bracket notation <code>[index]</code> and drill into nested fields with dot notation.</p>
<p><strong>Syntax patterns:</strong></p>
<ul>
<li><code>${map.results[0]}</code> - First agent result (full object)</li>
<li><code>${map.results[0].output}</code> - Output from first agent</li>
<li><code>${map.results[0].item_id}</code> - Item ID processed by first agent</li>
<li><code>${map.results[0].success}</code> - Success status (“true” or “false”)</li>
</ul>
<p><strong>Example:</strong></p>
<pre><code class="language-yaml">reduce:
  # Access specific agent results
  - shell: "echo 'First result: ${map.results[0]}'"
  - shell: "echo 'First output: ${map.results[0].output}'"
  - shell: "echo 'Second agent processed: ${map.results[1].item_id}'"

  # Combine with shell commands
  - shell: |
      if [ "${map.results[0].success}" = "true" ]; then
        echo "First agent succeeded"
      fi

  # Process multiple results
  - shell: |
      echo "Results 0-2:"
      echo "${map.results[0].item_id}"
      echo "${map.results[1].item_id}"
      echo "${map.results[2].item_id}"
</code></pre>
<h4 id="full-array-processing"><a class="header" href="#full-array-processing">Full Array Processing</a></h4>
<p>For processing all results, use <code>${map.results}</code> with JSON tools like <code>jq</code>:</p>
<pre><code class="language-yaml">reduce:
  # Count errors using jq
  - shell: |
      echo '${map.results}' | jq '[.[] | select(.status == "error")] | length'
    capture_output: "error_count"

  # Extract all item IDs
  - shell: |
      echo '${map.results}' | jq -r '.[].item_id'
    capture_output: "processed_items"

  # Calculate average score
  - shell: |
      echo '${map.results}' | jq '[.[].score] | add / length'
    capture_output: "avg_score"

  # Filter successful results
  - shell: |
      echo '${map.results}' | jq '[.[] | select(.success == true)]'
    capture_output: "successful_results"

  # Generate summary
  - claude: "/summarize ${map.results} --total ${map.total} --failed ${map.failed}"
</code></pre>
<p><strong>Note:</strong> <code>${map.results}</code> and <code>${map.results_json}</code> are equivalent - use whichever is clearer in your context.</p>
<h3 id="git-context-variables"><a class="header" href="#git-context-variables">Git Context Variables</a></h3>
<p>Variables tracking git changes throughout workflow execution:</p>
<div class="table-wrapper"><table><thead><tr><th>Variable</th><th>Description</th><th>Example</th></tr></thead><tbody>
<tr><td><code>${step.files_added}</code></td><td>Files added in current step</td><td><code>echo ${step.files_added}</code></td></tr>
<tr><td><code>${step.files_modified}</code></td><td>Files modified in current step</td><td><code>echo ${step.files_modified}</code></td></tr>
<tr><td><code>${step.files_deleted}</code></td><td>Files deleted in current step</td><td><code>echo ${step.files_deleted}</code></td></tr>
<tr><td><code>${step.files_changed}</code></td><td>All changed files (added + modified + deleted)</td><td><code>echo ${step.files_changed}</code></td></tr>
<tr><td><code>${step.commits}</code></td><td>Commits in current step</td><td><code>echo ${step.commits}</code></td></tr>
<tr><td><code>${step.commit_count}</code></td><td>Number of commits in step</td><td><code>echo "${step.commit_count} commits"</code></td></tr>
<tr><td><code>${step.insertions}</code></td><td>Lines inserted in step</td><td><code>echo "+${step.insertions}"</code></td></tr>
<tr><td><code>${step.deletions}</code></td><td>Lines deleted in step</td><td><code>echo "-${step.deletions}"</code></td></tr>
<tr><td><code>${workflow.commits}</code></td><td>All commits in workflow</td><td><code>git show ${workflow.commits}</code></td></tr>
<tr><td><code>${workflow.commit_count}</code></td><td>Total number of commits</td><td><code>echo "${workflow.commit_count} commits"</code></td></tr>
</tbody></table>
</div>
<p><strong>Available in:</strong> All phases (requires git repository)</p>
<h4 id="format-modifiers"><a class="header" href="#format-modifiers">Format Modifiers</a></h4>
<p><strong>Important:</strong> These format modifiers work with <strong>all git context variables that return file or commit lists</strong>, not just the examples shown. Apply them to any of: <code>step.files_added</code>, <code>step.files_modified</code>, <code>step.files_deleted</code>, <code>step.files_changed</code>, <code>step.commits</code>, <code>workflow.commits</code>, and merge phase git variables.</p>
<p>Git context variables support multiple output formats:</p>
<div class="table-wrapper"><table><thead><tr><th>Modifier</th><th>Description</th><th>Example</th></tr></thead><tbody>
<tr><td>(default)</td><td>Space-separated list</td><td><code>${step.files_added}</code> → <code>file1.rs file2.rs</code></td></tr>
<tr><td><code>:json</code></td><td>JSON array format</td><td><code>${step.files_added:json}</code> → <code>["file1.rs", "file2.rs"]</code></td></tr>
<tr><td><code>:lines</code></td><td>Newline-separated list</td><td><code>${step.files_added:lines}</code> → <code>file1.rs\nfile2.rs</code></td></tr>
<tr><td><code>:csv</code></td><td>Comma-separated list</td><td><code>${step.files_added:csv}</code> → <code>file1.rs,file2.rs</code></td></tr>
<tr><td><code>:*.ext</code></td><td>Glob pattern filter</td><td><code>${step.files_added:*.rs}</code> → only Rust files</td></tr>
<tr><td><code>:path/**/*.ext</code></td><td>Path with glob</td><td><code>${step.files_added:src/**/*.rs}</code> → Rust files in src/</td></tr>
</tbody></table>
</div>
<p><strong>Format Examples:</strong></p>
<pre><code class="language-yaml"># JSON format for jq processing
- shell: "echo '${step.files_added:json}' | jq -r '.[]'"

# Newline format for iteration
- shell: |
    echo '${step.files_modified:lines}' | while read file; do
      cargo fmt "$file"
    done

# Glob filtering for language-specific operations
- shell: "cargo clippy ${step.files_modified:*.rs}"

# Multiple glob patterns
- shell: "git diff ${step.files_modified:*.rs,*.toml}"
</code></pre>
<h3 id="merge-variables-merge-phase-only"><a class="header" href="#merge-variables-merge-phase-only">Merge Variables (Merge Phase Only)</a></h3>
<p>Variables available during the merge phase when integrating worktree changes. Merge variables include both basic context and comprehensive git tracking information.</p>
<p><strong>Source:</strong> src/worktree/merge_orchestrator.rs:340-423</p>
<h4 id="basic-merge-context"><a class="header" href="#basic-merge-context">Basic Merge Context</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Variable</th><th>Description</th><th>Example</th></tr></thead><tbody>
<tr><td><code>${merge.worktree}</code></td><td>Worktree name being merged</td><td><code>echo ${merge.worktree}</code></td></tr>
<tr><td><code>${merge.source_branch}</code></td><td>Source branch from worktree</td><td><code>git log ${merge.source_branch}</code></td></tr>
<tr><td><code>${merge.target_branch}</code></td><td>Target branch (where you started)</td><td><code>git merge ${merge.source_branch}</code></td></tr>
<tr><td><code>${merge.session_id}</code></td><td>Session ID for correlation</td><td><code>echo ${merge.session_id}</code></td></tr>
</tbody></table>
</div>
<h4 id="merge-git-context-variables"><a class="header" href="#merge-git-context-variables">Merge Git Context Variables</a></h4>
<p>Additional variables tracking git changes during the merge operation:</p>
<div class="table-wrapper"><table><thead><tr><th>Variable</th><th>Description</th><th>Format</th><th>Example</th></tr></thead><tbody>
<tr><td><code>${merge.commits}</code></td><td>All commits from worktree</td><td>JSON array</td><td><code>echo '${merge.commits}' | jq</code></td></tr>
<tr><td><code>${merge.commit_count}</code></td><td>Number of commits</td><td>Integer</td><td><code>echo "${merge.commit_count} commits"</code></td></tr>
<tr><td><code>${merge.commit_ids}</code></td><td>Short commit IDs</td><td>Comma-separated</td><td><code>git show ${merge.commit_ids}</code></td></tr>
<tr><td><code>${merge.modified_files}</code></td><td>Modified files with metadata</td><td>JSON array</td><td><code>echo '${merge.modified_files}' | jq</code></td></tr>
<tr><td><code>${merge.file_count}</code></td><td>Number of modified files</td><td>Integer</td><td><code>echo "${merge.file_count} files"</code></td></tr>
<tr><td><code>${merge.file_list}</code></td><td>File paths</td><td>Comma-separated</td><td><code>echo ${merge.file_list}</code></td></tr>
</tbody></table>
</div>
<p><strong>Available in:</strong> Merge phase only</p>
<p><strong>Limits:</strong> Capped at 100 commits and 500 files to prevent overwhelming workflows (configurable in GitOperationsConfig).</p>
<h4 id="merge-context-examples"><a class="header" href="#merge-context-examples">Merge Context Examples</a></h4>
<p><strong>Basic merge workflow:</strong></p>
<pre><code class="language-yaml">merge:
  commands:
    - shell: "git fetch origin"
    - shell: "git merge origin/${merge.target_branch}"
    - claude: "/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}"
</code></pre>
<p><strong>Using git context variables:</strong></p>
<pre><code class="language-yaml">merge:
  commands:
    # Show merge summary
    - shell: |
        echo "Merging worktree: ${merge.worktree}"
        echo "Commits: ${merge.commit_count}"
        echo "Files modified: ${merge.file_count}"

    # List all commits being merged
    - shell: "echo 'Commit IDs: ${merge.commit_ids}'"

    # Process commits as JSON
    - shell: |
        echo '${merge.commits}' | jq -r '.[] | "\(.short_id): \(.message)"'

    # Check specific files
    - shell: |
        echo '${merge.modified_files}' | jq -r '.[].path'

    # Conditional merge based on file count
    - shell: |
        if [ ${merge.file_count} -gt 50 ]; then
          echo "Large merge detected, requesting review"
        fi

    # Perform merge
    - claude: "/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}"
</code></pre>
<h4 id="commit-object-structure"><a class="header" href="#commit-object-structure">Commit Object Structure</a></h4>
<p>The <code>${merge.commits}</code> variable contains an array of commit objects with this structure:</p>
<pre><code class="language-json">[
  {
    "id": "full-sha-hash",
    "short_id": "abc1234",
    "author": {
      "name": "Author Name",
      "email": "author@example.com"
    },
    "message": "Commit message",
    "timestamp": "2025-01-10T12:00:00Z",
    "files_changed": ["file1.rs", "file2.rs"]
  }
]
</code></pre>
<p><strong>Source:</strong> src/cook/execution/mapreduce/resources/git_operations.rs:280-293</p>
<h4 id="file-object-structure"><a class="header" href="#file-object-structure">File Object Structure</a></h4>
<p>The <code>${merge.modified_files}</code> variable contains an array of file modification objects:</p>
<pre><code class="language-json">[
  {
    "path": "src/main.rs",
    "modification_type": "Modified",
    "size_before": 1024,
    "size_after": 1156,
    "last_modified": "2025-01-10T12:00:00Z",
    "commit_id": "abc1234"
  }
]
</code></pre>
<p><strong>Source:</strong> src/cook/execution/mapreduce/resources/git_operations.rs:311-322</p>
<h3 id="validation-variables"><a class="header" href="#validation-variables">Validation Variables</a></h3>
<p>Variables for workflow validation and completion tracking:</p>
<div class="table-wrapper"><table><thead><tr><th>Variable</th><th>Description</th><th>Example</th></tr></thead><tbody>
<tr><td><code>${validation.completion}</code></td><td>Completion percentage (0-100)</td><td><code>echo "${validation.completion}%"</code></td></tr>
<tr><td><code>${validation.gaps}</code></td><td>Array of missing requirements</td><td><code>echo '${validation.gaps}'</code></td></tr>
<tr><td><code>${validation.status}</code></td><td>Status: complete/incomplete/failed</td><td><code>if [ "${validation.status}" = "complete" ]</code></td></tr>
</tbody></table>
</div>
<p><strong>Available in:</strong> Goal seek and validation phases</p>
<h3 id="variable-interpolation-syntax"><a class="header" href="#variable-interpolation-syntax">Variable Interpolation Syntax</a></h3>
<p>Prodigy supports two interpolation syntaxes:</p>
<ul>
<li><strong><code>${VAR}</code></strong> - Preferred syntax, works in all contexts (recommended)</li>
<li><strong><code>$VAR</code></strong> - Shell-style syntax, simpler but may have limitations</li>
</ul>
<p><strong>When to use <code>${VAR}</code>:</strong></p>
<ul>
<li>In YAML values with special characters</li>
<li>For nested field access: <code>${item.nested.field}</code></li>
<li>When combining with text: <code>prefix_${var}_suffix</code></li>
<li>For format modifiers: <code>${step.files:json}</code></li>
</ul>
<p><strong>When <code>$VAR</code> works:</strong></p>
<ul>
<li>Simple variable names in shell commands</li>
<li>Environment variables in shell context</li>
<li>Quick substitutions without special characters</li>
</ul>
<p><strong>Best Practice:</strong> Always use <code>${VAR}</code> syntax for consistency and reliability.</p>
<h3 id="legacy-variable-aliases"><a class="header" href="#legacy-variable-aliases">Legacy Variable Aliases</a></h3>
<p>For backward compatibility, Prodigy supports legacy variable aliases from earlier versions. These are still functional but <strong>deprecated</strong> - prefer the current variable names in new workflows.</p>
<p><strong>Source:</strong> src/cook/workflow/variables.rs (legacy alias definitions)</p>
<div class="table-wrapper"><table><thead><tr><th>Legacy Alias</th><th>Current Variable</th><th>Context</th><th>Status</th></tr></thead><tbody>
<tr><td><code>${ARG}</code></td><td><code>${item.value}</code></td><td>Map phase</td><td>Deprecated</td></tr>
<tr><td><code>${ARGUMENT}</code></td><td><code>${item.value}</code></td><td>Map phase</td><td>Deprecated</td></tr>
<tr><td><code>${FILE}</code></td><td><code>${item.path}</code></td><td>Map phase</td><td>Deprecated</td></tr>
<tr><td><code>${FILE_PATH}</code></td><td><code>${item.path}</code></td><td>Map phase</td><td>Deprecated</td></tr>
</tbody></table>
</div>
<p><strong>Example:</strong></p>
<pre><code class="language-yaml"># Old style (still works but discouraged)
map:
  agent_template:
    - shell: "process ${ARG}"
    - shell: "cat ${FILE}"

# New style (recommended)
map:
  agent_template:
    - shell: "process ${item.value}"
    - shell: "cat ${item.path}"
</code></pre>
<p><strong>Migration Recommendation:</strong> Update legacy aliases to current variable names when maintaining older workflows. The current names are more explicit and work better with arbitrary JSON field access.</p>
<h4 id="default-values"><a class="header" href="#default-values">Default Values</a></h4>
<p>Provide fallback values for undefined or missing variables using the <code>:-</code> syntax (bash/shell convention):</p>
<p><strong>Syntax:</strong> <code>${variable:-default_value}</code></p>
<p><strong>Source:</strong> src/cook/execution/interpolation.rs:277</p>
<p><strong>Examples:</strong></p>
<pre><code class="language-yaml"># Use default if variable is undefined
- shell: "echo 'Timeout: ${timeout:-600}'"
  # Output: "Timeout: 600" if timeout is not defined

# Fallback for optional configuration
- shell: "cargo build --profile ${build_profile:-dev}"
  # Uses "dev" profile if build_profile not set

# Default for MapReduce variables
- shell: "echo 'Processed ${map.successful:-0} items'"
  # Shows "0" if map.successful is not available
</code></pre>
<p><strong>Behavior with Interpolation Modes:</strong></p>
<ul>
<li><strong>Non-strict mode (default):</strong> Uses default value if variable is undefined</li>
<li><strong>Strict mode:</strong> Default value syntax prevents errors for optional variables</li>
</ul>
<h4 id="interpolation-modes"><a class="header" href="#interpolation-modes">Interpolation Modes</a></h4>
<p>Prodigy supports two modes for handling undefined variables:</p>
<p><strong>Non-strict Mode (Default):</strong></p>
<ul>
<li>Leaves placeholders unresolved when variable is undefined</li>
<li>Example: <code>${undefined}</code> remains as <code>${undefined}</code> in output</li>
<li>With default: <code>${undefined:-fallback}</code> becomes <code>fallback</code></li>
<li>Use case: Workflows that can handle partial variable resolution</li>
</ul>
<p><strong>Strict Mode:</strong></p>
<ul>
<li>Fails immediately on undefined variables</li>
<li>Example: <code>${undefined}</code> causes workflow to fail with comprehensive error</li>
<li>Error message lists all available variables for debugging</li>
<li>Use case: Production workflows requiring all variables to be properly defined</li>
</ul>
<p><strong>Source:</strong> src/cook/execution/interpolation.rs:16-17, 104-137</p>
<p><strong>Configuration:</strong>
Strict mode is configured per InterpolationEngine instance and controlled at the workflow execution level.</p>
<p><strong>Best Practice:</strong> Use strict mode during development to catch variable name typos and scope issues early. Use default values (<code>${var:-default}</code>) for truly optional configuration.</p>
<p><strong>Examples:</strong></p>
<pre><code class="language-yaml"># Non-strict mode (graceful degradation)
- shell: "echo 'Config: ${optional_config:-none}'"
  # Works even if optional_config is undefined

# Strict mode (fail fast)
# If required_var is undefined, workflow stops with error:
# "Variable interpolation failed: required_var not found.
#  Available variables: workflow.name, workflow.id, step.index, ..."
- shell: "echo 'Required: ${required_var}'"
</code></pre>
<h3 id="variable-scoping-and-precedence"><a class="header" href="#variable-scoping-and-precedence">Variable Scoping and Precedence</a></h3>
<h4 id="scope-by-phase"><a class="header" href="#scope-by-phase">Scope by Phase</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Phase</th><th>Variables Available</th></tr></thead><tbody>
<tr><td>Setup</td><td>Standard, workflow context, step context, git context, custom captured</td></tr>
<tr><td>Map</td><td>Standard, workflow context, step context, git context, item variables, custom captured</td></tr>
<tr><td>Reduce</td><td>Standard, workflow context, step context, git context, MapReduce variables, custom captured</td></tr>
<tr><td>Merge</td><td>Standard, workflow context, step context, merge variables, custom captured</td></tr>
</tbody></table>
</div>
<p><strong>Important:</strong> Setup phase captures are available in map and reduce phases. Map phase captures are only available within that specific agent. Reduce phase captures are available to subsequent reduce steps.</p>
<h4 id="variable-precedence-highest-to-lowest"><a class="header" href="#variable-precedence-highest-to-lowest">Variable Precedence (highest to lowest)</a></h4>
<ol>
<li><strong>Custom captured variables</strong> (<code>capture_output</code>)</li>
<li><strong>Phase-specific built-in variables</strong> (<code>item.*</code>, <code>map.*</code>, <code>merge.*</code>)</li>
<li><strong>Step context variables</strong> (<code>step.*</code>)</li>
<li><strong>Workflow context variables</strong> (<code>workflow.*</code>)</li>
<li><strong>Standard output variables</strong> (<code>last.output</code>, <code>shell.output</code>)</li>
<li><strong>Environment variables</strong> (static workflow-level <code>env</code> block)</li>
<li><strong>Computed variables</strong> (<code>env.*</code>, <code>file:*</code>, <code>cmd:*</code>, <code>json:*</code>, <code>date:*</code>, <code>uuid</code>)</li>
</ol>
<p><strong>Note:</strong> Computed variables have lowest precedence because they’re evaluated on-demand. If a custom variable has the same name as a computed variable, the custom variable wins.</p>
<p><strong>Shadowing Warning:</strong> Custom captures can shadow built-in variable names. Avoid using names like <code>item</code>, <code>map</code>, <code>workflow</code>, etc. as custom variable names.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-yaml"># Bad: shadows built-in ${item}
- shell: "custom command"
  capture_output: "item"  # Don't do this!

# Good: descriptive custom name
- shell: "custom command"
  capture_output: "custom_result"
</code></pre>
<h4 id="parent-context-resolution"><a class="header" href="#parent-context-resolution">Parent Context Resolution</a></h4>
<p>Variable resolution walks up a parent context chain when variables are not found in the current context. This enables variable inheritance across workflow phases and nested contexts.</p>
<p><strong>Source:</strong> src/cook/execution/interpolation.rs:200-226, InterpolationContext struct at :376-381</p>
<p><strong>Resolution Order:</strong></p>
<ol>
<li>Check current context</li>
<li>If not found, check parent context</li>
<li>If not found in parent, check parent’s parent</li>
<li>Continue until variable is found or no parent exists</li>
<li>If not found and has default value, use default</li>
<li>If not found in strict mode, fail with error listing available variables</li>
</ol>
<p><strong>Benefits:</strong></p>
<ul>
<li>Nested workflow contexts inherit variables from parent workflows</li>
<li>Foreach loops access both loop-level and workflow-level variables</li>
<li>Map agents access setup phase variables</li>
<li>Reduce phase accesses both map results and setup variables</li>
</ul>
<p><strong>Example:</strong></p>
<pre><code class="language-yaml">setup:
  - shell: "pwd"
    capture_output: "workspace_root"  # Available to all agents via parent context
  - shell: "git rev-parse HEAD"
    capture_output: "base_commit"     # Also inherited by map agents

map:
  input: "items.json"
  json_path: "$.items[*]"
  agent_template:
    - shell: "echo 'processing ${item.name}'"
      capture_output: "item_status"  # Only in this agent's context
    - shell: "cd ${workspace_root}"  # Resolved from parent (setup) context
    - shell: "git diff ${base_commit}" # Also from parent context
    - shell: "echo 'Status: ${item_status}'" # From current agent context

reduce:
  # Can access setup variables but NOT individual agent's item_status
  - shell: "cd ${workspace_root}"  # From setup phase parent context
  - shell: "echo 'Base: ${base_commit}'"  # Also from setup phase
</code></pre>
<p><strong>Context Hierarchy:</strong></p>
<pre><code>Setup Context (workspace_root, base_commit)
    ↓ parent
Map Agent Context (item, item_status, workspace_root*, base_commit*)
    ↓ parent
Reduce Context (map.results, workspace_root*, base_commit*)
</code></pre>
<p>*Inherited from parent context</p>
<h3 id="reduce-phase-access-to-item-data"><a class="header" href="#reduce-phase-access-to-item-data">Reduce Phase Access to Item Data</a></h3>
<p>In the reduce phase, individual item variables (<code>${item.*}</code>) are not directly available, but you can access all item data through <code>${map.results}</code>, which contains the aggregated results from all map agents.</p>
<p><strong>Examples:</strong></p>
<pre><code class="language-yaml">reduce:
  # Count items with specific property
  - shell: |
      echo '${map.results}' | jq '[.[] | select(.type == "error")] | length'
    capture_output: "error_count"

  # Extract all file paths processed
  - shell: |
      echo '${map.results}' | jq -r '.[].item.path'
    capture_output: "all_paths"

  # Aggregate numeric field
  - shell: |
      echo '${map.results}' | jq '[.[].coverage] | add / length'
    capture_output: "avg_coverage"

  # Filter and transform results
  - shell: |
      echo '${map.results}' | jq '[.[] | select(.item.priority &gt; 5) | .item.name]'
    capture_output: "high_priority_items"
</code></pre>
<h3 id="performance-template-caching"><a class="header" href="#performance-template-caching">Performance: Template Caching</a></h3>
<p>Prodigy implements <strong>dual caching</strong> for optimal performance: template parsing cache and operation result cache.</p>
<p><strong>Source:</strong> src/cook/execution/interpolation.rs:18-19, 68-75; src/cook/execution/variables.rs:218-256</p>
<h4 id="template-parse-caching"><a class="header" href="#template-parse-caching">Template Parse Caching</a></h4>
<p>When the same variable template is used multiple times, the template is parsed once and reused:</p>
<p><strong>How It Works:</strong></p>
<ul>
<li>First use: Template is parsed and cached</li>
<li>Subsequent uses: Cached template is reused (no re-parsing)</li>
<li>Cache key: Exact template string</li>
<li>Automatic: No configuration needed</li>
</ul>
<p><strong>Example:</strong></p>
<pre><code class="language-yaml"># Template "${item.path} --priority ${item.metadata.priority:-5}"
# is parsed once, then reused for all 1000 items
map:
  input: "items.json"  # 1000 items
  json_path: "$.items[*]"
  agent_template:
    - shell: "process ${item.path} --priority ${item.metadata.priority:-5}"
</code></pre>
<h4 id="computed-variable-caching-1"><a class="header" href="#computed-variable-caching-1">Computed Variable Caching</a></h4>
<p>Expensive computed operations (file reads, command execution) have separate result caching:</p>
<p><strong>Cached Operations:</strong></p>
<ul>
<li><code>${env.VAR}</code> - Environment variable lookups</li>
<li><code>${file:path}</code> - File system reads</li>
<li><code>${cmd:command}</code> - Shell command execution</li>
</ul>
<p><strong>Not Cached:</strong></p>
<ul>
<li><code>${json:path:from:var}</code> - JSON parsing is fast</li>
<li><code>${date:format}</code> - Values change over time</li>
<li><code>${uuid}</code> - Must be unique</li>
</ul>
<p><strong>Cache Details:</strong></p>
<ul>
<li><strong>Type:</strong> LRU (Least Recently Used) cache</li>
<li><strong>Size:</strong> 100 entries maximum</li>
<li><strong>Scope:</strong> Per workflow execution</li>
<li><strong>Thread Safety:</strong> Async RwLock protection</li>
</ul>
<p><strong>Performance Impact:</strong></p>
<pre><code class="language-yaml"># First shell command: Reads .commit-message.txt from disk
- shell: "git commit -m '${file:.commit-message.txt}'"

# Second shell command: Uses cached file content (no disk read)
- shell: "echo 'Message: ${file:.commit-message.txt}'"

# Third shell command: Still uses cache
- shell: "test -n '${file:.commit-message.txt}'"
</code></pre>
<p><strong>Benefits:</strong></p>
<ul>
<li><strong>Faster interpolation</strong> for repeated templates (template cache)</li>
<li><strong>Reduced I/O</strong> for repeated file reads (operation cache)</li>
<li><strong>Lower CPU</strong> for repeated command execution (operation cache)</li>
<li><strong>Reduced latency</strong> in MapReduce workflows</li>
</ul>
<p><strong>When It Matters Most:</strong></p>
<ul>
<li>MapReduce workflows with many work items (&gt;100)</li>
<li>Workflows using the same computed variables repeatedly</li>
<li>High-frequency variable interpolation in loops</li>
<li>Templates with multiple variables and nested field access</li>
</ul>
<p><strong>Note:</strong> All caching is transparent and automatic. You don’t need any configuration to benefit from it. Both caches persist for the lifetime of the workflow execution.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="custom-variable-capture"><a class="header" href="#custom-variable-capture">Custom Variable Capture</a></h2>
<p>Custom capture variables allow you to save command output with explicit names for later use. This provides fine-grained control over what data is captured, how it’s formatted, and when it’s available to subsequent steps.</p>
<h3 id="basic-syntax"><a class="header" href="#basic-syntax">Basic Syntax</a></h3>
<p>The <code>capture_output</code> field can be used with any command type (shell, claude, handler):</p>
<pre><code class="language-yaml"># Option 1: No capture (default)
- shell: "cargo test"
  capture_output: false

# Option 2: Boolean - use default variable name
- shell: "cargo test"
  capture_output: true  # Creates ${shell.output}

# Option 3: Custom variable name (recommended)
- shell: "ls -la | wc -l"
  capture_output: "file_count"  # Creates ${file_count}
</code></pre>
<h3 id="default-variable-names"><a class="header" href="#default-variable-names">Default Variable Names</a></h3>
<p>When using <code>capture_output: true</code>, the variable name depends on the command type:</p>
<div class="table-wrapper"><table><thead><tr><th>Command Type</th><th>Default Variable</th><th>Example</th></tr></thead><tbody>
<tr><td><code>shell:</code></td><td><code>${shell.output}</code></td><td><code>echo ${shell.output}</code></td></tr>
<tr><td><code>claude:</code></td><td><code>${claude.output}</code></td><td><code>echo ${claude.output}</code></td></tr>
<tr><td><code>handler:</code></td><td><code>${handler.output}</code></td><td><code>echo ${handler.output}</code></td></tr>
</tbody></table>
</div>
<p><strong>Recommendation:</strong> Use custom variable names for clarity and to avoid overwriting previous captures.</p>
<h3 id="capture-formats"><a class="header" href="#capture-formats">Capture Formats</a></h3>
<p>Control how output is parsed and structured using <code>capture_format</code>:</p>
<pre><code class="language-yaml"># String format (default) - raw output
- shell: "cat readme.txt"
  capture_output: "readme"
  capture_format: "string"  # default, can be omitted

# JSON format - parse as JSON, access nested fields
- shell: "cargo metadata --format-version 1"
  capture_output: "metadata"
  capture_format: "json"
- shell: "echo 'Package: ${metadata.packages[0].name}'"

# Lines format - split into array
- shell: "git log --oneline -5"
  capture_output: "recent_commits"
  capture_format: "lines"

# Number format - parse as numeric
- shell: "cargo test 2&gt;&amp;1 | grep -c 'test result: ok'"
  capture_output: "passed_tests"
  capture_format: "number"

# Boolean format - parse as boolean or use exit status
- shell: "cargo clippy -- -D warnings"
  capture_output: "clippy_clean"
  capture_format: "boolean"
</code></pre>
<h3 id="metadata-fields"><a class="header" href="#metadata-fields">Metadata Fields</a></h3>
<p>Captured variables automatically include metadata fields:</p>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Description</th><th>Example</th></tr></thead><tbody>
<tr><td><code>${var}</code></td><td>Primary output value</td><td><code>echo ${file_count}</code></td></tr>
<tr><td><code>${var.stderr}</code></td><td>Standard error output</td><td><code>echo ${build.stderr}</code></td></tr>
<tr><td><code>${var.exit_code}</code></td><td>Command exit code</td><td><code>if [ ${build.exit_code} -eq 0 ]</code></td></tr>
<tr><td><code>${var.success}</code></td><td>Boolean success status</td><td><code>if ${build.success}</code></td></tr>
<tr><td><code>${var.duration}</code></td><td>Execution time in seconds</td><td><code>echo "Took ${build.duration}s"</code></td></tr>
</tbody></table>
</div>
<p><strong>Example:</strong></p>
<pre><code class="language-yaml">- shell: "cargo build --release"
  capture_output: "build"
- shell: |
    if ${build.success}; then
      echo "Build succeeded in ${build.duration}s"
    else
      echo "Build failed: ${build.stderr}"
      exit ${build.exit_code}
    fi
</code></pre>
<h3 id="capture-streams"><a class="header" href="#capture-streams">Capture Streams</a></h3>
<p>Customize which streams and metadata are captured using <code>capture_streams</code>:</p>
<pre><code class="language-yaml">- shell: "cargo test 2&gt;&amp;1"
  capture_output: "test_output"
  capture_streams:
    stdout: true      # default: true
    stderr: true      # default: false (include stderr in output)
    exit_code: true   # default: true
    success: true     # default: true
    duration: true    # default: true
</code></pre>
<p><strong>Default Behavior Explained:</strong></p>
<p><strong>Source:</strong> src/cook/workflow/variables.rs:269-291</p>
<ul>
<li><code>stdout: true</code> - Command output is captured (default)</li>
<li><code>stderr: false</code> - Error output is NOT captured by default (prevents noise from warnings)</li>
<li><code>exit_code: true</code> - Exit code is always captured</li>
<li><code>success: true</code> - Boolean success status (exit_code == 0) is always captured</li>
<li><code>duration: true</code> - Execution duration is always captured</li>
</ul>
<p><strong>When stderr is false (default):</strong></p>
<ul>
<li>Error output is discarded</li>
<li>Only stdout is captured in the variable</li>
<li>Use this when stderr contains progress bars, warnings, or noise</li>
</ul>
<p><strong>When stderr is true:</strong></p>
<ul>
<li>Both stdout and stderr are combined in the captured output</li>
<li>Useful for debugging failures or when errors contain useful information</li>
</ul>
<p><strong>Common Patterns:</strong></p>
<pre><code class="language-yaml"># Capture both stdout and stderr for debugging
- shell: "cargo build 2&gt;&amp;1"
  capture_output: "build_output"
  capture_streams:
    stdout: true
    stderr: true  # Override default to capture errors

# Only capture exit code for validation (no output)
- shell: "test -f config.toml"
  capture_output: "config_exists"
  capture_streams:
    stdout: false  # Don't capture output
    stderr: false
    exit_code: true  # Just check if it succeeded
    success: true    # Get boolean result

# Capture output but measure performance
- shell: "cargo test --lib"
  capture_output: "test_results"
  capture_streams:
    stdout: true
    stderr: false
    duration: true  # Track how long tests took
</code></pre>
<h3 id="variable-scoping"><a class="header" href="#variable-scoping">Variable Scoping</a></h3>
<p>Captured variables have different scopes depending on the workflow phase:</p>
<div class="table-wrapper"><table><thead><tr><th>Phase</th><th>Scope</th><th>Available To</th></tr></thead><tbody>
<tr><td>Setup</td><td>Workflow-wide</td><td>All map agents and reduce steps</td></tr>
<tr><td>Map</td><td>Agent-local</td><td>Only that specific agent’s steps</td></tr>
<tr><td>Reduce</td><td>Step-forward</td><td>Current and subsequent reduce steps</td></tr>
<tr><td>Merge</td><td>Merge-local</td><td>Only merge phase commands</td></tr>
</tbody></table>
</div>
<p><strong>Important:</strong> Setup phase captures are the only way to share data across all map agents.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-yaml">setup:
  - shell: "cargo metadata --format-version 1"
    capture_output: "cargo_metadata"
    capture_format: "json"
  - shell: "echo '${cargo_metadata.workspace_root}'"
    capture_output: "workspace_root"

map:
  input: "packages.json"
  agent_template:
    # ${workspace_root} and ${cargo_metadata} available here
    - shell: "cd ${workspace_root} &amp;&amp; cargo test -p ${item.name}"
      capture_output: "test_result"  # only available in this agent
</code></pre>
<h3 id="json-field-access"><a class="header" href="#json-field-access">JSON Field Access</a></h3>
<p>When using <code>capture_format: "json"</code>, access nested fields with dot notation:</p>
<pre><code class="language-yaml">- shell: "cargo metadata --format-version 1"
  capture_output: "metadata"
  capture_format: "json"

# Access nested fields
- shell: "echo 'Workspace: ${metadata.workspace_root}'"
- shell: "echo 'Target: ${metadata.target_directory}'"
- shell: "echo 'First package: ${metadata.packages[0].name}'"
- shell: "echo 'Version: ${metadata.packages[0].version}'"
</code></pre>
<p><strong>Tip:</strong> Use <code>jq</code> to explore JSON structure first:</p>
<pre><code class="language-yaml">- shell: "cargo metadata --format-version 1 | jq -r '.packages[0] | keys'"
</code></pre>
<h3 id="complete-examples"><a class="header" href="#complete-examples">Complete Examples</a></h3>
<h4 id="example-1-setup-phase-variables"><a class="header" href="#example-1-setup-phase-variables">Example 1: Setup Phase Variables</a></h4>
<pre><code class="language-yaml">name: analyze-codebase
mode: mapreduce

setup:
  # Capture workspace info for all agents
  - shell: "cargo metadata --format-version 1"
    capture_output: "metadata"
    capture_format: "json"

  - shell: "find . -name '*.rs' | wc -l"
    capture_output: "total_rust_files"
    capture_format: "number"

  - shell: "git rev-parse --abbrev-ref HEAD"
    capture_output: "current_branch"

map:
  input: "packages.json"
  agent_template:
    # All setup variables available here
    - shell: "echo 'Processing ${item.name} on ${current_branch}'"
    - shell: "echo 'Workspace has ${total_rust_files} Rust files'"
</code></pre>
<h4 id="example-2-map-phase-local-variables"><a class="header" href="#example-2-map-phase-local-variables">Example 2: Map Phase Local Variables</a></h4>
<pre><code class="language-yaml">map:
  input: "tests.json"
  agent_template:
    - shell: "cargo test ${item.test_name}"
      capture_output: "test_result"
      capture_streams:
        stdout: true
        stderr: true

    - shell: |
        if ${test_result.success}; then
          echo "PASS: ${item.test_name}"
        else
          echo "FAIL: ${item.test_name}"
          echo "${test_result.stderr}"
        fi
</code></pre>
<h4 id="example-3-reduce-phase-aggregation"><a class="header" href="#example-3-reduce-phase-aggregation">Example 3: Reduce Phase Aggregation</a></h4>
<pre><code class="language-yaml">reduce:
  # Extract error count
  - shell: "echo '${map.results}' | jq '[.[] | select(.status == \"failed\")] | length'"
    capture_output: "failure_count"
    capture_format: "number"

  # Extract failed test names
  - shell: "echo '${map.results}' | jq -r '[.[] | select(.status == \"failed\") | .item.test_name] | join(\", \")'"
    capture_output: "failed_tests"

  # Generate summary
  - shell: |
      echo "Total: ${map.total}"
      echo "Passed: ${map.successful}"
      echo "Failed: ${failure_count}"
      if [ ${failure_count} -gt 0 ]; then
        echo "Failed tests: ${failed_tests}"
      fi
</code></pre>
<h4 id="example-4-conditional-execution-based-on-captures"><a class="header" href="#example-4-conditional-execution-based-on-captures">Example 4: Conditional Execution Based on Captures</a></h4>
<pre><code class="language-yaml">- shell: "cargo clippy -- -D warnings 2&gt;&amp;1"
  capture_output: "clippy"
  capture_format: "boolean"

- shell: "cargo fmt --check"
  capture_output: "fmt"
  capture_format: "boolean"

- shell: |
    if ${clippy.success} &amp;&amp; ${fmt.success}; then
      echo "All checks passed!"
      exit 0
    else
      echo "Checks failed:"
      [ ! ${clippy.success} ] &amp;&amp; echo "  - clippy: ${clippy.stderr}"
      [ ! ${fmt.success} ] &amp;&amp; echo "  - fmt: ${fmt.stderr}"
      exit 1
    fi
</code></pre>
<h3 id="best-practices-8"><a class="header" href="#best-practices-8">Best Practices</a></h3>
<ol>
<li><strong>Use descriptive variable names</strong>: <code>test_results</code> instead of <code>output</code></li>
<li><strong>Specify capture_format</strong>: Make JSON parsing explicit</li>
<li><strong>Capture in setup for sharing</strong>: Setup variables available to all agents</li>
<li><strong>Use metadata fields</strong>: Check <code>.success</code> before using output</li>
<li><strong>Document expected structure</strong>: Comment complex JSON captures</li>
<li><strong>Avoid shadowing built-ins</strong>: Don’t name variables <code>item</code>, <code>map</code>, or <code>workflow</code></li>
<li><strong>Clean up large captures</strong>: Don’t keep unnecessary data in scope</li>
</ol>
<h4 id="choosing-between-capture_format-json-and-manual-parsing"><a class="header" href="#choosing-between-capture_format-json-and-manual-parsing">Choosing Between capture_format: json and Manual Parsing</a></h4>
<p><strong>Use capture_format: json when:</strong></p>
<ul>
<li>You need to access nested fields directly: <code>${metadata.workspace.root}</code></li>
<li>The JSON structure is stable and well-defined</li>
<li>You want simple field access without complex transformations</li>
<li>The command always outputs valid JSON</li>
</ul>
<p><strong>Example:</strong></p>
<pre><code class="language-yaml">- shell: "cargo metadata --format-version 1"
  capture_output: "metadata"
  capture_format: "json"

# Access fields directly
- shell: "cd ${metadata.workspace_root}"
- shell: "echo 'Package: ${metadata.packages[0].name}'"
</code></pre>
<p><strong>Use manual jq parsing when:</strong></p>
<ul>
<li>You need complex filtering or transformations</li>
<li>You want to extract only specific fields (reduce memory)</li>
<li>The JSON structure varies or is deeply nested</li>
<li>You need to combine or reshape data</li>
<li>You want error handling for invalid JSON</li>
</ul>
<p><strong>Example:</strong></p>
<pre><code class="language-yaml"># Complex transformation with jq
- shell: |
    cargo test --format json 2&gt;&amp;1 | \
      jq -s '[.[] | select(.type == "test")] |
             group_by(.event) |
             map({event: .[0].event, count: length})'
  capture_output: "test_summary"
  capture_format: "json"

# Filter to specific data only
- shell: |
    cargo metadata --format-version 1 | \
      jq '.packages[] | select(.name == "prodigy") | {name, version, edition}'
  capture_output: "prodigy_info"
  capture_format: "json"
</code></pre>
<p><strong>Combined Approach:</strong></p>
<pre><code class="language-yaml"># Capture full JSON for direct field access
- shell: "cargo metadata --format-version 1"
  capture_output: "metadata"
  capture_format: "json"

# Access simple fields directly (fast, no jq needed)
- shell: "echo 'Workspace: ${metadata.workspace_root}'"

# Use jq for complex operations on the same data
- shell: |
    echo '${metadata}' | jq '.packages[] | select(.name == "prodigy")'
  capture_output: "prodigy_package"
  capture_format: "json"
</code></pre>
<p><strong>Performance Considerations:</strong></p>
<ul>
<li><strong>capture_format: json</strong> parses once, enables direct field access</li>
<li><strong>Manual jq</strong> can filter early to reduce memory for large JSON</li>
<li><strong>Combined approach</strong> gives both direct access and complex queries</li>
</ul>
<p><strong>Source:</strong> Examples from book/src/variables/custom-variable-capture.md:143-161, 289-297</p>
<h3 id="common-patterns"><a class="header" href="#common-patterns">Common Patterns</a></h3>
<p><strong>Pattern 1: Conditional Claude Command</strong></p>
<pre><code class="language-yaml">- shell: "cargo test 2&gt;&amp;1"
  capture_output: "tests"

- claude: "/fix-failing-tests '${tests}'"
  condition: "! ${tests.success}"
</code></pre>
<p><strong>Pattern 2: Counting and Statistics</strong></p>
<pre><code class="language-yaml">- shell: "find . -name '*.rs' | wc -l"
  capture_output: "rust_files"
  capture_format: "number"

- shell: "echo 'Found ${rust_files} Rust files'"
</code></pre>
<p><strong>Pattern 3: Passing Structured Data</strong></p>
<pre><code class="language-yaml">- shell: "cargo metadata --format-version 1"
  capture_output: "metadata"
  capture_format: "json"

- claude: "/analyze-dependencies '${metadata.packages:json}'"
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="troubleshooting-variable-interpolation"><a class="header" href="#troubleshooting-variable-interpolation">Troubleshooting Variable Interpolation</a></h2>
<p>This guide helps you diagnose and fix common variable interpolation issues in Prodigy workflows.</p>
<h3 id="issue-variables-not-interpolating"><a class="header" href="#issue-variables-not-interpolating">Issue: Variables Not Interpolating</a></h3>
<p><strong>Symptom:</strong> Literal <code>${var}</code> or <code>$VAR</code> appears in output instead of the value.</p>
<p><strong>Common Causes:</strong></p>
<ol>
<li>
<p><strong>Variable name typo or case mismatch</strong></p>
<pre><code class="language-yaml"># Wrong
- shell: "echo ${Item.path}"  # Should be lowercase: ${item.path}

# Correct
- shell: "echo ${item.path}"
</code></pre>
</li>
<li>
<p><strong>Variable doesn’t exist in current scope</strong></p>
<pre><code class="language-yaml"># Wrong - using item variable in reduce phase
reduce:
  - shell: "echo ${item.name}"  # ${item.*} not available here!

# Correct - access via map.results
reduce:
  - shell: "echo '${map.results}' | jq -r '.[].item.name'"
</code></pre>
</li>
<li>
<p><strong>Command failed before setting variable</strong></p>
<pre><code class="language-yaml">- shell: "cargo test"  # If this fails...
  capture_output: "tests"
- shell: "echo ${tests}"  # ...this might be empty

# Better: Check success first
- shell: |
    if ${tests.success}; then
      echo "Tests: ${tests}"
    else
      echo "Tests failed: ${tests.stderr}"
    fi
</code></pre>
</li>
</ol>
<p><strong>Solutions:</strong></p>
<ul>
<li>Check spelling and case sensitivity</li>
<li>Verify variable exists in current phase (see phase availability table)</li>
<li>Use verbose mode (<code>-v</code>) to see variable values during execution</li>
<li>Echo variables to debug: <code>shell: "echo 'DEBUG: var=${my_var}'"</code></li>
</ul>
<h3 id="issue-variable-empty-or-undefined"><a class="header" href="#issue-variable-empty-or-undefined">Issue: Variable Empty or Undefined</a></h3>
<p><strong>Symptom:</strong> Variable interpolates but contains empty string or null.</p>
<p><strong>Common Causes:</strong></p>
<ol>
<li>
<p><strong>Variable used before being set</strong></p>
<pre><code class="language-yaml"># Wrong - using before capture
- shell: "echo ${count}"
- shell: "wc -l file.txt"
  capture_output: "count"

# Correct - capture first, use second
- shell: "wc -l file.txt"
  capture_output: "count"
- shell: "echo ${count}"
</code></pre>
</li>
<li>
<p><strong>Command produced no output</strong></p>
<pre><code class="language-yaml">- shell: "find . -name 'nonexistent.txt'"
  capture_output: "result"  # Will be empty if no matches
- shell: "echo ${result}"  # Empty string
</code></pre>
</li>
<li>
<p><strong>Capture not configured</strong></p>
<pre><code class="language-yaml"># Wrong - forgot capture_output
- shell: "cargo --version"
- shell: "echo ${shell.output}"  # Empty unless capture_output: true

# Correct
- shell: "cargo --version"
  capture_output: true  # or capture_output: "cargo_version"
</code></pre>
</li>
</ol>
<p><strong>Solutions:</strong></p>
<ul>
<li>Ensure capture_output is set when you need to save output</li>
<li>Check command actually produces output</li>
<li>Use verbose mode to see when variables are set</li>
<li>Provide defaults: <code>${var:-default_value}</code> (shell syntax)</li>
</ul>
<h3 id="issue-phase-specific-variable-not-available"><a class="header" href="#issue-phase-specific-variable-not-available">Issue: Phase-Specific Variable Not Available</a></h3>
<p><strong>Symptom:</strong> Error about undefined variable or empty value when using phase-specific variables.</p>
<p><strong>Common Causes:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>Variable</th><th>Wrong Phase</th><th>Correct Phase</th><th>Fix</th></tr></thead><tbody>
<tr><td><code>${item.*}</code></td><td>Reduce, Setup, Merge</td><td>Map only</td><td>Use <code>${map.results}</code> in reduce</td></tr>
<tr><td><code>${map.*}</code></td><td>Setup, Map, Merge</td><td>Reduce only</td><td>Move logic to reduce phase</td></tr>
<tr><td><code>${merge.*}</code></td><td>Setup, Map, Reduce</td><td>Merge only</td><td>Only use in merge commands</td></tr>
</tbody></table>
</div>
<p><strong>Example Problem:</strong></p>
<pre><code class="language-yaml"># Wrong - can't use ${item} in reduce
reduce:
  - shell: "process ${item.name}"  # ERROR!

# Correct - iterate through map.results
reduce:
  - shell: "echo '${map.results}' | jq -r '.[] | .item.name' | while read name; do process \"$name\"; done"
</code></pre>
<p><strong>Solutions:</strong></p>
<ul>
<li>Review phase availability table in main Variables documentation</li>
<li>Move variable usage to appropriate phase</li>
<li>In reduce, access item data through <code>${map.results}</code></li>
<li>Restructure workflow if necessary</li>
</ul>
<h3 id="issue-nested-field-access-fails"><a class="header" href="#issue-nested-field-access-fails">Issue: Nested Field Access Fails</a></h3>
<p><strong>Symptom:</strong> Can’t access nested JSON fields like <code>${var.field.nested}</code>.</p>
<p><strong>Common Causes:</strong></p>
<ol>
<li>
<p><strong>Format not specified as JSON</strong></p>
<pre><code class="language-yaml"># Wrong - no format specification
- shell: "cargo metadata --format-version 1"
  capture_output: "metadata"
- shell: "echo ${metadata.workspace_root}"  # Won't work!

# Correct - specify JSON format
- shell: "cargo metadata --format-version 1"
  capture_output: "metadata"
  capture_format: "json"
- shell: "echo ${metadata.workspace_root}"  # Works!
</code></pre>
</li>
<li>
<p><strong>Field doesn’t exist in JSON</strong></p>
<pre><code class="language-yaml">- shell: "echo ${item.nonexistent_field}"  # Empty if field missing
</code></pre>
</li>
<li>
<p><strong>JSON is invalid</strong></p>
<pre><code class="language-yaml"># Command produces malformed JSON
- shell: "echo '{incomplete json'"
  capture_output: "data"
  capture_format: "json"  # Will fail to parse
</code></pre>
</li>
</ol>
<p><strong>Solutions:</strong></p>
<ul>
<li>Always use <code>capture_format: "json"</code> for JSON output</li>
<li>Verify JSON structure with <code>jq</code>: <code>echo '${var}' | jq .</code></li>
<li>Check field exists: <code>echo '${var}' | jq -r '.field // "default"'</code></li>
<li>Validate JSON before capture</li>
</ul>
<h3 id="issue-git-context-variables-empty"><a class="header" href="#issue-git-context-variables-empty">Issue: Git Context Variables Empty</a></h3>
<p><strong>Symptom:</strong> Git variables like <code>${step.files_added}</code> are empty.</p>
<p><strong>Common Causes:</strong></p>
<ol>
<li>
<p><strong>No commits created</strong></p>
<pre><code class="language-yaml">- shell: "echo 'hello' &gt; file.txt"
- shell: "echo ${step.files_added}"  # Empty - no commit yet!

# Correct - ensure command creates commit
- shell: "echo 'hello' &gt; file.txt"
  commit_required: true  # Forces commit
- shell: "echo ${step.files_added}"  # Now has value
</code></pre>
</li>
<li>
<p><strong>Not in a git repository</strong></p>
<pre><code class="language-yaml"># Git variables require git repo
- shell: "echo ${step.files_added}"  # Empty if not in git repo
</code></pre>
</li>
<li>
<p><strong>No files changed in step</strong></p>
<pre><code class="language-yaml">- shell: "cargo check"  # Doesn't modify files
- shell: "echo ${step.files_added}"  # Empty - no files added
</code></pre>
</li>
</ol>
<p><strong>Solutions:</strong></p>
<ul>
<li>Ensure commands that modify files use <code>commit_required: true</code></li>
<li>Verify you’re in a git repository</li>
<li>Check that commands actually modify files</li>
<li>Use <code>git status</code> to verify changes exist</li>
</ul>
<h3 id="issue-format-modifiers-not-working"><a class="header" href="#issue-format-modifiers-not-working">Issue: Format Modifiers Not Working</a></h3>
<p><strong>Symptom:</strong> Format modifiers like <code>:json</code> or <code>:*.rs</code> don’t apply.</p>
<p><strong>Common Causes:</strong></p>
<ol>
<li>
<p><strong>Wrong variable type</strong></p>
<pre><code class="language-yaml"># Wrong - not a git context variable
- shell: "ls"
  capture_output: "files"
- shell: "echo ${files:json}"  # Format modifiers only work on git vars!

# Correct - use capture_format instead
- shell: "ls"
  capture_output: "files"
  capture_format: "json"
</code></pre>
</li>
<li>
<p><strong>Syntax error</strong></p>
<pre><code class="language-yaml"># Wrong syntax
- shell: "echo ${step.files_added:.rs}"  # Missing * in glob

# Correct
- shell: "echo ${step.files_added:*.rs}"
</code></pre>
</li>
</ol>
<p><strong>Solutions:</strong></p>
<ul>
<li>Format modifiers (<code>:json</code>, <code>:lines</code>, <code>:csv</code>, <code>:*.ext</code>) only work on git context variables</li>
<li>For custom captures, use <code>capture_format</code> instead</li>
<li>Check glob pattern syntax</li>
</ul>
<h3 id="debugging-techniques"><a class="header" href="#debugging-techniques">Debugging Techniques</a></h3>
<h4 id="1-use-verbose-mode"><a class="header" href="#1-use-verbose-mode">1. Use Verbose Mode</a></h4>
<pre><code class="language-bash"># Run with verbose flag to see variable resolution
prodigy run workflow.yml -v
</code></pre>
<p>Verbose mode shows:</p>
<ul>
<li>Variable values at each step</li>
<li>When variables are captured</li>
<li>Interpolated command strings before execution</li>
</ul>
<h4 id="2-echo-variables-for-debugging"><a class="header" href="#2-echo-variables-for-debugging">2. Echo Variables for Debugging</a></h4>
<pre><code class="language-yaml">- shell: "echo 'DEBUG: item=${item}'"
- shell: "echo 'DEBUG: item.path=${item.path}'"
- shell: "echo 'DEBUG: item_index=${item_index}'"
</code></pre>
<h4 id="3-check-claude-json-logs"><a class="header" href="#3-check-claude-json-logs">3. Check Claude JSON Logs</a></h4>
<p>Claude command logs contain variable interpolation details:</p>
<pre><code class="language-bash"># View most recent Claude command log
cat ~/.claude/projects/*/latest.jsonl | jq -c 'select(.type == "assistant")'
</code></pre>
<h4 id="4-verify-variables-in-checkpoint-files"><a class="header" href="#4-verify-variables-in-checkpoint-files">4. Verify Variables in Checkpoint Files</a></h4>
<p>For resume issues, check checkpoint files:</p>
<pre><code class="language-bash"># View checkpoint variables
cat ~/.prodigy/state/*/checkpoints/latest.json | jq .variables
</code></pre>
<h4 id="5-use-jq-to-explore-json-variables"><a class="header" href="#5-use-jq-to-explore-json-variables">5. Use jq to Explore JSON Variables</a></h4>
<pre><code class="language-yaml"># Explore structure
- shell: "echo '${map.results}' | jq ."

# List available keys
- shell: "echo '${metadata}' | jq 'keys'"

# Pretty print
- shell: "echo '${item}' | jq -C ."
</code></pre>
<h3 id="common-syntax-issues"><a class="header" href="#common-syntax-issues">Common Syntax Issues</a></h3>
<h4 id="issue-special-characters-in-variables"><a class="header" href="#issue-special-characters-in-variables">Issue: Special Characters in Variables</a></h4>
<p><strong>Problem:</strong></p>
<pre><code class="language-yaml"># Variable contains spaces or special chars
- shell: echo ${item.name}  # Breaks if name has spaces!
</code></pre>
<p><strong>Solution:</strong></p>
<pre><code class="language-yaml"># Always quote variables in shell commands
- shell: "echo \"${item.name}\""
</code></pre>
<h4 id="issue-yaml-string-escaping"><a class="header" href="#issue-yaml-string-escaping">Issue: YAML String Escaping</a></h4>
<p><strong>Problem:</strong></p>
<pre><code class="language-yaml"># Single quotes prevent interpolation
- shell: 'echo ${item.name}'  # Literal ${item.name} printed!
</code></pre>
<p><strong>Solution:</strong></p>
<pre><code class="language-yaml"># Use double quotes for interpolation
- shell: "echo ${item.name}"
</code></pre>
<h4 id="issue-combining-variables-with-text"><a class="header" href="#issue-combining-variables-with-text">Issue: Combining Variables with Text</a></h4>
<p><strong>Problem:</strong></p>
<pre><code class="language-yaml"># Ambiguous variable name
- shell: "echo $item_path"  # Is it ${item_path} or ${item}_path?
</code></pre>
<p><strong>Solution:</strong></p>
<pre><code class="language-yaml"># Use ${} syntax to clarify boundaries
- shell: "echo ${item}_path"
- shell: "echo prefix_${item}_suffix"
</code></pre>
<h3 id="best-practices-for-avoiding-issues"><a class="header" href="#best-practices-for-avoiding-issues">Best Practices for Avoiding Issues</a></h3>
<ol>
<li><strong>Always use <code>${VAR}</code> syntax</strong> - More reliable than <code>$VAR</code></li>
<li><strong>Check phase availability</strong> - Review phase table before using variables</li>
<li><strong>Quote shell variables</strong> - Use <code>"${var}"</code> in shell commands</li>
<li><strong>Capture before use</strong> - Set <code>capture_output</code> before referencing</li>
<li><strong>Specify JSON format</strong> - Use <code>capture_format: "json"</code> for structured data</li>
<li><strong>Use verbose mode</strong> - Debug with <code>-v</code> flag</li>
<li><strong>Validate JSON</strong> - Test with <code>jq</code> before using in workflow</li>
<li><strong>Document assumptions</strong> - Comment expected variable structure</li>
<li><strong>Provide fallbacks</strong> - Handle empty variables gracefully</li>
<li><strong>Test incrementally</strong> - Add variables one at a time</li>
</ol>
<h3 id="getting-help"><a class="header" href="#getting-help">Getting Help</a></h3>
<p>If you’re still stuck after trying these debugging techniques:</p>
<ol>
<li><strong>Check logs</strong>: Review Claude JSON logs for variable resolution</li>
<li><strong>Inspect checkpoints</strong>: Look at stored variable values</li>
<li><strong>Simplify workflow</strong>: Remove complexity to isolate issue</li>
<li><strong>Review examples</strong>: Check working examples in documentation</li>
<li><strong>Verify phase</strong>: Double-check variable is available in current phase</li>
</ol>
<p><strong>See Also:</strong></p>
<ul>
<li><a href="variables/available-variables.html">Available Variables</a> - Full variable reference with phase availability</li>
<li><a href="variables/custom-variable-capture.html">Custom Variable Capture</a> - Capture configuration and formats</li>
<li><a href="variables/../examples.html">Examples</a> - Working examples of variable usage</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="see-also-7"><a class="header" href="#see-also-7">See Also</a></h2>
<h3 id="related-documentation"><a class="header" href="#related-documentation">Related Documentation</a></h3>
<ul>
<li><strong><a href="variables/../environment/">Environment Variables</a></strong> - Configure environment variables, secrets, and profiles for workflow parameterization</li>
<li><strong><a href="variables/../git-context-advanced.html">Git Context Advanced</a></strong> - Deep dive into git variable formats, filtering, and advanced use cases</li>
<li><strong><a href="variables/../mapreduce/">MapReduce Workflows</a></strong> - Using item.* and map.* variables in distributed parallel processing</li>
<li><strong><a href="variables/../commands.html">Commands</a></strong> - Command-specific capture_output configuration and command types</li>
<li><strong><a href="variables/../examples.html">Examples</a></strong> - Real-world variable usage patterns and complete workflows</li>
<li><strong><a href="variables/../troubleshooting/">Troubleshooting</a></strong> - Debugging workflows with variable-related issues</li>
</ul>
<h3 id="key-concepts-1"><a class="header" href="#key-concepts-1">Key Concepts</a></h3>
<ul>
<li><strong>Built-in vs Custom Variables</strong>: Built-in variables are automatic (workflow.<em>, item.</em>, etc.), while custom variables require explicit capture_output configuration</li>
<li><strong>Phase Availability</strong>: Variables have different availability depending on workflow phase (setup, map, reduce, merge)</li>
<li><strong>Variable Scoping</strong>: Setup captures are workflow-wide, map captures are agent-local, reduce captures are step-forward</li>
<li><strong>Format Modifiers</strong>: Git context variables support format modifiers (:json, :lines, :csv, :*.ext), while custom captures use capture_format</li>
<li><strong>Metadata Fields</strong>: All captured variables include .success, .exit_code, .stderr, and .duration fields. Goal-seeking commands also provide validation.completion, validation.gaps, and validation.status metadata</li>
</ul>
<h3 id="external-resources"><a class="header" href="#external-resources">External Resources</a></h3>
<ul>
<li><strong><a href="https://goessner.net/articles/JsonPath/">JSONPath Syntax</a></strong> - For json_path in MapReduce input extraction</li>
<li><strong><a href="https://en.wikipedia.org/wiki/Glob_(programming)">Glob Pattern Syntax</a></strong> - For git context filtering with <code>:*.ext</code> modifiers</li>
<li><strong><a href="https://stedolan.github.io/jq/manual/">jq Manual</a></strong> - Essential tool for working with JSON variables in workflows</li>
</ul>
<h3 id="quick-reference"><a class="header" href="#quick-reference">Quick Reference</a></h3>
<p><strong>Phase-Specific Variables:</strong></p>
<ul>
<li>Setup: All standard variables, custom captures (workflow-wide scope)</li>
<li>Map: item.*, item_index, item_total, worker.id + inherited setup captures</li>
<li>Reduce: map.total, map.successful, map.failed, map.results</li>
<li>Merge: merge.worktree, merge.source_branch, merge.target_branch</li>
</ul>
<p><strong>Git Context Variables:</strong></p>
<ul>
<li>step.files_added, step.files_modified, step.files_deleted, step.files_changed</li>
<li>step.commits, step.commit_count, step.insertions, step.deletions</li>
<li>workflow.commits, workflow.commit_count</li>
<li>Format modifiers: :json, :lines (or :newline), :csv (or :comma), :*.ext</li>
</ul>
<p><strong>Custom Capture:</strong></p>
<ul>
<li>capture_output: “var_name” - Creates ${var_name}</li>
<li>capture_format: “json|string|lines|number|boolean”</li>
<li>Metadata: ${var.success}, ${var.exit_code}, ${var.stderr}, ${var.duration}</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="environment-configuration-1"><a class="header" href="#environment-configuration-1">Environment Configuration</a></h1>
<p>Prodigy provides flexible environment configuration for workflows, allowing you to manage environment variables, secrets, profiles, and step-specific settings. This chapter explains the user-facing configuration options available in workflow YAML files.</p>
<h2 id="architecture-overview"><a class="header" href="#architecture-overview">Architecture Overview</a></h2>
<p>Prodigy uses a two-layer architecture for environment management:</p>
<ol>
<li><strong>WorkflowConfig</strong>: User-facing YAML configuration with <code>env</code>, <code>secrets</code>, <code>profiles</code>, and <code>env_files</code> fields</li>
<li><strong>EnvironmentConfig</strong>: Internal runtime configuration that extends workflow config with additional features</li>
</ol>
<p>This chapter documents the WorkflowConfig layer - the fields you write in workflow YAML files (<code>env</code>, <code>secrets</code>, <code>env_files</code>, <code>profiles</code>). The EnvironmentConfig is Prodigy’s internal runtime that processes these YAML fields and adds internal-only features like dynamic command-based values and conditional expressions.</p>
<p><strong>Internal vs. User-Facing Capabilities:</strong></p>
<p>The internal <code>EnvironmentConfig</code> supports richer environment value types through the <code>EnvValue</code> enum:</p>
<ul>
<li><code>Static</code>: Simple string values (what WorkflowConfig exposes)</li>
<li><code>Dynamic</code>: Values from command output (internal only)</li>
<li><code>Conditional</code>: Expression-based values (internal only)</li>
</ul>
<p>In workflow YAML, the <code>env</code> field only supports static string values (<code>HashMap&lt;String, String&gt;</code>). The Dynamic and Conditional variants are internal runtime features not exposed in workflow configuration.</p>
<p><strong>Note on Internal Features:</strong> The <code>EnvironmentConfig</code> runtime layer includes a <code>StepEnvironment</code> struct with fields like <code>env</code>, <code>working_dir</code>, <code>clear_env</code>, and <code>temporary</code>. These are internal implementation details not exposed in <code>WorkflowStepCommand</code> YAML syntax. Per-command environment changes must use shell syntax (e.g., <code>ENV=value command</code>).</p>
<hr />
<h2 id="global-environment-variables-1"><a class="header" href="#global-environment-variables-1">Global Environment Variables</a></h2>
<p>Define static environment variables that apply to all commands in your workflow:</p>
<pre><code class="language-yaml"># Global environment variables (static strings only)
env:
  NODE_ENV: production
  PORT: "3000"
  API_URL: https://api.example.com
  DEBUG: "false"

commands:
  - shell: "echo $NODE_ENV"  # Uses global environment
</code></pre>
<p><strong>Important:</strong> The <code>env</code> field at the workflow level only supports static string values. Dynamic or conditional environment variables are handled internally by the runtime but are not directly exposed in workflow YAML.</p>
<p><strong>Environment Inheritance:</strong> Parent process environment variables are always inherited by default. All global environment variables are merged with the parent environment.</p>
<hr />
<h2 id="variable-interpolation-3"><a class="header" href="#variable-interpolation-3">Variable Interpolation</a></h2>
<p>Prodigy supports two syntaxes for referencing environment variables in workflows:</p>
<h3 id="simple-syntax-var"><a class="header" href="#simple-syntax-var">Simple Syntax: <code>$VAR</code></a></h3>
<p>The simple <code>$VAR</code> syntax works for basic variable references:</p>
<pre><code class="language-yaml">env:
  API_URL: https://api.example.com
  PORT: "3000"

commands:
  - shell: "curl $API_URL/health"
  - shell: "echo Server running on port $PORT"
</code></pre>
<p><strong>Use <code>$VAR</code> when:</strong></p>
<ul>
<li>Variable name is standalone (not adjacent to other text)</li>
<li>You’re passing to shell commands</li>
<li>Simple, clear usage without ambiguity</li>
</ul>
<h3 id="bracketed-syntax-var"><a class="header" href="#bracketed-syntax-var">Bracketed Syntax: <code>${VAR}</code></a></h3>
<p>The bracketed <code>${VAR}</code> syntax is preferred for clarity and is required in some cases:</p>
<pre><code class="language-yaml">env:
  PROJECT: my-app
  VERSION: "1.0.0"
  ENVIRONMENT: prod

commands:
  - shell: "deploy-${PROJECT}-${VERSION}.sh"  # Required: adjacent to text
  - shell: "echo Deploying ${PROJECT} v${VERSION}"  # Preferred: explicit
  - shell: "mkdir -p /var/log/${PROJECT}/${ENVIRONMENT}"  # Required: in paths
</code></pre>
<p><strong>Use <code>${VAR}</code> when:</strong></p>
<ul>
<li>Variable is adjacent to other text (e.g., <code>${VAR}-suffix</code>, <code>prefix-${VAR}</code>)</li>
<li>Variable is part of a path (e.g., <code>config/${VAR}/file.json</code>)</li>
<li>Complex expressions or nested usage</li>
<li>You want explicit, unambiguous references (recommended)</li>
</ul>
<h3 id="when-var-is-required"><a class="header" href="#when-var-is-required">When <code>${VAR}</code> is Required</a></h3>
<p><strong>1. Adjacent to text:</strong></p>
<pre><code class="language-yaml">env:
  NAME: api
  VERSION: "2.1"

commands:
  # Wrong - Shell interprets as variable named "NAME_VERSION"
  - shell: "echo $NAME_VERSION"

  # Correct - Explicitly separates variables
  - shell: "echo ${NAME}_${VERSION}"
</code></pre>
<p><strong>2. In file paths:</strong></p>
<pre><code class="language-yaml">env:
  ENVIRONMENT: staging
  PROJECT: my-app

commands:
  # Preferred - Clear variable boundaries
  - shell: "cp config.json /etc/${PROJECT}/${ENVIRONMENT}/config.json"
</code></pre>
<p><strong>3. In URLs and complex strings:</strong></p>
<pre><code class="language-yaml">env:
  API_BASE: https://api.example.com
  VERSION: v1

commands:
  # Required - Variable within URL
  - shell: "curl ${API_BASE}/${VERSION}/users"
</code></pre>
<h3 id="interpolation-in-different-contexts"><a class="header" href="#interpolation-in-different-contexts">Interpolation in Different Contexts</a></h3>
<p><strong>Shell commands:</strong>
Both syntaxes work, but <code>${VAR}</code> is safer:</p>
<pre><code class="language-yaml">env:
  DATABASE_URL: postgresql://localhost:5432/app
  TIMEOUT: "30"

commands:
  - shell: "psql $DATABASE_URL"           # Simple case: $VAR works
  - shell: "timeout ${TIMEOUT} ./app"     # Preferred: ${VAR} is explicit
</code></pre>
<p><strong>Claude commands:</strong>
Use <code>${VAR}</code> for consistency:</p>
<pre><code class="language-yaml">env:
  SPEC_FILE: spec-123.md
  PROJECT_NAME: my-project

commands:
  - claude: "/implement-spec ${SPEC_FILE} --project ${PROJECT_NAME}"
</code></pre>
<p><strong>File paths:</strong>
Always use <code>${VAR}</code> in paths:</p>
<pre><code class="language-yaml">env:
  OUTPUT_DIR: /tmp/results
  TIMESTAMP: "20240101"

commands:
  - shell: "mkdir -p ${OUTPUT_DIR}/${TIMESTAMP}"
  - write_file:
      path: ${OUTPUT_DIR}/${TIMESTAMP}/report.json
      content: "..."
</code></pre>
<p><strong>MapReduce configurations:</strong>
Combine with MapReduce variables like <code>${item}</code>:</p>
<pre><code class="language-yaml">env:
  MAX_WORKERS: "10"
  OUTPUT_PATH: /results

map:
  max_parallel: ${MAX_WORKERS}
  agent_template:
    - shell: "process ${item.file} --output ${OUTPUT_PATH}/${item.id}.result"
</code></pre>
<h3 id="escaping-variables"><a class="header" href="#escaping-variables">Escaping Variables</a></h3>
<p>If you need a literal <code>$</code> character, use shell escaping:</p>
<pre><code class="language-yaml">commands:
  # Using single quotes (no interpolation)
  - shell: 'echo "Price: $100"'

  # Using double quotes with escape
  - shell: "echo \"Price: \\$100\""

  # Double $$ for literal $ in some contexts
  - shell: "echo Price: $$100"
</code></pre>
<h3 id="best-practices-9"><a class="header" href="#best-practices-9">Best Practices</a></h3>
<ol>
<li><strong>Prefer <code>${VAR}</code> for consistency</strong>: More explicit and works in all cases</li>
<li><strong>Use <code>$VAR</code> only for simple standalone cases</strong>: When passing to shell and no ambiguity</li>
<li><strong>Always use <code>${VAR}</code> in paths</strong>: Prevents parsing issues</li>
<li><strong>Be consistent within a workflow</strong>: Pick one style and stick to it</li>
<li><strong>Use quotes for values with spaces</strong>: <code>env: MESSAGE: "Hello World"</code></li>
</ol>
<h3 id="examples-comparison"><a class="header" href="#examples-comparison">Examples Comparison</a></h3>
<p><strong>Simple case (both work):</strong></p>
<pre><code class="language-yaml">env:
  PORT: "3000"

# Both acceptable
- shell: "echo Port: $PORT"
- shell: "echo Port: ${PORT}"
</code></pre>
<p><strong>Complex case (requires <code>${VAR}</code>):</strong></p>
<pre><code class="language-yaml">env:
  PROJECT: api
  VERSION: "1.0"
  ENVIRONMENT: prod

# Required - variables adjacent to text and in paths
- shell: "deploy-${PROJECT}-v${VERSION}.sh --env ${ENVIRONMENT}"
- shell: "cp /src/config.${ENVIRONMENT}.json /etc/${PROJECT}/config.json"
</code></pre>
<p><strong>Recommended approach (always use <code>${VAR}</code>):</strong></p>
<pre><code class="language-yaml">env:
  DATABASE: myapp
  USER: admin
  HOST: localhost

commands:
  - shell: "psql -h ${HOST} -U ${USER} -d ${DATABASE}"
  - shell: "backup-${DATABASE}-$(date +%Y%m%d).sql"
</code></pre>
<hr />
<h2 id="additional-topics-3"><a class="header" href="#additional-topics-3">Additional Topics</a></h2>
<h3 id="environment-configuration-subsections"><a class="header" href="#environment-configuration-subsections">Environment Configuration Subsections</a></h3>
<ul>
<li><a href="environment/mapreduce-environment-variables.html">MapReduce Environment Variables</a> - Environment variables specific to MapReduce workflows</li>
<li><a href="environment/environment-files.html">Environment Files</a> - Using .env files for configuration</li>
<li><a href="environment/secrets-management.html">Secrets Management</a> - Handling sensitive data securely</li>
<li><a href="environment/environment-profiles.html">Environment Profiles</a> - Profile-based configuration for different environments</li>
<li><a href="environment/per-command-environment-overrides.html">Per-Command Environment Overrides</a> - Step-level environment customization</li>
<li><a href="environment/environment-precedence.html">Environment Precedence</a> - Understanding variable resolution order</li>
<li><a href="environment/best-practices.html">Best Practices</a> - Recommended patterns and approaches</li>
<li><a href="environment/common-patterns.html">Common Patterns</a> - Real-world usage examples</li>
</ul>
<h3 id="related-chapters"><a class="header" href="#related-chapters">Related Chapters</a></h3>
<ul>
<li><a href="environment/../mapreduce/index.html">MapReduce Workflows</a> - Parallel processing with environment configuration</li>
<li><a href="environment/../variables/index.html">Variables and Interpolation</a> - Understanding variable syntax and usage</li>
<li><a href="environment/../configuration/index.html">Configuration</a> - Overall workflow and project configuration</li>
<li><a href="environment/../configuration/workflow-configuration.html">Workflow Configuration</a> - Complete workflow file structure</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="mapreduce-environment-variables-1"><a class="header" href="#mapreduce-environment-variables-1">MapReduce Environment Variables</a></h2>
<p>In MapReduce workflows, environment variables provide powerful parameterization across all phases (setup, map, reduce, and merge). This enables workflows to be reusable across different projects and configurations.</p>
<h3 id="overview-5"><a class="header" href="#overview-5">Overview</a></h3>
<p>Environment variables in MapReduce workflows are available in all execution phases:</p>
<ul>
<li><strong>Setup phase</strong>: Initialize environment, generate configuration</li>
<li><strong>Map phase</strong>: Parameterize agent templates, configure timeouts and parallelism</li>
<li><strong>Reduce phase</strong>: Aggregate results, format output</li>
<li><strong>Merge phase</strong>: Control merge behavior, validation</li>
</ul>
<h3 id="setup-phase-environment-variables"><a class="header" href="#setup-phase-environment-variables">Setup Phase Environment Variables</a></h3>
<p>Environment variables are fully available in setup phase commands:</p>
<pre><code class="language-yaml">env:
  PROJECT_NAME: prodigy
  DATA_SOURCE: https://api.example.com/items
  TIMEOUT: "30"

setup:
  - shell: "curl $DATA_SOURCE &gt; items.json"
  - shell: "echo 'Processing $PROJECT_NAME workflow'"
  - shell: "mkdir -p output/$PROJECT_NAME"
</code></pre>
<p><strong>Use cases in setup:</strong></p>
<ul>
<li>Configure data sources and API endpoints</li>
<li>Set project-specific paths</li>
<li>Initialize environment-specific settings</li>
</ul>
<h3 id="map-phase-environment-variables"><a class="header" href="#map-phase-environment-variables">Map Phase Environment Variables</a></h3>
<p>Environment variables can be used throughout the map phase configuration:</p>
<pre><code class="language-yaml">env:
  MAX_WORKERS: "10"
  AGENT_TIMEOUT: "600"
  PROJECT_DIR: /path/to/project

map:
  input: items.json
  max_parallel: ${MAX_WORKERS}     # Parameterize parallelism
  agent_timeout_secs: ${AGENT_TIMEOUT}

  agent_template:
    - claude: "/process-item '${item.name}' --project $PROJECT_NAME"
    - shell: "test -f $PROJECT_DIR/${item.file}"
    - shell: "timeout ${AGENT_TIMEOUT} ./analyze.sh ${item.path}"
</code></pre>
<p><strong>Parameterizing max_parallel and agent_timeout_secs:</strong></p>
<p>Both <code>max_parallel</code> and <code>agent_timeout_secs</code> accept numeric values or environment variable references. These are resolved at <strong>configuration parse time</strong> (when the workflow is loaded), not at execution time.</p>
<pre><code class="language-yaml">env:
  MAX_WORKERS: "5"   # Can be overridden per environment
  AGENT_TIMEOUT: "600"

map:
  max_parallel: ${MAX_WORKERS}           # Resolved to 5 at parse time
  agent_timeout_secs: ${AGENT_TIMEOUT}   # Resolved to 600 at parse time
  # OR
  max_parallel: 10                       # Static value
  agent_timeout_secs: 300                # Static value
</code></pre>
<p><strong>Environment Variable Resolution Timing:</strong></p>
<ul>
<li><strong>Parse time</strong> (when workflow is loaded): <code>max_parallel</code> and <code>agent_timeout_secs</code> are resolved using <code>resolve_env_or_parse</code> method</li>
<li><strong>Execution time</strong> (when commands run): Command interpolation happens (e.g., <code>${item.name}</code>, <code>$PROJECT_NAME</code>)</li>
</ul>
<p>This distinction matters because parse-time resolution fails fast if environment variables are undefined, while execution-time interpolation happens dynamically.</p>
<p>Source: src/config/mapreduce.rs:527-540 (to_map_phase method with resolve_env_or_parse)</p>
<p><strong>Integration with MapReduce-specific variables:</strong></p>
<p>Environment variables work seamlessly with MapReduce variables like <code>${item}</code> and <code>${map.results}</code>:</p>
<pre><code class="language-yaml">env:
  OUTPUT_DIR: /tmp/results
  CONFIG_FILE: config.json

map:
  agent_template:
    - shell: "process --config $CONFIG_FILE --input ${item.path} --output $OUTPUT_DIR/${item.id}.json"
</code></pre>
<h3 id="reduce-phase-environment-variables"><a class="header" href="#reduce-phase-environment-variables">Reduce Phase Environment Variables</a></h3>
<p>Use environment variables in reduce commands to parameterize aggregation:</p>
<pre><code class="language-yaml">env:
  OUTPUT_PATH: results/summary.json
  MIN_SUCCESS_RATE: "80"

reduce:
  - shell: "echo 'Processed ${map.successful}/${map.total} items'"
  - write_file:
      path: "${OUTPUT_PATH}"
      content: "${map.results}"
      format: json
  - shell: |
      SUCCESS_RATE=$((${map.successful} * 100 / ${map.total}))
      if [ $SUCCESS_RATE -lt $MIN_SUCCESS_RATE ]; then
        echo "Warning: Success rate below threshold"
        exit 1
      fi
</code></pre>
<p><strong>Important: write_file format field</strong></p>
<p>The <code>format</code> field in <code>write_file</code> accepts enum literals only (<code>text</code>, <code>json</code>, or <code>yaml</code>), not environment variable interpolation. Only the <code>path</code> and <code>content</code> fields support variable interpolation.</p>
<p>Source: src/config/command.rs:303-314 (WriteFileFormat enum)</p>
<h3 id="merge-phase-environment-variables"><a class="header" href="#merge-phase-environment-variables">Merge Phase Environment Variables</a></h3>
<p>Environment variables are available in merge workflows alongside merge-specific variables:</p>
<pre><code class="language-yaml">env:
  CI_MODE: "true"
  TEST_TIMEOUT: "300"

merge:
  commands:
    - shell: "git fetch origin"
    - shell: "timeout $TEST_TIMEOUT cargo test"
    - shell: |
        if [ "$CI_MODE" = "true" ]; then
          git merge --no-edit ${merge.source_branch}
        else
          claude "/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}"
        fi
</code></pre>
<h3 id="complete-example-parameterized-mapreduce-workflow"><a class="header" href="#complete-example-parameterized-mapreduce-workflow">Complete Example: Parameterized MapReduce Workflow</a></h3>
<pre><code class="language-yaml">name: parameterized-processing
mode: mapreduce

env:
  # Project configuration
  PROJECT_NAME: my-project
  VERSION: "1.0.0"

  # Execution parameters
  MAX_WORKERS: "10"
  AGENT_TIMEOUT: "600"

  # Paths
  INPUT_FILE: items.json
  OUTPUT_DIR: results
  CONFIG_PATH: config/settings.json

  # Thresholds
  MIN_COVERAGE: "80"

setup:
  - shell: "echo 'Starting $PROJECT_NAME v$VERSION'"
  - shell: "mkdir -p $OUTPUT_DIR"
  - shell: "generate-items.sh &gt; $INPUT_FILE"

map:
  input: ${INPUT_FILE}
  max_parallel: ${MAX_WORKERS}
  agent_timeout_secs: ${AGENT_TIMEOUT}

  agent_template:
    - claude: "/process '${item.name}' --project $PROJECT_NAME --config $CONFIG_PATH"
    - shell: "test -f $OUTPUT_DIR/${item.id}.result"

reduce:
  - shell: "echo 'Project: $PROJECT_NAME, Version: $VERSION'"
  - shell: "echo 'Results: ${map.successful}/${map.total} succeeded'"
  - write_file:
      path: "${OUTPUT_DIR}/summary.json"
      content: |
        {
          "project": "$PROJECT_NAME",
          "version": "$VERSION",
          "total": ${map.total},
          "successful": ${map.successful},
          "failed": ${map.failed}
        }
      format: json

merge:
  commands:
    - shell: "cargo test --timeout $AGENT_TIMEOUT"
    - claude: "/validate-merge --project $PROJECT_NAME"
</code></pre>
<h3 id="best-practices-for-mapreduce-environment-variables"><a class="header" href="#best-practices-for-mapreduce-environment-variables">Best Practices for MapReduce Environment Variables</a></h3>
<ol>
<li><strong>Parameterize resource limits</strong>: Use env vars for <code>max_parallel</code>, <code>agent_timeout_secs</code>, and <code>timeout</code></li>
<li><strong>Project-agnostic workflows</strong>: Use <code>PROJECT_NAME</code> env var to make workflows reusable</li>
<li><strong>Environment-specific values</strong>: Combine with profiles for dev/staging/prod configurations</li>
<li><strong>Path parameterization</strong>: Use env vars for input files, output directories, and config paths</li>
<li><strong>Threshold configuration</strong>: Parameterize quality thresholds, success rates, and limits</li>
<li><strong>Combine with MapReduce variables</strong>: Use both <code>${ENV_VAR}</code> and <code>${item.field}</code> together</li>
<li><strong>Understand precedence chain</strong>: Environment variables are resolved in this order:
<ul>
<li>Step-level env (per-command overrides)</li>
<li>Workflow profile env (when <code>--profile</code> is active)</li>
<li>Workflow env (global env block)</li>
<li>System environment variables</li>
</ul>
</li>
</ol>
<p><strong>Environment Variable Precedence:</strong></p>
<p>When the same variable is defined in multiple places, Prodigy uses this precedence chain (highest to lowest priority):</p>
<ol>
<li><strong>Step env</strong> - Environment variables defined at the command level</li>
<li><strong>Workflow profile</strong> - Profile-specific values (activated with <code>--profile</code>)</li>
<li><strong>Workflow env</strong> - Global env block at workflow level</li>
<li><strong>System env</strong> - Environment variables from the parent process</li>
</ol>
<p>Source: .prodigy/book-analysis/features.json:596 (configuration.precedence_chain)</p>
<h3 id="common-patterns-1"><a class="header" href="#common-patterns-1">Common Patterns</a></h3>
<p><strong>Multi-environment deployment:</strong></p>
<pre><code class="language-yaml">env:
  MAX_WORKERS:
    default: "5"
    dev: "2"
    prod: "20"
  API_URL:
    default: http://localhost:3000
    prod: https://api.production.com

map:
  max_parallel: ${MAX_WORKERS}
  agent_template:
    - shell: "curl $API_URL/process -d '${item}'"
</code></pre>
<p><strong>Resource-constrained execution:</strong></p>
<pre><code class="language-yaml">env:
  CPU_LIMIT: "4"
  MEMORY_LIMIT: "8G"
  TIMEOUT: "300"

map:
  max_parallel: ${CPU_LIMIT}
  agent_timeout_secs: ${TIMEOUT}

  agent_template:
    - shell: "docker run --cpus=${CPU_LIMIT} --memory=${MEMORY_LIMIT} processor ${item.path}"
</code></pre>
<p><strong>Dynamic input generation with secrets:</strong></p>
<pre><code class="language-yaml">env:
  DATA_SOURCE: https://api.example.com/v1/items
  API_KEY:
    secret: true
    value: "sk-actual-api-key-here"

setup:
  - shell: "curl -H 'Authorization: Bearer ${API_KEY}' $DATA_SOURCE &gt; items.json"

map:
  input: items.json
  agent_template:
    - shell: "process --api-key ${API_KEY} ${item.id}"
</code></pre>
<p><strong>Secret Configuration:</strong></p>
<p>Secrets are defined within the <code>env</code> block using the <code>secret: true</code> flag. This masks the value in logs and output. There is no separate <code>secrets:</code> block.</p>
<pre><code class="language-yaml">env:
  REGULAR_VAR: "visible-value"
  SECRET_API_KEY:
    secret: true
    value: "sk-secret-value"   # Masked as *** in logs
</code></pre>
<p>Source: src/config/workflow.rs (WorkflowConfig.env), features.json:environment.secrets</p>
<p>See also:</p>
<ul>
<li><a href="environment/environment-profiles.html">Environment Profiles</a> for multi-environment configurations</li>
<li><a href="environment/secrets-management.html">Secrets Management</a> for handling sensitive variables</li>
<li><a href="environment/environment-precedence.html">Environment Precedence</a> for understanding resolution order</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="environment-files-1"><a class="header" href="#environment-files-1">Environment Files</a></h2>
<p>Load environment variables from <code>.env</code> files:</p>
<pre><code class="language-yaml"># Environment files to load
env_files:
  - .env
  - .env.local
  - config/.env.production

commands:
  - shell: "echo $DATABASE_URL"
</code></pre>
<p><strong>Environment File Format:</strong></p>
<p>Environment files use the standard <code>.env</code> format with <code>KEY=value</code> pairs:</p>
<pre><code class="language-bash"># .env file example
DATABASE_URL=postgresql://localhost:5432/mydb
REDIS_HOST=localhost
REDIS_PORT=6379

# Comments are supported (lines starting with #)
API_KEY=secret-key-here

# Empty lines are ignored

# Multi-line values use quotes
PRIVATE_KEY="-----BEGIN PRIVATE KEY-----
MIIEvQIBADANBg...
-----END PRIVATE KEY-----"
</code></pre>
<p><strong>Quote Handling:</strong></p>
<p>Prodigy automatically strips both single and double quotes from the start and end of values during parsing:</p>
<pre><code class="language-bash"># These all produce the same value: myvalue
KEY1=myvalue
KEY2="myvalue"
KEY3='myvalue'

# For multi-line values, quotes are required but will be stripped
PRIVATE_KEY="-----BEGIN PRIVATE KEY-----
MIIEvQIBADANBg...
-----END PRIVATE KEY-----"
# Resulting value does NOT include the surrounding quotes
</code></pre>
<p><strong>Source:</strong> <code>src/cook/environment/manager.rs:200-207</code></p>
<p><strong>Parsing Rules:</strong></p>
<ul>
<li>Lines starting with <code>#</code> are treated as comments and skipped</li>
<li>Empty lines are ignored</li>
<li>Each line must contain an <code>=</code> character to be valid</li>
<li>Key is everything before the first <code>=</code> (trimmed)</li>
<li>Value is everything after the first <code>=</code> (trimmed)</li>
<li>Surrounding quotes (<code>"</code> or <code>'</code>) are automatically removed from values</li>
</ul>
<p><strong>Source:</strong> <code>src/cook/environment/manager.rs:190-211</code></p>
<p><strong>Loading Order and Precedence:</strong></p>
<p>Environment files are loaded in order (if they exist), with later files overriding earlier files. Missing files are silently skipped with debug logging. This enables layered configuration:</p>
<pre><code class="language-yaml">env_files:
  - .env                # Base configuration
  - .env.local          # Local overrides (gitignored)
  - .env.production     # Environment-specific settings
</code></pre>
<p>Example override behavior:</p>
<pre><code class="language-bash"># .env (base)
DATABASE_URL=postgresql://localhost:5432/dev
API_TIMEOUT=30

# .env.production (overrides)
DATABASE_URL=postgresql://prod-server:5432/app
# API_TIMEOUT remains 30 from base file
</code></pre>
<p>Precedence order (highest to lowest):</p>
<ol>
<li>Global <code>env</code> field in workflow YAML</li>
<li>Later files in <code>env_files</code> list</li>
<li>Earlier files in <code>env_files</code> list</li>
<li>Parent process environment</li>
</ol>
<p><strong>File Paths and Resolution:</strong></p>
<p>Environment file paths can be:</p>
<ul>
<li><strong>Absolute paths:</strong> <code>/etc/myapp/.env</code></li>
<li><strong>Relative paths:</strong> Resolved relative to the workflow file location (e.g., <code>.env</code>, <code>config/.env.production</code>)</li>
</ul>
<p><strong>Source:</strong> <code>src/cook/environment/manager.rs:182-215</code></p>
<p><strong>Error Handling:</strong></p>
<p>Prodigy handles environment files with the following behavior:</p>
<ul>
<li><strong>Missing files:</strong> Silently skipped with debug logging (<code>"Environment file not found: {path}"</code>). This allows optional configuration files and environment-specific files that may not exist in all contexts.</li>
<li><strong>File read errors:</strong> Will halt workflow execution with an error (e.g., permission denied)</li>
<li><strong>Invalid syntax:</strong> Will halt workflow execution with an error (e.g., malformed <code>.env</code> format)</li>
</ul>
<p><strong>Source:</strong> <code>src/cook/environment/manager.rs:184-186</code></p>
<p><strong>Example with missing file:</strong></p>
<pre><code class="language-yaml">env_files:
  - .env                    # Always exists (base config)
  - .env.local              # May not exist (personal overrides)
  - .env.${ENVIRONMENT}     # May not exist (environment-specific)
</code></pre>
<p>In this example, if <code>.env.local</code> doesn’t exist, Prodigy logs a debug message and continues. Only <code>.env</code> needs to exist. This is useful for:</p>
<ul>
<li>Personal configuration files that are gitignored</li>
<li>Environment-specific files (<code>.env.production</code>, <code>.env.staging</code>)</li>
<li>Optional feature flags or overrides</li>
</ul>
<p><strong>Troubleshooting:</strong></p>
<p>To verify which env files are being loaded, run Prodigy with verbose logging:</p>
<pre><code class="language-bash"># Enable debug logging to see which files are loaded/skipped
RUST_LOG=debug prodigy run workflow.yml
</code></pre>
<p>You’ll see messages like:</p>
<pre><code>DEBUG prodigy::cook::environment - Environment file not found: .env.local
INFO  prodigy::cook::environment - Loaded environment from: .env
</code></pre>
<p><strong>Common Syntax Errors:</strong></p>
<p>These will cause workflow execution to halt:</p>
<pre><code class="language-bash"># INVALID - No = character
INVALID_LINE

# VALID - Value can be empty
EMPTY_VALUE=

# INVALID - Unbalanced quotes (opening quote without closing)
BAD_QUOTE="unclosed value

# VALID - Quotes must match
GOOD_QUOTE_1="value with spaces"
GOOD_QUOTE_2='single quoted value'

# INVALID - Mixed quotes
MIXED_QUOTES="value'

# VALID - Embedded quotes of opposite type
EMBEDDED="value with 'single' quotes inside"
</code></pre>
<hr />
<h3 id="integration-with-profiles-and-secrets"><a class="header" href="#integration-with-profiles-and-secrets">Integration with Profiles and Secrets</a></h3>
<p>Environment files work seamlessly with other environment features like profiles and secrets management.</p>
<p><strong>Combining env_files with profiles:</strong></p>
<pre><code class="language-yaml"># Base configuration in .env file
env_files:
  - .env

# Profile-specific overrides
profiles:
  dev:
    API_URL: http://localhost:3000
    DEBUG: "true"
  prod:
    API_URL: https://api.production.com
    DEBUG: "false"

commands:
  - shell: "echo $API_URL"  # Uses profile value if active, otherwise .env value
</code></pre>
<p>The precedence order is:</p>
<ol>
<li>Profile-specific values (if profile active)</li>
<li>Global <code>env</code> field values</li>
<li>Environment file values (later files override earlier)</li>
<li>Parent process environment</li>
</ol>
<p><strong>Loading secrets from env_files:</strong></p>
<p>Environment files can contain secrets, but you must explicitly mark them as secrets in the workflow configuration:</p>
<pre><code class="language-yaml"># .env.secrets file
API_KEY=sk-abc123xyz
DATABASE_PASSWORD=secret-password

# Workflow configuration
env_files:
  - .env.secrets

secrets:
  # Retrieve from environment variable (loaded from .env.secrets)
  API_KEY: "${env:API_KEY}"
  DATABASE_PASSWORD: "${env:DATABASE_PASSWORD}"
</code></pre>
<p><strong>Note:</strong> Variables loaded from env_files are NOT automatically masked. You must explicitly mark them as secrets in the <code>secrets</code> section for masking in logs.</p>
<p><strong>Complete integration example:</strong></p>
<pre><code class="language-yaml"># Layered configuration strategy
env_files:
  - .env                # Base configuration
  - .env.local          # Local overrides (gitignored)
  - .env.${ENVIRONMENT} # Environment-specific (e.g., .env.production)

env:
  PROJECT_NAME: my-project
  VERSION: "1.0.0"

secrets:
  # Secrets loaded from env files, masked in logs
  API_KEY: "${env:API_KEY}"
  DATABASE_URL: "${env:DATABASE_URL}"

profiles:
  dev:
    MAX_WORKERS: "2"
    TIMEOUT: "60"
  prod:
    MAX_WORKERS: "20"
    TIMEOUT: "30"

commands:
  - shell: "echo 'Project: $PROJECT_NAME v$VERSION'"
  - shell: "echo 'Workers: $MAX_WORKERS, Timeout: $TIMEOUT'"
  - shell: "curl -H 'Authorization: Bearer ***' $API_URL"  # API_KEY masked
</code></pre>
<p><strong>Best practices for organizing env files:</strong></p>
<ol>
<li><strong>.env</strong>: Base configuration, safe to commit (no secrets)</li>
<li><strong>.env.local</strong>: Personal overrides, add to .gitignore</li>
<li><strong>.env.production / .env.staging / .env.dev</strong>: Environment-specific, may contain encrypted secrets</li>
<li><strong>.env.secrets</strong>: Sensitive values, NEVER commit, always in .gitignore</li>
</ol>
<p><strong>Precedence example:</strong></p>
<pre><code class="language-bash"># .env (base)
API_URL=http://localhost:3000
MAX_WORKERS=5
TIMEOUT=30

# .env.production (overrides)
API_URL=https://api.production.com
MAX_WORKERS=20
</code></pre>
<pre><code class="language-yaml">env_files:
  - .env
  - .env.production  # Overrides API_URL and MAX_WORKERS

env:
  TIMEOUT: "60"      # Overrides TIMEOUT from both files

profiles:
  prod:
    MAX_WORKERS: "50"  # Overrides MAX_WORKERS when --profile prod used
</code></pre>
<p>Final values when running with <code>--profile prod</code>:</p>
<ul>
<li><code>API_URL</code>: <code>https://api.production.com</code> (from .env.production)</li>
<li><code>MAX_WORKERS</code>: <code>50</code> (from prod profile - highest precedence)</li>
<li><code>TIMEOUT</code>: <code>60</code> (from global env field - overrides files)</li>
</ul>
<hr />
<h2 id="see-also-8"><a class="header" href="#see-also-8">See Also</a></h2>
<ul>
<li><a href="environment/environment-precedence.html">Environment Precedence</a> - Detailed precedence rules for all environment sources</li>
<li><a href="environment/environment-profiles.html">Environment Profiles</a> - Profile-specific environment configuration</li>
<li><a href="environment/secrets-management.html">Secrets Management</a> - How to securely handle secrets loaded from env files</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="secrets-management-1"><a class="header" href="#secrets-management-1">Secrets Management</a></h2>
<p>Prodigy provides secure secret management through the <code>secrets</code> field in <code>EnvironmentConfig</code>. Secrets are environment variables that are masked in logs and output for security.</p>
<h3 id="configuration-structure"><a class="header" href="#configuration-structure">Configuration Structure</a></h3>
<p>Secrets are defined in the workflow-level <code>secrets:</code> block using the <code>SecretValue</code> type, which supports two variants:</p>
<p><strong>Source</strong>: <code>src/cook/environment/config.rs:84-96</code></p>
<pre><code class="language-yaml"># Simple secret reference (SecretValue::Simple)
secrets:
  API_KEY: "${env:SECRET_API_KEY}"

# Provider-based secrets (SecretValue::Provider)
secrets:
  DATABASE_URL:
    provider: env
    key: "DB_CONNECTION_STRING"

  SSH_KEY:
    provider: file
    key: "~/.ssh/deploy_key"
    version: "v1"  # Optional version field
</code></pre>
<h3 id="secretvalue-types"><a class="header" href="#secretvalue-types">SecretValue Types</a></h3>
<p>The <code>SecretValue</code> enum has two variants defined in <code>src/cook/environment/config.rs:84-96</code>:</p>
<h4 id="1-simple-string-reference"><a class="header" href="#1-simple-string-reference">1. Simple String Reference</a></h4>
<p>The <code>Simple</code> variant allows direct string references, typically using environment variable interpolation:</p>
<pre><code class="language-yaml">secrets:
  # Reference environment variable directly
  API_TOKEN: "${env:GITHUB_TOKEN}"

  # Reference another env var with fallback
  DATABASE_PASS: "${env:DB_PASSWORD}"
</code></pre>
<p><strong>Resolution</strong>: Simple values are resolved by looking up the referenced environment variable name (<code>src/cook/environment/manager.rs:316-319</code>).</p>
<h4 id="2-provider-based-secrets"><a class="header" href="#2-provider-based-secrets">2. Provider-Based Secrets</a></h4>
<p>The <code>Provider</code> variant supports structured secret resolution with different providers:</p>
<pre><code class="language-yaml">secrets:
  # Environment variable provider
  API_KEY:
    provider: env
    key: "SECRET_API_KEY"

  # File-based secret
  SSH_PRIVATE_KEY:
    provider: file
    key: "/etc/secrets/ssh_key"

  # Custom provider (extensible)
  CUSTOM_SECRET:
    provider: custom
    key: "my-secret-id"
</code></pre>
<p><strong>Source</strong>: Example from <code>workflows/mapreduce-env-example.yml:23-26</code></p>
<h3 id="supported-secret-providers"><a class="header" href="#supported-secret-providers">Supported Secret Providers</a></h3>
<p>Prodigy defines five secret providers in the <code>SecretProvider</code> enum (<code>src/cook/environment/config.rs:98-112</code>):</p>
<div class="table-wrapper"><table><thead><tr><th>Provider</th><th>Status</th><th>Description</th><th>Source Reference</th></tr></thead><tbody>
<tr><td><code>env</code></td><td>✅ Implemented</td><td>Reads from environment variables</td><td><code>manager.rs:322-323</code></td></tr>
<tr><td><code>file</code></td><td>✅ Implemented</td><td>Reads from filesystem</td><td><code>manager.rs:324-329</code></td></tr>
<tr><td><code>vault</code></td><td>🔮 Planned</td><td>HashiCorp Vault integration</td><td><code>config.rs:107</code></td></tr>
<tr><td><code>aws</code></td><td>🔮 Planned</td><td>AWS Secrets Manager</td><td><code>config.rs:109</code></td></tr>
<tr><td><code>custom</code></td><td>⚙️ Extensible</td><td>Custom provider via SecretStore</td><td><code>config.rs:111</code></td></tr>
</tbody></table>
</div>
<p><strong>Important</strong>: Only <code>env</code> and <code>file</code> providers are fully implemented in <code>EnvironmentManager.resolve_secret()</code> (<code>src/cook/environment/manager.rs:313-337</code>). Vault and AWS providers are defined in the enum but delegate to <code>SecretStore</code> for implementation, which currently returns “not found” errors.</p>
<h3 id="secret-resolution-flow"><a class="header" href="#secret-resolution-flow">Secret Resolution Flow</a></h3>
<p>Secrets are resolved during environment setup with the following flow (<code>src/cook/environment/manager.rs:128-136</code>):</p>
<pre><code>1. EnvironmentConfig.secrets loaded from YAML
   ↓
2. For each secret, EnvironmentManager.resolve_secret() is called
   ↓
3. Resolution strategy based on SecretValue variant:
   - Simple: Look up in std::env
   - Provider(Env): Look up in std::env
   - Provider(File): Read from filesystem
   - Provider(Other): Delegate to SecretStore
   ↓
4. Resolved value added to environment HashMap
   ↓
5. Secret key tracked in EnvironmentContext.secrets Vec for masking
</code></pre>
<p><strong>Source</strong>: <code>src/cook/environment/manager.rs:128-136</code></p>
<h3 id="secret-masking"><a class="header" href="#secret-masking">Secret Masking</a></h3>
<p>Secret values are masked in logs and command output to prevent accidental exposure:</p>
<pre><code class="language-yaml">secrets:
  API_TOKEN:
    provider: env
    key: "GITHUB_TOKEN"

commands:
  - shell: "curl -H 'Authorization: Bearer $API_TOKEN' https://api.github.com"
</code></pre>
<p><strong>Output</strong> (masked):</p>
<pre><code>$ curl -H 'Authorization: Bearer ***' https://api.github.com
</code></pre>
<p><strong>How it works</strong>: The <code>EnvironmentContext</code> struct tracks secret keys in a <code>Vec&lt;String&gt;</code> field (<code>src/cook/environment/manager.rs:23-30</code>). When commands are executed, output is scanned and secret values are replaced with <code>***</code>.</p>
<h3 id="secretstore-architecture"><a class="header" href="#secretstore-architecture">SecretStore Architecture</a></h3>
<p>For extensibility, Prodigy provides a <code>SecretStore</code> system that supports custom secret providers (<code>src/cook/environment/secret_store.rs:26-107</code>):</p>
<p><strong>Built-in Providers</strong>:</p>
<ul>
<li><code>EnvSecretProvider</code> - Environment variable lookup (<code>secret_store.rs:120-131</code>)</li>
<li><code>FileSecretProvider</code> - File-based secrets (<code>secret_store.rs:134-148</code>)</li>
</ul>
<p><strong>Custom Providers</strong>:</p>
<p>You can add custom secret providers by implementing the <code>SecretProvider</code> trait:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Example custom provider (for reference)
#[async_trait::async_trait]
pub trait SecretProvider: Send + Sync {
    async fn get_secret(&amp;self, key: &amp;str) -&gt; Result&lt;String&gt;;
    async fn has_secret(&amp;self, key: &amp;str) -&gt; bool;
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Source</strong>: <code>src/cook/environment/secret_store.rs:110-117</code></p>
<p>Custom providers can be registered with <code>SecretStore.add_provider()</code> (<code>secret_store.rs:79-81</code>).</p>
<h3 id="real-world-examples-2"><a class="header" href="#real-world-examples-2">Real-World Examples</a></h3>
<h4 id="example-1-simple-environment-variable-secrets"><a class="header" href="#example-1-simple-environment-variable-secrets">Example 1: Simple Environment Variable Secrets</a></h4>
<p>From <code>workflows/environment-example.yml:21-23</code>:</p>
<pre><code class="language-yaml">secrets:
  API_KEY: "${env:SECRET_API_KEY}"

commands:
  - shell: "echo 'Deploying with API key: $API_KEY'"  # Value masked in logs
</code></pre>
<h4 id="example-2-provider-based-secrets-in-mapreduce"><a class="header" href="#example-2-provider-based-secrets-in-mapreduce">Example 2: Provider-Based Secrets in MapReduce</a></h4>
<p>From <code>workflows/mapreduce-env-example.yml:23-26</code>:</p>
<pre><code class="language-yaml">secrets:
  API_TOKEN:
    provider: env
    key: "GITHUB_TOKEN"

reduce:
  - shell: "curl -H 'Authorization: Bearer $API_TOKEN' https://api.github.com/repos/notify"
</code></pre>
<h4 id="example-3-file-based-secrets"><a class="header" href="#example-3-file-based-secrets">Example 3: File-Based Secrets</a></h4>
<pre><code class="language-yaml">secrets:
  DATABASE_PASSWORD:
    provider: file
    key: "/run/secrets/db_password"  # Docker secrets pattern

  SSH_DEPLOY_KEY:
    provider: file
    key: "~/.ssh/deploy_key"

commands:
  - shell: "psql postgresql://user:$DATABASE_PASSWORD@localhost/db"
  - shell: "ssh -i $SSH_DEPLOY_KEY deploy@server 'systemctl restart app'"
</code></pre>
<h3 id="integration-with-environment-configuration"><a class="header" href="#integration-with-environment-configuration">Integration with Environment Configuration</a></h3>
<p>Secrets are part of the global <code>EnvironmentConfig</code> structure and work alongside other environment features:</p>
<pre><code class="language-yaml"># Global environment configuration
env:
  NODE_ENV: production
  API_URL: https://api.example.com

# Secrets (masked in logs)
secrets:
  API_KEY:
    provider: env
    key: "SECRET_API_KEY"

# Environment profiles
profiles:
  production:
    NODE_ENV: production
  development:
    NODE_ENV: development

commands:
  - shell: "curl -H 'X-API-Key: $API_KEY' $API_URL/deploy"
</code></pre>
<p><strong>Source</strong>: Structure from <code>src/cook/environment/config.rs:11-36</code></p>
<h3 id="security-best-practices"><a class="header" href="#security-best-practices">Security Best Practices</a></h3>
<ol>
<li>
<p><strong>Never commit secrets to version control</strong></p>
<ul>
<li>Use environment variables or secret files</li>
<li>Add secret files to <code>.gitignore</code></li>
</ul>
</li>
<li>
<p><strong>Use the <code>secrets:</code> field for sensitive data</strong></p>
<ul>
<li>Ensures masking in logs and output</li>
<li>Prevents accidental exposure in error messages</li>
</ul>
</li>
<li>
<p><strong>Prefer environment variables or secure files</strong></p>
<ul>
<li>Only <code>env</code> and <code>file</code> providers are currently implemented</li>
<li>Vault and AWS providers are planned for future releases</li>
</ul>
</li>
<li>
<p><strong>Use profiles for environment-specific secrets</strong></p>
<pre><code class="language-yaml">profiles:
  production:
    DB_HOST: "prod-db.example.com"
  development:
    DB_HOST: "localhost"

secrets:
  DB_PASSWORD:
    provider: env
    key: "DATABASE_PASSWORD"
</code></pre>
</li>
<li>
<p><strong>Test secret resolution in development</strong></p>
<ul>
<li>Verify secrets load correctly before deploying</li>
<li>Use different secret sources per profile</li>
</ul>
</li>
</ol>
<h3 id="troubleshooting-3"><a class="header" href="#troubleshooting-3">Troubleshooting</a></h3>
<h4 id="issue-secret-not-found-in-environment"><a class="header" href="#issue-secret-not-found-in-environment">Issue: “Secret not found in environment”</a></h4>
<p><strong>Cause</strong>: Secret key doesn’t exist in environment variables</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># Verify environment variable exists
echo $SECRET_API_KEY

# Set the variable before running workflow
export SECRET_API_KEY="your-secret-value"
prodigy run workflow.yml
</code></pre>
<h4 id="issue-failed-to-read-secret-file"><a class="header" href="#issue-failed-to-read-secret-file">Issue: “Failed to read secret file”</a></h4>
<p><strong>Cause</strong>: Secret file doesn’t exist or insufficient permissions</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># Verify file exists and is readable
ls -la ~/.ssh/deploy_key

# Fix permissions if needed
chmod 600 ~/.ssh/deploy_key
</code></pre>
<h4 id="issue-secrets-not-masked-in-output"><a class="header" href="#issue-secrets-not-masked-in-output">Issue: Secrets not masked in output</a></h4>
<p><strong>Cause</strong>: Secret not defined in <code>secrets:</code> field</p>
<p><strong>Solution</strong>: Move sensitive variables from <code>env:</code> to <code>secrets:</code> block:</p>
<pre><code class="language-yaml"># Before (NOT masked)
env:
  API_KEY: "${env:SECRET_API_KEY}"

# After (masked)
secrets:
  API_KEY: "${env:SECRET_API_KEY}"
</code></pre>
<h3 id="related-documentation-1"><a class="header" href="#related-documentation-1">Related Documentation</a></h3>
<ul>
<li><a href="environment/environment-profiles.html">Environment Variables</a> - General environment variable configuration</li>
<li><a href="environment/environment-profiles.html">Environment Profiles</a> - Profile-based configuration</li>
<li><a href="environment/environment-precedence.html">Environment Precedence</a> - How environment values are resolved</li>
<li><a href="environment/best-practices.html">Best Practices</a> - Environment configuration best practices</li>
</ul>
<h3 id="implementation-references"><a class="header" href="#implementation-references">Implementation References</a></h3>
<ul>
<li><strong>Configuration Types</strong>: <code>src/cook/environment/config.rs:84-112</code></li>
<li><strong>Secret Resolution</strong>: <code>src/cook/environment/manager.rs:313-337</code></li>
<li><strong>Secret Store</strong>: <code>src/cook/environment/secret_store.rs:26-107</code></li>
<li><strong>Environment Setup</strong>: <code>src/cook/environment/manager.rs:128-136</code></li>
<li><strong>Test Examples</strong>: <code>tests/environment_workflow_test.rs:19-59</code></li>
<li><strong>Workflow Examples</strong>: <code>workflows/environment-example.yml</code>, <code>workflows/mapreduce-env-example.yml</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="environment-profiles-1"><a class="header" href="#environment-profiles-1">Environment Profiles</a></h2>
<p>Environment profiles allow you to define named sets of environment variables for different execution contexts (development, staging, production, etc.). Each profile contains environment variables that are applied when the profile is activated.</p>
<p><strong>Source</strong>: Profile infrastructure implemented in <a href="environment/../../../src/cook/environment/config.rs">src/cook/environment/config.rs</a> (EnvironmentConfig struct) and <a href="environment/../../../src/cook/environment/manager.rs">src/cook/environment/manager.rs</a> (profile application logic).</p>
<h3 id="defining-profiles"><a class="header" href="#defining-profiles">Defining Profiles</a></h3>
<p>Profiles use a flat structure where environment variables are defined directly at the profile level (not nested under an <code>env:</code> key):</p>
<pre><code class="language-yaml"># Define multiple profiles for different environments
profiles:
  development:
    description: "Development environment with debug enabled"
    NODE_ENV: development
    DEBUG: "true"
    API_URL: http://localhost:3000
    LOG_LEVEL: debug

  staging:
    description: "Staging environment for QA"
    NODE_ENV: staging
    DEBUG: "true"
    API_URL: https://staging.api.example.com
    LOG_LEVEL: info

  production:
    description: "Production environment configuration"
    NODE_ENV: production
    DEBUG: "false"
    API_URL: https://api.example.com
    LOG_LEVEL: error

# Global environment variables (apply to all profiles)
env:
  APP_NAME: "my-app"
  VERSION: "1.0.0"

commands:
  - shell: "npm run build"
</code></pre>
<p><strong>Source</strong>: Profile structure defined in <a href="environment/../../../src/cook/environment/config.rs">src/cook/environment/config.rs</a> (EnvProfile type).</p>
<p><strong>Profile Structure Details</strong>:</p>
<ul>
<li><strong>description</strong> (optional): Human-readable description of the profile’s purpose</li>
<li><strong>Environment variables</strong>: Direct key-value pairs at the profile level</li>
<li>All variable values must be strings in YAML</li>
</ul>
<h3 id="activating-profiles"><a class="header" href="#activating-profiles">Activating Profiles</a></h3>
<p><strong>Design Note</strong>: The profile activation infrastructure is architecturally complete in the codebase. The <code>EnvironmentConfig</code> struct has an <code>active_profile</code> field (<a href="environment/../../../src/cook/environment/config.rs">src/cook/environment/config.rs:33-35</a>), and the <code>EnvironmentManager</code> applies active profiles during environment setup (<a href="environment/../../../src/cook/environment/manager.rs">src/cook/environment/manager.rs:118-120</a>). Comprehensive integration tests demonstrate profile activation (<a href="environment/../../../tests/environment_workflow_test.rs">tests/environment_workflow_test.rs:63-132</a>).</p>
<p><strong>Current Implementation Status</strong>: As of this documentation, the CLI wiring for profile activation (<code>--profile</code> flag and <code>PRODIGY_PROFILE</code> environment variable) is not yet connected to the argument parser. The profile application infrastructure exists and is tested, but requires the active profile to be set programmatically rather than via command-line arguments.</p>
<p><strong>Intended Usage</strong> (when CLI wiring is complete):</p>
<pre><code class="language-bash"># Activate profile via command line flag
prodigy run workflow.yml --profile production

# Activate profile via environment variable
export PRODIGY_PROFILE=staging
prodigy run workflow.yml
</code></pre>
<p><strong>Current Workaround</strong>: Profiles can be activated programmatically in tests or by directly setting the <code>active_profile</code> field when constructing <code>EnvironmentConfig</code> objects.</p>
<h3 id="common-use-cases-1"><a class="header" href="#common-use-cases-1">Common Use Cases</a></h3>
<p>Profiles are ideal for managing environment-specific configuration:</p>
<ol>
<li>
<p><strong>Different API Endpoints</strong></p>
<pre><code class="language-yaml">profiles:
  development:
    API_URL: http://localhost:3000
    AUTH_URL: http://localhost:4000

  production:
    API_URL: https://api.example.com
    AUTH_URL: https://auth.example.com
</code></pre>
</li>
<li>
<p><strong>Environment-Specific Credentials</strong></p>
<pre><code class="language-yaml">profiles:
  development:
    DB_HOST: localhost
    DB_NAME: myapp_dev
    DB_USER: dev_user

  production:
    DB_HOST: prod-db.example.com
    DB_NAME: myapp_prod
    DB_USER: prod_user
</code></pre>
</li>
<li>
<p><strong>Deployment Target Configuration</strong></p>
<pre><code class="language-yaml">profiles:
  aws:
    CLOUD_PROVIDER: aws
    REGION: us-east-1
    DEPLOY_COMMAND: "aws deploy"

  gcp:
    CLOUD_PROVIDER: gcp
    REGION: us-central1
    DEPLOY_COMMAND: "gcloud deploy"
</code></pre>
</li>
</ol>
<h3 id="environment-variable-precedence"><a class="header" href="#environment-variable-precedence">Environment Variable Precedence</a></h3>
<p>When a profile is active, environment variables are resolved in this order (highest to lowest precedence):</p>
<ol>
<li><strong>Step-level environment</strong> - Variables defined in individual command <code>env:</code> blocks</li>
<li><strong>Active profile environment</strong> - Variables from the activated profile</li>
<li><strong>Global environment</strong> - Variables from top-level <code>env:</code> block</li>
<li><strong>System environment</strong> - Variables inherited from the shell</li>
</ol>
<p><strong>Source</strong>: Precedence chain implemented in <a href="environment/../../../src/cook/environment/manager.rs">src/cook/environment/manager.rs</a> and tested in <a href="environment/../../../tests/environment_workflow_test.rs">tests/environment_workflow_test.rs</a>.</p>
<p>For detailed information on precedence rules, see <a href="environment/environment-precedence.html">Environment Precedence</a>.</p>
<h3 id="profile-best-practices"><a class="header" href="#profile-best-practices">Profile Best Practices</a></h3>
<p><strong>Define sensible defaults</strong>:</p>
<pre><code class="language-yaml">profiles:
  development:
    description: "Local development with debug enabled"
    DEBUG: "true"
    LOG_LEVEL: debug

  production:
    description: "Production environment with minimal logging"
    DEBUG: "false"
    LOG_LEVEL: error
</code></pre>
<p><strong>Combine with env_files for secrets</strong>:</p>
<pre><code class="language-yaml">profiles:
  production:
    API_URL: https://api.example.com
    DEBUG: "false"

env_files:
  - path: .env.production
    required: true  # Contains secrets like API_KEY
</code></pre>
<p>See <a href="environment/environment-files.html">Environment Files</a> for more on combining profiles with environment files.</p>
<p><strong>Override profile values at step level</strong>:</p>
<pre><code class="language-yaml">profiles:
  production:
    LOG_LEVEL: error

commands:
  - shell: "run-diagnostics.sh"
    env:
      LOG_LEVEL: debug  # Override for this step only
</code></pre>
<p>See <a href="environment/per-command-environment-overrides.html">Per-Command Environment Overrides</a> for step-level overrides.</p>
<h3 id="troubleshooting-4"><a class="header" href="#troubleshooting-4">Troubleshooting</a></h3>
<p><strong>Profile not applied</strong>:</p>
<ul>
<li>Verify profile name matches exactly (case-sensitive)</li>
<li>Check that profile is defined in <code>profiles:</code> section</li>
<li>Confirm profile activation method is used correctly</li>
</ul>
<p><strong>Variables not resolved</strong>:</p>
<ul>
<li>Ensure variable names are correct in profile definition</li>
<li>Check precedence - higher-precedence sources may override profile values</li>
<li>Verify string values are quoted in YAML if they contain special characters</li>
</ul>
<p><strong>See Also</strong>:</p>
<ul>
<li><a href="environment/environment-precedence.html">Environment Precedence</a> - Understanding variable resolution order</li>
<li><a href="environment/environment-files.html">Environment Files</a> - Loading variables from external files</li>
<li><a href="environment/best-practices.html">Best Practices</a> - Recommended patterns for environment configuration</li>
</ul>
<hr />
<div style="break-before: page; page-break-before: always;"></div><h2 id="per-command-environment-overrides"><a class="header" href="#per-command-environment-overrides">Per-Command Environment Overrides</a></h2>
<p><strong>IMPORTANT:</strong> WorkflowStepCommand does NOT have an <code>env</code> field. All per-command environment changes must use shell syntax.</p>
<p><strong>Note:</strong> The legacy Command struct (structured format) has an <code>env</code> field via CommandMetadata, but the modern WorkflowStepCommand format does not. For workflows using the modern <code>claude:</code>/<code>shell:</code> syntax, use shell-level environment syntax (<code>ENV=value command</code>).</p>
<p>You can override environment variables for individual commands using shell environment syntax:</p>
<pre><code class="language-yaml">env:
  RUST_LOG: info
  API_URL: "https://api.example.com"

# Steps go directly in the workflow
- shell: "cargo run"  # Uses RUST_LOG=info from global env

# Override environment for this command only using shell syntax
- shell: "RUST_LOG=debug cargo run --verbose"

# Change directory and set environment in shell
- shell: "cd frontend &amp;&amp; PATH=./node_modules/.bin:$PATH npm run build"
</code></pre>
<h2 id="shell-based-environment-techniques"><a class="header" href="#shell-based-environment-techniques">Shell-Based Environment Techniques</a></h2>
<h3 id="basic-overrides"><a class="header" href="#basic-overrides">Basic Overrides</a></h3>
<ul>
<li><strong>Single variable override:</strong> <code>ENV_VAR=value command</code></li>
<li><strong>Multiple variables:</strong> <code>VAR1=value1 VAR2=value2 command</code></li>
<li><strong>Change directory:</strong> <code>cd path &amp;&amp; command</code></li>
<li><strong>Combine both:</strong> <code>cd path &amp;&amp; ENV_VAR=value command</code></li>
</ul>
<h3 id="advanced-shell-patterns"><a class="header" href="#advanced-shell-patterns">Advanced Shell Patterns</a></h3>
<p>You can combine shell environment overrides with redirection and other shell features:</p>
<pre><code class="language-yaml"># Redirect output while overriding environment
- shell: "RUST_LOG=trace cargo test &gt; test-output.txt 2&gt;&amp;1"

# Pipe commands with environment overrides
- shell: "COLUMNS=120 cargo fmt --check | tee fmt-report.txt"

# Multiple environment variables with complex shell operations
- shell: |
    RUST_BACKTRACE=1 RUST_LOG=debug cargo run \
      --release \
      --bin my-app &gt; app.log 2&gt;&amp;1 || echo "Build failed"
</code></pre>
<p><strong>Source</strong>: Implementation in <code>src/config/command.rs:320-401</code> (WorkflowStepCommand definition)</p>
<h2 id="interaction-with-environment-precedence"><a class="header" href="#interaction-with-environment-precedence">Interaction with Environment Precedence</a></h2>
<p>Shell-level environment overrides take <strong>highest precedence</strong> and apply only to the specific command where they’re defined. These overrides shadow:</p>
<ul>
<li>Global <code>env</code> variables</li>
<li>Profile-specific environment</li>
<li><code>.env</code> file values</li>
<li>System environment variables</li>
</ul>
<p>For detailed precedence rules, see <a href="environment/environment-precedence.html">Environment Precedence</a>.</p>
<h2 id="future-plans-stepenvironment-struct"><a class="header" href="#future-plans-stepenvironment-struct">Future Plans: StepEnvironment Struct</a></h2>
<p>A <code>StepEnvironment</code> struct exists in the internal runtime (defined in <code>src/cook/environment/config.rs:126-144</code>) with support for:</p>
<ul>
<li><code>env</code>: HashMap of environment variables</li>
<li><code>working_dir</code>: Optional working directory override</li>
<li><code>clear_env</code>: Clear parent environment before applying step env</li>
<li><code>temporary</code>: Restore environment after step execution</li>
</ul>
<p>This struct may be exposed in future versions to provide more structured per-step environment control directly in YAML syntax, eliminating the need for shell-based workarounds. However, currently it is <strong>not exposed</strong> in WorkflowStepCommand, so all per-command environment changes must use shell syntax as demonstrated above.</p>
<p><strong>Source</strong>: <code>src/cook/environment/config.rs:126-144</code></p>
<h2 id="troubleshooting-5"><a class="header" href="#troubleshooting-5">Troubleshooting</a></h2>
<h3 id="environment-variable-not-taking-effect"><a class="header" href="#environment-variable-not-taking-effect">Environment Variable Not Taking Effect</a></h3>
<p><strong>Problem</strong>: Shell environment override doesn’t apply to command</p>
<p><strong>Cause</strong>: Quote escaping or shell evaluation order issues</p>
<p><strong>Solution</strong>: Use proper quoting and verify variable expansion:</p>
<pre><code class="language-yaml"># ✓ Correct
- shell: 'API_URL="https://example.com" ./script.sh'

# ✗ Incorrect (quotes broken)
- shell: "API_URL="https://example.com" ./script.sh"
</code></pre>
<h3 id="variable-expansion-issues"><a class="header" href="#variable-expansion-issues">Variable Expansion Issues</a></h3>
<p><strong>Problem</strong>: Variable contains shell special characters (<code>$</code>, <code>\</code>, etc.)</p>
<p><strong>Solution</strong>: Use single quotes to prevent shell expansion:</p>
<pre><code class="language-yaml"># If variable value is literal (no shell expansion needed)
- shell: "PASSWORD='$ecr3t!' ./deploy.sh"
</code></pre>
<h3 id="debugging-environment-resolution"><a class="header" href="#debugging-environment-resolution">Debugging Environment Resolution</a></h3>
<p>To debug which environment values are active:</p>
<pre><code class="language-yaml"># Print all environment variables
- shell: "env | sort"

# Check specific variable resolution
- shell: 'echo "RUST_LOG is: $RUST_LOG"'

# Verify override works
- shell: 'RUST_LOG=trace sh -c "echo RUST_LOG is: $RUST_LOG"'
</code></pre>
<h2 id="see-also-9"><a class="header" href="#see-also-9">See Also</a></h2>
<ul>
<li><a href="environment/environment-precedence.html">Environment Precedence</a> - How environment variables are resolved</li>
<li><a href="environment/environment-profiles.html">Environment Profiles</a> - Named environment configurations</li>
<li><a href="environment/secrets-management.html">Secrets Management</a> - Handling sensitive values</li>
</ul>
<hr />
<div style="break-before: page; page-break-before: always;"></div><h2 id="environment-precedence-2"><a class="header" href="#environment-precedence-2">Environment Precedence</a></h2>
<p>Environment variables are resolved with a clear precedence order, ensuring predictable behavior when the same variable is defined in multiple locations.</p>
<p><strong>Source</strong>: Implementation in <code>src/cook/environment/manager.rs:95-137</code></p>
<h2 id="precedence-order"><a class="header" href="#precedence-order">Precedence Order</a></h2>
<p>Environment variables are applied in the following order (later sources override earlier ones):</p>
<ol>
<li><strong>Parent environment</strong> - Inherited from the parent process</li>
<li><strong>Environment files</strong> - Loaded from <code>env_files</code> (later files override earlier)</li>
<li><strong>Global <code>env</code></strong> - Defined at workflow level in YAML</li>
<li><strong>Active profile</strong> - Applied if a profile is set (internal infrastructure)</li>
<li><strong>Step-specific <code>env</code></strong> - Per-step environment variables</li>
<li><strong>Secrets</strong> - Loaded from secrets configuration (applied after step env)</li>
<li><strong>Shell-level overrides</strong> - Using <code>ENV=value command</code> syntax</li>
</ol>
<p><strong>Source</strong>: Precedence implementation in <code>src/cook/environment/manager.rs:95-137</code></p>
<h2 id="implementation-details"><a class="header" href="#implementation-details">Implementation Details</a></h2>
<h3 id="parent-environment-inheritance"><a class="header" href="#parent-environment-inheritance">Parent Environment Inheritance</a></h3>
<p>By default, workflows inherit all environment variables from the parent process. You can disable this with <code>inherit: false</code> in the environment configuration.</p>
<pre><code class="language-yaml"># Disable parent environment inheritance
inherit: false

env:
  # Only these variables will be available
  NODE_ENV: production
</code></pre>
<p><strong>Source</strong>: Parent environment loading at <code>src/cook/environment/manager.rs:98-102</code></p>
<h3 id="environment-files-precedence"><a class="header" href="#environment-files-precedence">Environment Files Precedence</a></h3>
<p>When multiple environment files are specified, later files override earlier ones. This allows layering of configuration (base + environment-specific).</p>
<pre><code class="language-yaml">env_files:
  - .env              # Base configuration
  - .env.production   # Overrides base values
</code></pre>
<p><strong>Source</strong>: Environment file loading at <code>src/cook/environment/manager.rs:107-109</code></p>
<h3 id="global-environment"><a class="header" href="#global-environment">Global Environment</a></h3>
<p>The global <code>env</code> block at the workflow level overrides both parent environment and environment files.</p>
<pre><code class="language-yaml">env:
  NODE_ENV: production  # Overrides .env files and parent environment
  API_URL: https://api.example.com
</code></pre>
<p><strong>Source</strong>: Global environment application at <code>src/cook/environment/manager.rs:112-115</code></p>
<h3 id="profile-infrastructure"><a class="header" href="#profile-infrastructure">Profile Infrastructure</a></h3>
<p>Prodigy includes internal profile infrastructure that can activate different environment configurations. However, this feature is not currently exposed via CLI flags.</p>
<pre><code class="language-yaml"># Profile infrastructure exists but no --profile CLI flag available
profiles:
  development:
    NODE_ENV: development
    API_URL: http://localhost:3000
</code></pre>
<p><strong>Source</strong>: Profile application at <code>src/cook/environment/manager.rs:118-120</code>; No CLI flag in <code>src/cli/args.rs</code></p>
<h3 id="step-specific-environment"><a class="header" href="#step-specific-environment">Step-Specific Environment</a></h3>
<p><strong>Note</strong>: The YAML command syntax (<code>WorkflowStepCommand</code>) does not expose step-level environment configuration. However, the internal runtime (<code>StepEnvironment</code>) supports it for future extensibility.</p>
<p><strong>Source</strong>:</p>
<ul>
<li><code>StepEnvironment</code> struct at <code>src/cook/environment/config.rs:128-144</code></li>
<li>Step environment application at <code>src/cook/environment/manager.rs:123-127</code></li>
</ul>
<h3 id="secrets-loading"><a class="header" href="#secrets-loading">Secrets Loading</a></h3>
<p>Secrets are loaded AFTER step-specific environment variables, ensuring they cannot be accidentally overridden by step configuration.</p>
<pre><code class="language-yaml">secrets:
  API_KEY: "${env:SECRET_API_KEY}"

# This takes precedence over global env, env_files, and step env
</code></pre>
<p><strong>Source</strong>: Secrets application at <code>src/cook/environment/manager.rs:130-137</code></p>
<h3 id="shell-level-overrides"><a class="header" href="#shell-level-overrides">Shell-Level Overrides</a></h3>
<p>Shell syntax provides the highest precedence override, applied at command execution time.</p>
<pre><code class="language-yaml">- shell: "NODE_ENV=test echo $NODE_ENV"  # Prints: test
</code></pre>
<p>This override is handled by the shell itself and takes precedence over all Prodigy environment sources.</p>
<h2 id="complete-example-4"><a class="header" href="#complete-example-4">Complete Example</a></h2>
<p>Here’s a comprehensive example demonstrating all precedence levels:</p>
<pre><code class="language-yaml"># Parent environment: NODE_ENV=local (inherited by default)

env_files:
  - .env  # Contains: NODE_ENV=development, API_URL=http://localhost:3000

env:
  NODE_ENV: production      # Overrides .env file and parent
  API_URL: https://api.prod.example.com  # Overrides .env file

secrets:
  API_KEY: "${env:SECRET_API_KEY}"  # Loaded after global env

commands:
  - shell: "echo $NODE_ENV"          # Prints: production (from global env)
  - shell: "echo $API_URL"           # Prints: https://api.prod.example.com
  - shell: "echo $API_KEY"           # Prints: *** (masked, from secrets)

  # Override using shell syntax (highest precedence)
  - shell: "NODE_ENV=staging echo $NODE_ENV"  # Prints: staging
</code></pre>
<p><strong>Source</strong>: Real-world example from <code>workflows/environment-example.yml</code></p>
<h2 id="precedence-resolution-flow"><a class="header" href="#precedence-resolution-flow">Precedence Resolution Flow</a></h2>
<p>When resolving an environment variable, Prodigy follows this flow:</p>
<pre><code>1. Start with parent environment (if inherit: true, default)
2. Apply each env_file in order (later files override)
3. Apply global env block (overrides files and parent)
4. Apply active profile if set (internal feature)
5. Apply step-specific env (runtime capability)
6. Apply secrets (highest Prodigy-level precedence)
7. Shell overrides apply at execution time (highest overall)
</code></pre>
<p><strong>Result</strong>: The last value set wins, creating a predictable override chain.</p>
<h2 id="debugging-precedence-issues"><a class="header" href="#debugging-precedence-issues">Debugging Precedence Issues</a></h2>
<p>When troubleshooting which environment source is active:</p>
<pre><code class="language-yaml"># Print all environment variables to debug precedence
- shell: "env | sort"
  capture_output: true

# Print specific variable to verify its value
- shell: "echo NODE_ENV=$NODE_ENV"
</code></pre>
<p>Common debugging scenarios:</p>
<ul>
<li><strong>Variable not set</strong>: Check if parent environment is being inherited</li>
<li><strong>Wrong value</strong>: Check which precedence level last set the variable</li>
<li><strong>Secrets not working</strong>: Verify secrets are loaded after step environment</li>
<li><strong>Override not applying</strong>: Ensure shell syntax is correct</li>
</ul>
<h2 id="related-topics"><a class="header" href="#related-topics">Related Topics</a></h2>
<ul>
<li><a href="environment/environment-files.html">Environment Files</a> - Loading configuration from files</li>
<li><a href="environment/secrets-management.html">Secrets Management</a> - Secure handling of sensitive values</li>
<li><a href="environment/environment-profiles.html">Environment Profiles</a> - Profile infrastructure details</li>
<li><a href="environment/per-command-environment-overrides.html">Per-Command Overrides</a> - Command-level environment control</li>
<li><a href="environment/best-practices.html">Best Practices</a> - Environment configuration recommendations</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="best-practices-10"><a class="header" href="#best-practices-10">Best Practices</a></h2>
<p>This section provides guidelines for effective environment variable management, secrets handling, and profile organization in Prodigy workflows.</p>
<h3 id="environment-variable-usage"><a class="header" href="#environment-variable-usage">Environment Variable Usage</a></h3>
<p><strong>When to use environment variables:</strong></p>
<ul>
<li>Configuration values that change between environments (dev/staging/prod)</li>
<li>Parameterization for reusable workflows</li>
<li>Non-sensitive API endpoints and URLs</li>
<li>Timeouts, limits, and resource constraints</li>
<li>Project-specific paths and file locations</li>
</ul>
<p><strong>When to use profiles instead:</strong></p>
<ul>
<li>Environment-specific configuration sets (dev/staging/prod)</li>
<li>Multiple related values that change together</li>
<li>When you need to switch entire configuration contexts</li>
</ul>
<p><strong>When to use env_files instead:</strong></p>
<ul>
<li>Loading many variables from external sources</li>
<li>Sharing configuration across multiple workflows</li>
<li>Managing .env files in version control (without secrets)</li>
<li>Local development overrides (.env.local)</li>
</ul>
<p><strong>When to use step-level env overrides:</strong></p>
<ul>
<li>Command-specific configuration that overrides global/profile values</li>
<li>Per-step resource limits or timeouts</li>
<li>Temporary environment changes for individual commands</li>
<li>Testing different configurations in the same workflow</li>
</ul>
<p>See <a href="environment/per-command-environment-overrides.html">Per-Command Environment Overrides</a> for detailed step-level env documentation.</p>
<h3 id="naming-conventions"><a class="header" href="#naming-conventions">Naming Conventions</a></h3>
<p><strong>Follow these naming conventions for clarity:</strong></p>
<ol>
<li><strong>Use UPPERCASE with underscores</strong>: <code>API_URL</code>, <code>MAX_WORKERS</code>, <code>DATABASE_URL</code></li>
<li><strong>Be descriptive</strong>: <code>AGENT_TIMEOUT</code> not <code>TIMEOUT</code>, <code>API_BASE_URL</code> not <code>URL</code></li>
<li><strong>Use prefixes for grouping</strong>: <code>DB_HOST</code>, <code>DB_PORT</code>, <code>DB_NAME</code> or <code>REDIS_HOST</code>, <code>REDIS_PORT</code></li>
<li><strong>Avoid abbreviations</strong>: <code>PROJECT_NAME</code> not <code>PROJ_NM</code>, <code>ENVIRONMENT</code> not <code>ENV</code></li>
</ol>
<p><strong>Examples:</strong></p>
<pre><code class="language-yaml">env:
  # Good - Clear and descriptive
  PROJECT_NAME: my-project
  API_BASE_URL: https://api.example.com
  MAX_PARALLEL_WORKERS: "10"
  CLAUDE_COMMAND_TIMEOUT: "600"

  # Avoid - Unclear or abbreviated
  PROJ: my-project
  URL: https://api.example.com
  MAX: "10"
  TIMEOUT: "600"
</code></pre>
<h3 id="secrets-management-guidelines"><a class="header" href="#secrets-management-guidelines">Secrets Management Guidelines</a></h3>
<p><strong>Critical security practices:</strong></p>
<ol>
<li>
<p><strong>NEVER commit secrets to version control</strong></p>
<ul>
<li>Add <code>.env.secrets</code>, <code>.env.local</code> to <code>.gitignore</code></li>
<li>Use secret providers (vault, aws-secrets) for production</li>
<li>Rotate secrets regularly</li>
</ul>
</li>
<li>
<p><strong>Always use the secrets block for sensitive values</strong></p>
<pre><code class="language-yaml">secrets:
  # Simple format - loads from environment variable
  API_KEY: "my-secret-value"  # Masked in all logs

  # Provider format - explicitly specify source
  DATABASE_PASSWORD:
    provider: env
    key: DB_PASSWORD  # Retrieves from $DB_PASSWORD env var
</code></pre>
</li>
<li>
<p><strong>Use external secret providers</strong></p>
<p><strong>Currently implemented providers</strong> (src/cook/environment/secret_store.rs:35-46):</p>
<ul>
<li><code>env</code> - Load from environment variable (EnvSecretProvider)</li>
<li><code>file</code> - Load from file path (FileSecretProvider)</li>
</ul>
<pre><code class="language-yaml">secrets:
  # Load from environment variable
  API_KEY:
    provider: env
    key: SECRET_API_KEY

  # Load from file
  DATABASE_PASSWORD:
    provider: file
    key: /etc/secrets/db-password
</code></pre>
<p><strong>Planned providers</strong> (not yet implemented):</p>
<ul>
<li><code>vault</code> - HashiCorp Vault integration (declared in config.rs:106)</li>
<li><code>aws</code> - AWS Secrets Manager (declared in config.rs:108)</li>
<li><code>custom</code> - Custom provider support (declared in config.rs:110)</li>
</ul>
</li>
<li>
<p><strong>Minimize secret exposure</strong></p>
<ul>
<li>Only expose secrets to commands that need them</li>
<li>Use short-lived tokens when possible</li>
<li>Use <code>file</code> provider to avoid secrets in environment variables</li>
</ul>
</li>
<li>
<p><strong>Verify secret masking</strong></p>
<ul>
<li>Check logs to ensure secrets are masked (appear as <code>***</code>)</li>
<li>Test with <code>-v</code> verbose mode to confirm masking works</li>
</ul>
</li>
</ol>
<p><strong>Secret organization pattern:</strong></p>
<pre><code class="language-yaml"># Good - Organized with file-based secrets
env_files:
  - .env              # Non-sensitive config
  - .env.local        # Local overrides (gitignored)

secrets:
  # Production secrets from files (managed by deployment system)
  DATABASE_PASSWORD:
    provider: file
    key: /run/secrets/db-password

  API_KEY:
    provider: file
    key: /run/secrets/api-key

  # Development secrets from environment (for local testing)
  DEV_API_KEY:
    provider: env
    key: DEV_API_KEY  # Loaded from environment, still masked in logs
</code></pre>
<p><strong>Source</strong>: Secret provider implementations in src/cook/environment/secret_store.rs:34-148</p>
<h3 id="profile-organization-strategies"><a class="header" href="#profile-organization-strategies">Profile Organization Strategies</a></h3>
<p><strong>Multi-environment structure:</strong></p>
<p>Use profiles to manage dev/staging/prod configurations:</p>
<pre><code class="language-yaml">env:
  # Common values shared across all environments
  PROJECT_NAME: my-project
  VERSION: "1.0.0"

profiles:
  dev:
    API_URL: http://localhost:3000
    MAX_WORKERS: "2"
    TIMEOUT: "60"
    DEBUG: "true"

  staging:
    API_URL: https://staging-api.example.com
    MAX_WORKERS: "10"
    TIMEOUT: "30"
    DEBUG: "true"

  prod:
    API_URL: https://api.example.com
    MAX_WORKERS: "20"
    TIMEOUT: "30"
    DEBUG: "false"
</code></pre>
<p><strong>Activate with:</strong> <code>prodigy run workflow.yml --profile prod</code></p>
<p><strong>Base + override pattern:</strong></p>
<pre><code class="language-yaml">env:
  # Base/default values
  API_URL: http://localhost:3000
  MAX_WORKERS: "5"
  CACHE_ENABLED: "true"

profiles:
  prod:
    # Only override what changes in prod
    API_URL: https://api.production.com
    MAX_WORKERS: "20"
    # CACHE_ENABLED inherits "true" from base
</code></pre>
<h3 id="avoiding-common-pitfalls"><a class="header" href="#avoiding-common-pitfalls">Avoiding Common Pitfalls</a></h3>
<p><strong>1. Variable name conflicts:</strong></p>
<pre><code class="language-yaml"># Problem - Confusing overlap with shell variables
env:
  PATH: /custom/bin  # Conflicts with system PATH
  HOME: /app         # Conflicts with system HOME

# Solution - Use prefixed names
env:
  APP_PATH: /custom/bin
  APP_HOME: /app
</code></pre>
<p><strong>2. Precedence confusion:</strong></p>
<p>Remember the order (highest to lowest priority):</p>
<ol>
<li><strong>Step-level env</strong> - Per-command overrides (see <a href="environment/per-command-environment-overrides.html">Per-Command Environment Overrides</a>)</li>
<li><strong>Profile values</strong> - When profile is active via <code>--profile</code> flag</li>
<li><strong>Global <code>env</code> field</strong> - Workflow-level environment variables</li>
<li><strong>Environment files</strong> - Later files override earlier ones (from <code>env_files</code>)</li>
<li><strong>Parent process environment</strong> - Inherited from shell (if <code>inherit: true</code>)</li>
</ol>
<p><strong>Source</strong>: Implementation in src/cook/environment/manager.rs:88-156</p>
<p>For detailed explanation of precedence rules, see <a href="environment/environment-precedence.html">Environment Precedence</a>.</p>
<p><strong>3. Forgetting to mark secrets:</strong></p>
<pre><code class="language-yaml"># Problem - Secret exposed in logs
env_files:
  - .env.secrets  # Contains API_KEY=sk-abc123

commands:
  - shell: "curl -H 'Authorization: Bearer $API_KEY' ..."
  # API_KEY appears in logs!

# Solution - Use secrets block to mask in logs
secrets:
  API_KEY:
    provider: env
    key: SECRET_API_KEY  # Retrieves from environment, masked in all output
</code></pre>
<p><strong>4. Hardcoding environment-specific values:</strong></p>
<pre><code class="language-yaml"># Problem - Not reusable across environments
commands:
  - shell: "curl https://api.production.com/data"
  - shell: "timeout 30 ./process.sh"

# Solution - Use environment variables
env:
  API_URL: https://api.production.com
  TIMEOUT: "30"

commands:
  - shell: "curl $API_URL/data"
  - shell: "timeout $TIMEOUT ./process.sh"
</code></pre>
<h3 id="documentation-practices"><a class="header" href="#documentation-practices">Documentation Practices</a></h3>
<p><strong>Document your environment configuration:</strong></p>
<ol>
<li>
<p><strong>List required variables</strong> in README or workflow comments:</p>
<pre><code class="language-yaml"># Required environment variables:
# - API_URL: Base URL for API endpoints
# - MAX_WORKERS: Number of parallel workers (default: 5)
# - TIMEOUT: Command timeout in seconds (default: 30)

env:
  API_URL: https://api.example.com
  MAX_WORKERS: "5"
  TIMEOUT: "30"
</code></pre>
</li>
<li>
<p><strong>Provide example .env file</strong>:</p>
<pre><code class="language-bash"># .env.example (safe to commit)
API_URL=https://api.example.com
MAX_WORKERS=5
TIMEOUT=30

# Copy to .env and fill in values:
# cp .env.example .env
</code></pre>
</li>
<li>
<p><strong>Document profile usage</strong>:</p>
<pre><code class="language-yaml"># Profiles available:
# - dev: Local development (low resource usage)
# - staging: Staging environment (medium resources)
# - prod: Production environment (high resources)
#
# Usage: prodigy run workflow.yml --profile prod
</code></pre>
</li>
</ol>
<h3 id="testing-environment-configurations"><a class="header" href="#testing-environment-configurations">Testing Environment Configurations</a></h3>
<p><strong>Validate your environment setup:</strong></p>
<ol>
<li>
<p><strong>Test with dry-run mode</strong> (if available)</p>
</li>
<li>
<p><strong>Verify variable interpolation</strong>:</p>
<pre><code class="language-yaml">commands:
  - shell: "echo 'API_URL: $API_URL'"
  - shell: "echo 'MAX_WORKERS: $MAX_WORKERS'"
  - shell: "echo 'TIMEOUT: $TIMEOUT'"
</code></pre>
</li>
<li>
<p><strong>Test each profile</strong>:</p>
<pre><code class="language-bash">prodigy run workflow.yml --profile dev
prodigy run workflow.yml --profile staging
prodigy run workflow.yml --profile prod
</code></pre>
</li>
<li>
<p><strong>Verify secret masking</strong>:</p>
<ul>
<li>Run with <code>-v</code> verbose mode</li>
<li>Check that secrets appear as <code>***</code> in output</li>
<li>Verify secrets don’t leak in error messages</li>
</ul>
</li>
</ol>
<h3 id="environment-variable-composition"><a class="header" href="#environment-variable-composition">Environment Variable Composition</a></h3>
<p><strong>Layered configuration strategy:</strong></p>
<pre><code class="language-yaml"># Layer 1: Base configuration (committed)
env_files:
  - .env

# Layer 2: Local overrides (gitignored)
env_files:
  - .env.local

# Layer 3: Global workflow values
env:
  PROJECT_NAME: my-project
  VERSION: "1.0.0"

# Layer 4: Profile-specific overrides
profiles:
  prod:
    MAX_WORKERS: "20"
    TIMEOUT: "30"

# Layer 5: Step-level overrides (per-command)
commands:
  - shell: "process-data"
    env:
      TIMEOUT: "60"  # Override for this command only

# Layer 6: Secrets (separate management)
secrets:
  API_KEY:
    provider: file
    key: /run/secrets/api-key
</code></pre>
<p><strong>Source</strong>: Precedence implementation in src/cook/environment/manager.rs:88-156</p>
<p><strong>Benefits:</strong></p>
<ul>
<li>Clear separation of concerns</li>
<li>Easy to understand precedence</li>
<li>Secure secret handling</li>
<li>Flexible environment switching</li>
<li>Local development friendly</li>
</ul>
<h3 id="performance-considerations-1"><a class="header" href="#performance-considerations-1">Performance Considerations</a></h3>
<p><strong>Use environment variables to optimize performance:</strong></p>
<ol>
<li>
<p><strong>Parameterize resource limits</strong>:</p>
<pre><code class="language-yaml">env:
  MAX_WORKERS: "10"      # Tune based on CPU cores
  MEMORY_LIMIT: "8G"     # Tune based on available RAM
  TIMEOUT: "300"         # Tune based on expected duration
</code></pre>
</li>
<li>
<p><strong>Environment-specific optimizations</strong>:</p>
<pre><code class="language-yaml">profiles:
  dev:
    MAX_WORKERS: "2"     # Low resource usage for local dev
    CACHE_ENABLED: "false"  # Disable caching for faster iteration

  prod:
    MAX_WORKERS: "20"    # High throughput for production
    CACHE_ENABLED: "true"   # Enable caching for performance
</code></pre>
</li>
<li>
<p><strong>Avoid expensive operations in variable expansion</strong>:</p>
<pre><code class="language-yaml"># Problem - Runs command on every variable access
env:
  TIMESTAMP: "$(date +%s)"  # Evaluated once, not per use

# Better - Capture once if dynamic value needed
commands:
  - shell: "date +%s"
    capture_output: TIMESTAMP
</code></pre>
</li>
</ol>
<h3 id="security-checklist"><a class="header" href="#security-checklist">Security Checklist</a></h3>
<p>Before deploying workflows to production:</p>
<ul>
<li><input disabled="" type="checkbox"/>
All secrets defined in <code>secrets</code> block for masking</li>
<li><input disabled="" type="checkbox"/>
Secret files in <code>.gitignore</code></li>
<li><input disabled="" type="checkbox"/>
Production secrets use <code>file</code> provider (vault/aws planned for future)</li>
<li><input disabled="" type="checkbox"/>
Verified secrets masked in logs with <code>-v</code> mode</li>
<li><input disabled="" type="checkbox"/>
No hardcoded credentials in workflow YAML</li>
<li><input disabled="" type="checkbox"/>
Environment variables documented in README</li>
<li><input disabled="" type="checkbox"/>
<code>.env.example</code> provided for team</li>
<li><input disabled="" type="checkbox"/>
Tested all profiles work correctly</li>
<li><input disabled="" type="checkbox"/>
No sensitive data in environment variable names</li>
<li><input disabled="" type="checkbox"/>
Secrets rotated regularly</li>
<li><input disabled="" type="checkbox"/>
File-based secrets have restricted permissions (chmod 600)</li>
</ul>
<p>See also:</p>
<ul>
<li><a href="environment/environment-precedence.html">Environment Precedence</a> for understanding resolution order</li>
<li><a href="environment/secrets-management.html">Secrets Management</a> for detailed secret handling</li>
<li><a href="environment/environment-profiles.html">Environment Profiles</a> for profile configuration</li>
<li><a href="environment/common-patterns.html">Common Patterns</a> for practical examples</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="common-patterns-2"><a class="header" href="#common-patterns-2">Common Patterns</a></h2>
<p>This section provides practical patterns and realistic examples for environment configuration in Prodigy workflows. All examples are validated against the actual implementation and real workflow files.</p>
<blockquote>
<p><strong>Source References</strong>: Examples based on:</p>
<ul>
<li>src/config/workflow.rs:12-39 (WorkflowConfig structure)</li>
<li>src/cook/environment/config.rs:12-144 (Environment configuration types)</li>
<li>workflows/mapreduce-env-example.yml (Complete working example)</li>
</ul>
</blockquote>
<h3 id="multi-environment-deployment-pattern"><a class="header" href="#multi-environment-deployment-pattern">Multi-Environment Deployment Pattern</a></h3>
<p>Use profiles to manage different deployment environments with environment-specific configurations.</p>
<blockquote>
<p><strong>Profile Support</strong>: Profiles provide environment-specific variable overrides (src/cook/environment/config.rs:116-124). Activate with <code>--profile &lt;name&gt;</code> flag.</p>
</blockquote>
<pre><code class="language-yaml">name: multi-env-deployment
mode: mapreduce

env:
  # Shared across all environments
  PROJECT_NAME: my-service
  VERSION: "2.1.0"
  LOG_LEVEL: info

profiles:
  dev:
    API_URL: http://localhost:3000
    DATABASE_URL: postgresql://localhost:5432/dev
    MAX_WORKERS: "2"
    CACHE_TTL: "60"
    DEBUG: "true"

  staging:
    API_URL: https://staging-api.example.com
    DATABASE_URL: postgresql://staging-db:5432/app
    MAX_WORKERS: "10"
    CACHE_TTL: "300"
    DEBUG: "true"

  prod:
    API_URL: https://api.example.com
    DATABASE_URL: postgresql://prod-db:5432/app
    MAX_WORKERS: "20"
    CACHE_TTL: "3600"
    DEBUG: "false"

secrets:
  # Secrets from environment variables (supported)
  DATABASE_PASSWORD:
    provider: env
    key: DB_PASSWORD

commands:
  - shell: "echo 'Deploying $PROJECT_NAME v$VERSION to $PROFILE environment'"
  - shell: "echo 'API: $API_URL, Workers: $MAX_WORKERS'"
  - shell: "deploy.sh --env $PROFILE --version $VERSION"
</code></pre>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">prodigy run deploy.yml --profile dev
prodigy run deploy.yml --profile staging
prodigy run deploy.yml --profile prod
</code></pre>
<h3 id="secrets-management-pattern"><a class="header" href="#secrets-management-pattern">Secrets Management Pattern</a></h3>
<p>Combine env files with secret providers for secure credential management:</p>
<blockquote>
<p><strong>Currently Supported Providers</strong> (src/cook/environment/secret_store.rs:40-41):</p>
<ul>
<li><code>env</code> - Environment variables</li>
<li><code>file</code> - File-based secrets</li>
</ul>
<p><strong>Planned Providers</strong> (defined but not yet implemented):</p>
<ul>
<li><code>vault</code> - HashiCorp Vault integration</li>
<li><code>aws</code> - AWS Secrets Manager</li>
<li><code>custom</code> - Custom provider support</li>
</ul>
</blockquote>
<pre><code class="language-yaml">name: secure-workflow

# Load non-sensitive config from files
env_files:
  - .env              # Base configuration (committed)
  - .env.local        # Local overrides (gitignored)

env:
  SERVICE_NAME: payment-processor
  REGION: us-west-2

secrets:
  # Secrets from environment variables (currently supported)
  AWS_ACCESS_KEY:
    provider: env
    key: AWS_ACCESS_KEY_ID

  AWS_SECRET_KEY:
    provider: env
    key: AWS_SECRET_ACCESS_KEY

  # Secrets from files (currently supported)
  DATABASE_URL:
    provider: file
    key: /path/to/secrets/database_url.txt

  # Simple secret reference (loaded from env)
  THIRD_PARTY_API_KEY: "${THIRD_PARTY_API_KEY}"

commands:
  - shell: "aws s3 ls --region $REGION"  # Uses AWS credentials
  - shell: "psql $DATABASE_URL -c 'SELECT version()'"
  - shell: "curl -H 'Authorization: Bearer ***' https://api.example.com"
</code></pre>
<p><strong>.env file (committed):</strong></p>
<pre><code class="language-bash">SERVICE_NAME=payment-processor
REGION=us-west-2
LOG_LEVEL=info
</code></pre>
<p><strong>.env.local file (gitignored):</strong></p>
<pre><code class="language-bash">THIRD_PARTY_API_KEY=sk-local-dev-key
DATABASE_URL=postgresql://localhost:5432/dev
</code></pre>
<h3 id="profile-based-configuration-strategy"><a class="header" href="#profile-based-configuration-strategy">Profile-Based Configuration Strategy</a></h3>
<p>Base configuration with environment-specific overrides:</p>
<pre><code class="language-yaml">name: profile-based-config

env:
  # Base configuration (defaults)
  PROJECT_NAME: analytics-pipeline
  MAX_WORKERS: "5"
  BATCH_SIZE: "100"
  TIMEOUT: "300"
  RETRY_ATTEMPTS: "3"
  CACHE_ENABLED: "true"

profiles:
  # Local development - minimal resources
  dev:
    MAX_WORKERS: "2"
    BATCH_SIZE: "10"
    TIMEOUT: "600"        # Longer timeout for debugging
    CACHE_ENABLED: "false"  # Disable caching for fresh data

  # CI/CD environment - controlled resources
  ci:
    MAX_WORKERS: "4"
    BATCH_SIZE: "50"
    TIMEOUT: "300"
    RETRY_ATTEMPTS: "1"  # Fail fast in CI

  # Production - optimized for throughput
  prod:
    MAX_WORKERS: "20"
    BATCH_SIZE: "500"
    TIMEOUT: "180"       # Strict timeout
    RETRY_ATTEMPTS: "5"  # More retries for resilience
    CACHE_ENABLED: "true"

map:
  input: data.json
  max_parallel: ${MAX_WORKERS}
  agent_timeout_secs: ${TIMEOUT}

  agent_template:
    - shell: "process-batch.sh --size $BATCH_SIZE --retries $RETRY_ATTEMPTS"
</code></pre>
<h3 id="environment-variable-composition-pattern"><a class="header" href="#environment-variable-composition-pattern">Environment Variable Composition Pattern</a></h3>
<p>Layer configuration from multiple sources with clear precedence (src/cook/environment/config.rs:12-36):</p>
<blockquote>
<p><strong>Note</strong>: env_files paths are static and don’t support variable interpolation. Use profiles to handle environment-specific file loading.</p>
</blockquote>
<pre><code class="language-yaml">name: layered-config

# Layer 1: Base defaults and local overrides
env_files:
  - .env              # Base configuration
  - .env.local        # Local overrides (if exists)

# Layer 4: Global workflow values (override env files)
env:
  WORKFLOW_VERSION: "3.0.0"
  EXECUTION_MODE: standard

# Layer 5: Profile values (override everything when active)
profiles:
  prod:
    EXECUTION_MODE: high-performance
    MAX_WORKERS: "50"

# Layer 6: Secrets (separate layer for security)
secrets:
  API_TOKEN:
    provider: vault
    key: secret/data/api-token

commands:
  - shell: "echo 'Mode: $EXECUTION_MODE, Workers: $MAX_WORKERS'"
</code></pre>
<p><strong>Precedence Order</strong> (highest to lowest):</p>
<ol>
<li>Profile variables (when profile is active)</li>
<li>Global <code>env</code> variables</li>
<li>Variables from <code>env_files</code> (later files override earlier)</li>
<li>Inherited system environment variables</li>
</ol>
<p><strong>File structure:</strong></p>
<pre><code>.env                  # Base: MAX_WORKERS=5, API_URL=http://localhost
.env.local           # Local: MAX_WORKERS=2 (overrides .env)
</code></pre>
<h3 id="cicd-integration-pattern"><a class="header" href="#cicd-integration-pattern">CI/CD Integration Pattern</a></h3>
<p>Use environment variables to make workflows portable across CI/CD systems.</p>
<blockquote>
<p><strong>Conditional Execution</strong>: Commands support the <code>when</code> field for conditional execution based on environment variables (src/config/command.rs:388).</p>
</blockquote>
<pre><code class="language-yaml">name: ci-cd-workflow

env:
  # CI/CD environment detection
  CI_MODE: "${CI:-false}"                    # GitHub Actions, GitLab CI set CI=true
  BUILD_NUMBER: "${BUILD_NUMBER:-local}"     # Jenkins BUILD_NUMBER
  COMMIT_SHA: "${GITHUB_SHA:-unknown}"       # GitHub Actions
  BRANCH_NAME: "${BRANCH_NAME:-main}"        # Can be set by CI

  # Resource limits for CI
  MAX_WORKERS: "${CI_MAX_WORKERS:-5}"
  TIMEOUT: "${CI_TIMEOUT:-300}"

  # Paths
  ARTIFACT_DIR: "${WORKSPACE:-./artifacts}"
  CACHE_DIR: "${CACHE_DIR:-./cache}"

commands:
  - shell: "echo 'CI Mode: $CI_MODE, Build: $BUILD_NUMBER'"
  - shell: "echo 'Branch: $BRANCH_NAME, Commit: $COMMIT_SHA'"

  - shell: "cargo build --release"
    when: "${CI_MODE} == 'true'"

  - shell: "cargo test --all"
    timeout: ${TIMEOUT}

  - shell: "mkdir -p $ARTIFACT_DIR"
  - shell: "cp target/release/app $ARTIFACT_DIR/"
</code></pre>
<p><strong>GitHub Actions example:</strong></p>
<pre><code class="language-yaml">- name: Run Prodigy workflow
  env:
    CI_MAX_WORKERS: 10
    CI_TIMEOUT: 600
  run: prodigy run workflow.yml
</code></pre>
<p><strong>Jenkins example:</strong></p>
<pre><code class="language-groovy">environment {
  CI_MAX_WORKERS = '10'
  CI_TIMEOUT = '600'
}
steps {
  sh 'prodigy run workflow.yml'
}
</code></pre>
<h3 id="local-development-pattern"><a class="header" href="#local-development-pattern">Local Development Pattern</a></h3>
<p>Optimize for local development with overridable defaults:</p>
<pre><code class="language-yaml">name: dev-friendly-workflow

env_files:
  - .env                # Base config (committed)
  - .env.local          # Personal settings (gitignored)

env:
  # Development defaults
  API_URL: http://localhost:3000
  DATABASE_URL: postgresql://localhost:5432/dev
  REDIS_URL: redis://localhost:6379

  # Resource limits for local dev
  MAX_WORKERS: "2"
  TIMEOUT: "60"

  # Feature flags
  ENABLE_CACHING: "false"
  ENABLE_ANALYTICS: "false"
  DEBUG_MODE: "true"

profiles:
  # Personal override for more powerful dev machines
  high-perf:
    MAX_WORKERS: "8"
    ENABLE_CACHING: "true"

commands:
  - shell: "echo 'Development mode: Debug=$DEBUG_MODE'"
  - shell: "docker-compose up -d"
    when: "${DATABASE_URL} =~ 'localhost'"

  - shell: "cargo run --bin migrate"
  - shell: "cargo test"
</code></pre>
<p><strong>.env.local.example (committed as template):</strong></p>
<pre><code class="language-bash"># Copy to .env.local and customize for your machine

# Override API endpoint for local backend
# API_URL=http://localhost:8080

# Use more workers if you have powerful CPU
# MAX_WORKERS=4

# Enable features for testing
# ENABLE_CACHING=true
# ENABLE_ANALYTICS=true
</code></pre>
<h3 id="template-parameterization-pattern"><a class="header" href="#template-parameterization-pattern">Template Parameterization Pattern</a></h3>
<p>Create reusable workflows parameterized with environment variables:</p>
<pre><code class="language-yaml">name: reusable-test-workflow

env:
  # Required parameters (set by caller or env)
  PROJECT_NAME: "${PROJECT_NAME}"           # Must be provided
  TEST_SUITE: "${TEST_SUITE:-all}"         # Default: all
  COVERAGE_THRESHOLD: "${COVERAGE_THRESHOLD:-80}"

  # Optional customization
  PARALLEL_JOBS: "${PARALLEL_JOBS:-5}"
  TIMEOUT: "${TIMEOUT:-300}"
  REPORT_FORMAT: "${REPORT_FORMAT:-json}"

commands:
  - shell: "echo 'Testing $PROJECT_NAME: $TEST_SUITE suite'"

  - shell: "cargo test --workspace"
    when: "${TEST_SUITE} == 'all'"
    timeout: ${TIMEOUT}

  - shell: "cargo test --package $PROJECT_NAME"
    when: "${TEST_SUITE} == 'unit'"

  - shell: "cargo tarpaulin --out $REPORT_FORMAT --output-dir coverage"
    capture_output: coverage_result
    capture_format: json

  - shell: |
      COVERAGE=$(echo '$coverage_result' | jq '.coverage')
      if (( $(echo "$COVERAGE &lt; $COVERAGE_THRESHOLD" | bc -l) )); then
        echo "Coverage $COVERAGE% below threshold $COVERAGE_THRESHOLD%"
        exit 1
      fi
</code></pre>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash"># With environment variables
PROJECT_NAME=my-app TEST_SUITE=unit prodigy run test-workflow.yml

# Or with .env file
echo "PROJECT_NAME=my-app" &gt; .env
echo "TEST_SUITE=integration" &gt;&gt; .env
echo "COVERAGE_THRESHOLD=90" &gt;&gt; .env
prodigy run test-workflow.yml
</code></pre>
<h3 id="regional-configuration-pattern"><a class="header" href="#regional-configuration-pattern">Regional Configuration Pattern</a></h3>
<p>Deploy to different regions with region-specific settings:</p>
<pre><code class="language-yaml">name: multi-region-deployment

env:
  SERVICE_NAME: api-gateway
  VERSION: "1.2.0"

profiles:
  us-west:
    REGION: us-west-2
    API_ENDPOINT: https://api-usw.example.com
    S3_BUCKET: my-app-usw-artifacts
    MAX_INSTANCES: "10"

  us-east:
    REGION: us-east-1
    API_ENDPOINT: https://api-use.example.com
    S3_BUCKET: my-app-use-artifacts
    MAX_INSTANCES: "20"

  eu-west:
    REGION: eu-west-1
    API_ENDPOINT: https://api-euw.example.com
    S3_BUCKET: my-app-euw-artifacts
    MAX_INSTANCES: "15"

secrets:
  # AWS credentials from environment (currently supported)
  AWS_ACCESS_KEY:
    provider: env
    key: AWS_ACCESS_KEY_ID
  AWS_SECRET_KEY:
    provider: env
    key: AWS_SECRET_ACCESS_KEY

commands:
  - shell: "echo 'Deploying to $REGION'"
  - shell: "aws s3 cp ./artifact.zip s3://$S3_BUCKET/$VERSION/ --region $REGION"
  - shell: "deploy-to-region.sh --region $REGION --instances $MAX_INSTANCES"
</code></pre>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">prodigy run deploy.yml --profile us-west
prodigy run deploy.yml --profile eu-west
</code></pre>
<h3 id="feature-flag-pattern"><a class="header" href="#feature-flag-pattern">Feature Flag Pattern</a></h3>
<p>Use environment variables to control feature availability:</p>
<pre><code class="language-yaml">name: feature-flag-workflow

env:
  # Feature flags
  ENABLE_NEW_PIPELINE: "${ENABLE_NEW_PIPELINE:-false}"
  ENABLE_EXPERIMENTAL: "${ENABLE_EXPERIMENTAL:-false}"
  ENABLE_BETA_FEATURES: "${ENABLE_BETA_FEATURES:-false}"

  # Version-based features
  MIN_VERSION: "2.0.0"
  CURRENT_VERSION: "2.1.0"

profiles:
  canary:
    ENABLE_EXPERIMENTAL: "true"

  beta:
    ENABLE_BETA_FEATURES: "true"

  production:
    ENABLE_NEW_PIPELINE: "true"
    ENABLE_EXPERIMENTAL: "false"
    ENABLE_BETA_FEATURES: "false"

commands:
  - shell: "run-legacy-pipeline.sh"
    when: "${ENABLE_NEW_PIPELINE} == 'false'"

  - shell: "run-new-pipeline.sh"
    when: "${ENABLE_NEW_PIPELINE} == 'true'"

  - shell: "run-experimental-features.sh"
    when: "${ENABLE_EXPERIMENTAL} == 'true'"

  - shell: "validate-version.sh --min $MIN_VERSION --current $CURRENT_VERSION"
</code></pre>
<h3 id="complete-real-world-example"><a class="header" href="#complete-real-world-example">Complete Real-World Example</a></h3>
<p>Combining multiple patterns for a production-ready workflow:</p>
<pre><code class="language-yaml">name: production-data-pipeline
mode: mapreduce

# Layer 1: Base configuration
env_files:
  - .env
  - .env.${ENVIRONMENT}

# Layer 2: Global settings
env:
  PROJECT_NAME: data-pipeline
  VERSION: "3.0.0"
  ENVIRONMENT: "${ENVIRONMENT:-dev}"

# Layer 3: Environment profiles
profiles:
  dev:
    DATA_SOURCE: s3://dev-data-bucket
    OUTPUT_PATH: s3://dev-results-bucket
    MAX_WORKERS: "5"
    BATCH_SIZE: "100"
    ENABLE_MONITORING: "false"

  staging:
    DATA_SOURCE: s3://staging-data-bucket
    OUTPUT_PATH: s3://staging-results-bucket
    MAX_WORKERS: "15"
    BATCH_SIZE: "500"
    ENABLE_MONITORING: "true"

  prod:
    DATA_SOURCE: s3://prod-data-bucket
    OUTPUT_PATH: s3://prod-results-bucket
    MAX_WORKERS: "50"
    BATCH_SIZE: "1000"
    ENABLE_MONITORING: "true"

# Layer 4: Secrets
secrets:
  # Currently supported: env and file providers
  AWS_ACCESS_KEY:
    provider: env
    key: AWS_ACCESS_KEY_ID

  DATABASE_URL:
    provider: file
    key: /secrets/${ENVIRONMENT}/database_url.txt

setup:
  - shell: "echo 'Starting $PROJECT_NAME v$VERSION in $ENVIRONMENT'"
  - shell: "aws s3 ls $DATA_SOURCE --region us-west-2"
  - shell: "generate-work-items.sh --source $DATA_SOURCE --batch-size $BATCH_SIZE &gt; items.json"

map:
  input: items.json
  max_parallel: ${MAX_WORKERS}

  agent_template:
    - shell: "process-batch.sh --input ${item.path} --output $OUTPUT_PATH/${item.id}.result"
    - shell: "validate-result.sh $OUTPUT_PATH/${item.id}.result"
      on_failure:
        shell: "log-failure.sh ${item.id}"

reduce:
  - shell: "echo 'Processed ${map.successful}/${map.total} batches'"
  - shell: "aggregate-results.sh --input $OUTPUT_PATH --output $OUTPUT_PATH/summary.json"
  - shell: "send-metrics.sh --completed ${map.successful} --failed ${map.failed}"
    when: "${ENABLE_MONITORING} == 'true'"

merge:
  commands:
    - shell: "cargo test"
    - shell: "validate-deployment.sh --env $ENVIRONMENT"
</code></pre>
<p>See also:</p>
<ul>
<li><a href="environment/best-practices.html">Best Practices</a> for guidelines on environment management</li>
<li><a href="environment/environment-profiles.html">Environment Profiles</a> for profile configuration details</li>
<li><a href="environment/secrets-management.html">Secrets Management</a> for secure credential handling</li>
<li><a href="environment/mapreduce-environment-variables.html">MapReduce Environment Variables</a> for MapReduce-specific usage</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="configuration-1"><a class="header" href="#configuration-1">Configuration</a></h1>
<p>Prodigy supports comprehensive configuration through multiple files with a clear precedence hierarchy. This chapter explains all configuration options and how to use them effectively.</p>
<h2 id="quick-start-2"><a class="header" href="#quick-start-2">Quick Start</a></h2>
<p>Prodigy uses two distinct types of configuration files:</p>
<ol>
<li><strong>Project Configuration</strong> (<code>.prodigy/config.yml</code>) - Project settings and metadata</li>
<li><strong>Workflow Configuration</strong> (<code>.prodigy/workflow.yml</code> or explicit path) - Workflow definitions</li>
</ol>
<h3 id="minimal-project-configuration"><a class="header" href="#minimal-project-configuration">Minimal Project Configuration</a></h3>
<p>Create <code>.prodigy/config.yml</code>:</p>
<pre><code class="language-yaml">name: my-project  # Required: Project identifier
</code></pre>
<p>The <code>name</code> field is the only required field. All other settings have sensible defaults.</p>
<p><strong>Source</strong>: ProjectConfig struct in src/config/mod.rs:66-74</p>
<h3 id="minimal-workflow-configuration"><a class="header" href="#minimal-workflow-configuration">Minimal Workflow Configuration</a></h3>
<p>Create <code>.prodigy/workflow.yml</code> or any <code>.yml</code> file:</p>
<pre><code class="language-yaml">commands:  # List of commands to execute in sequence
  - prodigy-code-review  # Slash prefix is optional in workflow files
  - /prodigy-lint        # Both styles work the same
</code></pre>
<p>Workflows define a sequence of commands to execute. Each command is a Prodigy slash command - the <code>/</code> prefix is optional in workflow files.</p>
<p><strong>Source</strong>: WorkflowConfig parsing in src/core/config/mod.rs:11-22, Examples in examples/mapreduce-json-input.yml</p>
<p>That’s all you need to get started! Prodigy provides sensible defaults for everything else. See the subsections below for detailed configuration options.</p>
<h2 id="configuration-file-locations"><a class="header" href="#configuration-file-locations">Configuration File Locations</a></h2>
<p>Prodigy uses a search hierarchy to find configuration files. Configuration can come from multiple sources with the following precedence (highest to lowest):</p>
<ol>
<li><strong>CLI Flags</strong> - Command-line arguments override all other settings</li>
<li><strong>Environment Variables</strong> - Environment variables (e.g., <code>PRODIGY_CLAUDE_API_KEY</code>)</li>
<li><strong>Project Config</strong> - <code>.prodigy/config.yml</code> in your project directory</li>
<li><strong>Global Config</strong> - <code>~/.prodigy/config.yml</code> in your home directory</li>
<li><strong>Defaults</strong> - Built-in default values</li>
</ol>
<p><strong>Source</strong>: ConfigLoader.load_with_explicit_path() in src/config/loader.rs:31-55</p>
<h3 id="workflow-file-search-hierarchy"><a class="header" href="#workflow-file-search-hierarchy">Workflow File Search Hierarchy</a></h3>
<p>For workflow files specifically (different from project config):</p>
<ol>
<li><strong>Explicit path</strong> - Path provided via <code>prodigy run path/to/workflow.yml</code> (error if not found)</li>
<li><strong>Default location</strong> - <code>.prodigy/workflow.yml</code> in the project directory (if exists)</li>
<li><strong>Built-in defaults</strong> - Default workflow configuration</li>
</ol>
<p><strong>Source</strong>: ConfigLoader.load_with_explicit_path() in src/config/loader.rs:40-54</p>
<h3 id="key-distinction-configyml-vs-workflowyml"><a class="header" href="#key-distinction-configyml-vs-workflowyml">Key Distinction: config.yml vs workflow.yml</a></h3>
<ul>
<li>
<p><strong><code>.prodigy/config.yml</code></strong>: Contains <strong>project settings</strong> (name, version, API keys, editor preferences)</p>
<ul>
<li>Maps to <code>ProjectConfig</code> struct (src/config/mod.rs:66-74)</li>
<li>Loaded via ConfigLoader.load_project() (src/config/loader.rs:85-104)</li>
</ul>
</li>
<li>
<p><strong><code>.prodigy/workflow.yml</code></strong>: Contains <strong>workflow definitions</strong> (commands to execute)</p>
<ul>
<li>Maps to <code>WorkflowConfig</code> struct (src/config/workflow.rs)</li>
<li>Loaded via ConfigLoader.load_with_explicit_path() (src/config/loader.rs:35-55)</li>
</ul>
</li>
</ul>
<p>Both can exist in the <code>.prodigy/</code> directory and serve different purposes.</p>
<h2 id="configuration-architecture"><a class="header" href="#configuration-architecture">Configuration Architecture</a></h2>
<p>Prodigy uses a three-tier configuration structure internally:</p>
<pre><code>Config (Root)
├── GlobalConfig      - User-wide settings (~/.prodigy/config.yml)
├── ProjectConfig     - Project-specific settings (.prodigy/config.yml)
└── WorkflowConfig    - Workflow definitions (.prodigy/workflow.yml or explicit path)
</code></pre>
<p><strong>Source</strong>: Config struct hierarchy in src/config/mod.rs:38-43</p>
<p><strong>How it works:</strong></p>
<ol>
<li><code>Config::new()</code> creates the root with default <code>GlobalConfig</code></li>
<li><code>merge_project_config()</code> adds project-specific settings (src/core/config/mod.rs:36-40)</li>
<li><code>merge_workflow_config()</code> adds workflow definitions (src/core/config/mod.rs:31-34)</li>
<li><code>Config.merge_env_vars()</code> applies environment variable overrides (src/config/mod.rs:111-131)</li>
</ol>
<p>This design allows:</p>
<ul>
<li><strong>Separation of concerns</strong>: Global settings vs project settings vs workflows</li>
<li><strong>Clear precedence</strong>: Project settings override global defaults</li>
<li><strong>Environment overrides</strong>: Runtime configuration via env vars</li>
<li><strong>Type safety</strong>: Each config tier has its own validated struct</li>
</ul>
<p>See <a href="configuration/global-configuration-structure.html">Global Configuration Structure</a> and <a href="configuration/project-configuration-structure.html">Project Configuration Structure</a> for detailed field definitions.</p>
<h2 id="common-configuration-patterns"><a class="header" href="#common-configuration-patterns">Common Configuration Patterns</a></h2>
<h3 id="quick-validation"><a class="header" href="#quick-validation">Quick Validation</a></h3>
<p>To verify which configuration Prodigy is using:</p>
<pre><code class="language-bash"># Check if config files exist
ls -la .prodigy/config.yml .prodigy/workflow.yml

# Validate YAML syntax
prodigy validate workflow.yml

# Run with verbose output to see loaded configuration
prodigy run workflow.yml -v
</code></pre>
<p><strong>Validation rules</strong> (src/core/config/mod.rs:43-50):</p>
<ul>
<li>Only <code>.yml</code> and <code>.yaml</code> extensions are supported (TOML is deprecated)</li>
<li>Config files must be valid YAML syntax</li>
<li>ProjectConfig requires <code>name</code> field</li>
<li>WorkflowConfig requires <code>commands</code> array</li>
</ul>
<h3 id="configuration-not-found"><a class="header" href="#configuration-not-found">Configuration Not Found?</a></h3>
<p>If Prodigy doesn’t find your configuration:</p>
<ol>
<li>
<p><strong>Workflow file</strong>: Check explicit path vs <code>.prodigy/workflow.yml</code></p>
<ul>
<li>Explicit path: <code>prodigy run path/to/workflow.yml</code> (must exist or error)</li>
<li>Default location: <code>.prodigy/workflow.yml</code> (optional, uses defaults if missing)</li>
</ul>
</li>
<li>
<p><strong>Project config</strong>: Must be at <code>.prodigy/config.yml</code> in project root</p>
<ul>
<li>Prodigy searches upward from current directory for <code>.prodigy/</code> folder</li>
<li>Check you’re running from within the project directory</li>
</ul>
</li>
<li>
<p><strong>Global config</strong>: Optional, located at <code>~/.prodigy/config.yml</code></p>
<ul>
<li>Use for API keys and editor preferences across all projects</li>
</ul>
</li>
</ol>
<p><strong>Source</strong>: ConfigLoader search logic in src/config/loader.rs:35-104</p>
<h3 id="debugging-configuration-issues"><a class="header" href="#debugging-configuration-issues">Debugging Configuration Issues</a></h3>
<p>Common issues and solutions:</p>
<div class="table-wrapper"><table><thead><tr><th>Issue</th><th>Cause</th><th>Solution</th></tr></thead><tbody>
<tr><td>“Config not found”</td><td>File in wrong location</td><td>Check <code>.prodigy/config.yml</code> exists in project root</td></tr>
<tr><td>“Invalid YAML”</td><td>Syntax error</td><td>Validate YAML with online parser or <code>yamllint</code></td></tr>
<tr><td>“Unknown field”</td><td>Typo in field name</td><td>Check struct definitions in src/config/mod.rs</td></tr>
<tr><td>Settings not applied</td><td>Wrong precedence</td><td>CLI flags &gt; env vars &gt; project &gt; global &gt; defaults</td></tr>
<tr><td>Workflow not loaded</td><td>Wrong file used</td><td>Verify workflow.yml vs config.yml distinction</td></tr>
</tbody></table>
</div>
<h2 id="additional-topics-4"><a class="header" href="#additional-topics-4">Additional Topics</a></h2>
<p>See also:</p>
<ul>
<li><a href="configuration/configuration-precedence-rules.html">Configuration Precedence Rules</a></li>
<li><a href="configuration/global-configuration-structure.html">Global Configuration Structure</a></li>
<li><a href="configuration/project-configuration-structure.html">Project Configuration Structure</a></li>
<li><a href="configuration/workflow-configuration.html">Workflow Configuration</a></li>
<li><a href="configuration/storage-configuration.html">Storage Configuration</a></li>
<li><a href="configuration/environment-variables.html">Environment Variables</a></li>
<li><a href="configuration/complete-configuration-examples.html">Complete Configuration Examples</a></li>
<li><a href="configuration/default-values-reference.html">Default Values Reference</a></li>
<li><a href="configuration/best-practices.html">Best Practices</a></li>
<li><a href="configuration/troubleshooting.html">Troubleshooting</a></li>
<li><a href="configuration/migration-guide-toml-to-yaml.html">Migration Guide: TOML to YAML</a></li>
<li><a href="configuration/related-documentation.html">Related Documentation</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="configuration-precedence-rules"><a class="header" href="#configuration-precedence-rules">Configuration Precedence Rules</a></h2>
<p>Prodigy loads configuration from multiple sources with a clear precedence hierarchy. Understanding how configuration is merged helps you control which settings take effect.</p>
<h3 id="precedence-hierarchy"><a class="header" href="#precedence-hierarchy">Precedence Hierarchy</a></h3>
<p>From highest to lowest priority:</p>
<ol>
<li>
<p><strong>Project Config</strong> (<code>.prodigy/config.yml</code>) - Highest priority</p>
<ul>
<li>Project-specific settings in your repository</li>
<li>Located at <code>.prodigy/config.yml</code> in your project directory</li>
<li>Overrides all default values</li>
<li>Committed to version control (be careful with secrets)</li>
</ul>
</li>
<li>
<p><strong>Defaults</strong> (lowest priority)</p>
<ul>
<li>Built-in default values defined in the code</li>
<li>Used when project config doesn’t provide a value</li>
<li>See source: <code>src/config/mod.rs:88-100</code></li>
</ul>
</li>
</ol>
<p><strong>Note</strong>: Global config (<code>~/.prodigy/config.yml</code>), environment variables (<code>PRODIGY_*</code>), and CLI flag overrides are defined in the code but not currently loaded in production. Only project-level configuration and defaults are active.</p>
<h3 id="how-settings-are-loaded"><a class="header" href="#how-settings-are-loaded">How Settings Are Loaded</a></h3>
<p>When Prodigy starts, it builds the final configuration with this process:</p>
<ol>
<li><strong>Initialize with defaults</strong> - Create <code>GlobalConfig</code> with built-in defaults (source: <code>src/config/mod.rs:88-100</code>)</li>
<li><strong>Load project config</strong> - Read <code>.prodigy/config.yml</code> from project directory (source: <code>src/config/loader.rs:85-104</code>)</li>
<li><strong>Merge at field level</strong> - Project config values override defaults on a per-field basis (source: <code>src/config/mod.rs:133-154</code>)</li>
</ol>
<p>Project config <strong>overrides</strong> default values at the individual field level. Settings not specified in project config inherit from defaults.</p>
<h3 id="examples-1"><a class="header" href="#examples-1">Examples</a></h3>
<h4 id="example-1-using-defaults"><a class="header" href="#example-1-using-defaults">Example 1: Using Defaults</a></h4>
<pre><code class="language-yaml"># No .prodigy/config.yml file exists
</code></pre>
<p><strong>Result</strong>: Prodigy uses all default values:</p>
<ul>
<li><code>log_level: "info"</code></li>
<li><code>auto_commit: true</code></li>
<li><code>max_concurrent_specs: 1</code></li>
</ul>
<p>(Source: <code>src/config/mod.rs:88-100</code>)</p>
<h4 id="example-2-project-config-override"><a class="header" href="#example-2-project-config-override">Example 2: Project Config Override</a></h4>
<pre><code class="language-yaml"># .prodigy/config.yml (project config)
name: my-project
claude_api_key: "sk-project-key"
auto_commit: false
</code></pre>
<p><strong>Result</strong>:</p>
<ul>
<li><code>claude_api_key: "sk-project-key"</code> (from project config)</li>
<li><code>auto_commit: false</code> (from project config)</li>
<li><code>log_level: "info"</code> (from defaults - not specified in project)</li>
<li><code>max_concurrent_specs: 1</code> (from defaults - not specified in project)</li>
</ul>
<p>Field-level precedence is implemented via getter methods (source: <code>src/config/mod.rs:133-154</code>):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn get_auto_commit(&amp;self) -&gt; bool {
    self.project
        .as_ref()
        .and_then(|p| p.auto_commit)
        .or(self.global.auto_commit)
        .unwrap_or(true)  // Default if neither provides value
}
<span class="boring">}</span></code></pre></pre>
<h4 id="example-3-partial-project-override"><a class="header" href="#example-3-partial-project-override">Example 3: Partial Project Override</a></h4>
<pre><code class="language-yaml"># .prodigy/config.yml
name: my-project
claude_api_key: "sk-abc123"
# Other fields not specified
</code></pre>
<p><strong>Result</strong>:</p>
<ul>
<li><code>claude_api_key: "sk-abc123"</code> (from project config)</li>
<li><code>log_level: "info"</code> (from defaults)</li>
<li><code>auto_commit: true</code> (from defaults)</li>
<li><code>max_concurrent_specs: 1</code> (from defaults)</li>
</ul>
<h3 id="field-level-precedence"><a class="header" href="#field-level-precedence">Field-Level Precedence</a></h3>
<p>Precedence is applied <strong>per field</strong>, not per file. Each configuration field is resolved independently using the precedence rules.</p>
<pre><code class="language-yaml"># .prodigy/config.yml (project config)
name: my-project
auto_commit: false  # Only override auto_commit
# Other fields inherited from defaults
</code></pre>
<p><strong>Precedence Logic</strong> (source: <code>src/config/mod.rs:133-154</code>):</p>
<ol>
<li>Check if project config has the field → use it</li>
<li>Otherwise, check if global config has the field → use it</li>
<li>Otherwise, use the default value</li>
</ol>
<p>This allows fine-grained configuration: override only what you need, inherit the rest.</p>
<h3 id="configuration-loading-implementation"><a class="header" href="#configuration-loading-implementation">Configuration Loading Implementation</a></h3>
<p>The configuration loading happens in these steps (source: <code>src/config/loader.rs</code>):</p>
<p><strong>Step 1: Initialize</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ConfigLoader::new() - line 23
let config = Config::new();  // Creates Config with GlobalConfig defaults
<span class="boring">}</span></code></pre></pre>
<p><strong>Step 2: Load Project Config</strong> (optional)</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ConfigLoader::load_project() - line 85
let config_path = project_path.join(".prodigy").join("config.yml");
if config_path.exists() {
    let content = fs::read_to_string(&amp;config_path).await?;
    let project_config = parse_project_config(&amp;content)?;
    *config = merge_project_config(config.clone(), project_config);
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Step 3: Access with Precedence</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Config::get_claude_api_key() - line 133
self.project
    .as_ref()
    .and_then(|p| p.claude_api_key.as_deref())  // Try project first
    .or(self.global.claude_api_key.as_deref())   // Fall back to global
<span class="boring">}</span></code></pre></pre>
<h3 id="default-values-1"><a class="header" href="#default-values-1">Default Values</a></h3>
<p>Built-in defaults (source: <code>src/config/mod.rs:88-100</code>):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl Default for GlobalConfig {
    fn default() -&gt; Self {
        Self {
            prodigy_home: get_global_prodigy_dir()
                .unwrap_or_else(|_| PathBuf::from("~/.prodigy")),
            default_editor: None,
            log_level: Some("info".to_string()),
            claude_api_key: None,
            max_concurrent_specs: Some(1),
            auto_commit: Some(true),
            plugins: None,
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="future-global-config-and-environment-variables"><a class="header" href="#future-global-config-and-environment-variables">Future: Global Config and Environment Variables</a></h3>
<p>The codebase includes infrastructure for additional configuration sources, but these are not currently loaded in production:</p>
<p><strong>Global Config</strong> (<code>~/.prodigy/config.yml</code>):</p>
<ul>
<li>Mentioned in documentation (line 49: <code>src/config/mod.rs</code>)</li>
<li>No loader implementation yet</li>
<li>Would provide user-level defaults across all projects</li>
</ul>
<p><strong>Environment Variables</strong>:</p>
<ul>
<li>Defined in <code>Config::merge_env_vars()</code> (lines 111-131: <code>src/config/mod.rs</code>)</li>
<li>Supports: <code>PRODIGY_CLAUDE_API_KEY</code>, <code>PRODIGY_LOG_LEVEL</code>, <code>PRODIGY_EDITOR</code>, <code>PRODIGY_AUTO_COMMIT</code></li>
<li>Only called in tests, not in production code</li>
<li>Would override file-based configuration when implemented</li>
</ul>
<p><strong>CLI Flag Overrides</strong>:</p>
<ul>
<li>No implementation yet</li>
<li>Would provide highest-priority overrides for individual runs</li>
</ul>
<h3 id="test-coverage"><a class="header" href="#test-coverage">Test Coverage</a></h3>
<p>Configuration precedence behavior is validated through comprehensive tests (source: <code>src/config/loader.rs:113-334</code>):</p>
<p><strong>Test: Default Configuration</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Line 120: test_new_creates_default_config
// Verifies GlobalConfig defaults are set correctly
assert_eq!(config.global.log_level, Some("info".to_string()));
assert_eq!(config.global.max_concurrent_specs, Some(1));
assert_eq!(config.global.auto_commit, Some(true));
<span class="boring">}</span></code></pre></pre>
<p><strong>Test: Project Config Loading</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Line 230: test_load_project_config
// Verifies .prodigy/config.yml is loaded and merged
let project = config.project.unwrap();
assert_eq!(project.name, "test-project");
assert_eq!(project.claude_api_key, Some("test-key".to_string()));
assert_eq!(project.auto_commit, Some(false));
<span class="boring">}</span></code></pre></pre>
<p><strong>Test: Field-Level Override</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// src/config/mod.rs:471 - test shows project overrides global
config.project = Some(ProjectConfig {
    name: "test".into(),
    claude_api_key: Some("project-key".into()),
    // ... other fields
});
assert_eq!(config.get_claude_api_key(), Some("project-key"));
<span class="boring">}</span></code></pre></pre>
<h3 id="see-also-10"><a class="header" href="#see-also-10">See Also</a></h3>
<ul>
<li><a href="configuration/global-configuration-structure.html">Global Configuration Structure</a> - Complete field reference</li>
<li><a href="configuration/default-values-reference.html">Default Values Reference</a> - All default values</li>
<li><a href="configuration/complete-configuration-examples.html">Complete Configuration Examples</a> - Real-world configuration patterns</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="global-configuration-structure"><a class="header" href="#global-configuration-structure">Global Configuration Structure</a></h2>
<p>Global configuration is stored in <code>~/.prodigy/config.yml</code> in your home directory. These settings apply to all Prodigy projects unless overridden by project-specific configuration.</p>
<h3 id="location"><a class="header" href="#location">Location</a></h3>
<ul>
<li><strong>File</strong>: <code>~/.prodigy/config.yml</code></li>
<li><strong>Created</strong>: Automatically on first run with defaults</li>
<li><strong>Format</strong>: YAML</li>
</ul>
<h3 id="fields"><a class="header" href="#fields">Fields</a></h3>
<h4 id="prodigy_home"><a class="header" href="#prodigy_home"><code>prodigy_home</code></a></h4>
<p><strong>Type</strong>: Path
<strong>Default</strong>: <code>~/.prodigy</code> (or platform-specific data directory)</p>
<p>Base directory for global Prodigy data including events, DLQ, state, and worktrees.</p>
<pre><code class="language-yaml">prodigy_home: /Users/username/.prodigy
</code></pre>
<h4 id="default_editor"><a class="header" href="#default_editor"><code>default_editor</code></a></h4>
<p><strong>Type</strong>: String (optional)
<strong>Default</strong>: None</p>
<p>Default text editor for interactive operations. Falls back to <code>EDITOR</code> environment variable if not set.</p>
<pre><code class="language-yaml">default_editor: vim
</code></pre>
<h4 id="log_level"><a class="header" href="#log_level"><code>log_level</code></a></h4>
<p><strong>Type</strong>: String (optional)
<strong>Default</strong>: <code>info</code>
<strong>Valid values</strong>: <code>trace</code>, <code>debug</code>, <code>info</code>, <code>warn</code>, <code>error</code></p>
<p>Controls logging verbosity for Prodigy operations.</p>
<pre><code class="language-yaml">log_level: debug
</code></pre>
<h4 id="claude_api_key"><a class="header" href="#claude_api_key"><code>claude_api_key</code></a></h4>
<p><strong>Type</strong>: String (optional)
<strong>Default</strong>: None</p>
<p>Claude API key for AI-powered commands. Can be overridden by project config or <code>PRODIGY_CLAUDE_API_KEY</code> environment variable.</p>
<pre><code class="language-yaml">claude_api_key: "sk-ant-api03-..."
</code></pre>
<p><strong>Security Note</strong>: Store API keys in environment variables or project config (not committed to version control) rather than global config.</p>
<h4 id="max_concurrent_specs"><a class="header" href="#max_concurrent_specs"><code>max_concurrent_specs</code></a></h4>
<p><strong>Type</strong>: Integer (optional)
<strong>Default</strong>: <code>1</code></p>
<p>Maximum number of concurrent spec implementations to run in parallel.</p>
<pre><code class="language-yaml">max_concurrent_specs: 3
</code></pre>
<h4 id="auto_commit"><a class="header" href="#auto_commit"><code>auto_commit</code></a></h4>
<p><strong>Type</strong>: Boolean (optional)
<strong>Default</strong>: <code>true</code></p>
<p>Whether to automatically commit changes after successful command execution.</p>
<pre><code class="language-yaml">auto_commit: false
</code></pre>
<h4 id="storage"><a class="header" href="#storage"><code>storage</code></a></h4>
<p><strong>Type</strong>: Object (optional)
<strong>Default</strong>: File storage in <code>~/.prodigy</code></p>
<p>Storage backend configuration for events, DLQ, state, and worktrees. See <a href="configuration/storage-configuration.html">Storage Configuration</a> for details.</p>
<pre><code class="language-yaml">storage:
  backend: file
  backend_config:
    base_dir: ~/.prodigy
    repository_grouping: true
</code></pre>
<p><strong>Storage Fields</strong>:</p>
<ul>
<li><code>backend</code>: Storage type (<code>file</code> or <code>memory</code>)</li>
<li><code>backend_config.base_dir</code>: Base directory for file storage</li>
<li><code>backend_config.repository_grouping</code>: Group data by repository name (default: true)</li>
</ul>
<p>See <a href="configuration/storage-configuration.html">Storage Configuration</a> for complete documentation.</p>
<h4 id="plugins"><a class="header" href="#plugins"><code>plugins</code></a></h4>
<p><strong>Type</strong>: Object (optional)
<strong>Default</strong>: None</p>
<p>Plugin system configuration. See <a href="configuration/global-configuration-structure.html#plugin-configuration">Plugin Configuration</a> below.</p>
<h3 id="complete-example-5"><a class="header" href="#complete-example-5">Complete Example</a></h3>
<pre><code class="language-yaml"># ~/.prodigy/config.yml
prodigy_home: /Users/username/.prodigy
default_editor: code
log_level: info
claude_api_key: "sk-ant-api03-..."
max_concurrent_specs: 2
auto_commit: true

storage:
  backend: file
  backend_config:
    base_dir: /Users/username/.prodigy
    repository_grouping: true

plugins:
  enabled: true
  directory: /Users/username/.prodigy/plugins
  auto_load:
    - github-integration
    - slack-notifications
</code></pre>
<h3 id="plugin-configuration"><a class="header" href="#plugin-configuration">Plugin Configuration</a></h3>
<p>The <code>plugins</code> field controls the plugin system:</p>
<h4 id="enabled"><a class="header" href="#enabled"><code>enabled</code></a></h4>
<p><strong>Type</strong>: Boolean
<strong>Default</strong>: <code>false</code></p>
<p>Enable or disable the plugin system.</p>
<pre><code class="language-yaml">plugins:
  enabled: true
</code></pre>
<h4 id="directory"><a class="header" href="#directory"><code>directory</code></a></h4>
<p><strong>Type</strong>: Path
<strong>Default</strong>: <code>~/.prodigy/plugins</code></p>
<p>Directory to search for plugins.</p>
<pre><code class="language-yaml">plugins:
  directory: /custom/plugin/path
</code></pre>
<h4 id="auto_load"><a class="header" href="#auto_load"><code>auto_load</code></a></h4>
<p><strong>Type</strong>: Array of strings
<strong>Default</strong>: <code>[]</code></p>
<p>List of plugin names to automatically load on startup.</p>
<pre><code class="language-yaml">plugins:
  auto_load:
    - plugin-name-1
    - plugin-name-2
</code></pre>
<h3 id="creating-global-config"><a class="header" href="#creating-global-config">Creating Global Config</a></h3>
<p>If the file doesn’t exist, create it manually:</p>
<pre><code class="language-bash">mkdir -p ~/.prodigy
cat &gt; ~/.prodigy/config.yml &lt;&lt; 'EOF'
log_level: info
auto_commit: true
EOF
</code></pre>
<h3 id="relationship-to-project-config"><a class="header" href="#relationship-to-project-config">Relationship to Project Config</a></h3>
<ul>
<li>Global config applies to <strong>all projects</strong></li>
<li>Project config (<code>.prodigy/config.yml</code>) <strong>overrides</strong> global config per field</li>
<li>Settings not specified in project config are <strong>inherited</strong> from global config</li>
<li>See <a href="configuration/configuration-precedence-rules.html">Configuration Precedence Rules</a> for details</li>
</ul>
<h3 id="see-also-11"><a class="header" href="#see-also-11">See Also</a></h3>
<ul>
<li><a href="configuration/project-configuration-structure.html">Project Configuration Structure</a></li>
<li><a href="configuration/configuration-precedence-rules.html">Configuration Precedence Rules</a></li>
<li><a href="configuration/environment-variables.html">Environment Variables</a></li>
<li><a href="configuration/default-values-reference.html">Default Values Reference</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="project-configuration-structure"><a class="header" href="#project-configuration-structure">Project Configuration Structure</a></h2>
<p>Project configuration is stored in <code>.prodigy/config.yml</code> within your project repository. These settings override global configuration for this specific project and are typically committed to version control (except for secrets).</p>
<h3 id="location-1"><a class="header" href="#location-1">Location</a></h3>
<ul>
<li><strong>File</strong>: <code>.prodigy/config.yml</code> (in project root)</li>
<li><strong>Created</strong>: Manually or via <code>prodigy init</code></li>
<li><strong>Format</strong>: YAML</li>
<li><strong>Version Control</strong>: Committed to git (recommended, except for secrets)</li>
</ul>
<h3 id="fields-1"><a class="header" href="#fields-1">Fields</a></h3>
<h4 id="name-required"><a class="header" href="#name-required"><code>name</code> (required)</a></h4>
<p><strong>Type</strong>: String
<strong>Default</strong>: None (required field)</p>
<p>Project identifier used in logs, events, and UI.</p>
<pre><code class="language-yaml">name: my-project
</code></pre>
<h4 id="description"><a class="header" href="#description"><code>description</code></a></h4>
<p><strong>Type</strong>: String (optional)
<strong>Default</strong>: None</p>
<p>Human-readable project description.</p>
<pre><code class="language-yaml">description: "AI-powered code analysis tool"
</code></pre>
<h4 id="version"><a class="header" href="#version"><code>version</code></a></h4>
<p><strong>Type</strong>: String (optional)
<strong>Default</strong>: None</p>
<p>Project version (semantic versioning recommended).</p>
<pre><code class="language-yaml">version: "1.2.3"
</code></pre>
<h4 id="spec_dir"><a class="header" href="#spec_dir"><code>spec_dir</code></a></h4>
<p><strong>Type</strong>: Path (optional)
<strong>Default</strong>: <code>specs</code></p>
<p>Directory containing Prodigy specification files.</p>
<pre><code class="language-yaml">spec_dir: custom/specs
</code></pre>
<h4 id="claude_api_key-1"><a class="header" href="#claude_api_key-1"><code>claude_api_key</code></a></h4>
<p><strong>Type</strong>: String (optional)
<strong>Default</strong>: None (inherits from global config or environment)</p>
<p>Project-specific Claude API key. <strong>Overrides</strong> global config and <strong>is overridden by</strong> environment variable.</p>
<pre><code class="language-yaml">claude_api_key: "sk-ant-api03-..."
</code></pre>
<p><strong>Security Warning</strong>: Do NOT commit API keys to version control. Use environment variables or <code>.prodigy/config.local.yml</code> (gitignored) instead.</p>
<h4 id="auto_commit-1"><a class="header" href="#auto_commit-1"><code>auto_commit</code></a></h4>
<p><strong>Type</strong>: Boolean (optional)
<strong>Default</strong>: Inherits from global config (default: <code>true</code>)</p>
<p>Whether to automatically commit changes after successful command execution.</p>
<pre><code class="language-yaml">auto_commit: false
</code></pre>
<h4 id="variables"><a class="header" href="#variables"><code>variables</code></a></h4>
<p><strong>Type</strong>: Object (optional)
<strong>Default</strong>: <code>{}</code></p>
<p>Project-specific variables available in workflows and commands.</p>
<pre><code class="language-yaml">variables:
  deploy_branch: production
  test_timeout: 300
  feature_flags:
    new_ui: true
    beta_features: false
</code></pre>
<p>These variables can be referenced in workflows using <code>${variable_name}</code> syntax.</p>
<p><strong>Note</strong>: For workflow-level environment variables (with secrets, profiles, and step-level overrides), use the <code>env:</code> block in workflow files instead. See <a href="configuration/environment-variables.html">Environment Variables</a> for details.</p>
<h4 id="storage-1"><a class="header" href="#storage-1"><code>storage</code></a></h4>
<p><strong>Type</strong>: Object (optional)
<strong>Default</strong>: Inherits from global config</p>
<p>Project-specific storage configuration. See <a href="configuration/storage-configuration.html">Storage Configuration</a> for details.</p>
<pre><code class="language-yaml">storage:
  backend: file
  backend_config:
    base_dir: /custom/project/storage
</code></pre>
<h3 id="complete-example-6"><a class="header" href="#complete-example-6">Complete Example</a></h3>
<pre><code class="language-yaml"># .prodigy/config.yml
name: prodigy
description: "Workflow orchestration tool for Claude Code"
version: "0.1.0"
spec_dir: specs

auto_commit: true

variables:
  default_branch: master
  test_suite: full
  timeout_seconds: 600
</code></pre>
<h3 id="secrets-management-2"><a class="header" href="#secrets-management-2">Secrets Management</a></h3>
<p>For sensitive values like API keys, use one of these approaches:</p>
<h4 id="option-1-environment-variables-recommended"><a class="header" href="#option-1-environment-variables-recommended">Option 1: Environment Variables (Recommended)</a></h4>
<pre><code class="language-yaml"># .prodigy/config.yml (committed)
name: my-project
# No claude_api_key here
</code></pre>
<pre><code class="language-bash"># Set in environment
export PRODIGY_CLAUDE_API_KEY="sk-ant-api03-..."
</code></pre>
<h4 id="option-2-local-config-file-not-committed"><a class="header" href="#option-2-local-config-file-not-committed">Option 2: Local Config File (Not Committed)</a></h4>
<p>Create <code>.prodigy/config.local.yml</code> and add it to <code>.gitignore</code>:</p>
<pre><code class="language-yaml"># .prodigy/config.local.yml (gitignored)
claude_api_key: "sk-ant-api03-..."
</code></pre>
<pre><code class="language-bash"># .gitignore
.prodigy/config.local.yml
</code></pre>
<h4 id="option-3-secret-management-service"><a class="header" href="#option-3-secret-management-service">Option 3: Secret Management Service</a></h4>
<p>Use a secret management service (AWS Secrets Manager, HashiCorp Vault, etc.) and retrieve the key at runtime via environment variables.</p>
<h3 id="relationship-to-global-config"><a class="header" href="#relationship-to-global-config">Relationship to Global Config</a></h3>
<p>Project config <strong>overrides</strong> global config on a per-field basis:</p>
<ul>
<li>Fields specified in project config <strong>replace</strong> global config values</li>
<li>Fields NOT specified in project config <strong>inherit</strong> global config values</li>
<li>See <a href="configuration/configuration-precedence-rules.html">Configuration Precedence Rules</a></li>
</ul>
<p><strong>Example</strong>:</p>
<pre><code class="language-yaml"># ~/.prodigy/config.yml (global)
log_level: info
auto_commit: true
max_concurrent_specs: 1
</code></pre>
<pre><code class="language-yaml"># .prodigy/config.yml (project)
name: my-project
log_level: debug  # Override global
# auto_commit and max_concurrent_specs inherited from global
</code></pre>
<p><strong>Result</strong>: Project uses <code>log_level: debug</code> but inherits <code>auto_commit: true</code> and <code>max_concurrent_specs: 1</code> from global config.</p>
<h3 id="project-variables-in-workflows"><a class="header" href="#project-variables-in-workflows">Project Variables in Workflows</a></h3>
<p>Variables defined in project config are available in workflows:</p>
<pre><code class="language-yaml"># .prodigy/config.yml
name: my-project
variables:
  environment: staging
  api_url: https://staging.api.example.com
</code></pre>
<pre><code class="language-yaml"># .prodigy/workflow.yml
commands:
  - name: deploy
    args: ["${environment}"]
    options:
      api_url: "${api_url}"
</code></pre>
<h3 id="creating-project-config"><a class="header" href="#creating-project-config">Creating Project Config</a></h3>
<p>Initialize a new project:</p>
<pre><code class="language-bash">cd my-project
mkdir -p .prodigy
cat &gt; .prodigy/config.yml &lt;&lt; 'EOF'
name: my-project
auto_commit: true
EOF
</code></pre>
<p>Or use the init command (if available):</p>
<pre><code class="language-bash">prodigy init
</code></pre>
<h3 id="see-also-12"><a class="header" href="#see-also-12">See Also</a></h3>
<ul>
<li><a href="configuration/global-configuration-structure.html">Global Configuration Structure</a></li>
<li><a href="configuration/configuration-precedence-rules.html">Configuration Precedence Rules</a></li>
<li><a href="configuration/workflow-configuration.html">Workflow Configuration</a></li>
<li><a href="configuration/environment-variables.html">Environment Variables</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="workflow-configuration"><a class="header" href="#workflow-configuration">Workflow Configuration</a></h2>
<p>Workflow configuration defines the sequence of commands to execute and their execution environment. Workflows are the core of Prodigy’s automation capabilities.</p>
<h3 id="overview-6"><a class="header" href="#overview-6">Overview</a></h3>
<p>Workflows are defined in <code>.prodigy/workflow.yml</code> files and specify:</p>
<ul>
<li>Commands to execute in sequence</li>
<li>Iteration and loop control</li>
<li>Error handling behavior</li>
<li>Environment variables</li>
<li>Timeout and retry settings</li>
</ul>
<p>For detailed workflow syntax and MapReduce capabilities, see the <a href="configuration/../workflow-basics.html">Workflow Basics</a> chapter.</p>
<h3 id="basic-workflow-structure"><a class="header" href="#basic-workflow-structure">Basic Workflow Structure</a></h3>
<pre><code class="language-yaml"># .prodigy/workflow.yml
commands:
  - prodigy-code-review
  - prodigy-lint
  - prodigy-test
</code></pre>
<h3 id="advanced-workflow-features"><a class="header" href="#advanced-workflow-features">Advanced Workflow Features</a></h3>
<h4 id="structured-commands"><a class="header" href="#structured-commands">Structured Commands</a></h4>
<p>Commands can include arguments and options:</p>
<pre><code class="language-yaml">commands:
  - name: prodigy-implement-spec
    args: ["${SPEC_ID}"]
    options:
      focus: performance
  - name: prodigy-code-review
    options:
      severity: high
</code></pre>
<h4 id="iteration-control"><a class="header" href="#iteration-control">Iteration Control</a></h4>
<pre><code class="language-yaml">max_iterations: 5
commands:
  - prodigy-fix-issues
  - prodigy-test
</code></pre>
<h4 id="error-handling-1"><a class="header" href="#error-handling-1">Error Handling</a></h4>
<pre><code class="language-yaml">commands:
  - name: prodigy-risky-operation
    metadata:
      continue_on_error: true
      retries: 3
      timeout: 600
</code></pre>
<h3 id="configuration-location"><a class="header" href="#configuration-location">Configuration Location</a></h3>
<p>Workflows can be specified in multiple ways:</p>
<ol>
<li><strong>Explicit path</strong>: <code>prodigy run /path/to/workflow.yml</code></li>
<li><strong>Project default</strong>: <code>.prodigy/workflow.yml</code> in project directory</li>
<li><strong>Embedded in config</strong>: Nested under <code>workflow:</code> in config files</li>
</ol>
<h3 id="environment-variables-in-workflows"><a class="header" href="#environment-variables-in-workflows">Environment Variables in Workflows</a></h3>
<p>Workflows have a dedicated <code>env:</code> block for defining environment variables with advanced features like secrets, profiles, and step-level overrides:</p>
<pre><code class="language-yaml"># .prodigy/workflow.yml
name: deployment

env:
  # Plain variables
  ENVIRONMENT: staging
  API_URL: https://staging.api.com

  # Secret variables (masked in logs)
  API_KEY:
    secret: true
    value: "${STAGING_API_KEY}"  # From system env

  # Profile-specific values
  DEPLOY_TARGET:
    default: dev-server
    staging: staging-cluster
    prod: prod-cluster

commands:
  - shell: "deploy --env ${ENVIRONMENT} --url ${API_URL} --key ${API_KEY}"
    # Output: deploy --env staging --url https://staging.api.com --key ***
</code></pre>
<p><strong>Key Features</strong>:</p>
<ul>
<li><strong>Secrets</strong>: Automatically masked in all output (<code>secret: true</code>)</li>
<li><strong>Profiles</strong>: Different values for dev/staging/prod environments</li>
<li><strong>Step Overrides</strong>: Override variables for specific commands</li>
<li><strong>Interpolation</strong>: Reference system environment variables</li>
</ul>
<p>See <a href="configuration/environment-variables.html#workflow-environment-variables">Environment Variables - Workflow Section</a> for complete documentation.</p>
<p><strong>Note</strong>: Project config <code>variables</code> are separate from workflow <code>env</code> and serve different purposes:</p>
<ul>
<li><strong>Workflow <code>env:</code></strong>: Runtime environment variables, supports secrets and profiles</li>
<li><strong>Config <code>variables:</code></strong>: Project metadata and settings (deprecated for workflow use)</li>
</ul>
<h3 id="mapreduce-workflows-1"><a class="header" href="#mapreduce-workflows-1">MapReduce Workflows</a></h3>
<p>For parallel processing of large datasets, use MapReduce mode:</p>
<pre><code class="language-yaml">name: process-all-files
mode: mapreduce

map:
  input: items.json
  json_path: "$.files[*]"
  agent_template:
    - claude: "/process-file '${item.path}'"
  max_parallel: 10

reduce:
  - claude: "/summarize ${map.results}"
</code></pre>
<p>See the <a href="configuration/../mapreduce-workflows.html">MapReduce Workflows</a> chapter for complete documentation.</p>
<h3 id="workflow-precedence"><a class="header" href="#workflow-precedence">Workflow Precedence</a></h3>
<p>When multiple workflow sources exist, Prodigy uses this precedence:</p>
<ol>
<li>Explicit path via <code>prodigy run workflow.yml</code> (highest)</li>
<li><code>.prodigy/workflow.yml</code> in project directory</li>
<li>Default workflow configuration (lowest)</li>
</ol>
<h3 id="see-also-13"><a class="header" href="#see-also-13">See Also</a></h3>
<ul>
<li><a href="configuration/../workflow-basics.html">Workflow Basics</a> - Complete workflow syntax and features</li>
<li><a href="configuration/../mapreduce-workflows.html">MapReduce Workflows</a> - Parallel execution patterns</li>
<li><a href="configuration/project-configuration-structure.html">Project Configuration Structure</a> - Project variables for workflows</li>
<li><a href="configuration/environment-variables.html">Environment Variables</a> - Using environment variables in workflows</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="storage-configuration"><a class="header" href="#storage-configuration">Storage Configuration</a></h2>
<p>Storage configuration controls where and how Prodigy stores data including events, state, DLQ (Dead Letter Queue), and worktrees. By default, Prodigy uses <strong>global storage</strong> (<code>~/.prodigy/</code>) to enable cross-worktree data sharing and centralized management.</p>
<h3 id="storage-backend-types"><a class="header" href="#storage-backend-types">Storage Backend Types</a></h3>
<h4 id="file-storage-default"><a class="header" href="#file-storage-default">File Storage (Default)</a></h4>
<p>Stores data as files in the filesystem. This is the recommended backend for production use.</p>
<pre><code class="language-yaml">storage:
  backend: file
  backend_config:
    base_dir: /Users/username/.prodigy
    use_global: true  # Default: use global storage
    enable_file_locks: true
    max_file_size: 104857600  # 100MB
    enable_compression: false
</code></pre>
<h4 id="memory-storage"><a class="header" href="#memory-storage">Memory Storage</a></h4>
<p>Stores data in memory. Useful for testing but data is lost when the process exits.</p>
<pre><code class="language-yaml">storage:
  backend: memory
  backend_config:
    max_memory: 104857600  # 100MB
    persist_to_disk: false
</code></pre>
<h3 id="file-storage-configuration"><a class="header" href="#file-storage-configuration">File Storage Configuration</a></h3>
<h4 id="base_dir"><a class="header" href="#base_dir"><code>base_dir</code></a></h4>
<p><strong>Type</strong>: Path
<strong>Default</strong>: <code>~/.prodigy</code></p>
<p>Base directory for all storage operations.</p>
<pre><code class="language-yaml">storage:
  backend: file
  backend_config:
    base_dir: /custom/storage/path
</code></pre>
<h4 id="use_global"><a class="header" href="#use_global"><code>use_global</code></a></h4>
<p><strong>Type</strong>: Boolean
<strong>Default</strong>: <code>true</code></p>
<p><strong>IMPORTANT</strong>: Global storage is the default and recommended setting. When <code>true</code>, Prodigy stores all data in <code>~/.prodigy/</code> organized by repository:</p>
<pre><code>~/.prodigy/
├── events/{repo_name}/{job_id}/
├── dlq/{repo_name}/{job_id}/
├── state/{repo_name}/mapreduce/jobs/{job_id}/
└── worktrees/{repo_name}/{session_id}/
</code></pre>
<p>Benefits of global storage:</p>
<ul>
<li><strong>Cross-worktree event aggregation</strong>: Multiple worktrees share event logs</li>
<li><strong>Persistent state</strong>: Job checkpoints survive worktree cleanup</li>
<li><strong>Centralized monitoring</strong>: All job data in one location</li>
<li><strong>Efficient storage</strong>: Deduplication across worktrees</li>
</ul>
<p>When <code>false</code> (deprecated, local storage):</p>
<pre><code>.prodigy/  # In project directory
├── events/
├── dlq/
└── state/
</code></pre>
<p><strong>Recommendation</strong>: Always use <code>use_global: true</code> (the default).</p>
<h4 id="enable_file_locks"><a class="header" href="#enable_file_locks"><code>enable_file_locks</code></a></h4>
<p><strong>Type</strong>: Boolean
<strong>Default</strong>: <code>true</code></p>
<p>Enable file-based locking to prevent concurrent access conflicts.</p>
<pre><code class="language-yaml">storage:
  backend: file
  backend_config:
    enable_file_locks: true
</code></pre>
<h4 id="max_file_size"><a class="header" href="#max_file_size"><code>max_file_size</code></a></h4>
<p><strong>Type</strong>: Integer (bytes)
<strong>Default</strong>: <code>104857600</code> (100MB)</p>
<p>Maximum file size before rotation.</p>
<pre><code class="language-yaml">storage:
  backend: file
  backend_config:
    max_file_size: 209715200  # 200MB
</code></pre>
<h4 id="enable_compression"><a class="header" href="#enable_compression"><code>enable_compression</code></a></h4>
<p><strong>Type</strong>: Boolean
<strong>Default</strong>: <code>false</code></p>
<p>Enable compression for archived files.</p>
<pre><code class="language-yaml">storage:
  backend: file
  backend_config:
    enable_compression: true
</code></pre>
<h3 id="connection-and-performance"><a class="header" href="#connection-and-performance">Connection and Performance</a></h3>
<h4 id="connection_pool_size"><a class="header" href="#connection_pool_size"><code>connection_pool_size</code></a></h4>
<p><strong>Type</strong>: Integer
<strong>Default</strong>: <code>10</code></p>
<p>Connection pool size for backend operations (future database backends).</p>
<pre><code class="language-yaml">storage:
  connection_pool_size: 20
</code></pre>
<h4 id="timeout-1"><a class="header" href="#timeout-1"><code>timeout</code></a></h4>
<p><strong>Type</strong>: Duration
<strong>Default</strong>: <code>30s</code></p>
<p>Default timeout for storage operations.</p>
<pre><code class="language-yaml">storage:
  timeout: 60s  # Format: 60s, 1m, 1h
</code></pre>
<h4 id="enable_locking"><a class="header" href="#enable_locking"><code>enable_locking</code></a></h4>
<p><strong>Type</strong>: Boolean
<strong>Default</strong>: <code>true</code></p>
<p>Enable distributed locking for concurrent access control.</p>
<pre><code class="language-yaml">storage:
  enable_locking: true
</code></pre>
<h3 id="retry-policy"><a class="header" href="#retry-policy">Retry Policy</a></h3>
<p>Configure automatic retries for transient failures:</p>
<pre><code class="language-yaml">storage:
  retry_policy:
    max_retries: 3
    initial_delay: 1s
    max_delay: 30s
    backoff_multiplier: 2.0
    jitter: true
</code></pre>
<h4 id="max_retries"><a class="header" href="#max_retries"><code>max_retries</code></a></h4>
<p><strong>Type</strong>: Integer
<strong>Default</strong>: <code>3</code></p>
<p>Maximum number of retry attempts.</p>
<h4 id="initial_delay"><a class="header" href="#initial_delay"><code>initial_delay</code></a></h4>
<p><strong>Type</strong>: Duration
<strong>Default</strong>: <code>1s</code></p>
<p>Initial delay before first retry.</p>
<h4 id="max_delay"><a class="header" href="#max_delay"><code>max_delay</code></a></h4>
<p><strong>Type</strong>: Duration
<strong>Default</strong>: <code>30s</code></p>
<p>Maximum delay between retries (with exponential backoff).</p>
<h4 id="backoff_multiplier"><a class="header" href="#backoff_multiplier"><code>backoff_multiplier</code></a></h4>
<p><strong>Type</strong>: Float
<strong>Default</strong>: <code>2.0</code></p>
<p>Multiplier for exponential backoff (delay doubles each retry).</p>
<h4 id="jitter"><a class="header" href="#jitter"><code>jitter</code></a></h4>
<p><strong>Type</strong>: Boolean
<strong>Default</strong>: <code>true</code></p>
<p>Add random jitter to retry delays to avoid thundering herd.</p>
<h3 id="cache-configuration"><a class="header" href="#cache-configuration">Cache Configuration</a></h3>
<p>Optional caching layer for improved performance:</p>
<pre><code class="language-yaml">storage:
  enable_cache: true
  cache_config:
    max_entries: 1000
    ttl: 1h
    cache_type: memory
</code></pre>
<h4 id="enable_cache"><a class="header" href="#enable_cache"><code>enable_cache</code></a></h4>
<p><strong>Type</strong>: Boolean
<strong>Default</strong>: <code>false</code></p>
<p>Enable in-memory caching.</p>
<h4 id="max_entries"><a class="header" href="#max_entries"><code>max_entries</code></a></h4>
<p><strong>Type</strong>: Integer
<strong>Default</strong>: <code>1000</code></p>
<p>Maximum number of cached entries.</p>
<h4 id="ttl"><a class="header" href="#ttl"><code>ttl</code></a></h4>
<p><strong>Type</strong>: Duration
<strong>Default</strong>: <code>1h</code></p>
<p>Cache time-to-live.</p>
<h4 id="cache_type"><a class="header" href="#cache_type"><code>cache_type</code></a></h4>
<p><strong>Type</strong>: String
<strong>Default</strong>: <code>memory</code>
<strong>Valid values</strong>: <code>memory</code></p>
<p>Cache implementation type (currently only memory is supported).</p>
<h3 id="complete-example-7"><a class="header" href="#complete-example-7">Complete Example</a></h3>
<pre><code class="language-yaml">storage:
  backend: file
  connection_pool_size: 10
  timeout: 30s
  enable_locking: true
  enable_cache: false

  backend_config:
    base_dir: /Users/username/.prodigy
    use_global: true
    enable_file_locks: true
    max_file_size: 104857600
    enable_compression: false

  retry_policy:
    max_retries: 3
    initial_delay: 1s
    max_delay: 30s
    backoff_multiplier: 2.0
    jitter: true

  cache_config:
    max_entries: 1000
    ttl: 1h
    cache_type: memory
</code></pre>
<h3 id="environment-variable-configuration"><a class="header" href="#environment-variable-configuration">Environment Variable Configuration</a></h3>
<p>Storage can be configured via environment variables:</p>
<pre><code class="language-bash"># Backend type (file or memory)
export PRODIGY_STORAGE_TYPE=file

# Base directory path
export PRODIGY_STORAGE_BASE_PATH=/custom/path

# Alternative variable names (deprecated)
export PRODIGY_STORAGE_DIR=/custom/path
export PRODIGY_STORAGE_PATH=/custom/path
</code></pre>
<p>Environment variables override file-based configuration. See <a href="configuration/environment-variables.html">Environment Variables</a> for complete list.</p>
<h3 id="storage-directory-structure"><a class="header" href="#storage-directory-structure">Storage Directory Structure</a></h3>
<p>With global storage enabled (default):</p>
<pre><code>~/.prodigy/
├── events/
│   └── {repo_name}/
│       └── {job_id}/
│           └── events-{timestamp}.jsonl
├── dlq/
│   └── {repo_name}/
│       └── {job_id}/
│           └── items.json
├── state/
│   └── {repo_name}/
│       └── mapreduce/
│           └── jobs/
│               └── {job_id}/
│                   ├── setup-checkpoint.json
│                   ├── map-checkpoint-{timestamp}.json
│                   └── reduce-checkpoint-v1-{timestamp}.json
├── worktrees/
│   └── {repo_name}/
│       └── session-{session_id}/
└── orphaned_worktrees/
    └── {repo_name}/
        └── {job_id}.json
</code></pre>
<h3 id="migration-from-local-storage"><a class="header" href="#migration-from-local-storage">Migration from Local Storage</a></h3>
<p>If you have existing local storage (<code>.prodigy/</code> in project directory), migrate to global storage:</p>
<ol>
<li><strong>Automatic</strong>: Set <code>use_global: true</code> (default) - Prodigy creates new global storage</li>
<li><strong>Manual Migration</strong>: Copy existing data to global storage structure</li>
<li><strong>Coexistence</strong>: Old local storage is ignored once global storage is active</li>
</ol>
<p><strong>Note</strong>: Global storage is the default and recommended approach. Local storage is deprecated.</p>
<h3 id="troubleshooting-6"><a class="header" href="#troubleshooting-6">Troubleshooting</a></h3>
<p><strong>Issue</strong>: “Failed to acquire storage lock”
<strong>Solution</strong>: Check for stuck processes, wait for lock release, or disable locking temporarily</p>
<p><strong>Issue</strong>: “Storage directory not writable”
<strong>Solution</strong>: Check permissions on <code>~/.prodigy</code> directory</p>
<p><strong>Issue</strong>: “Disk space full”
<strong>Solution</strong>: Clean up old events/DLQ data, increase <code>max_file_size</code>, enable compression</p>
<h3 id="see-also-14"><a class="header" href="#see-also-14">See Also</a></h3>
<ul>
<li><a href="configuration/environment-variables.html">Environment Variables</a></li>
<li><a href="configuration/default-values-reference.html">Default Values Reference</a></li>
<li><a href="configuration/troubleshooting.html">Troubleshooting</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="environment-variables"><a class="header" href="#environment-variables">Environment Variables</a></h2>
<p>Prodigy supports two types of environment variables:</p>
<ol>
<li><strong>System Environment Variables</strong>: Standard Unix environment variables that control Prodigy’s behavior globally</li>
<li><strong>Workflow Environment Variables</strong>: Variables defined in workflow YAML files that are available during workflow execution</li>
</ol>
<p>This page documents both types. For details on how environment variables interact with other configuration sources, see <a href="configuration/configuration-precedence-rules.html">Configuration Precedence Rules</a>.</p>
<hr />
<h2 id="workflow-environment-variables"><a class="header" href="#workflow-environment-variables">Workflow Environment Variables</a></h2>
<p>Workflows can define custom environment variables using the <code>env:</code> block. These variables are available to all commands within the workflow and support advanced features like secrets, profiles, and interpolation.</p>
<h3 id="basic-syntax-1"><a class="header" href="#basic-syntax-1">Basic Syntax</a></h3>
<pre><code class="language-yaml">name: my-workflow

env:
  # Plain variables
  PROJECT_NAME: "prodigy"
  VERSION: "1.0.0"
  BUILD_DIR: "target/release"

commands:
  - shell: "echo Building $PROJECT_NAME version $VERSION"
  - shell: "cargo build --release --target-dir $BUILD_DIR"
</code></pre>
<h3 id="variable-interpolation-4"><a class="header" href="#variable-interpolation-4">Variable Interpolation</a></h3>
<p>Workflow environment variables can be referenced using two syntaxes:</p>
<ul>
<li><strong><code>$VAR</code></strong> - Simple variable reference (shell-style)</li>
<li><strong><code>${VAR}</code></strong> - Bracketed reference (recommended for clarity and complex expressions)</li>
</ul>
<pre><code class="language-yaml">env:
  API_URL: "https://api.example.com"
  API_VERSION: "v2"
  ENDPOINT: "${API_URL}/${API_VERSION}"

commands:
  - shell: "curl ${ENDPOINT}/status"
  - claude: "/deploy --url $API_URL --version $API_VERSION"
</code></pre>
<h3 id="secrets-and-sensitive-data"><a class="header" href="#secrets-and-sensitive-data">Secrets and Sensitive Data</a></h3>
<p>Mark sensitive values as secrets to automatically mask them in logs and output:</p>
<pre><code class="language-yaml">env:
  # Public configuration
  DATABASE_HOST: "db.example.com"

  # Secret configuration (masked in logs)
  DATABASE_PASSWORD:
    secret: true
    value: "super-secret-password"

  API_KEY:
    secret: true
    value: "sk-abc123..."

commands:
  - shell: "psql -h $DATABASE_HOST -p $DATABASE_PASSWORD"
  # Output: psql -h db.example.com -p ***
</code></pre>
<p><strong>Security Best Practices</strong>:</p>
<ul>
<li>Always mark API keys, passwords, and tokens as secrets</li>
<li>Never commit secret values to version control</li>
<li>Use environment variable references for secrets: <code>value: "${PROD_API_KEY}"</code></li>
<li>Rotate secrets regularly</li>
</ul>
<h3 id="profiles-for-multiple-environments"><a class="header" href="#profiles-for-multiple-environments">Profiles for Multiple Environments</a></h3>
<p>Profiles allow different values for different environments (dev, staging, prod):</p>
<pre><code class="language-yaml">env:
  # API endpoints vary by environment
  API_URL:
    default: "http://localhost:3000"
    staging: "https://staging.api.com"
    prod: "https://api.com"

  # Credentials vary by environment
  API_KEY:
    secret: true
    default: "dev-key-123"
    staging:
      secret: true
      value: "${STAGING_API_KEY}"  # From system env
    prod:
      secret: true
      value: "${PROD_API_KEY}"

commands:
  - shell: "curl -H 'Authorization: Bearer ${API_KEY}' ${API_URL}/health"
</code></pre>
<p><strong>Activate a profile</strong>:</p>
<pre><code class="language-bash"># Use staging profile
prodigy run workflow.yml --profile staging

# Use prod profile via environment variable
export PRODIGY_PROFILE=prod
prodigy run workflow.yml
</code></pre>
<h3 id="step-level-environment-overrides-2"><a class="header" href="#step-level-environment-overrides-2">Step-Level Environment Overrides</a></h3>
<p>Individual commands can override workflow environment variables:</p>
<pre><code class="language-yaml">env:
  NODE_ENV: "development"
  LOG_LEVEL: "info"

commands:
  # Uses workflow-level NODE_ENV
  - shell: "npm test"

  # Override for this command only
  - shell: "npm run build"
    env:
      NODE_ENV: "production"
      LOG_LEVEL: "warn"

  # Back to workflow-level NODE_ENV
  - shell: "npm start"
</code></pre>
<p><strong>Precedence</strong>: Step env &gt; Profile env &gt; Workflow env &gt; System env</p>
<h3 id="mapreduce-environment-variables-2"><a class="header" href="#mapreduce-environment-variables-2">MapReduce Environment Variables</a></h3>
<p>Environment variables work across all MapReduce phases (setup, map, reduce, merge):</p>
<pre><code class="language-yaml">name: parallel-processing
mode: mapreduce

env:
  MAX_PARALLEL: "10"
  TIMEOUT: "300"
  OUTPUT_DIR: "/tmp/results"

setup:
  - shell: "mkdir -p $OUTPUT_DIR"
  - shell: "generate-work-items.sh &gt; items.json"

map:
  input: "items.json"
  json_path: "$[*]"
  max_parallel: ${MAX_PARALLEL}  # Use env var for parallelism

  agent_template:
    - claude: "/process ${item.file} --timeout $TIMEOUT"
    - shell: "cp result.json ${OUTPUT_DIR}/${item.name}.json"

reduce:
  - shell: "echo Processed ${map.total} items to $OUTPUT_DIR"
</code></pre>
<p><strong>Advanced MapReduce Usage</strong>:</p>
<ul>
<li>Use env vars for <code>max_parallel</code>, <code>timeout</code>, <code>agent_timeout_secs</code></li>
<li>Reference in <code>filter</code> and <code>sort_by</code> expressions</li>
<li>Pass to validation and gap-filling commands</li>
</ul>
<h3 id="environment-files-env"><a class="header" href="#environment-files-env">Environment Files (<code>.env</code>)</a></h3>
<p>Load variables from dotenv-format files (not yet implemented in Prodigy, but planned):</p>
<pre><code class="language-yaml">env:
  env_files:
    - ".env"
    - ".env.${PRODIGY_PROFILE}"

# .env file format:
# PROJECT_NAME=prodigy
# VERSION=1.0.0
# API_KEY=sk-abc123
</code></pre>
<p><strong>Note</strong>: This feature is planned but not yet available. Use system environment variables as a workaround.</p>
<h3 id="complete-workflow-example"><a class="header" href="#complete-workflow-example">Complete Workflow Example</a></h3>
<pre><code class="language-yaml">name: deployment-workflow

env:
  # Project configuration
  PROJECT_NAME: "my-app"
  VERSION: "2.1.0"

  # Environment-specific settings
  DEPLOY_TARGET:
    default: "dev-server"
    staging: "staging-cluster"
    prod: "prod-cluster"

  # Secrets (masked in logs)
  DEPLOY_TOKEN:
    secret: true
    default: "${DEV_TOKEN}"
    prod:
      secret: true
      value: "${PROD_TOKEN}"

commands:
  - shell: "echo Deploying $PROJECT_NAME v$VERSION to $DEPLOY_TARGET"
  - shell: "docker build -t ${PROJECT_NAME}:${VERSION} ."
  - shell: "deploy --target $DEPLOY_TARGET --token $DEPLOY_TOKEN"
  # Output: deploy --target prod-cluster --token ***
</code></pre>
<p>Run with:</p>
<pre><code class="language-bash"># Development deployment
prodigy run deploy.yml

# Production deployment
export PROD_TOKEN="secret-prod-token"
prodigy run deploy.yml --profile prod
</code></pre>
<hr />
<h2 id="system-environment-variables"><a class="header" href="#system-environment-variables">System Environment Variables</a></h2>
<p>System environment variables control Prodigy’s global behavior and configuration.</p>
<h3 id="claude-api-configuration"><a class="header" href="#claude-api-configuration">Claude API Configuration</a></h3>
<h4 id="prodigy_claude_api_key"><a class="header" href="#prodigy_claude_api_key"><code>PRODIGY_CLAUDE_API_KEY</code></a></h4>
<p><strong>Purpose</strong>: Claude API key for AI-powered commands
<strong>Default</strong>: None
<strong>Overrides</strong>: Global and project <code>claude_api_key</code> settings</p>
<pre><code class="language-bash">export PRODIGY_CLAUDE_API_KEY="sk-ant-api03-..."
</code></pre>
<p>This is the <strong>recommended</strong> way to provide API keys (more secure than storing in config files).</p>
<h4 id="prodigy_claude_streaming"><a class="header" href="#prodigy_claude_streaming"><code>PRODIGY_CLAUDE_STREAMING</code></a></h4>
<p><strong>Purpose</strong>: Control Claude JSON streaming output
<strong>Default</strong>: <code>true</code> (streaming enabled by default)
<strong>Valid values</strong>: <code>true</code>, <code>false</code></p>
<pre><code class="language-bash">export PRODIGY_CLAUDE_STREAMING=false  # Disable streaming
</code></pre>
<p>When <code>false</code>, uses legacy print mode instead of JSON streaming.</p>
<h3 id="general-configuration"><a class="header" href="#general-configuration">General Configuration</a></h3>
<h4 id="prodigy_log_level"><a class="header" href="#prodigy_log_level"><code>PRODIGY_LOG_LEVEL</code></a></h4>
<p><strong>Purpose</strong>: Logging verbosity
<strong>Default</strong>: <code>info</code>
<strong>Valid values</strong>: <code>trace</code>, <code>debug</code>, <code>info</code>, <code>warn</code>, <code>error</code>
<strong>Overrides</strong>: Global and project <code>log_level</code> settings</p>
<pre><code class="language-bash">export PRODIGY_LOG_LEVEL=debug
</code></pre>
<h4 id="prodigy_editor"><a class="header" href="#prodigy_editor"><code>PRODIGY_EDITOR</code></a></h4>
<p><strong>Purpose</strong>: Default text editor for interactive operations
<strong>Default</strong>: None
<strong>Overrides</strong>: Global <code>default_editor</code> setting
<strong>Fallback</strong>: <code>EDITOR</code> environment variable</p>
<pre><code class="language-bash">export PRODIGY_EDITOR=vim
</code></pre>
<p>If neither <code>PRODIGY_EDITOR</code> nor <code>EDITOR</code> is set, Prodigy uses system defaults.</p>
<h4 id="editor"><a class="header" href="#editor"><code>EDITOR</code></a></h4>
<p><strong>Purpose</strong>: Standard Unix editor variable (fallback)
<strong>Default</strong>: None
<strong>Fallback for</strong>: <code>PRODIGY_EDITOR</code></p>
<pre><code class="language-bash">export EDITOR=nano
</code></pre>
<p><strong>Precedence</strong>: <code>PRODIGY_EDITOR</code> takes precedence over <code>EDITOR</code> if both are set.</p>
<h4 id="prodigy_auto_commit"><a class="header" href="#prodigy_auto_commit"><code>PRODIGY_AUTO_COMMIT</code></a></h4>
<p><strong>Purpose</strong>: Automatic commit after successful commands
<strong>Default</strong>: <code>true</code>
<strong>Valid values</strong>: <code>true</code>, <code>false</code>
<strong>Overrides</strong>: Global and project <code>auto_commit</code> settings</p>
<pre><code class="language-bash">export PRODIGY_AUTO_COMMIT=false
</code></pre>
<h3 id="storage-configuration-1"><a class="header" href="#storage-configuration-1">Storage Configuration</a></h3>
<h4 id="prodigy_storage_type"><a class="header" href="#prodigy_storage_type"><code>PRODIGY_STORAGE_TYPE</code></a></h4>
<p><strong>Purpose</strong>: Storage backend type
<strong>Default</strong>: <code>file</code>
<strong>Valid values</strong>: <code>file</code>, <code>memory</code>
<strong>Overrides</strong>: Storage <code>backend</code> setting</p>
<pre><code class="language-bash">export PRODIGY_STORAGE_TYPE=file
</code></pre>
<h4 id="prodigy_storage_base_path"><a class="header" href="#prodigy_storage_base_path"><code>PRODIGY_STORAGE_BASE_PATH</code></a></h4>
<p><strong>Purpose</strong>: Base directory for file storage
<strong>Default</strong>: <code>~/.prodigy</code>
<strong>Overrides</strong>: Storage <code>backend_config.base_dir</code> setting</p>
<pre><code class="language-bash">export PRODIGY_STORAGE_BASE_PATH=/custom/storage/path
</code></pre>
<p><strong>Alternative names</strong> (deprecated, use <code>PRODIGY_STORAGE_BASE_PATH</code>):</p>
<ul>
<li><code>PRODIGY_STORAGE_DIR</code></li>
<li><code>PRODIGY_STORAGE_PATH</code></li>
</ul>
<h3 id="workflow-execution"><a class="header" href="#workflow-execution">Workflow Execution</a></h3>
<h4 id="prodigy_automation"><a class="header" href="#prodigy_automation"><code>PRODIGY_AUTOMATION</code></a></h4>
<p><strong>Purpose</strong>: Signal automated execution mode
<strong>Default</strong>: Not set
<strong>Set by</strong>: Prodigy when executing workflows</p>
<pre><code class="language-bash">export PRODIGY_AUTOMATION=true
</code></pre>
<p>This variable is <strong>set automatically</strong> by Prodigy during workflow execution. It signals to Claude and other tools that execution is automated (not interactive).</p>
<h4 id="prodigy_claude_console_output"><a class="header" href="#prodigy_claude_console_output"><code>PRODIGY_CLAUDE_CONSOLE_OUTPUT</code></a></h4>
<p><strong>Purpose</strong>: Force Claude streaming output regardless of verbosity
<strong>Default</strong>: Not set
<strong>Valid values</strong>: <code>true</code>, <code>false</code></p>
<pre><code class="language-bash">export PRODIGY_CLAUDE_CONSOLE_OUTPUT=true
</code></pre>
<p>When set to <code>true</code>, forces JSON streaming output even when verbosity is 0. Useful for debugging specific runs without changing command flags.</p>
<h3 id="complete-example-8"><a class="header" href="#complete-example-8">Complete Example</a></h3>
<p>Set up a complete Prodigy environment:</p>
<pre><code class="language-bash"># API key (recommended method)
export PRODIGY_CLAUDE_API_KEY="sk-ant-api03-..."

# Logging
export PRODIGY_LOG_LEVEL=info

# Editor
export PRODIGY_EDITOR=code

# Behavior
export PRODIGY_AUTO_COMMIT=true

# Storage
export PRODIGY_STORAGE_TYPE=file
export PRODIGY_STORAGE_BASE_PATH=/Users/username/.prodigy

# Development/debugging
export PRODIGY_CLAUDE_STREAMING=true
export PRODIGY_CLAUDE_CONSOLE_OUTPUT=false
</code></pre>
<h3 id="environment-files-2"><a class="header" href="#environment-files-2">Environment Files</a></h3>
<p>You can use <code>.env</code> files (not committed to version control) to manage environment variables:</p>
<pre><code class="language-bash"># .env (add to .gitignore)
PRODIGY_CLAUDE_API_KEY=sk-ant-api03-...
PRODIGY_LOG_LEVEL=debug
PRODIGY_AUTO_COMMIT=false
</code></pre>
<p>Load with:</p>
<pre><code class="language-bash"># Using direnv
eval "$(cat .env)"

# Using dotenv tool
dotenv run prodigy run workflow.yml

# Manually
export $(cat .env | xargs)
</code></pre>
<h3 id="security-best-practices-1"><a class="header" href="#security-best-practices-1">Security Best Practices</a></h3>
<ol>
<li><strong>Never commit API keys</strong> to version control</li>
<li><strong>Use environment variables</strong> for secrets (not config files)</li>
<li><strong>Use <code>.env</code> files</strong> (gitignored) for local development</li>
<li><strong>Use secret managers</strong> (AWS Secrets Manager, Vault) in production</li>
<li><strong>Rotate keys regularly</strong> and use project-specific keys when possible</li>
</ol>
<p><strong>Example <code>.gitignore</code></strong>:</p>
<pre><code>.env
.env.*
!.env.example
.prodigy/config.local.yml
</code></pre>
<h3 id="precedence-summary"><a class="header" href="#precedence-summary">Precedence Summary</a></h3>
<p>For any given setting, the effective value comes from (highest to lowest):</p>
<ol>
<li><strong>CLI flags</strong> (if applicable)</li>
<li><strong>Environment variables</strong> ← This level</li>
<li><strong>Project config</strong> (<code>.prodigy/config.yml</code>)</li>
<li><strong>Global config</strong> (<code>~/.prodigy/config.yml</code>)</li>
<li><strong>Defaults</strong> (built-in values)</li>
</ol>
<p>Example:</p>
<pre><code class="language-yaml"># ~/.prodigy/config.yml
log_level: info
</code></pre>
<pre><code class="language-yaml"># .prodigy/config.yml
log_level: warn
</code></pre>
<pre><code class="language-bash">export PRODIGY_LOG_LEVEL=debug  # This wins
</code></pre>
<p><strong>Result</strong>: <code>log_level: debug</code></p>
<h3 id="checking-environment-variables"><a class="header" href="#checking-environment-variables">Checking Environment Variables</a></h3>
<p>To see which environment variables are active:</p>
<pre><code class="language-bash"># List all PRODIGY_* variables
env | grep PRODIGY_

# Check effective configuration (merges all sources)
prodigy config show
</code></pre>
<h3 id="see-also-15"><a class="header" href="#see-also-15">See Also</a></h3>
<ul>
<li><a href="configuration/configuration-precedence-rules.html">Configuration Precedence Rules</a></li>
<li><a href="configuration/global-configuration-structure.html">Global Configuration Structure</a></li>
<li><a href="configuration/project-configuration-structure.html">Project Configuration Structure</a></li>
<li><a href="configuration/storage-configuration.html">Storage Configuration</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="complete-configuration-examples"><a class="header" href="#complete-configuration-examples">Complete Configuration Examples</a></h2>
<p>This subsection provides comprehensive, production-ready workflow examples demonstrating all major Prodigy configuration features. Each example is extracted from real workflows in the repository and includes detailed annotations explaining configuration choices.</p>
<h3 id="quick-reference-1"><a class="header" href="#quick-reference-1">Quick Reference</a></h3>
<p>Complete workflow configurations include:</p>
<div class="table-wrapper"><table><thead><tr><th>Feature</th><th>Standard Workflow</th><th>MapReduce Workflow</th></tr></thead><tbody>
<tr><td><strong>Basic Structure</strong></td><td><code>commands: []</code></td><td><code>mode: mapreduce</code> with <code>setup</code>, <code>map</code>, <code>reduce</code></td></tr>
<tr><td><strong>Environment Variables</strong></td><td><code>env:</code>, <code>secrets:</code>, <code>profiles:</code></td><td>Same + phase-specific overrides</td></tr>
<tr><td><strong>Command Types</strong></td><td><code>claude:</code>, <code>shell:</code>, <code>goal_seek:</code>, <code>foreach:</code>, <code>write_file:</code></td><td>Same + <code>agent_template</code></td></tr>
<tr><td><strong>Error Handling</strong></td><td><code>on_failure:</code>, <code>on_success:</code>, <code>retry:</code></td><td>Same + <code>error_policy:</code>, <code>on_item_failure:</code></td></tr>
<tr><td><strong>Validation</strong></td><td><code>validate:</code> with <code>threshold</code>, <code>on_incomplete</code></td><td>Per-step validation + gap filling</td></tr>
<tr><td><strong>Output Capture</strong></td><td><code>capture_output:</code>, <code>outputs:</code></td><td><code>capture_outputs:</code> in setup phase</td></tr>
<tr><td><strong>Timeouts</strong></td><td><code>timeout:</code> per command</td><td><code>timeout:</code> per phase + <code>agent_timeout_secs</code></td></tr>
<tr><td><strong>Merge Workflow</strong></td><td><code>merge:</code> with custom commands</td><td>Same with <code>${merge.*}</code> variables</td></tr>
</tbody></table>
</div>
<hr />
<h3 id="1-complete-standard-workflow-example"><a class="header" href="#1-complete-standard-workflow-example">1. Complete Standard Workflow Example</a></h3>
<p>This example demonstrates a full standard workflow with all major configuration options.</p>
<p><strong>Source</strong>: workflows/debtmap.yml (lines 1-56)</p>
<pre><code class="language-yaml"># Sequential workflow for technical debt analysis and remediation
# Demonstrates: validation, goal-seeking, error handlers, output capture

# Phase 1: Generate coverage data
- shell: "just coverage-lcov"
  timeout: 300

# Phase 2: Analyze tech debt and capture baseline
- shell: "debtmap analyze . --lcov target/coverage/lcov.info --output .prodigy/debtmap-before.json --format json"
  capture_output: true

# Phase 3: Create implementation plan with validation
- claude: "/prodigy-debtmap-plan --before .prodigy/debtmap-before.json --output .prodigy/IMPLEMENTATION_PLAN.md"
  commit_required: true
  validate:
    commands:
      - claude: "/prodigy-validate-debtmap-plan --before .prodigy/debtmap-before.json --plan .prodigy/IMPLEMENTATION_PLAN.md --output .prodigy/plan-validation.json"
    result_file: ".prodigy/plan-validation.json"
    threshold: 75  # Must achieve 75% completeness
    on_incomplete:
      commands:
        - claude: "/prodigy-revise-debtmap-plan --gaps ${validation.gaps} --plan .prodigy/IMPLEMENTATION_PLAN.md"
      max_attempts: 3
      fail_workflow: false

# Phase 4: Execute the plan with comprehensive validation
- claude: "/prodigy-debtmap-implement --plan .prodigy/IMPLEMENTATION_PLAN.md"
  commit_required: true
  validate:
    commands:
      - shell: "just coverage-lcov"
      - shell: "debtmap analyze . --lcov target/coverage/lcov.info --output .prodigy/debtmap-after.json --format json"
      - shell: "debtmap compare --before .prodigy/debtmap-before.json --after .prodigy/debtmap-after.json --plan .prodigy/IMPLEMENTATION_PLAN.md --output .prodigy/comparison.json --format json"
      - claude: "/prodigy-validate-debtmap-improvement --comparison .prodigy/comparison.json --output .prodigy/debtmap-validation.json"
    result_file: ".prodigy/debtmap-validation.json"
    threshold: 75
    on_incomplete:
      commands:
        - claude: "/prodigy-complete-debtmap-fix --plan .prodigy/IMPLEMENTATION_PLAN.md --validation .prodigy/debtmap-validation.json --attempt ${validation.attempt_number}"
          commit_required: true
        - shell: "just coverage-lcov"
        - shell: "debtmap analyze . --lcov target/coverage/lcov.info --output .prodigy/debtmap-after.json --format json"
        - shell: "debtmap compare --before .prodigy/debtmap-before.json --after .prodigy/debtmap-after.json --plan .prodigy/IMPLEMENTATION_PLAN.md --output .prodigy/comparison.json --format json"
      max_attempts: 5
      fail_workflow: true

# Phase 5: Verify tests pass with error recovery
- shell: "just test"
  on_failure:
    claude: "/prodigy-debug-test-failure --output ${shell.output}"
    max_attempts: 5
    fail_workflow: true

# Phase 6: Enforce code quality standards
- shell: "just fmt-check &amp;&amp; just lint"
  on_failure:
    claude: "/prodigy-lint ${shell.output}"
    max_attempts: 5
    fail_workflow: true
</code></pre>
<p><strong>Key Features Demonstrated</strong>:</p>
<ul>
<li><strong>Validation with gap filling</strong>: <code>validate:</code> block with <code>threshold</code> and <code>on_incomplete</code> handler</li>
<li><strong>Error recovery</strong>: <code>on_failure:</code> handlers with <code>max_attempts</code> for automatic fixing</li>
<li><strong>Output capture</strong>: Shell output captured and passed to Claude for debugging</li>
<li><strong>Commit control</strong>: <code>commit_required: true</code> ensures changes are tracked</li>
<li><strong>Timeouts</strong>: Per-command timeout to prevent hanging</li>
<li><strong>Sequential orchestration</strong>: Each phase builds on previous results</li>
</ul>
<p><strong>Configuration Details</strong> (from src/config/command.rs:WorkflowStepCommand):</p>
<ul>
<li><code>commit_required: bool</code> - Whether step must create a git commit (default: false)</li>
<li><code>timeout: u64</code> - Maximum execution time in seconds</li>
<li><code>validate: ValidationConfig</code> - Validation specification with threshold and handlers</li>
<li><code>on_failure: TestDebugConfig</code> - Error handler with max_attempts and fail_workflow</li>
<li><code>capture_output: bool</code> - Capture command output for use in subsequent steps</li>
</ul>
<hr />
<h3 id="2-complete-mapreduce-workflow-example"><a class="header" href="#2-complete-mapreduce-workflow-example">2. Complete MapReduce Workflow Example</a></h3>
<p>This example demonstrates a production MapReduce workflow with all phases and configuration options.</p>
<p><strong>Source</strong>: workflows/book-docs-drift.yml (lines 1-101)</p>
<pre><code class="language-yaml">name: prodigy-book-docs-drift-detection
mode: mapreduce

# Global environment configuration
env:
  # Project configuration
  PROJECT_NAME: "Prodigy"
  PROJECT_CONFIG: ".prodigy/book-config.json"
  FEATURES_PATH: ".prodigy/book-analysis/features.json"

  # Book-specific settings
  BOOK_DIR: "book"
  ANALYSIS_DIR: ".prodigy/book-analysis"
  CHAPTERS_FILE: "workflows/data/prodigy-chapters.json"

  # Workflow settings
  MAX_PARALLEL: "3"

# Setup phase: Analyze codebase and prepare work items
setup:
  - shell: "mkdir -p $ANALYSIS_DIR"

  # Step 1: Analyze codebase features
  - claude: "/prodigy-analyze-features-for-book --project $PROJECT_NAME --config $PROJECT_CONFIG"

  # Step 2: Detect gaps and generate work items
  - claude: "/prodigy-detect-documentation-gaps --project $PROJECT_NAME --config $PROJECT_CONFIG --features $FEATURES_PATH --chapters $CHAPTERS_FILE --book-dir $BOOK_DIR"

# Map phase: Process each documentation subsection in parallel
map:
  input: "${ANALYSIS_DIR}/flattened-items.json"
  json_path: "$[*]"

  agent_template:
    # Step 1: Analyze subsection for drift
    - claude: "/prodigy-analyze-subsection-drift --project $PROJECT_NAME --json '${item}' --features $FEATURES_PATH"
      commit_required: true

    # Step 2: Fix drift with validation
    - claude: "/prodigy-fix-subsection-drift --project $PROJECT_NAME --json '${item}'"
      commit_required: true
      validate:
        claude: "/prodigy-validate-doc-fix --project $PROJECT_NAME --json '${item}' --output .prodigy/validation-result.json"
        result_file: ".prodigy/validation-result.json"
        threshold: 100  # Documentation must meet 100% quality standards
        on_incomplete:
          claude: "/prodigy-complete-doc-fix --project $PROJECT_NAME --json '${item}' --gaps ${validation.gaps}"
          max_attempts: 3
          fail_workflow: false
          commit_required: true

  max_parallel: ${MAX_PARALLEL}

# Reduce phase: Aggregate results and validate build
reduce:
  # Rebuild the book to ensure all chapters compile
  - shell: "cd book &amp;&amp; mdbook build"
    on_failure:
      claude: "/prodigy-fix-book-build-errors --project $PROJECT_NAME"
      commit_required: true

  # Clean up temporary analysis files
  - shell: "rm -rf ${ANALYSIS_DIR}"
  - shell: "git add -A &amp;&amp; git commit -m 'chore: remove temporary book analysis files for ${PROJECT_NAME}' || true"

# Error handling policy
error_policy:
  on_item_failure: dlq          # Send failures to Dead Letter Queue
  continue_on_failure: true     # Don't stop on individual item failures
  max_failures: 2               # Stop if more than 2 items fail
  error_collection: aggregate   # Collect errors for batch reporting

# Custom merge workflow
merge:
  commands:
    - shell: "git fetch origin"
    - claude: "/prodigy-merge-master --project ${PROJECT_NAME}"
    - claude: "/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}"
</code></pre>
<p><strong>Key Features Demonstrated</strong>:</p>
<ul>
<li><strong>Environment parameterization</strong>: All paths and settings in <code>env:</code> block for easy customization</li>
<li><strong>Setup phase</strong>: Generate work items before parallel processing</li>
<li><strong>Agent template</strong>: Commands execute in isolation per work item</li>
<li><strong>Work item access</strong>: <code>${item}</code> variable provides access to current item fields</li>
<li><strong>Parallel execution</strong>: <code>max_parallel</code> controls concurrency (can reference env vars)</li>
<li><strong>Validation with gap filling</strong>: Automatic quality improvement until threshold met</li>
<li><strong>Error policy</strong>: Comprehensive failure handling with DLQ and thresholds</li>
<li><strong>Merge workflow</strong>: Custom merge process with branch variables</li>
</ul>
<p><strong>MapReduce Configuration Details</strong> (from src/config/mapreduce.rs):</p>
<p><strong>SetupPhaseConfig</strong>:</p>
<ul>
<li><code>commands: Vec&lt;WorkflowStep&gt;</code> - Commands to execute during setup</li>
<li><code>timeout: Option&lt;String&gt;</code> - Phase timeout (supports env var references like <code>"$TIMEOUT"</code>)</li>
<li><code>capture_outputs: HashMap&lt;String, CaptureConfig&gt;</code> - Variables to capture from setup</li>
</ul>
<p><strong>MapPhaseYaml</strong>:</p>
<ul>
<li><code>input: String</code> - Path to work items JSON or command to generate items</li>
<li><code>json_path: String</code> - JSONPath expression to extract items (default: <code>""</code> for array root)</li>
<li><code>agent_template: AgentTemplate</code> - Commands to execute per item</li>
<li><code>max_parallel: String</code> - Concurrency limit (supports env vars like <code>"${MAX_PARALLEL}"</code>)</li>
<li><code>filter: Option&lt;String&gt;</code> - Filter expression (e.g., <code>"item.priority &gt;= 5"</code>)</li>
<li><code>sort_by: Option&lt;String&gt;</code> - Sort field with direction (<code>"item.priority DESC"</code>)</li>
<li><code>max_items: Option&lt;usize&gt;</code> - Limit number of items to process</li>
<li><code>offset: Option&lt;usize&gt;</code> - Skip first N items</li>
<li><code>agent_timeout_secs: Option&lt;String&gt;</code> - Per-agent timeout (supports env vars)</li>
</ul>
<p><strong>Error Policy</strong> (from src/cook/workflow/error_policy.rs:WorkflowErrorPolicy):</p>
<ul>
<li><code>on_item_failure: ItemFailureAction</code> - Action on failure: <code>dlq</code>, <code>retry</code>, <code>skip</code>, <code>stop</code> (default: <code>dlq</code>)</li>
<li><code>continue_on_failure: bool</code> - Continue processing after failures (default: <code>true</code>)</li>
<li><code>max_failures: Option&lt;usize&gt;</code> - Stop after N failures</li>
<li><code>failure_threshold: Option&lt;f64&gt;</code> - Stop if failure rate exceeds threshold (0.0 to 1.0)</li>
<li><code>error_collection: ErrorCollectionStrategy</code> - Collection mode: <code>aggregate</code>, <code>immediate</code>, <code>batched</code> (default: <code>aggregate</code>)</li>
</ul>
<p><strong>Merge Workflow Variables</strong>:</p>
<ul>
<li><code>${merge.worktree}</code> - Worktree name being merged</li>
<li><code>${merge.source_branch}</code> - Source branch (worktree branch)</li>
<li><code>${merge.target_branch}</code> - Target branch (original branch)</li>
<li><code>${merge.session_id}</code> - Session ID for correlation</li>
</ul>
<hr />
<h3 id="3-environment-variables-and-secrets-example"><a class="header" href="#3-environment-variables-and-secrets-example">3. Environment Variables and Secrets Example</a></h3>
<p>This example demonstrates comprehensive environment configuration with static variables, dynamic values, secrets, and profiles.</p>
<p><strong>Source</strong>: workflows/environment-example.yml (lines 1-70)</p>
<pre><code class="language-yaml"># Global environment configuration
env:
  # Static environment variables
  NODE_ENV: production
  API_URL: https://api.example.com

  # Dynamic environment variable (computed from command)
  WORKERS:
    command: "nproc 2&gt;/dev/null || echo 4"
    cache: true  # Cache the result for workflow duration

  # Conditional environment variable (based on git branch)
  DEPLOY_ENV:
    condition: "${branch} == 'main'"
    when_true: "production"
    when_false: "staging"

# Secret environment variables (masked in logs)
secrets:
  # Reference to environment variable
  API_KEY: "${env:SECRET_API_KEY}"

# Environment files to load (.env format)
env_files:
  - .env.production

# Environment profiles for different contexts
profiles:
  development:
    NODE_ENV: development
    API_URL: http://localhost:3000
    DEBUG: "true"

  testing:
    NODE_ENV: test
    API_URL: http://localhost:4000
    COVERAGE: "true"

# Workflow steps demonstrating environment features
commands:
  - name: "Show environment"
    shell: "echo NODE_ENV=$NODE_ENV API_URL=$API_URL WORKERS=$WORKERS"
    capture_output: true

  - name: "Build frontend"
    shell: "echo 'Building frontend with NODE_ENV='$NODE_ENV"
    env:
      BUILD_TARGET: production  # Step-specific environment override
      OPTIMIZE: "true"
    working_dir: ./frontend

  - name: "Run tests"
    shell: "echo 'Running tests in test environment'"
    env:
      PYTHONPATH: "./src:./tests"
      TEST_ENV: "true"
    working_dir: ./backend
    temporary: true  # Environment restored after this step

  - name: "Deploy application"
    shell: "echo 'Deploying to '$DEPLOY_ENV' environment'"
    working_dir: "${env.DEPLOY_DIR}"

  - name: "Cleanup"
    shell: "echo 'Cleaning up temporary files'"
    clear_env: true  # Clear all environment variables except step-specific
    env:
      CLEANUP_MODE: "full"
</code></pre>
<p><strong>Environment Configuration Details</strong> (from src/cook/environment/config.rs):</p>
<p><strong>EnvValue Types</strong>:</p>
<ul>
<li><strong>Static</strong>: Simple string value</li>
<li><strong>Dynamic</strong>: Computed from command with optional caching
<ul>
<li><code>command: String</code> - Command to execute for value</li>
<li><code>cache: bool</code> - Cache result (default: false)</li>
</ul>
</li>
<li><strong>Conditional</strong>: Value based on expression evaluation
<ul>
<li><code>condition: String</code> - Expression to evaluate</li>
<li><code>when_true: String</code> - Value when condition is true</li>
<li><code>when_false: String</code> - Value when condition is false</li>
</ul>
</li>
</ul>
<p><strong>Secret Management</strong>:</p>
<ul>
<li>Marked with <code>secret: true</code> or defined in <code>secrets:</code> block</li>
<li>Automatically masked in logs, error messages, and event streams</li>
<li>Supports environment variable references: <code>"${env:VAR_NAME}"</code></li>
</ul>
<p><strong>Profile Usage</strong>:</p>
<pre><code class="language-bash"># Activate a profile at runtime
prodigy run workflow.yml --profile development
prodigy run workflow.yml --profile testing
</code></pre>
<p><strong>Step-Level Environment</strong> (from src/config/command.rs:WorkflowStepCommand):</p>
<ul>
<li><code>env: HashMap&lt;String, String&gt;</code> - Step-specific environment variables</li>
<li><code>working_dir: Option&lt;PathBuf&gt;</code> - Working directory for this step</li>
<li><code>temporary: bool</code> - Restore environment after step (default: false)</li>
<li><code>clear_env: bool</code> - Clear parent environment before applying step env (default: false)</li>
</ul>
<hr />
<h3 id="4-error-handling-and-retry-strategies-example"><a class="header" href="#4-error-handling-and-retry-strategies-example">4. Error Handling and Retry Strategies Example</a></h3>
<p>This example demonstrates comprehensive error handling patterns including retry strategies, backoff configurations, and circuit breakers.</p>
<p><strong>Source</strong>: workflows/implement-with-tests.yml (lines 1-79) and workflows/debtmap.yml</p>
<pre><code class="language-yaml"># Nested error handling with automatic recovery
commands:
  # Step 1: Implement specification
  - claude: "/prodigy-implement-spec $ARG"
    analysis:
      max_cache_age: 300

  # Step 2: Run tests with nested error recovery
  - shell: "cargo test"
    capture_output: "test_output"
    commit_required: false
    on_failure:
      # First attempt: Debug test failures
      claude: "/prodigy-debug-test-failures '${test_output}'"
      commit_required: true
      on_success:
        # Verify fixes work
        shell: "cargo test"
        commit_required: false
        on_failure:
          # Second attempt: Deep analysis if still failing
          claude: "/prodigy-fix-test-failures '${shell.output}' --deep-analysis"
          commit_required: true

  # Step 3: Run linting
  - claude: "/prodigy-lint"
    commit_required: false

  # Step 4: Run benchmarks (non-critical)
  - shell: "cargo bench --no-run"
    commit_required: false
    on_failure:
      shell: "echo 'Skipping benchmarks due to compilation issues'"
      commit_required: false

  # Step 5: Final verification with status reporting
  - shell: "cargo test --release"
    capture_output: "final_test_results"
    commit_required: false
    on_failure:
      # Report persistent failures
      claude: "/prodigy-report-test-status failed '${final_test_results}' --notify"
      commit_required: false
    on_success:
      shell: "echo '✅ All tests passing! Implementation complete.'"
      commit_required: false
</code></pre>
<p><strong>Error Handler Configuration</strong> (from src/config/command.rs:TestDebugConfig):</p>
<ul>
<li><code>claude: String</code> - Command to run on failure</li>
<li><code>max_attempts: u32</code> - Maximum retry attempts (default: 3)</li>
<li><code>fail_workflow: bool</code> - Stop workflow if max attempts exceeded (default: false)</li>
<li><code>commit_required: bool</code> - Whether handler must create commits (default: true)</li>
</ul>
<p><strong>Backoff Strategy Types</strong> (from src/cook/workflow/error_policy.rs:BackoffStrategy):</p>
<pre><code class="language-yaml"># Fixed delay between retries
retry:
  backoff:
    fixed:
      delay: 5s

# Linear backoff (delay increases linearly)
retry:
  backoff:
    linear:
      initial: 1s
      increment: 2s

# Exponential backoff (default: 2x multiplier)
retry:
  backoff:
    exponential:
      initial: 1s
      multiplier: 2.0

# Fibonacci sequence delays
retry:
  backoff:
    fibonacci:
      initial: 1s
</code></pre>
<p><strong>Circuit Breaker Configuration</strong> (from src/cook/workflow/error_policy.rs:CircuitBreakerConfig):</p>
<pre><code class="language-yaml">error_policy:
  circuit_breaker:
    failure_threshold: 5       # Open circuit after 5 failures
    success_threshold: 3       # Close after 3 successes
    timeout: 30s              # Time before attempting to close
    half_open_requests: 3     # Requests allowed in half-open state
</code></pre>
<hr />
<h3 id="5-goal-seeking-and-validation-examples"><a class="header" href="#5-goal-seeking-and-validation-examples">5. Goal-Seeking and Validation Examples</a></h3>
<p>This example demonstrates iterative refinement with validation and automatic gap filling.</p>
<p><strong>Source</strong>: workflows/goal-seeking-examples.yml (lines 1-129)</p>
<pre><code class="language-yaml"># Example 1: Test Coverage Improvement
- goal_seek:
    goal: "Achieve 90% test coverage"
    claude: "/prodigy-coverage --improve"
    validate: "cargo tarpaulin --print-summary 2&gt;/dev/null | grep 'Coverage' | sed 's/.*Coverage=\\([0-9]*\\).*/score: \\1/'"
    threshold: 90
    max_attempts: 5
    timeout_seconds: 300
    fail_on_incomplete: true
  commit_required: true

# Example 2: Performance Optimization
- goal_seek:
    goal: "Optimize algorithm performance to under 100ms"
    claude: "/optimize-performance --target 100ms"
    validate: "cargo bench --bench main_bench 2&gt;/dev/null | grep 'time:' | awk '{if ($2 &lt; 100) print \"score: 95\"; else print \"score:\", int(10000/$2)}'"
    threshold: 90
    max_attempts: 4
    timeout_seconds: 600
  commit_required: true

# Example 3: Code Quality with Custom Validation
- goal_seek:
    goal: "Fix all clippy warnings and improve code quality"
    claude: "/fix-clippy-warnings"
    validate: |
      warnings=$(cargo clippy 2&gt;&amp;1 | grep -c warning || echo 0)
      if [ "$warnings" -eq 0 ]; then
        echo "score: 100"
      else
        score=$((100 - warnings * 5))
        echo "score: $score"
      fi
    threshold: 95
    max_attempts: 3
    fail_on_incomplete: false
  commit_required: true

# Example 4: Multi-stage Goal Seeking
- name: "Complete feature implementation with quality checks"
  goal_seek:
    goal: "Implement user profile feature"
    claude: "/implement-feature user-profile"
    validate: "test -f src/features/user_profile.rs &amp;&amp; echo 'score: 100' || echo 'score: 0'"
    threshold: 100
    max_attempts: 2

- name: "Add comprehensive tests"
  goal_seek:
    goal: "Add tests for user profile feature"
    claude: "/add-tests src/features/user_profile.rs"
    validate: |
      test_count=$(grep -c "#\\[test\\]" src/features/user_profile.rs || echo 0)
      if [ "$test_count" -ge 5 ]; then
        echo "score: 100"
      else
        score=$((test_count * 20))
        echo "score: $score"
      fi
    threshold: 100
    max_attempts: 3

- name: "Ensure tests pass"
  goal_seek:
    goal: "Make all user profile tests pass"
    claude: "/fix-tests user_profile"
    validate: "cargo test user_profile 2&gt;&amp;1 | grep -q 'test result: ok' &amp;&amp; echo 'score: 100' || echo 'score: 0'"
    threshold: 100
    max_attempts: 4
    fail_on_incomplete: true
</code></pre>
<p><strong>Goal-Seeking Configuration</strong> (from src/cook/goal_seek/mod.rs):</p>
<ul>
<li><code>goal: String</code> - Human-readable description of the goal</li>
<li><code>claude: String</code> - Claude command to execute for improvement</li>
<li><code>validate: String</code> - Shell command that outputs “score: N” (0-100)</li>
<li><code>threshold: u32</code> - Minimum score required for success (0-100)</li>
<li><code>max_attempts: u32</code> - Maximum refinement iterations</li>
<li><code>timeout_seconds: u64</code> - Maximum time for all attempts</li>
<li><code>fail_on_incomplete: bool</code> - Fail workflow if threshold not reached</li>
</ul>
<p><strong>Validation Output Format</strong>:
The validation command must output a single line with the score:</p>
<pre><code>score: 85
</code></pre>
<p>The score should be between 0 and 100, where 100 indicates complete success.</p>
<hr />
<h3 id="6-foreach-parallel-iteration-example"><a class="header" href="#6-foreach-parallel-iteration-example">6. Foreach Parallel Iteration Example</a></h3>
<p>This example demonstrates parallel iteration over work items with different input sources and concurrency controls.</p>
<p><strong>Configuration Details</strong> (from src/config/command.rs:ForeachConfig):</p>
<pre><code class="language-yaml"># Static list input with parallel execution
- foreach:
    input: ["file1.rs", "file2.rs", "file3.rs"]
    parallel: 3  # Process 3 files concurrently
    do:
      - claude: "/lint ${item}"
      - shell: "rustfmt ${item}"
    continue_on_error: true  # Don't stop on individual item failures

# Command input (output becomes items)
- foreach:
    input:
      command: "find src -name '*.rs' -type f"
    parallel: 5
    do:
      - claude: "/analyze ${item}"
      - shell: "cargo check --file ${item}"
    max_items: 50  # Limit to first 50 files

# Sequential execution (no parallelism)
- foreach:
    input: ["step1", "step2", "step3"]
    parallel: false  # Execute sequentially
    do:
      - claude: "/execute-step ${item}"
</code></pre>
<p><strong>ForeachConfig Structure</strong>:</p>
<ul>
<li><code>input: ForeachInput</code> - Source of items (command or static list)
<ul>
<li><code>command: String</code> - Command whose output (one item per line) becomes items</li>
<li><code>list: Vec&lt;String&gt;</code> - Static list of items</li>
</ul>
</li>
<li><code>parallel: ParallelConfig</code> - Concurrency control
<ul>
<li><code>boolean: bool</code> - Enable/disable parallelism (true = default count, false = sequential)</li>
<li><code>count: usize</code> - Specific number of concurrent items</li>
</ul>
</li>
<li><code>do: Vec&lt;WorkflowStepCommand&gt;</code> - Commands to execute per item</li>
<li><code>continue_on_error: bool</code> - Continue if individual item fails (default: false)</li>
<li><code>max_items: Option&lt;usize&gt;</code> - Limit number of items to process</li>
</ul>
<p><strong>Item Access</strong>:</p>
<ul>
<li>Use <code>${item}</code> to reference the current item in commands</li>
<li>Each iteration runs in a clean environment</li>
<li>Failures are isolated to individual items</li>
</ul>
<hr />
<h3 id="7-write-file-command-example"><a class="header" href="#7-write-file-command-example">7. Write File Command Example</a></h3>
<p>This example demonstrates the <code>write_file</code> command for generating files during workflow execution.</p>
<p><strong>Configuration Details</strong> (from src/config/command.rs:WriteFileConfig):</p>
<pre><code class="language-yaml"># Write plain text file
- write_file:
    path: "reports/summary.txt"
    content: |
      Workflow Summary
      ================
      Project: ${PROJECT_NAME}
      Completed: ${map.successful}/${map.total} items
      Duration: ${workflow.duration}
    format: text
    create_dirs: true  # Create parent directories if needed

# Write JSON file with validation
- write_file:
    path: "config/generated.json"
    content: |
      {
        "version": "${VERSION}",
        "timestamp": "${timestamp}",
        "items_processed": ${map.total}
      }
    format: json  # Validates JSON syntax and pretty-prints
    mode: "0644"

# Write YAML configuration
- write_file:
    path: "config/deploy.yml"
    content: |
      environment: ${DEPLOY_ENV}
      version: ${VERSION}
      features:
        - feature1
        - feature2
    format: yaml  # Validates YAML syntax and formats
    create_dirs: true
</code></pre>
<p><strong>WriteFileConfig Structure</strong>:</p>
<ul>
<li><code>path: String</code> - File path (supports variable interpolation)</li>
<li><code>content: String</code> - Content to write (supports variable interpolation)</li>
<li><code>format: WriteFileFormat</code> - Output format (default: <code>text</code>)
<ul>
<li><code>text</code> - Plain text (no processing)</li>
<li><code>json</code> - JSON with validation and pretty-printing</li>
<li><code>yaml</code> - YAML with validation and formatting</li>
</ul>
</li>
<li><code>mode: String</code> - File permissions in octal format (default: <code>"0644"</code>)</li>
<li><code>create_dirs: bool</code> - Create parent directories (default: false)</li>
</ul>
<hr />
<h3 id="8-advanced-timeout-configuration-example"><a class="header" href="#8-advanced-timeout-configuration-example">8. Advanced Timeout Configuration Example</a></h3>
<p>This example demonstrates timeout configuration at multiple levels.</p>
<p><strong>Configuration Details</strong>:</p>
<pre><code class="language-yaml"># Global timeout for workflow
timeout: 3600  # 1 hour for entire workflow

commands:
  # Command-level timeout
  - shell: "long-running-task.sh"
    timeout: 600  # 10 minutes for this command

# MapReduce with phase-specific timeouts
setup:
  - shell: "setup-task.sh"
  timeout: 300  # 5 minutes for setup phase

map:
  agent_template:
    - claude: "/process ${item}"
      timeout: 180  # 3 minutes per command
  agent_timeout_secs: 600  # 10 minutes total per agent
  timeout_config:
    total_timeout_secs: 3600     # Max time for entire map phase
    idle_timeout_secs: 300       # Kill agent if idle for 5 minutes
    per_item_timeout_secs: 180   # Max time per work item

merge:
  commands:
    - claude: "/merge"
  timeout: 600  # 10 minutes for merge phase
</code></pre>
<p><strong>Timeout Hierarchy</strong> (most specific wins):</p>
<ol>
<li>Command-level <code>timeout:</code> - Per command</li>
<li>Agent-level <code>agent_timeout_secs:</code> - Per MapReduce agent</li>
<li>Phase-level <code>timeout:</code> - Per workflow phase (setup, reduce, merge)</li>
<li>Global-level <code>timeout:</code> - Entire workflow</li>
</ol>
<hr />
<h3 id="cross-references-5"><a class="header" href="#cross-references-5">Cross-References</a></h3>
<p>For more detailed information on specific features:</p>
<ul>
<li><strong>Workflow Structure</strong>: See <a href="configuration/../workflow-basics/full-workflow-structure.html">../workflow-basics/full-workflow-structure.md</a></li>
<li><strong>Environment Variables</strong>: See <a href="configuration/environment-variables.html">environment-variables.md</a></li>
<li><strong>Error Handling</strong>: See <a href="configuration/../error-handling.html">../error-handling.md</a></li>
<li><strong>MapReduce Basics</strong>: See <a href="configuration/../mapreduce/index.html">../mapreduce/index.md</a></li>
<li><strong>Validation</strong>: See <a href="configuration/../advanced/implementation-validation.html">../advanced/implementation-validation.md</a></li>
<li><strong>Goal Seeking</strong>: See <a href="configuration/../advanced/goal-seeking-operations.html">../advanced/goal-seeking-operations.md</a></li>
</ul>
<hr />
<h3 id="best-practices-11"><a class="header" href="#best-practices-11">Best Practices</a></h3>
<ol>
<li><strong>Start Simple</strong>: Begin with basic workflows and add complexity incrementally</li>
<li><strong>Use Environment Variables</strong>: Parameterize workflows for reusability across projects</li>
<li><strong>Add Validation</strong>: Use <code>validate:</code> blocks with <code>on_incomplete</code> for quality gates</li>
<li><strong>Handle Errors Gracefully</strong>: Add <code>on_failure:</code> handlers for automatic recovery</li>
<li><strong>Capture Outputs</strong>: Use <code>capture_output:</code> to pass data between steps</li>
<li><strong>Set Timeouts</strong>: Always set timeouts to prevent hanging workflows</li>
<li><strong>Test Incrementally</strong>: Test each phase separately before running full workflow</li>
<li><strong>Use Annotations</strong>: Add comments explaining configuration choices</li>
<li><strong>Profile for Environments</strong>: Use <code>profiles:</code> for dev/staging/prod configurations</li>
<li><strong>Monitor Progress</strong>: Use <code>-v</code> flag to watch Claude streaming output during development</li>
</ol>
<hr />
<h3 id="common-configuration-pitfalls"><a class="header" href="#common-configuration-pitfalls">Common Configuration Pitfalls</a></h3>
<ol>
<li><strong>Missing <code>mode: mapreduce</code></strong>: Standard workflows default to sequential mode</li>
<li><strong>Wrong JSONPath</strong>: Use <code>"$[*]"</code> for array root, not <code>"$.items[*]"</code> if items are at root</li>
<li><strong>Environment Variable Syntax</strong>: Use <code>${VAR}</code> for Prodigy variables, <code>$VAR</code> for shell variables</li>
<li><strong>Validation Score Format</strong>: Must output exactly <code>"score: N"</code> (0-100)</li>
<li><strong>Forgetting <code>commit_required</code></strong>: Set to <code>true</code> when commits are expected</li>
<li><strong>Merge Variables</strong>: Always use <code>${merge.source_branch}</code> and <code>${merge.target_branch}</code>, not hardcoded branches</li>
<li><strong>Timeout Units</strong>: Always in seconds, not milliseconds</li>
<li><strong>Parallel Count</strong>: Must be a number or env var reference (string), not interpolated expression</li>
</ol>
<hr />
<h3 id="next-steps-3"><a class="header" href="#next-steps-3">Next Steps</a></h3>
<ul>
<li>Explore real-world workflows in the <code>workflows/</code> directory</li>
<li>Read the <a href="configuration/best-practices.html">Configuration Best Practices</a> guide</li>
<li>Learn about <a href="configuration/../composition/index.html">Workflow Composition</a> for reusable templates</li>
<li>Review <a href="configuration/../mapreduce/troubleshooting.html">MapReduce Troubleshooting</a> for debugging tips</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="default-values-reference"><a class="header" href="#default-values-reference">Default Values Reference</a></h2>
<p>Complete reference of all default configuration values in Prodigy. These defaults are used when settings are not explicitly configured in global config, project config, or environment variables.</p>
<h3 id="global-configuration-defaults"><a class="header" href="#global-configuration-defaults">Global Configuration Defaults</a></h3>
<p>Source: <code>src/config/mod.rs:51-59, 88-100</code></p>
<div class="table-wrapper"><table><thead><tr><th>Setting</th><th>Default Value</th><th>Description</th></tr></thead><tbody>
<tr><td><code>prodigy_home</code></td><td><code>~/.prodigy</code></td><td>Global storage directory (platform-specific)</td></tr>
<tr><td><code>default_editor</code></td><td>None</td><td>Text editor (falls back to <code>$EDITOR</code>)</td></tr>
<tr><td><code>log_level</code></td><td><code>"info"</code></td><td>Logging verbosity (<code>trace</code>, <code>debug</code>, <code>info</code>, <code>warn</code>, <code>error</code>)</td></tr>
<tr><td><code>claude_api_key</code></td><td>None</td><td>Claude API key (use environment variable)</td></tr>
<tr><td><code>max_concurrent_specs</code></td><td><code>1</code></td><td>Maximum concurrent spec implementations</td></tr>
<tr><td><code>auto_commit</code></td><td><code>true</code></td><td>Automatically commit after successful commands</td></tr>
<tr><td><code>plugins.enabled</code></td><td><code>false</code></td><td>Enable plugin system</td></tr>
<tr><td><code>plugins.directory</code></td><td><code>~/.prodigy/plugins</code></td><td>Plugin directory</td></tr>
<tr><td><code>plugins.auto_load</code></td><td><code>[]</code></td><td>Plugins to load on startup</td></tr>
</tbody></table>
</div>
<h3 id="project-configuration-defaults"><a class="header" href="#project-configuration-defaults">Project Configuration Defaults</a></h3>
<p>Source: <code>src/config/mod.rs:66-74</code></p>
<div class="table-wrapper"><table><thead><tr><th>Setting</th><th>Default Value</th><th>Description</th></tr></thead><tbody>
<tr><td><code>name</code></td><td><strong>Required</strong></td><td>Project identifier (no default)</td></tr>
<tr><td><code>description</code></td><td>None</td><td>Human-readable project description</td></tr>
<tr><td><code>version</code></td><td>None</td><td>Project version</td></tr>
<tr><td><code>spec_dir</code></td><td><code>"specs"</code></td><td>Directory containing specification files</td></tr>
<tr><td><code>claude_api_key</code></td><td>Inherits from global</td><td>Project-specific API key</td></tr>
<tr><td><code>auto_commit</code></td><td>Inherits from global</td><td>Project-specific auto-commit setting</td></tr>
<tr><td><code>variables</code></td><td><code>{}</code></td><td>Project variables for workflows</td></tr>
</tbody></table>
</div>
<h3 id="storage-configuration-defaults"><a class="header" href="#storage-configuration-defaults">Storage Configuration Defaults</a></h3>
<p>Source: <code>src/storage/config.rs:24-55, 228-241</code></p>
<div class="table-wrapper"><table><thead><tr><th>Setting</th><th>Default Value</th><th>Description</th></tr></thead><tbody>
<tr><td><code>backend</code></td><td><code>"file"</code></td><td>Storage backend type (<code>file</code> or <code>memory</code>)</td></tr>
<tr><td><code>connection_pool_size</code></td><td><code>10</code></td><td>Connection pool size (for future backends)</td></tr>
<tr><td><code>timeout</code></td><td><code>30s</code></td><td>Default operation timeout</td></tr>
<tr><td><code>enable_locking</code></td><td><code>true</code></td><td>Enable distributed locking</td></tr>
<tr><td><code>enable_cache</code></td><td><code>false</code></td><td>Enable caching layer</td></tr>
</tbody></table>
</div>
<h3 id="file-storage-defaults"><a class="header" href="#file-storage-defaults">File Storage Defaults</a></h3>
<p>Source: <code>src/storage/config.rs:66-86, 196-198</code></p>
<div class="table-wrapper"><table><thead><tr><th>Setting</th><th>Default Value</th><th>Description</th></tr></thead><tbody>
<tr><td><code>base_dir</code></td><td><code>~/.prodigy</code></td><td>Base storage directory</td></tr>
<tr><td><code>use_global</code></td><td><code>true</code></td><td>Use global storage (recommended)</td></tr>
<tr><td><code>enable_file_locks</code></td><td><code>true</code></td><td>Enable file-based locking</td></tr>
<tr><td><code>max_file_size</code></td><td><code>104857600</code> (100MB)</td><td>Max file size before rotation</td></tr>
<tr><td><code>enable_compression</code></td><td><code>false</code></td><td>Compress archived files</td></tr>
</tbody></table>
</div>
<h3 id="memory-storage-defaults"><a class="header" href="#memory-storage-defaults">Memory Storage Defaults</a></h3>
<p>Source: <code>src/storage/config.rs:89-111, 200-201</code></p>
<div class="table-wrapper"><table><thead><tr><th>Setting</th><th>Default Value</th><th>Description</th></tr></thead><tbody>
<tr><td><code>max_memory</code></td><td><code>1073741824</code> (1GB)</td><td>Maximum memory usage</td></tr>
<tr><td><code>persist_to_disk</code></td><td><code>false</code></td><td>Persist memory storage to disk</td></tr>
<tr><td><code>persistence_path</code></td><td>None</td><td>Path for disk persistence</td></tr>
</tbody></table>
</div>
<h3 id="retry-policy-defaults"><a class="header" href="#retry-policy-defaults">Retry Policy Defaults</a></h3>
<p>Source: <code>src/storage/config.rs:114-147, 204-212</code></p>
<div class="table-wrapper"><table><thead><tr><th>Setting</th><th>Default Value</th><th>Description</th></tr></thead><tbody>
<tr><td><code>max_retries</code></td><td><code>3</code></td><td>Maximum retry attempts</td></tr>
<tr><td><code>initial_delay</code></td><td><code>1s</code></td><td>Initial retry delay</td></tr>
<tr><td><code>max_delay</code></td><td><code>30s</code></td><td>Maximum retry delay (with backoff)</td></tr>
<tr><td><code>backoff_multiplier</code></td><td><code>2.0</code></td><td>Exponential backoff multiplier</td></tr>
<tr><td><code>jitter</code></td><td><code>true</code></td><td>Add random jitter to delays</td></tr>
</tbody></table>
</div>
<h3 id="cache-configuration-defaults"><a class="header" href="#cache-configuration-defaults">Cache Configuration Defaults</a></h3>
<p>Source: <code>src/storage/config.rs:150-173</code></p>
<div class="table-wrapper"><table><thead><tr><th>Setting</th><th>Default Value</th><th>Description</th></tr></thead><tbody>
<tr><td><code>max_entries</code></td><td><code>1000</code></td><td>Maximum cached entries</td></tr>
<tr><td><code>ttl</code></td><td><code>3600s</code> (1 hour)</td><td>Cache time-to-live</td></tr>
<tr><td><code>cache_type</code></td><td><code>"memory"</code></td><td>Cache implementation type</td></tr>
</tbody></table>
</div>
<h3 id="environment-variable-defaults"><a class="header" href="#environment-variable-defaults">Environment Variable Defaults</a></h3>
<p>These settings can be overridden by environment variables (see <a href="configuration/environment-variables.html">Environment Variables</a>):</p>
<div class="table-wrapper"><table><thead><tr><th>Environment Variable</th><th>Corresponding Setting</th><th>Default</th></tr></thead><tbody>
<tr><td><code>PRODIGY_CLAUDE_API_KEY</code></td><td><code>claude_api_key</code></td><td>None</td></tr>
<tr><td><code>PRODIGY_LOG_LEVEL</code></td><td><code>log_level</code></td><td><code>"info"</code></td></tr>
<tr><td><code>PRODIGY_EDITOR</code></td><td><code>default_editor</code></td><td>None</td></tr>
<tr><td><code>PRODIGY_AUTO_COMMIT</code></td><td><code>auto_commit</code></td><td><code>true</code></td></tr>
<tr><td><code>PRODIGY_STORAGE_TYPE</code></td><td><code>storage.backend</code></td><td><code>"file"</code></td></tr>
<tr><td><code>PRODIGY_STORAGE_BASE_PATH</code></td><td><code>storage.base_dir</code></td><td><code>~/.prodigy</code></td></tr>
<tr><td><code>PRODIGY_CLAUDE_STREAMING</code></td><td>-</td><td><code>true</code></td></tr>
<tr><td><code>PRODIGY_AUTOMATION</code></td><td>-</td><td>Not set (set by Prodigy)</td></tr>
</tbody></table>
</div>
<h3 id="cli-parameter-defaults"><a class="header" href="#cli-parameter-defaults">CLI Parameter Defaults</a></h3>
<p>Source: <code>src/cook/command.rs:28-29</code></p>
<p>These are CLI-level parameters, not workflow configuration fields:</p>
<div class="table-wrapper"><table><thead><tr><th>Parameter</th><th>Default Value</th><th>Description</th></tr></thead><tbody>
<tr><td><code>--max-iterations</code></td><td><code>1</code></td><td>Number of workflow iterations to run</td></tr>
<tr><td><code>--path</code></td><td>Current directory</td><td>Repository path to run in</td></tr>
</tbody></table>
</div>
<h3 id="command-metadata-defaults"><a class="header" href="#command-metadata-defaults">Command Metadata Defaults</a></h3>
<p>Source: <code>src/config/command.rs:130-154, src/config/mod.rs:363-365</code></p>
<p>Applied to individual commands when not specified:</p>
<div class="table-wrapper"><table><thead><tr><th>Setting</th><th>Default Value</th><th>Description</th></tr></thead><tbody>
<tr><td><code>retries</code></td><td><code>2</code></td><td>Retry attempts for failed commands</td></tr>
<tr><td><code>timeout</code></td><td><code>300</code></td><td>Command timeout (seconds)</td></tr>
<tr><td><code>continue_on_error</code></td><td><code>false</code></td><td>Continue workflow on command failure</td></tr>
<tr><td><code>commit_required</code></td><td><code>false</code></td><td>Whether command must create git commits</td></tr>
<tr><td><code>env</code></td><td><code>{}</code></td><td>Environment variables for command</td></tr>
</tbody></table>
</div>
<h3 id="understanding-defaults"><a class="header" href="#understanding-defaults">Understanding Defaults</a></h3>
<p><strong>How defaults work:</strong></p>
<ol>
<li>Prodigy starts with built-in defaults</li>
<li>Global config (<code>~/.prodigy/config.yml</code>) overrides defaults</li>
<li>Project config (<code>.prodigy/config.yml</code>) overrides global config</li>
<li>Environment variables override file config</li>
<li>CLI flags override everything</li>
</ol>
<p><strong>Example precedence flow:</strong></p>
<pre><code>Built-in default: log_level = "info"
       ↓
Global config: log_level = "warn"  (overrides default)
       ↓
Project config: (not specified, inherits "warn")
       ↓
Environment: PRODIGY_LOG_LEVEL=debug  (overrides all configs)
       ↓
Result: log_level = "debug"
</code></pre>
<h3 id="practical-example-overriding-storage-defaults"><a class="header" href="#practical-example-overriding-storage-defaults">Practical Example: Overriding Storage Defaults</a></h3>
<p>This example shows how to override storage defaults in a project config:</p>
<pre><code class="language-yaml"># .prodigy/config.yml
name: my-project

storage:
  backend: file
  timeout: 60s  # Override default 30s
  enable_cache: true  # Override default false

  backend_config:
    file:
      base_dir: /custom/storage  # Override default ~/.prodigy
      max_file_size: 524288000  # 500MB (override default 100MB)
      enable_compression: true   # Override default false

  cache_config:
    max_entries: 5000  # Override default 1000
    ttl: 7200s  # 2 hours (override default 1 hour)
</code></pre>
<p>With this configuration:</p>
<ul>
<li>Storage timeout increases from 30s → 60s</li>
<li>Caching is enabled (default: disabled)</li>
<li>Files can be 500MB instead of 100MB</li>
<li>Cache holds 5000 entries instead of 1000</li>
</ul>
<h3 id="see-also-16"><a class="header" href="#see-also-16">See Also</a></h3>
<ul>
<li><a href="configuration/configuration-precedence-rules.html">Configuration Precedence Rules</a></li>
<li><a href="configuration/global-configuration-structure.html">Global Configuration Structure</a></li>
<li><a href="configuration/project-configuration-structure.html">Project Configuration Structure</a></li>
<li><a href="configuration/storage-configuration.html">Storage Configuration</a></li>
<li><a href="configuration/environment-variables.html">Environment Variables</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="best-practices-12"><a class="header" href="#best-practices-12">Best Practices</a></h2>
<p>This section provides practical guidance for configuring Prodigy workflows effectively, based on the implementation patterns and defaults in the codebase.</p>
<h2 id="start-with-sensible-defaults"><a class="header" href="#start-with-sensible-defaults">Start with Sensible Defaults</a></h2>
<p>Prodigy provides carefully chosen defaults that work well for most use cases. You typically only need to configure exceptions.</p>
<p><strong>Default Storage Configuration</strong> (src/storage/config.rs:184-226):</p>
<pre><code class="language-yaml"># These defaults are already set - no need to configure unless you need different values
storage:
  connection_pool_size: 10
  timeout_secs: 30
  max_retries: 3
  initial_delay_ms: 1000
  max_delay_ms: 30000
  backoff_multiplier: 2.0
  max_file_size_mb: 100
  cache_size: 1000
  cache_ttl_secs: 3600
</code></pre>
<p><strong>Default Cleanup Configuration</strong> (src/cook/execution/mapreduce/cleanup/config.rs:39-87):</p>
<pre><code class="language-yaml"># Default preset: balanced cleanup
cleanup:
  auto_cleanup: true
  cleanup_delay_secs: 30
  max_worktrees_per_job: 50
  max_total_worktrees: 200
  disk_threshold_mb: 1024
</code></pre>
<p><strong>When to Override Defaults:</strong></p>
<ul>
<li><strong>Storage</strong>: Only if you have specific latency/throughput requirements</li>
<li><strong>Cleanup</strong>: Use “aggressive” preset for disk-constrained environments</li>
<li><strong>Timeouts</strong>: Increase for long-running operations</li>
</ul>
<h2 id="use-configuration-layering-effectively"><a class="header" href="#use-configuration-layering-effectively">Use Configuration Layering Effectively</a></h2>
<p>Prodigy uses a three-tier configuration hierarchy (src/config/mod.rs:102-154):</p>
<p><strong>Configuration Precedence</strong> (highest to lowest):</p>
<ol>
<li><strong>Workflow configuration</strong> - Specific to a single workflow YAML file</li>
<li><strong>Project configuration</strong> - <code>.prodigy/config.toml</code> in your project</li>
<li><strong>Global configuration</strong> - <code>~/.prodigy/config.toml</code> in your home directory</li>
<li><strong>Built-in defaults</strong> - Sensible defaults from the codebase</li>
</ol>
<p><strong>Recommended Usage:</strong></p>
<pre><code class="language-yaml"># Global config (~/.prodigy/config.toml): User-wide preferences
[global]
log_level = "info"
auto_commit = true

# Project config (.prodigy/config.toml): Project-specific overrides
[project]
claude_api_key = "${CLAUDE_API_KEY}"  # Environment variable reference
auto_commit = false  # Override global setting

# Workflow YAML: Workflow-specific settings
env:
  MAX_PARALLEL: 5
  PROJECT_NAME: "my-project"
</code></pre>
<p>See <a href="configuration/configuration-precedence-rules.html">Configuration Precedence Rules</a> for detailed examples.</p>
<h2 id="parameterize-with-environment-variables"><a class="header" href="#parameterize-with-environment-variables">Parameterize with Environment Variables</a></h2>
<p>Use environment variables for flexible, reusable workflows (src/config/mapreduce.rs:395-449).</p>
<p><strong>Variable Syntax:</strong></p>
<pre><code class="language-yaml">env:
  PROJECT_NAME: "prodigy"
  MAX_PARALLEL: 3
  API_URL: "https://api.example.com"

map:
  max_parallel: $MAX_PARALLEL  # Simple reference
  agent_template:
    - shell: "echo Processing ${PROJECT_NAME}"  # Bracketed reference
    - shell: "curl ${API_URL}/endpoint"
</code></pre>
<p><strong>Resolution Order</strong> (src/config/mapreduce.rs:395-449):</p>
<ol>
<li>Workflow <code>env:</code> block</li>
<li>System environment variables</li>
<li>Error if not found</li>
</ol>
<p><strong>Type Flexibility</strong> (src/config/mapreduce.rs:354-393):</p>
<pre><code class="language-yaml"># Both numeric literals and environment variables are supported
map:
  max_parallel: 3              # Numeric literal
  max_parallel: $MAX_PARALLEL  # Environment variable
</code></pre>
<p>See <a href="configuration/environment-variables.html">Environment Variables</a> for comprehensive documentation.</p>
<h2 id="protect-sensitive-values"><a class="header" href="#protect-sensitive-values">Protect Sensitive Values</a></h2>
<p>Prodigy automatically masks common secret patterns in logs (src/cook/environment/config.rs:78-119).</p>
<p><strong>Automatic Masking Patterns:</strong></p>
<ul>
<li>API keys (contains “api” or “key”)</li>
<li>Tokens (contains “token”)</li>
<li>Passwords (contains “password”, “pwd”, “secret”)</li>
<li>Auth values (contains “auth”)</li>
</ul>
<p><strong>Example:</strong></p>
<pre><code class="language-yaml">env:
  API_KEY: "sk-abc123def456"      # Auto-masked as "***" in logs
  PASSWORD: "hunter2"              # Auto-masked
  AUTH_TOKEN: "ghp_abcdefg"        # Auto-masked
  NORMAL_VAR: "visible-value"      # Not masked
</code></pre>
<p><strong>Explicit Secret Marking:</strong></p>
<pre><code class="language-yaml">env:
  CUSTOM_SECRET:
    secret: true
    value: "my-sensitive-value"
</code></pre>
<p><strong>Best Practices:</strong></p>
<ol>
<li>Never hardcode secrets in workflow files</li>
<li>Use environment variable references: <code>${SECRET_NAME}</code></li>
<li>Store secrets in system environment or <code>.env</code> files (excluded from git)</li>
<li>Mark custom sensitive fields with <code>secret: true</code></li>
<li>Review logs to verify sensitive values are masked</li>
</ol>
<h2 id="use-profiles-for-different-environments"><a class="header" href="#use-profiles-for-different-environments">Use Profiles for Different Environments</a></h2>
<p>Profiles allow environment-specific configuration (src/cook/environment/config.rs:146-162):</p>
<pre><code class="language-yaml">env:
  API_URL:
    default: "http://localhost:3000"
    staging: "https://staging-api.example.com"
    prod: "https://api.example.com"

  MAX_PARALLEL:
    default: 2
    prod: 10

  DEBUG_MODE:
    default: true
    prod: false
</code></pre>
<p><strong>Activate a profile:</strong></p>
<pre><code class="language-bash">prodigy run workflow.yml --profile prod
</code></pre>
<p><strong>Use cases:</strong></p>
<ul>
<li>Development vs production API endpoints</li>
<li>Resource limits (lower parallelism in dev)</li>
<li>Feature flags (enable debug logging in dev)</li>
<li>Storage backends (local files vs S3 in prod)</li>
</ul>
<h2 id="configure-storage-appropriately"><a class="header" href="#configure-storage-appropriately">Configure Storage Appropriately</a></h2>
<p><strong>Global Storage (Default - Recommended):</strong></p>
<pre><code class="language-yaml">storage:
  use_global: true  # Default
  base_path: "~/.prodigy"  # Default
</code></pre>
<p><strong>Benefits:</strong></p>
<ul>
<li>Cross-worktree event aggregation for parallel jobs</li>
<li>Persistent state survives worktree cleanup</li>
<li>Centralized monitoring and debugging</li>
<li>Efficient storage deduplication</li>
</ul>
<p><strong>Local Storage (Deprecated):</strong></p>
<pre><code class="language-yaml">storage:
  use_global: false  # ⚠️ Deprecated
  base_path: ".prodigy"
</code></pre>
<p><strong>Warning:</strong> Local storage is deprecated (src/storage/config.rs:71-73). Use global storage unless you have specific isolation requirements.</p>
<p><strong>Environment Variable Fallbacks</strong> (src/storage/config.rs:244-286):</p>
<pre><code class="language-bash"># Precedence: highest to lowest
export PRODIGY_STORAGE_TYPE="file"        # or "memory"
export PRODIGY_STORAGE_BASE_PATH="/data/.prodigy"
export PRODIGY_STORAGE_DIR="/tmp/.prodigy"  # Fallback
export PRODIGY_STORAGE_PATH="/var/.prodigy"  # Secondary fallback
</code></pre>
<p>See <a href="configuration/storage-configuration.html">Storage Configuration</a> for detailed options.</p>
<h2 id="set-appropriate-timeouts"><a class="header" href="#set-appropriate-timeouts">Set Appropriate Timeouts</a></h2>
<p>Configure timeouts based on operation characteristics (src/app/config.rs:10-63):</p>
<p><strong>Workflow Timeouts:</strong></p>
<pre><code class="language-yaml"># Default timeout: 30 seconds per command
commands:
  - shell: "quick-test"  # Uses default
  - shell: "long-build"
    timeout_secs: 600    # 10 minutes for long operations
</code></pre>
<p><strong>MapReduce Timeouts:</strong></p>
<pre><code class="language-yaml">map:
  agent_timeout_secs: 300  # 5 minutes per agent
  max_parallel: 5
</code></pre>
<p><strong>Retry Configuration:</strong></p>
<pre><code class="language-yaml">retry_config:
  max_attempts: 3           # Default
  initial_delay_ms: 1000    # 1 second
  max_delay_ms: 30000       # 30 seconds cap
  backoff: exponential      # Backoff strategy
</code></pre>
<p><strong>Backoff Strategies</strong> (src/storage/config.rs:184-226):</p>
<ul>
<li><code>exponential</code>: Delay doubles each retry (2^n * initial_delay)</li>
<li><code>linear</code>: Delay increases linearly (n * initial_delay)</li>
<li><code>fibonacci</code>: Delay follows fibonacci sequence</li>
</ul>
<h2 id="prefer-simplified-yaml-syntax"><a class="header" href="#prefer-simplified-yaml-syntax">Prefer Simplified YAML Syntax</a></h2>
<p>Prodigy supports both simplified and verbose syntax (src/config/mapreduce.rs:287-351).</p>
<p><strong>Simplified Syntax (Recommended):</strong></p>
<pre><code class="language-yaml">map:
  agent_template:
    - claude: "/process ${item.path}"
    - shell: "test -f ${item.path}"
</code></pre>
<p><strong>Verbose Syntax (Backward Compatible):</strong></p>
<pre><code class="language-yaml">map:
  agent_template:
    commands:  # ⚠️ Deprecated nested array
      - claude: "/process ${item.path}"
      - shell: "test -f ${item.path}"
</code></pre>
<p><strong>Why simplified is better:</strong></p>
<ul>
<li>Less indentation</li>
<li>Clearer intent</li>
<li>Matches standard workflow syntax</li>
<li>Forward-compatible</li>
</ul>
<h2 id="validate-configuration-early"><a class="header" href="#validate-configuration-early">Validate Configuration Early</a></h2>
<p><strong>Type Safety</strong> (src/config/mapreduce.rs:354-393):</p>
<ul>
<li>Workflow YAML is validated at parse time</li>
<li>Clear error messages include context</li>
<li>Type mismatches caught before execution</li>
</ul>
<p><strong>Example Error Message:</strong></p>
<pre><code>Error: Environment variable 'MAX_PARALLEL' not found
Context: Required by map.max_parallel in workflow.yml:12
Resolution order: workflow env → system environment
</code></pre>
<p><strong>Configuration Debugging:</strong></p>
<pre><code class="language-bash"># View effective configuration after all precedence rules
prodigy config show

# Validate workflow without running
prodigy validate workflow.yml
</code></pre>
<h2 id="common-configuration-patterns-1"><a class="header" href="#common-configuration-patterns-1">Common Configuration Patterns</a></h2>
<p><strong>Capture Command Output:</strong></p>
<pre><code class="language-yaml">setup:
  - shell: "git rev-parse HEAD"
    capture: "commit_hash"
  - shell: "echo 'Building commit ${commit_hash}'"
</code></pre>
<p><strong>Conditional Execution:</strong></p>
<pre><code class="language-yaml">map:
  filter: "item.priority &gt;= 5"  # Only process high-priority items
  sort_by: "item.priority DESC"  # Process highest priority first
</code></pre>
<p><strong>Error Handling:</strong></p>
<pre><code class="language-yaml">map:
  agent_template:
    - shell: "risky-operation ${item.id}"
      on_failure:
        - claude: "/diagnose-failure ${item.id}"
</code></pre>
<p><strong>Resource Limits:</strong></p>
<pre><code class="language-yaml">map:
  max_parallel: 10              # Concurrent agents
  max_items: 100                # Limit total items processed
  agent_timeout_secs: 300       # Per-agent timeout
</code></pre>
<h2 id="anti-patterns-to-avoid"><a class="header" href="#anti-patterns-to-avoid">Anti-Patterns to Avoid</a></h2>
<p><strong>❌ Don’t: Hardcode Paths</strong></p>
<pre><code class="language-yaml">commands:
  - shell: "cd /Users/alice/project &amp;&amp; make test"
</code></pre>
<p><strong>✅ Do: Use Variables</strong></p>
<pre><code class="language-yaml">env:
  PROJECT_DIR: "${PWD}"
commands:
  - shell: "cd ${PROJECT_DIR} &amp;&amp; make test"
</code></pre>
<p><strong>❌ Don’t: Expose Secrets</strong></p>
<pre><code class="language-yaml">env:
  API_KEY: "sk-real-key-here"  # Committed to git!
</code></pre>
<p><strong>✅ Do: Reference Environment</strong></p>
<pre><code class="language-yaml">env:
  API_KEY: "${CLAUDE_API_KEY}"  # From system environment
</code></pre>
<p><strong>❌ Don’t: Disable Global Storage Unnecessarily</strong></p>
<pre><code class="language-yaml">storage:
  use_global: false  # Deprecated and limits functionality
</code></pre>
<p><strong>✅ Do: Use Default Global Storage</strong></p>
<pre><code class="language-yaml"># No storage configuration needed - global is default
</code></pre>
<p><strong>❌ Don’t: Skip Timeouts for Long Operations</strong></p>
<pre><code class="language-yaml">commands:
  - shell: "npm install"  # May hang indefinitely
</code></pre>
<p><strong>✅ Do: Set Explicit Timeouts</strong></p>
<pre><code class="language-yaml">commands:
  - shell: "npm install"
    timeout_secs: 300
</code></pre>
<h2 id="troubleshooting-configuration-issues"><a class="header" href="#troubleshooting-configuration-issues">Troubleshooting Configuration Issues</a></h2>
<p><strong>Configuration Not Found:</strong></p>
<pre><code class="language-bash"># Check search paths
prodigy config show --verbose

# Verify file exists and is valid YAML
cat .prodigy/config.toml
yamllint workflow.yml
</code></pre>
<p><strong>Wrong Precedence:</strong></p>
<pre><code class="language-bash"># View effective configuration
prodigy config show

# Check which file is setting a value
PRODIGY_LOG_LEVEL=debug prodigy run workflow.yml -v
</code></pre>
<p><strong>Environment Variable Not Resolved:</strong></p>
<pre><code class="language-yaml"># Check variable is defined
echo $MY_VAR

# Use correct syntax: $VAR or ${VAR}
# NOT: %VAR% (Windows) or $env:VAR (PowerShell)
</code></pre>
<p><strong>Timeout Too Short:</strong></p>
<pre><code class="language-yaml"># Increase timeouts for slow operations
commands:
  - shell: "cargo build --release"
    timeout_secs: 600  # 10 minutes
</code></pre>
<p>See <a href="configuration/troubleshooting.html">Troubleshooting</a> for more configuration issues and solutions.</p>
<h2 id="advanced-configuration"><a class="header" href="#advanced-configuration">Advanced Configuration</a></h2>
<p><strong>Custom Cleanup Presets:</strong></p>
<pre><code class="language-yaml"># Aggressive cleanup for CI/CD
cleanup:
  preset: "aggressive"
  cleanup_delay_secs: 5
  max_worktrees_per_job: 20
  disk_threshold_mb: 512
</code></pre>
<p><strong>Custom Storage Backend:</strong></p>
<pre><code class="language-yaml">storage:
  type: "file"  # or "memory" for testing
  base_path: "/mnt/fast-disk/.prodigy"
  connection_pool_size: 20  # High-concurrency workloads
</code></pre>
<p><strong>Workflow Validation:</strong></p>
<pre><code class="language-yaml">validation:
  threshold: 80  # Minimum 80% success rate
  timeout_secs: 600
  output_schema: "output-schema.json"  # Validate JSON outputs
</code></pre>
<h2 id="summary"><a class="header" href="#summary">Summary</a></h2>
<p><strong>Key Takeaways:</strong></p>
<ol>
<li><strong>Trust the defaults</strong> - They’re production-tested and sensible</li>
<li><strong>Layer your config</strong> - Global → Project → Workflow hierarchy</li>
<li><strong>Parameterize everything</strong> - Use environment variables for flexibility</li>
<li><strong>Protect secrets</strong> - Automatic masking + explicit <code>secret: true</code></li>
<li><strong>Use profiles</strong> - Separate dev, staging, prod configurations</li>
<li><strong>Global storage</strong> - Default and recommended for all use cases</li>
<li><strong>Set timeouts</strong> - Prevent hanging on long operations</li>
<li><strong>Simplified syntax</strong> - Clearer and forward-compatible</li>
<li><strong>Validate early</strong> - Catch errors before execution</li>
<li><strong>Monitor effective config</strong> - Use <code>prodigy config show</code> for debugging</li>
</ol>
<h2 id="see-also-17"><a class="header" href="#see-also-17">See Also</a></h2>
<ul>
<li><a href="configuration/configuration-precedence-rules.html">Configuration Precedence Rules</a> - Detailed precedence examples</li>
<li><a href="configuration/environment-variables.html">Environment Variables</a> - Comprehensive variable documentation</li>
<li><a href="configuration/storage-configuration.html">Storage Configuration</a> - Storage backend options</li>
<li><a href="configuration/global-configuration-structure.html">Global Configuration Structure</a> - GlobalConfig fields</li>
<li><a href="configuration/complete-configuration-examples.html">Complete Configuration Examples</a> - Real-world examples</li>
<li><a href="configuration/troubleshooting.html">Troubleshooting</a> - Common configuration issues</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="troubleshooting-7"><a class="header" href="#troubleshooting-7">Troubleshooting</a></h2>
<p>Common configuration issues and their solutions.</p>
<h3 id="configuration-file-issues"><a class="header" href="#configuration-file-issues">Configuration File Issues</a></h3>
<h4 id="configuration-file-not-found"><a class="header" href="#configuration-file-not-found">“Configuration file not found”</a></h4>
<p><strong>Symptoms</strong>:</p>
<pre><code>Error: Configuration file not found at ~/.prodigy/config.yml
</code></pre>
<p><strong>Causes</strong>:</p>
<ul>
<li>Config file doesn’t exist</li>
<li>Wrong file extension (<code>.yaml</code> instead of <code>.yml</code>, or <code>.toml</code> instead of <code>.yml</code>)</li>
<li>File is in the wrong location</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li>
<p>Create the file:</p>
<pre><code class="language-bash">mkdir -p ~/.prodigy
cat &gt; ~/.prodigy/config.yml &lt;&lt; 'EOF'
log_level: info
auto_commit: true
EOF
</code></pre>
</li>
<li>
<p>Check file extension:</p>
<pre><code class="language-bash">ls -la ~/.prodigy/config.*
# Should show config.yml, not config.yaml or config.toml
</code></pre>
</li>
<li>
<p>Verify location:</p>
<pre><code class="language-bash"># Global config
ls ~/.prodigy/config.yml

# Project config
ls .prodigy/config.yml
</code></pre>
</li>
</ol>
<h4 id="invalid-yaml-syntax"><a class="header" href="#invalid-yaml-syntax">“Invalid YAML syntax”</a></h4>
<p><strong>Symptoms</strong>:</p>
<pre><code>Error: Failed to parse configuration: invalid YAML syntax at line 10
</code></pre>
<p><strong>Causes</strong>:</p>
<ul>
<li>Incorrect indentation (must use 2 spaces, no tabs)</li>
<li>Missing space after colon (<code>key:value</code> instead of <code>key: value</code>)</li>
<li>Unquoted strings with special characters</li>
<li>Mixing TOML and YAML syntax</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li>
<p>Check indentation (must be 2 spaces):</p>
<pre><code class="language-yaml"># ✗ Wrong (tabs or 4 spaces)
storage:
    backend: file

# ✓ Correct (2 spaces)
storage:
  backend: file
</code></pre>
</li>
<li>
<p>Add space after colons:</p>
<pre><code class="language-yaml"># ✗ Wrong
log_level:debug

# ✓ Correct
log_level: debug
</code></pre>
</li>
<li>
<p>Quote strings with special characters:</p>
<pre><code class="language-yaml"># ✗ Wrong
message: Error: something failed

# ✓ Correct
message: "Error: something failed"
</code></pre>
</li>
<li>
<p>Use a YAML validator:</p>
<pre><code class="language-bash">yamllint ~/.prodigy/config.yml
</code></pre>
</li>
</ol>
<h4 id="unknown-field-in-configuration"><a class="header" href="#unknown-field-in-configuration">“Unknown field in configuration”</a></h4>
<p><strong>Symptoms</strong>:</p>
<pre><code>Warning: Unknown field 'unknown_setting' in configuration
</code></pre>
<p><strong>Causes</strong>:</p>
<ul>
<li>Typo in field name</li>
<li>Using deprecated field name</li>
<li>Field from old TOML format</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li>
<p>Check spelling against <a href="configuration/global-configuration-structure.html">Global Configuration Structure</a> or <a href="configuration/project-configuration-structure.html">Project Configuration Structure</a></p>
</li>
<li>
<p>Remove deprecated fields:</p>
<pre><code class="language-yaml"># ✗ Deprecated TOML-style
[storage]
backend = "file"

# ✓ Correct YAML
storage:
  backend: file
</code></pre>
</li>
<li>
<p>Update field names from old versions</p>
</li>
</ol>
<h3 id="environment-variable-issues"><a class="header" href="#environment-variable-issues">Environment Variable Issues</a></h3>
<h4 id="environment-variable-not-resolving"><a class="header" href="#environment-variable-not-resolving">“Environment variable not resolving”</a></h4>
<p><strong>Symptoms</strong>:</p>
<pre><code>Error: Variable 'API_KEY' not found
</code></pre>
<p><strong>Causes</strong>:</p>
<ul>
<li>Variable not defined in any configuration source</li>
<li>Incorrect variable syntax in workflow</li>
<li>Profile not activated</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li>
<p>Check variable is defined:</p>
<pre><code class="language-bash"># System env
echo $PRODIGY_API_KEY

# Workflow env (check workflow.yml)
grep API_KEY workflow.yml
</code></pre>
</li>
<li>
<p>Use correct syntax:</p>
<pre><code class="language-yaml"># ✗ Wrong
command: "curl $API_KEY"

# ✓ Correct
command: "curl ${API_KEY}"
</code></pre>
</li>
<li>
<p>Activate profile if using profile-specific values:</p>
<pre><code class="language-bash">prodigy run workflow.yml --profile prod
</code></pre>
</li>
<li>
<p>Check precedence chain: Step env &gt; Profile env &gt; Workflow env &gt; System env</p>
</li>
</ol>
<h4 id="secret-not-being-masked-in-logs"><a class="header" href="#secret-not-being-masked-in-logs">“Secret not being masked in logs”</a></h4>
<p><strong>Symptoms</strong>:</p>
<pre><code>Output: curl -H 'Authorization: Bearer sk-abc123...'
</code></pre>
<p><strong>Causes</strong>:</p>
<ul>
<li>Secret not marked with <code>secret: true</code> in workflow env block</li>
<li>Using system env vars (not masked automatically)</li>
</ul>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-yaml"># Mark as secret in workflow
env:
  API_KEY:
    secret: true
    value: "${PROD_API_KEY}"  # From system env
</code></pre>
<p><strong>Note</strong>: Only workflow env vars marked as <code>secret: true</code> are masked. System environment variables are not automatically masked.</p>
<h3 id="storage-issues"><a class="header" href="#storage-issues">Storage Issues</a></h3>
<h4 id="storage-directory-not-writable"><a class="header" href="#storage-directory-not-writable">“Storage directory not writable”</a></h4>
<p><strong>Symptoms</strong>:</p>
<pre><code>Error: Failed to write to storage: Permission denied
</code></pre>
<p><strong>Causes</strong>:</p>
<ul>
<li>Insufficient permissions on <code>~/.prodigy</code> directory</li>
<li>Directory owned by different user</li>
<li>Disk full</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li>
<p>Check permissions:</p>
<pre><code class="language-bash">ls -ld ~/.prodigy
# Should show: drwxr-xr-x username username
</code></pre>
</li>
<li>
<p>Fix ownership:</p>
<pre><code class="language-bash">sudo chown -R $USER:$USER ~/.prodigy
chmod -R u+rwX ~/.prodigy
</code></pre>
</li>
<li>
<p>Check disk space:</p>
<pre><code class="language-bash">df -h ~/.prodigy
</code></pre>
</li>
</ol>
<h4 id="failed-to-acquire-storage-lock"><a class="header" href="#failed-to-acquire-storage-lock">“Failed to acquire storage lock”</a></h4>
<p><strong>Symptoms</strong>:</p>
<pre><code>Error: Failed to acquire storage lock after 30s
</code></pre>
<p><strong>Causes</strong>:</p>
<ul>
<li>Another Prodigy process holding the lock</li>
<li>Stale lock from crashed process</li>
<li>File locking disabled but concurrent access occurring</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li>
<p>Check for running processes:</p>
<pre><code class="language-bash">ps aux | grep prodigy
</code></pre>
</li>
<li>
<p>Remove stale lock (if no processes running):</p>
<pre><code class="language-bash">rm ~/.prodigy/storage.lock
</code></pre>
</li>
<li>
<p>Wait for lock release (if process is running)</p>
</li>
<li>
<p>Disable locking temporarily (not recommended for production):</p>
<pre><code class="language-yaml">storage:
  enable_locking: false
</code></pre>
</li>
</ol>
<h3 id="workflow-configuration-issues"><a class="header" href="#workflow-configuration-issues">Workflow Configuration Issues</a></h3>
<h4 id="workflow-variables-not-interpolating"><a class="header" href="#workflow-variables-not-interpolating">“Workflow variables not interpolating”</a></h4>
<p><strong>Symptoms</strong>:</p>
<pre><code>Output: Deploying ${PROJECT_NAME} to ${ENVIRONMENT}
</code></pre>
<p><strong>Causes</strong>:</p>
<ul>
<li>Incorrect variable syntax</li>
<li>Variable not defined in workflow or config</li>
<li>Using project config variables instead of workflow env</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li>
<p>Use correct syntax:</p>
<pre><code class="language-yaml"># ✓ Workflow env vars
env:
  PROJECT_NAME: myapp
commands:
  - shell: "echo ${PROJECT_NAME}"

# ✗ Project config variables (different namespace)
# .prodigy/config.yml
variables:
  PROJECT_NAME: myapp  # Not available in workflows
</code></pre>
</li>
<li>
<p>Define variable in workflow <code>env:</code> block or as system env</p>
</li>
<li>
<p>Check variable exists:</p>
<pre><code class="language-bash">prodigy run workflow.yml -vv  # Verbose mode shows variable resolution
</code></pre>
</li>
</ol>
<h4 id="mapreduce-items-not-found"><a class="header" href="#mapreduce-items-not-found">“MapReduce items not found”</a></h4>
<p><strong>Symptoms</strong>:</p>
<pre><code>Error: No items found at JSONPath: $.items[*]
</code></pre>
<p><strong>Causes</strong>:</p>
<ul>
<li>Incorrect JSONPath expression</li>
<li>Input file not generated in setup phase</li>
<li>JSON structure doesn’t match path</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li>
<p>Validate JSONPath:</p>
<pre><code class="language-bash">cat items.json | jq '.items[0]'
</code></pre>
</li>
<li>
<p>Check setup phase output:</p>
<pre><code class="language-yaml">setup:
  - shell: "generate-items.sh &gt; items.json"
  - shell: "cat items.json"  # Verify file exists and has content
</code></pre>
</li>
<li>
<p>Test JSONPath expression:</p>
<pre><code class="language-bash"># Use jq to test
cat items.json | jq '$[*]'  # Root array
cat items.json | jq '.items[*]'  # Nested array
</code></pre>
</li>
</ol>
<h4 id="validation-always-fails"><a class="header" href="#validation-always-fails">“Validation always fails”</a></h4>
<p><strong>Symptoms</strong>:</p>
<pre><code>Error: Validation failed: completion_percentage 85 below threshold 100
</code></pre>
<p><strong>Causes</strong>:</p>
<ul>
<li>Threshold set too high (default: 100)</li>
<li>Validation command not returning expected format</li>
<li>Expected schema mismatch</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li>
<p>Adjust threshold:</p>
<pre><code class="language-yaml">validate:
  threshold: 80  # Accept 80% instead of 100%
</code></pre>
</li>
<li>
<p>Check validation output format:</p>
<pre><code class="language-bash"># Validation must output:
{
  "completion_percentage": 85,
  "status": "incomplete",
  "gaps": ["Missing feature X", "Incomplete test Y"]
}
</code></pre>
</li>
<li>
<p>Test validation command manually:</p>
<pre><code class="language-bash"># Run validation command outside workflow
./validate-script.sh
</code></pre>
</li>
</ol>
<h3 id="api-and-authentication-issues"><a class="header" href="#api-and-authentication-issues">API and Authentication Issues</a></h3>
<h4 id="claude-api-key-not-recognized"><a class="header" href="#claude-api-key-not-recognized">“Claude API key not recognized”</a></h4>
<p><strong>Symptoms</strong>:</p>
<pre><code>Error: Invalid Claude API key
</code></pre>
<p><strong>Causes</strong>:</p>
<ul>
<li>API key not set</li>
<li>Key set in wrong location</li>
<li>Invalid key format</li>
<li>Key expired or revoked</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li>
<p>Check key is set (precedence order):</p>
<pre><code class="language-bash"># Highest precedence: Environment variable
echo $PRODIGY_CLAUDE_API_KEY

# Project config
grep claude_api_key .prodigy/config.yml

# Global config
grep claude_api_key ~/.prodigy/config.yml
</code></pre>
</li>
<li>
<p>Verify key format (should start with <code>sk-ant-</code>):</p>
<pre><code class="language-bash">echo $PRODIGY_CLAUDE_API_KEY | head -c 10
# Should show: sk-ant-api
</code></pre>
</li>
<li>
<p>Use environment variable (recommended):</p>
<pre><code class="language-bash">export PRODIGY_CLAUDE_API_KEY="sk-ant-api03-..."
</code></pre>
</li>
<li>
<p>Verify key is valid at <a href="https://console.anthropic.com/">Anthropic Console</a></p>
</li>
</ol>
<h3 id="performance-issues"><a class="header" href="#performance-issues">Performance Issues</a></h3>
<h4 id="workflow-running-slowly"><a class="header" href="#workflow-running-slowly">“Workflow running slowly”</a></h4>
<p><strong>Causes</strong>:</p>
<ul>
<li>Excessive parallelism exhausting resources</li>
<li>Large work items in MapReduce</li>
<li>Slow validation commands</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li>
<p>Reduce parallelism:</p>
<pre><code class="language-yaml">map:
  max_parallel: 3  # Reduce from 10
</code></pre>
</li>
<li>
<p>Add timeout limits:</p>
<pre><code class="language-yaml">commands:
  - shell: "long-running-command"
    timeout: 300  # 5 minutes
</code></pre>
</li>
<li>
<p>Enable caching (if available):</p>
<pre><code class="language-yaml">storage:
  enable_cache: true
</code></pre>
</li>
</ol>
<h4 id="storage-growing-too-large"><a class="header" href="#storage-growing-too-large">“Storage growing too large”</a></h4>
<p><strong>Causes</strong>:</p>
<ul>
<li>Old events and DLQ data accumulating</li>
<li>Large checkpoint files</li>
<li>No compression enabled</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li>
<p>Clean up old data:</p>
<pre><code class="language-bash"># Remove old events (older than 30 days)
find ~/.prodigy/events -type f -mtime +30 -delete

# Clean DLQ for completed jobs
prodigy dlq clean --completed
</code></pre>
</li>
<li>
<p>Enable compression:</p>
<pre><code class="language-yaml">storage:
  backend_config:
    enable_compression: true
</code></pre>
</li>
<li>
<p>Reduce file retention:</p>
<pre><code class="language-yaml">storage:
  backend_config:
    max_file_size: 52428800  # 50MB instead of 100MB
</code></pre>
</li>
</ol>
<h3 id="debugging-tools"><a class="header" href="#debugging-tools">Debugging Tools</a></h3>
<h4 id="check-effective-configuration"><a class="header" href="#check-effective-configuration">Check Effective Configuration</a></h4>
<p>View the merged configuration from all sources:</p>
<pre><code class="language-bash">prodigy config show
</code></pre>
<h4 id="verbose-logging"><a class="header" href="#verbose-logging">Verbose Logging</a></h4>
<p>Enable detailed logging:</p>
<pre><code class="language-bash"># Verbose mode (shows Claude streaming)
prodigy run workflow.yml -v

# Debug mode (shows variable resolution)
prodigy run workflow.yml -vv

# Trace mode (shows all internal operations)
prodigy run workflow.yml -vvv
</code></pre>
<p>Or set log level in config:</p>
<pre><code class="language-yaml">log_level: debug  # trace, debug, info, warn, error
</code></pre>
<h4 id="validate-configuration-files"><a class="header" href="#validate-configuration-files">Validate Configuration Files</a></h4>
<pre><code class="language-bash"># Validate YAML syntax
yamllint ~/.prodigy/config.yml
yamllint .prodigy/config.yml

# Validate workflow syntax
prodigy validate workflow.yml
</code></pre>
<h4 id="check-claude-logs"><a class="header" href="#check-claude-logs">Check Claude Logs</a></h4>
<p>View Claude execution logs for debugging:</p>
<pre><code class="language-bash"># Latest log
prodigy logs --latest

# Tail live log
prodigy logs --latest --tail

# View specific log
cat ~/.local/state/claude/logs/session-abc123.json | jq
</code></pre>
<h3 id="common-error-messages"><a class="header" href="#common-error-messages">Common Error Messages</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Error Message</th><th>Likely Cause</th><th>Solution</th></tr></thead><tbody>
<tr><td>“Configuration file not found”</td><td>Missing config file</td><td>Create <code>~/.prodigy/config.yml</code> or <code>.prodigy/config.yml</code></td></tr>
<tr><td>“Invalid YAML syntax”</td><td>Syntax error in YAML</td><td>Check indentation, colons, quotes</td></tr>
<tr><td>“Unknown field”</td><td>Typo or deprecated field</td><td>Check docs for correct field names</td></tr>
<tr><td>“Variable not found”</td><td>Undefined variable</td><td>Define in workflow <code>env:</code> or system env</td></tr>
<tr><td>“Storage lock timeout”</td><td>Concurrent access</td><td>Wait or remove stale lock</td></tr>
<tr><td>“Permission denied”</td><td>Insufficient permissions</td><td>Fix ownership/permissions on <code>~/.prodigy</code></td></tr>
<tr><td>“JSONPath not found”</td><td>Wrong path or missing data</td><td>Verify JSON structure and path</td></tr>
<tr><td>“API key invalid”</td><td>Wrong or expired key</td><td>Check key format and validity</td></tr>
</tbody></table>
</div>
<h3 id="getting-help-1"><a class="header" href="#getting-help-1">Getting Help</a></h3>
<p>If you can’t resolve an issue:</p>
<ol>
<li><strong>Check logs</strong>: Use <code>-vvv</code> for maximum verbosity</li>
<li><strong>Verify config</strong>: Run <code>prodigy config show</code></li>
<li><strong>Check docs</strong>: See <a href="configuration/global-configuration-structure.html">Configuration Structure</a> and <a href="configuration/../workflow-basics.html">Workflow Basics</a></li>
<li><strong>File an issue</strong>: <a href="https://github.com/anthropics/prodigy/issues">Prodigy GitHub Issues</a> with:
<ul>
<li>Error message (redact secrets)</li>
<li>Relevant config snippets</li>
<li>Prodigy version (<code>prodigy --version</code>)</li>
<li>Operating system</li>
</ul>
</li>
</ol>
<h3 id="see-also-18"><a class="header" href="#see-also-18">See Also</a></h3>
<ul>
<li><a href="configuration/configuration-precedence-rules.html">Configuration Precedence Rules</a></li>
<li><a href="configuration/environment-variables.html">Environment Variables</a></li>
<li><a href="configuration/storage-configuration.html">Storage Configuration</a></li>
<li><a href="configuration/default-values-reference.html">Default Values Reference</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="migration-guide-toml-to-yaml"><a class="header" href="#migration-guide-toml-to-yaml">Migration Guide: TOML to YAML</a></h2>
<p>Prodigy has migrated from TOML to YAML for all configuration files. This guide helps you migrate existing configurations.</p>
<h3 id="quick-migration"><a class="header" href="#quick-migration">Quick Migration</a></h3>
<p><strong>Before (TOML):</strong></p>
<pre><code class="language-toml"># .prodigy/config.toml
name = "my-project"
description = "My project"

[variables]
PROJECT_ROOT = "/app"
VERSION = "1.0.0"

[claude]
api_key = "sk-ant-api03-..."
model = "claude-3-sonnet"
</code></pre>
<p><strong>After (YAML):</strong></p>
<pre><code class="language-yaml"># .prodigy/config.yml
name: my-project
description: My project

variables:
  PROJECT_ROOT: /app
  VERSION: 1.0.0

claude:
  api_key: sk-ant-api03-...
  model: claude-3-sonnet
</code></pre>
<h3 id="key-syntax-differences"><a class="header" href="#key-syntax-differences">Key Syntax Differences</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Feature</th><th>TOML</th><th>YAML</th></tr></thead><tbody>
<tr><td>Assignment</td><td><code>key = value</code></td><td><code>key: value</code></td></tr>
<tr><td>Sections</td><td><code>[section]</code></td><td><code>section:</code> (nested with indentation)</td></tr>
<tr><td>Strings</td><td><code>"quoted"</code> or <code>'quoted'</code></td><td>Usually unquoted (quote if contains <code>:</code>, <code>#</code>, etc.)</td></tr>
<tr><td>Indentation</td><td>Doesn’t matter</td><td><strong>Critical</strong> - use 2 spaces per level</td></tr>
<tr><td>Comments</td><td><code># comment</code></td><td><code># comment</code> (same)</td></tr>
<tr><td>Booleans</td><td><code>true</code>, <code>false</code></td><td><code>true</code>, <code>false</code> (same)</td></tr>
<tr><td>Arrays</td><td><code>[1, 2, 3]</code></td><td><code>[1, 2, 3]</code> or newline-separated with <code>-</code></td></tr>
<tr><td>Nested tables</td><td><code>[table.subtable]</code></td><td>Indented structure</td></tr>
</tbody></table>
</div>
<h3 id="configuration-file-locations-1"><a class="header" href="#configuration-file-locations-1">Configuration File Locations</a></h3>
<p>Both global and project configurations have moved from <code>.toml</code> to <code>.yml</code>:</p>
<div class="table-wrapper"><table><thead><tr><th>Type</th><th>Old Location</th><th>New Location</th></tr></thead><tbody>
<tr><td>Global Config</td><td><code>~/.prodigy/config.toml</code></td><td><code>~/.prodigy/config.yml</code></td></tr>
<tr><td>Project Config</td><td><code>.prodigy/config.toml</code></td><td><code>.prodigy/config.yml</code></td></tr>
</tbody></table>
</div>
<h3 id="global-configuration-migration"><a class="header" href="#global-configuration-migration">Global Configuration Migration</a></h3>
<p><strong>Before (<code>~/.prodigy/config.toml</code>):</strong></p>
<pre><code class="language-toml">prodigy_home = "/Users/username/.prodigy"
default_editor = "vim"
log_level = "info"
auto_commit = true
max_concurrent_specs = 2

[storage]
backend = "file"

[storage.backend_config]
base_dir = "/Users/username/.prodigy"
repository_grouping = true

[plugins]
enabled = true
directory = "/Users/username/.prodigy/plugins"
auto_load = ["plugin1", "plugin2"]
</code></pre>
<p><strong>After (<code>~/.prodigy/config.yml</code>):</strong></p>
<pre><code class="language-yaml">prodigy_home: /Users/username/.prodigy
default_editor: vim
log_level: info
auto_commit: true
max_concurrent_specs: 2

storage:
  backend: file
  backend_config:
    base_dir: /Users/username/.prodigy
    repository_grouping: true

plugins:
  enabled: true
  directory: /Users/username/.prodigy/plugins
  auto_load:
    - plugin1
    - plugin2
</code></pre>
<h3 id="project-configuration-migration"><a class="header" href="#project-configuration-migration">Project Configuration Migration</a></h3>
<p><strong>Before (<code>.prodigy/config.toml</code>):</strong></p>
<pre><code class="language-toml">name = "my-project"
description = "A sample project"

[variables]
PROJECT_ROOT = "/app"
VERSION = "1.0.0"

[claude]
api_key = "sk-ant-api03-..."
model = "claude-3-sonnet"

[storage]
backend = "file"
</code></pre>
<p><strong>After (<code>.prodigy/config.yml</code>):</strong></p>
<pre><code class="language-yaml">name: my-project
description: A sample project

variables:
  PROJECT_ROOT: /app
  VERSION: "1.0.0"

claude:
  api_key: sk-ant-api03-...
  model: claude-3-sonnet

storage:
  backend: file
</code></pre>
<h3 id="array-syntax-migration"><a class="header" href="#array-syntax-migration">Array Syntax Migration</a></h3>
<p>TOML and YAML handle arrays differently:</p>
<p><strong>TOML:</strong></p>
<pre><code class="language-toml">auto_load = ["plugin1", "plugin2", "plugin3"]
</code></pre>
<p><strong>YAML (inline style):</strong></p>
<pre><code class="language-yaml">auto_load: [plugin1, plugin2, plugin3]
</code></pre>
<p><strong>YAML (block style - recommended):</strong></p>
<pre><code class="language-yaml">auto_load:
  - plugin1
  - plugin2
  - plugin3
</code></pre>
<h3 id="nested-structure-migration"><a class="header" href="#nested-structure-migration">Nested Structure Migration</a></h3>
<p>TOML uses dotted keys or table headers for nesting. YAML uses indentation.</p>
<p><strong>TOML:</strong></p>
<pre><code class="language-toml">[storage]
backend = "file"

[storage.backend_config]
base_dir = "/Users/username/.prodigy"
repository_grouping = true
</code></pre>
<p><strong>YAML:</strong></p>
<pre><code class="language-yaml">storage:
  backend: file
  backend_config:
    base_dir: /Users/username/.prodigy
    repository_grouping: true
</code></pre>
<h3 id="string-quoting-rules"><a class="header" href="#string-quoting-rules">String Quoting Rules</a></h3>
<p>YAML is more relaxed about string quoting:</p>
<p><strong>When quotes are NOT needed:</strong></p>
<pre><code class="language-yaml">name: my-project
path: /usr/local/bin
url: https://example.com
</code></pre>
<p><strong>When quotes ARE needed:</strong></p>
<pre><code class="language-yaml"># Contains colon
message: "Error: something failed"

# Contains hash (would be a comment)
tag: "#important"

# Starts with special character
value: "@username"

# Looks like a number but should be string
version: "1.0"

# Contains YAML reserved words
status: "yes"  # Would be boolean without quotes
</code></pre>
<h3 id="boolean-values"><a class="header" href="#boolean-values">Boolean Values</a></h3>
<p>Both use the same boolean syntax:</p>
<pre><code class="language-yaml">auto_commit: true
enabled: false
</code></pre>
<p><strong>Note</strong>: In YAML, these are also booleans: <code>yes</code>, <code>no</code>, <code>on</code>, <code>off</code>, <code>true</code>, <code>false</code>. To use them as strings, quote them: <code>"yes"</code>, <code>"no"</code>.</p>
<h3 id="comments"><a class="header" href="#comments">Comments</a></h3>
<p>Both use <code>#</code> for comments:</p>
<pre><code class="language-yaml"># This is a comment
log_level: info  # Inline comment
</code></pre>
<h3 id="migrating-workflows"><a class="header" href="#migrating-workflows">Migrating Workflows</a></h3>
<p>Workflows were always YAML, so no migration is needed. However, if you referenced TOML config in workflows, update the file extension:</p>
<p><strong>Before:</strong></p>
<pre><code class="language-yaml">commands:
  - shell: "cat .prodigy/config.toml"
</code></pre>
<p><strong>After:</strong></p>
<pre><code class="language-yaml">commands:
  - shell: "cat .prodigy/config.yml"
</code></pre>
<h3 id="migration-checklist"><a class="header" href="#migration-checklist">Migration Checklist</a></h3>
<ul>
<li><input disabled="" type="checkbox"/>
Rename <code>~/.prodigy/config.toml</code> → <code>~/.prodigy/config.yml</code></li>
<li><input disabled="" type="checkbox"/>
Rename <code>.prodigy/config.toml</code> → <code>.prodigy/config.yml</code></li>
<li><input disabled="" type="checkbox"/>
Convert TOML syntax to YAML syntax</li>
<li><input disabled="" type="checkbox"/>
Update section headers <code>[section]</code> to <code>section:</code> with indentation</li>
<li><input disabled="" type="checkbox"/>
Convert assignments <code>key = value</code> to <code>key: value</code></li>
<li><input disabled="" type="checkbox"/>
Fix array syntax (use <code>-</code> for lists)</li>
<li><input disabled="" type="checkbox"/>
Ensure consistent 2-space indentation</li>
<li><input disabled="" type="checkbox"/>
Quote strings with special characters</li>
<li><input disabled="" type="checkbox"/>
Test configuration with <code>prodigy config show</code></li>
<li><input disabled="" type="checkbox"/>
Update any workflow references to config files</li>
</ul>
<h3 id="validation"><a class="header" href="#validation">Validation</a></h3>
<p>After migration, validate your configuration:</p>
<pre><code class="language-bash"># Check effective configuration (merges global + project)
prodigy config show

# Validate YAML syntax
yamllint .prodigy/config.yml

# Run a simple workflow to verify settings work
prodigy run test-workflow.yml
</code></pre>
<h3 id="troubleshooting-8"><a class="header" href="#troubleshooting-8">Troubleshooting</a></h3>
<p><strong>“Invalid YAML syntax” error:</strong></p>
<ul>
<li>Check indentation (must be 2 spaces, no tabs)</li>
<li>Ensure colons have space after them (<code>key: value</code>, not <code>key:value</code>)</li>
<li>Quote strings with special characters</li>
</ul>
<p><strong>“Configuration not found” error:</strong></p>
<ul>
<li>Ensure file is named <code>config.yml</code> (not <code>config.yaml</code> or <code>config.toml</code>)</li>
<li>Check file is in correct location (<code>.prodigy/</code> or <code>~/.prodigy/</code>)</li>
</ul>
<p><strong>“Invalid field” warnings:</strong></p>
<ul>
<li>Remove deprecated TOML-specific fields</li>
<li>Check spelling of field names</li>
<li>Refer to <a href="configuration/global-configuration-structure.html">Configuration Structure</a> for valid fields</li>
</ul>
<h3 id="backward-compatibility"><a class="header" href="#backward-compatibility">Backward Compatibility</a></h3>
<p><strong>TOML support is deprecated</strong> and may be removed in future versions. Migration to YAML is strongly recommended.</p>
<p>If you must support both formats temporarily:</p>
<ol>
<li>Keep both <code>config.toml</code> and <code>config.yml</code></li>
<li>YAML takes precedence if both exist</li>
<li>Migrate fully before upgrading to newer Prodigy versions</li>
</ol>
<h3 id="see-also-19"><a class="header" href="#see-also-19">See Also</a></h3>
<ul>
<li><a href="configuration/global-configuration-structure.html">Global Configuration Structure</a></li>
<li><a href="configuration/project-configuration-structure.html">Project Configuration Structure</a></li>
<li><a href="configuration/configuration-precedence-rules.html">Configuration Precedence Rules</a></li>
<li><a href="https://yaml.org/spec/">YAML Specification</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="related-documentation-2"><a class="header" href="#related-documentation-2">Related Documentation</a></h2>
<p>This section provides links to related configuration and feature documentation.</p>
<h3 id="core-configuration"><a class="header" href="#core-configuration">Core Configuration</a></h3>
<ul>
<li><a href="configuration/global-configuration-structure.html">Global Configuration Structure</a> - System-wide settings in <code>~/.prodigy/config.yml</code></li>
<li><a href="configuration/project-configuration-structure.html">Project Configuration Structure</a> - Project settings in <code>.prodigy/config.yml</code></li>
<li><a href="configuration/environment-variables.html">Environment Variables</a> - System and workflow environment variables</li>
<li><a href="configuration/configuration-precedence-rules.html">Configuration Precedence Rules</a> - How settings override each other</li>
<li><a href="configuration/default-values-reference.html">Default Values Reference</a> - Built-in default values for all settings</li>
<li><a href="configuration/storage-configuration.html">Storage Configuration</a> - Data storage backends and options</li>
</ul>
<h3 id="workflow-features"><a class="header" href="#workflow-features">Workflow Features</a></h3>
<ul>
<li><a href="configuration/workflow-configuration.html">Workflow Configuration</a> - Workflow file structure and options</li>
<li><a href="configuration/environment-variables.html#workflow-environment-variables">Environment Variables - Workflow Section</a> - Using <code>env:</code> blocks in workflows</li>
<li><a href="configuration/best-practices.html">Best Practices</a> - Configuration and workflow best practices</li>
</ul>
<h3 id="migration-and-troubleshooting"><a class="header" href="#migration-and-troubleshooting">Migration and Troubleshooting</a></h3>
<ul>
<li><a href="configuration/migration-guide-toml-to-yaml.html">Migration Guide: TOML to YAML</a> - Migrate from old TOML format</li>
<li><a href="configuration/troubleshooting.html">Troubleshooting</a> - Common configuration issues and solutions</li>
</ul>
<h3 id="advanced-topics-1"><a class="header" href="#advanced-topics-1">Advanced Topics</a></h3>
<ul>
<li><a href="configuration/../git-context-advanced.html">Git Context (Advanced)</a> - Pattern filtering and format modifiers for git variables</li>
<li><a href="configuration/../mapreduce-worktree-architecture.html">MapReduce Worktree Architecture</a> - Understanding worktree isolation in MapReduce workflows</li>
</ul>
<h3 id="feature-documentation"><a class="header" href="#feature-documentation">Feature Documentation</a></h3>
<ul>
<li><a href="configuration/../mapreduce.html">MapReduce</a> - Parallel workflow execution</li>
<li><a href="configuration/../variables.html">Variables</a> - Variable interpolation and usage</li>
<li><a href="configuration/../commands.html">Command Types</a> - Available command types (claude, shell, goal_seek, etc.)</li>
<li><a href="configuration/../error-handling.html">Error Handling</a> - Error handling strategies and retry policies</li>
</ul>
<h3 id="reference-material"><a class="header" href="#reference-material">Reference Material</a></h3>
<ul>
<li><a href="configuration/complete-configuration-examples.html">Complete Configuration Examples</a> - Real-world configuration examples</li>
<li><a href="configuration/../cli-reference.html">CLI Reference</a> - Command-line interface documentation</li>
<li><a href="https://yaml.org/spec/">YAML Specification</a> - Official YAML documentation</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="advanced-features"><a class="header" href="#advanced-features">Advanced Features</a></h1>
<p>This chapter covers advanced workflow features for building sophisticated automation pipelines. These features enable conditional execution, parallel processing, validation, and complex control flow.</p>
<hr />
<h2 id="conditional-execution-1"><a class="header" href="#conditional-execution-1">Conditional Execution</a></h2>
<p>Control when commands execute based on expressions or previous command results.</p>
<h3 id="expression-based-conditions"><a class="header" href="#expression-based-conditions">Expression-Based Conditions</a></h3>
<p>Use the <code>when</code> field to conditionally execute commands based on variable values:</p>
<pre><code class="language-yaml"># Execute only when variable is true
- shell: "cargo build --release"
  when: "${tests_passed}"

# Execute based on complex expression
- shell: "deploy.sh"
  when: "${environment == 'production' &amp;&amp; tests_passed}"
</code></pre>
<h4 id="expression-syntax-for-when-clauses"><a class="header" href="#expression-syntax-for-when-clauses">Expression Syntax for When Clauses</a></h4>
<p>The <code>when</code> clause supports a flexible expression syntax for conditional logic:</p>
<p><strong>Variable Interpolation:</strong></p>
<ul>
<li>Use <code>${variable}</code> to reference captured outputs or environment variables</li>
<li>Variables are evaluated in the context of previous command results</li>
<li>Boolean variables are evaluated as truthy/falsy values</li>
</ul>
<p><strong>Comparison Operators:</strong></p>
<ul>
<li><code>==</code> - Equality comparison (e.g., <code>${status == 'success'}</code>)</li>
<li><code>!=</code> - Inequality comparison (e.g., <code>${exit_code != 0}</code>)</li>
<li><code>&gt;</code> - Greater than (e.g., <code>${score &gt; 80}</code>)</li>
<li><code>&lt;</code> - Less than (e.g., <code>${errors &lt; 5}</code>)</li>
<li><code>&gt;=</code> - Greater than or equal to (e.g., <code>${coverage &gt;= 90}</code>)</li>
<li><code>&lt;=</code> - Less than or equal to (e.g., <code>${warnings &lt;= 10}</code>)</li>
<li><code>contains</code> - String matching (e.g., <code>${output contains 'success'}</code>)</li>
</ul>
<p><strong>Logical Operators:</strong></p>
<ul>
<li><code>&amp;&amp;</code> - Logical AND (e.g., <code>${tests_passed &amp;&amp; build_succeeded}</code>)</li>
<li><code>||</code> - Logical OR (e.g., <code>${is_dev || is_staging}</code>)</li>
</ul>
<p><strong>Type Coercion:</strong></p>
<ul>
<li>String values: Non-empty strings are truthy, empty strings are falsy</li>
<li>Numeric values: Non-zero numbers are truthy, zero is falsy</li>
<li>Boolean values: <code>true</code> is truthy, <code>false</code> is falsy</li>
</ul>
<p><strong>Complex Expressions:</strong></p>
<pre><code class="language-yaml"># Multiple conditions with logical operators
- shell: "deploy.sh"
  when: "${environment == 'production' &amp;&amp; tests_passed &amp;&amp; coverage &gt;= 80}"

# Nested logic with parentheses
- shell: "run-checks.sh"
  when: "${(is_pr || is_main) &amp;&amp; tests_passed}"

# Comparing captured outputs
- shell: "notify-team.sh"
  when: "${test-step.exit_code == 0 &amp;&amp; build-step.success}"
</code></pre>
<h3 id="on-success-handlers"><a class="header" href="#on-success-handlers">On Success Handlers</a></h3>
<p>Execute follow-up commands when a command succeeds:</p>
<pre><code class="language-yaml">- shell: "cargo test"
  on_success:
    shell: "cargo bench"
</code></pre>
<p><strong>Note</strong>: The <code>on_success</code> field supports any workflow step command with all its features, including nested conditionals, output capture, validation, and error handlers. You can create complex success workflows by combining multiple handlers or using <code>when</code> clauses for sophisticated control flow.</p>
<p><strong>Complex On Success Example:</strong></p>
<p>The <code>on_success</code> field accepts a complete workflow step command with all its features:</p>
<pre><code class="language-yaml">- shell: "cargo build --release"
  on_success:
    shell: "check-binary-size.sh"
    validate:
      threshold: 100
    on_failure:
      claude: "/optimize-binary-size"
      max_attempts: 2
</code></pre>
<p><strong>Source</strong>: Based on WorkflowStepCommand structure (src/config/command.rs:376)</p>
<h3 id="on-failure-handlers"><a class="header" href="#on-failure-handlers">On Failure Handlers</a></h3>
<p>Handle failures with automatic remediation:</p>
<pre><code class="language-yaml">- shell: "cargo clippy"
  on_failure:
    claude: "/fix-warnings"
    max_attempts: 3
    fail_workflow: false
    commit_required: true
</code></pre>
<p>The <code>on_failure</code> configuration supports:</p>
<ul>
<li><code>max_attempts</code>: Maximum retry attempts (default: 3)</li>
<li><code>fail_workflow</code>: Whether to fail entire workflow on final failure (default: false)</li>
<li><code>commit_required</code>: Whether the remediation command should create a git commit (default: true)</li>
</ul>
<p><strong>Note</strong>: These defaults come from the <code>TestDebugConfig</code> which provides sensible defaults for error recovery workflows.</p>
<p><strong>Source</strong>: TestDebugConfig struct definition (src/config/command.rs:168-183)</p>
<h3 id="nested-conditionals"><a class="header" href="#nested-conditionals">Nested Conditionals</a></h3>
<p>Chain multiple levels of conditional execution:</p>
<pre><code class="language-yaml">- shell: "cargo check"
  on_success:
    shell: "cargo build --release"
    on_success:
      shell: "cargo test --release"
      on_failure:
        claude: "/debug-failures '${shell.output}'"
</code></pre>
<p><strong>Note</strong>: For multi-step error recovery, nest individual <code>on_failure</code> handlers at each step rather than using a commands array. The <code>TestDebugConfig</code> supports only a single <code>claude</code> command per handler.</p>
<hr />
<h2 id="output-capture-and-variable-management"><a class="header" href="#output-capture-and-variable-management">Output Capture and Variable Management</a></h2>
<p>Capture command output in different formats for use in subsequent steps.</p>
<h3 id="capture-variable"><a class="header" href="#capture-variable">Capture Variable</a></h3>
<p>Capture output to a named variable using the <code>capture_output</code> field:</p>
<pre><code class="language-yaml"># Capture as string (backward compatible)
- shell: "git rev-parse HEAD"
  capture_output: "commit_hash"

# Reference in later steps
- shell: "echo 'Commit: ${commit_hash}'"
</code></pre>
<h3 id="command-agnostic-capture"><a class="header" href="#command-agnostic-capture">Command-Agnostic Capture</a></h3>
<p>The <code>last.*</code> variables capture output from any command type without needing explicit <code>capture_output</code>:</p>
<pre><code class="language-yaml"># Shell command output
- shell: "cargo test"
  # Output automatically available as ${last.output} and ${last.exit_code}

# Use in next command (any type)
- claude: "/analyze ${last.output}"

# Or reference in conditional
- shell: "notify-failure.sh"
  when: "${last.exit_code != 0}"
</code></pre>
<p><strong>Available Variables:</strong></p>
<ul>
<li><code>${last.output}</code> - Output from the last command of any type (shell, claude, etc.)</li>
<li><code>${last.exit_code}</code> - Exit code from the last command</li>
</ul>
<p>These variables work across all command types, making them ideal for generic workflows where you don’t want to hard-code command-specific variables like <code>${shell.output}</code> or <code>${claude.output}</code>.</p>
<p><strong>Source</strong>: Variable constants defined in src/cook/workflow/variables.rs:35-36</p>
<h3 id="capture-formats-1"><a class="header" href="#capture-formats-1">Capture Formats</a></h3>
<p>Control how output is parsed with <code>capture_format</code>:</p>
<pre><code class="language-yaml"># String (default) - trimmed output as single string
- shell: "git rev-parse HEAD"
  capture_output: "commit_hash"
  capture_format: "string"

# Number - parse output as number
- shell: "wc -l &lt; file.txt"
  capture_output: "line_count"
  capture_format: "number"

# JSON - parse output as JSON object
- shell: "cargo metadata --format-version 1"
  capture_output: "metadata"
  capture_format: "json"

# Lines - split output into array of lines
- shell: "find . -name '*.rs'"
  capture_output: "rust_files"
  capture_format: "lines"

# Boolean - parse "true"/"false" as boolean
- shell: "test -f README.md &amp;&amp; echo true || echo false"
  capture_output: "has_readme"
  capture_format: "boolean"
</code></pre>
<blockquote>
<p><strong>Error Handling:</strong> If parsing fails (e.g., non-numeric output with <code>capture_format: number</code>), the command will fail with a descriptive error. Use <code>capture_format: string</code> (default) when output format is unreliable.</p>
</blockquote>
<p><strong>Source</strong>: CaptureFormat enum (src/cook/workflow/variables.rs:260-265)</p>
<h3 id="stream-capture-control"><a class="header" href="#stream-capture-control">Stream Capture Control</a></h3>
<p>The <code>capture_streams</code> field supports two formats for flexible output capture.</p>
<p><strong>Simple String Format</strong> - For basic stream selection:</p>
<pre><code class="language-yaml"># Capture only stdout (default)
- shell: "cargo build"
  capture_output: "build_log"
  capture_streams: "stdout"

# Capture only stderr
- shell: "cargo test"
  capture_output: "error_log"
  capture_streams: "stderr"

# Capture both streams merged
- shell: "npm install"
  capture_output: "install_log"
  capture_streams: "both"
</code></pre>
<p><strong>Structured Object Format</strong> - For advanced control with exit code, success status, and duration:</p>
<pre><code class="language-yaml"># Structured format with all fields
- shell: "cargo test"
  capture_output: "test_result"
  capture_streams:
    stdout: true      # Capture stdout stream (default: true)
    stderr: true      # Capture stderr stream (default: false)
    exit_code: true   # Capture exit code (default: true)
    success: true     # Capture success status (default: true)
    duration: true    # Capture execution duration in seconds (default: true)

# Access individual fields
- shell: "echo 'Test exit code: ${test_result.exit_code}'"
- shell: "echo 'Test passed: ${test_result.success}'"
- shell: "echo 'Duration: ${test_result.duration}s'"
</code></pre>
<p><strong>Source</strong>: CaptureStreams struct definition (src/cook/workflow/variables.rs:268-292)</p>
<p><strong>Format Flexibility:</strong></p>
<p>The <code>capture_streams</code> field accepts two formats:</p>
<ol>
<li><strong>Simple string</strong> (<code>"stdout"</code>, <code>"stderr"</code>, <code>"both"</code>) - Stored as <code>Option&lt;String&gt;</code> in YAML config, best for basic stream selection</li>
<li><strong>Structured object</strong> - Parsed into <code>CaptureStreams</code> struct during execution, enables fine-grained control over <code>exit_code</code>, <code>success</code>, and <code>duration</code> capture</li>
</ol>
<p>Use simple format for basic cases, structured format when you need detailed execution metadata.</p>
<p><strong>Source</strong>: WorkflowStepCommand.capture_streams field (src/config/command.rs:396), CaptureStreams struct (src/cook/workflow/variables.rs:268-292)</p>
<h3 id="output-file-redirection"><a class="header" href="#output-file-redirection">Output File Redirection</a></h3>
<p>Write output directly to a file instead of capturing it:</p>
<pre><code class="language-yaml"># Redirect stdout to file
- shell: "cargo doc --no-deps"
  output_file: "docs/build.log"

# Combine with capture for dual output
- shell: "cargo test"
  capture_output: "test_status"
  output_file: "test-results.log"
</code></pre>
<h3 id="execution-context-1"><a class="header" href="#execution-context-1">Execution Context</a></h3>
<p>Configure where and how commands execute in your workflow.</p>
<p><strong>Note</strong>: Step-level environment variable configuration (<code>env</code>, <code>clear_env</code>, <code>inherit</code>) and working directory (<code>working_dir</code>, <code>cwd</code>) are internal features available in the execution layer but not currently exposed in the YAML configuration layer (WorkflowStepCommand). These features exist in the runtime <code>WorkflowStep</code> type for internal use.</p>
<p>For workflow-level environment configuration, see the <a href="advanced/../workflow-basics.html#environment-variables">Environment Variables</a> section in Workflow Basics.</p>
<hr />
<h2 id="additional-topics-5"><a class="header" href="#additional-topics-5">Additional Topics</a></h2>
<p>See also:</p>
<ul>
<li><a href="advanced/step-identification.html">Step Identification</a></li>
<li><a href="advanced/timeout-configuration.html">Timeout Configuration</a></li>
<li><a href="advanced/implementation-validation.html">Implementation Validation</a></li>
<li><a href="advanced/parallel-iteration-with-foreach.html">Parallel Iteration with Foreach</a></li>
<li><a href="advanced/goal-seeking-operations.html">Goal-Seeking Operations</a></li>
<li><a href="advanced/best-practices.html">Best Practices</a></li>
<li><a href="advanced/common-patterns.html">Common Patterns</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="step-identification"><a class="header" href="#step-identification">Step Identification</a></h2>
<p>Assign unique IDs to steps for explicit output referencing. This is particularly useful in complex workflows where multiple steps produce outputs and you need to reference specific results.</p>
<h3 id="available-step-reference-fields"><a class="header" href="#available-step-reference-fields">Available Step Reference Fields</a></h3>
<p>When you assign an ID to a step, you can reference multiple fields from that step’s execution:</p>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Description</th><th>Example</th></tr></thead><tbody>
<tr><td><code>${step-id.output}</code></td><td>string</td><td>Standard output (stdout) from step</td><td><code>${test-step.output}</code></td></tr>
<tr><td><code>${step-id.exit_code}</code></td><td>number</td><td>Process exit code (0 = success)</td><td><code>${build.exit_code}</code></td></tr>
<tr><td><code>${step-id.success}</code></td><td>boolean</td><td>Whether step succeeded (exit_code == 0)</td><td><code>${lint.success}</code></td></tr>
</tbody></table>
</div>
<p><strong>Source</strong>: Field resolution implemented in <code>src/cook/expression/mod.rs:187-200</code></p>
<p>These fields are automatically available for any step with an <code>id</code> field. They’re commonly used in conditionals and error handling:</p>
<h3 id="basic-step-ids-with-auto-captured-fields"><a class="header" href="#basic-step-ids-with-auto-captured-fields">Basic Step IDs with Auto-Captured Fields</a></h3>
<pre><code class="language-yaml">- shell: "cargo test"
  id: "test-step"

# Reference step's automatic fields
- shell: "echo 'Exit code: ${test-step.exit_code}'"
- shell: "echo 'Success: ${test-step.success}'"
- claude: "/analyze-test-output '${test-step.output}'"
  when: "${test-step.exit_code != 0}"
</code></pre>
<p><strong>Note</strong>: The <code>.output</code>, <code>.exit_code</code>, and <code>.success</code> fields are automatically captured for any step with an <code>id</code>. You don’t need to explicitly configure output capture for these standard fields.</p>
<h3 id="custom-output-fields"><a class="header" href="#custom-output-fields">Custom Output Fields</a></h3>
<p>For capturing specific files or structured data, use the <code>outputs</code> field:</p>
<pre><code class="language-yaml">- claude: "/prodigy-code-review"
  id: "review"
  outputs:
    spec:
      file_pattern: "*-spec.md"

# Reference custom output field
- claude: "/prodigy-implement-spec ${review.spec}"
  id: "implement"
</code></pre>
<p><strong>Source</strong>: Real example from <code>src/cook/mod_tests.rs:223-228</code> and <code>src/config/command.rs:1049</code></p>
<p>Custom outputs are useful for:</p>
<ul>
<li>Capturing generated files (specs, reports, configs)</li>
<li>Passing structured data between steps</li>
<li>Referencing specific artifacts by name</li>
</ul>
<h3 id="when-to-use-step-ids"><a class="header" href="#when-to-use-step-ids">When to Use Step IDs</a></h3>
<p><strong>1. Conditional Execution Based on Step Results</strong></p>
<p>Step IDs enable precise control flow using the auto-captured <code>.exit_code</code> and <code>.success</code> fields:</p>
<pre><code class="language-yaml">- shell: "cargo test --lib"
  id: "unit-tests"

- shell: "cargo test --test integration"
  id: "integration-tests"

# Execute only if specific test suite failed
- claude: "/analyze-failures '${unit-tests.output}'"
  when: "${unit-tests.exit_code != 0}"

- claude: "/analyze-failures '${integration-tests.output}'"
  when: "${integration-tests.exit_code != 0}"
</code></pre>
<p><strong>Source</strong>: Conditional evaluation from <code>src/cook/workflow/conditional_tests.rs:95-97</code></p>
<p><strong>2. Error Handling with Step-Specific Outputs</strong></p>
<p>Use step IDs to pass the exact output from a failed step to error handlers:</p>
<pre><code class="language-yaml">- shell: "npm run build"
  id: "build"

- shell: "npm run lint"
  id: "lint"

- shell: "npm test"
  id: "test"

# Pass step output to Claude for analysis
- claude: "/debug-build-failure '${build.output}'"
  when: "${build.exit_code != 0}"
</code></pre>
<p><strong>Source</strong>: Real pattern from <code>src/cook/mod.rs:721</code> and <code>src/cook/execution/mapreduce_integration_tests.rs:230</code></p>
<p><strong>3. Multi-Step Conditional Logic</strong></p>
<p>Combine multiple step results to control workflow execution:</p>
<pre><code class="language-yaml">- shell: "cargo clippy"
  id: "clippy-check"

- shell: "cargo fmt --check"
  id: "format-check"

# Only proceed if both checks pass
- shell: "cargo build --release"
  when: "${clippy-check.exit_code == 0 &amp;&amp; format-check.exit_code == 0}"

# Fix clippy warnings if present
- claude: "/fix-clippy-warnings '${clippy-check.output}'"
  when: "${clippy-check.exit_code != 0}"
</code></pre>
<p><strong>Source</strong>: Boolean logic support from <code>src/cook/workflow/conditional_tests.rs:56</code></p>
<p><strong>4. Passing Step Outputs to Subsequent Commands</strong></p>
<p>Reference earlier step outputs in later commands, including in <code>on_failure</code> handlers:</p>
<pre><code class="language-yaml">- shell: "cargo test"
  id: "test"
  on_failure:
    claude: "/fix-failing-tests '${test.output}'"
    max_attempts: 3

# Use test results in summary
- shell: "generate-report.sh '${test.output}' ${test.exit_code}"
</code></pre>
<p><strong>Source</strong>: Real <code>on_failure</code> pattern from <code>src/cook/mod.rs:721-722</code></p>
<h3 id="when-not-to-use-step-ids"><a class="header" href="#when-not-to-use-step-ids">When NOT to Use Step IDs</a></h3>
<p>Step IDs add complexity, so skip them when:</p>
<ul>
<li><strong>Single command workflows</strong>: If you only have one or two steps, <code>${shell.output}</code> is clear enough</li>
<li><strong>No output references</strong>: If you never reference a step’s output, exit code, or success status</li>
<li><strong>Simple sequential execution</strong>: Steps that always run in order without conditionals</li>
</ul>
<p><strong>Example of when step IDs are unnecessary:</strong></p>
<pre><code class="language-yaml">- shell: "cargo build"
- shell: "cargo test"
- shell: "cargo clippy"
</code></pre>
<p>None of these steps reference each other, so IDs would add no value.</p>
<h3 id="implementation-details-1"><a class="header" href="#implementation-details-1">Implementation Details</a></h3>
<p><strong>How It Works:</strong></p>
<ol>
<li>
<p>When a step with <code>id: "my-step"</code> executes, Prodigy automatically captures:</p>
<ul>
<li><code>${my-step.output}</code> - stdout from the command</li>
<li><code>${my-step.exit_code}</code> - process exit code (0 = success, non-zero = failure)</li>
<li><code>${my-step.success}</code> - boolean indicating success (computed as <code>exit_code == 0</code>)</li>
</ul>
</li>
<li>
<p>Custom outputs (via <code>outputs:</code> field) are stored separately and accessed by their output name:</p>
<ul>
<li><code>${my-step.custom-output-name}</code> - file path matching the pattern</li>
</ul>
</li>
<li>
<p>Variable resolution happens at runtime when constructing subsequent commands</p>
</li>
</ol>
<p><strong>Source</strong>: Implementation in <code>src/cook/expression/mod.rs:187-200</code> and <code>src/cook/orchestrator/execution_pipeline.rs:650-654</code></p>
<h3 id="troubleshooting-9"><a class="header" href="#troubleshooting-9">Troubleshooting</a></h3>
<p><strong>“Variable not found” errors:</strong></p>
<ul>
<li>Ensure the step has completed before referencing its outputs</li>
<li>Verify the step ID matches exactly (case-sensitive)</li>
<li>Check that the step actually has an <code>id</code> field</li>
</ul>
<p><strong>Empty output values:</strong></p>
<ul>
<li>Step output is only captured if the command writes to stdout</li>
<li>Use <code>outputs:</code> for file-based artifacts instead of stdout</li>
<li>Verify the command actually produces output</li>
</ul>
<p><strong>Conditional not working:</strong></p>
<ul>
<li>The <code>.success</code> and <code>.exit_code</code> fields are only available for steps with <code>id</code> set</li>
<li>Ensure your <code>when:</code> expression uses correct syntax: <code>"${step-id.exit_code != 0}"</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="timeout-configuration"><a class="header" href="#timeout-configuration">Timeout Configuration</a></h2>
<p>Set execution timeouts to prevent workflows from hanging indefinitely. Prodigy supports two distinct timeout mechanisms: command-level timeouts for standard workflows and MapReduce-specific timeouts with advanced configuration options.</p>
<h3 id="command-level-timeouts"><a class="header" href="#command-level-timeouts">Command-Level Timeouts</a></h3>
<p>Command-level timeouts apply to individual commands in standard workflows. These accept numeric values only (in seconds).</p>
<p><strong>Source</strong>: <code>src/config/command.rs:384</code> - <code>pub timeout: Option&lt;u64&gt;</code></p>
<pre><code class="language-yaml">commands:
  # Shell command with 10 minute timeout
  - shell: "cargo bench"
    timeout: 600

  # Claude command with 30 minute timeout
  - claude: "/analyze-codebase"
    timeout: 1800

  # No timeout specified = no limit
  - shell: "cargo build"
</code></pre>
<p><strong>Real-world examples from workflows:</strong></p>
<p>From <code>workflows/complex-build-pipeline.yml:12</code>:</p>
<pre><code class="language-yaml">- shell: "cargo bench"
  timeout: 600  # 10 minutes
  capture_output: "benchmark_results"
</code></pre>
<p>From <code>workflows/documentation-drift.yml:15</code>:</p>
<pre><code class="language-yaml">- shell: "cargo test --doc"
  timeout: 300  # 5 minutes
  on_failure:
    claude: "/prodigy-debug-test-failure --output ${shell.output}"
</code></pre>
<p><strong>Important</strong>: Command-level timeouts only accept numeric values. For environment variable support, use MapReduce timeouts (see below).</p>
<h3 id="mapreduce-timeouts"><a class="header" href="#mapreduce-timeouts">MapReduce Timeouts</a></h3>
<p>MapReduce workflows support more sophisticated timeout configuration with environment variable support and advanced policies.</p>
<h4 id="setup-phase-timeout"><a class="header" href="#setup-phase-timeout">Setup Phase Timeout</a></h4>
<p>Control how long the setup phase can run before timing out.</p>
<p><strong>Source</strong>: <code>src/config/mapreduce.rs:148</code> - Uses <code>deserialize_optional_u64_or_string</code></p>
<pre><code class="language-yaml">mode: mapreduce

setup:
  timeout: 300  # 5 minutes for setup
  commands:
    - shell: "generate-work-items.sh"
</code></pre>
<p><strong>With environment variables:</strong></p>
<pre><code class="language-yaml">setup:
  timeout: $SETUP_TIMEOUT  # References environment variable
  commands:
    - shell: "cargo build"
</code></pre>
<h4 id="map-phase-agent-timeout"><a class="header" href="#map-phase-agent-timeout">Map Phase Agent Timeout</a></h4>
<p>Set a global timeout for all map agents or configure per-agent policies.</p>
<p><strong>Source</strong>: <code>src/config/mapreduce.rs:269</code> - <code>pub agent_timeout_secs: Option&lt;String&gt;</code></p>
<p><strong>Simple agent timeout:</strong></p>
<pre><code class="language-yaml">map:
  agent_timeout_secs: 600  # 10 minutes per agent
  agent_template:
    - claude: "/process '${item}'"
</code></pre>
<p><strong>With environment variable:</strong></p>
<pre><code class="language-yaml">map:
  agent_timeout_secs: $AGENT_TIMEOUT  # Configurable via environment
  agent_template:
    - claude: "/process '${item}'"
</code></pre>
<h4 id="advanced-timeout-configuration"><a class="header" href="#advanced-timeout-configuration">Advanced Timeout Configuration</a></h4>
<p>For fine-grained control, use <code>timeout_config</code> to specify policies, per-command overrides, and timeout actions.</p>
<p><strong>Source</strong>: <code>src/cook/execution/mapreduce/timeout.rs:38-63</code> - <code>TimeoutConfig</code> struct</p>
<pre><code class="language-yaml">map:
  timeout_config:
    agent_timeout_secs: 600          # Global 10 minute timeout
    timeout_policy: hybrid           # Apply per-agent with overrides
    cleanup_grace_period_secs: 30    # 30s to clean up after timeout
    timeout_action: dlq              # Send timed-out items to DLQ
    enable_monitoring: true          # Track timeout metrics

    # Per-command timeout overrides
    command_timeouts:
      claude: 300                    # Claude commands: 5 minutes
      shell: 60                      # Shell commands: 1 minute
      claude_0: 600                  # First Claude command: 10 minutes

  agent_template:
    - claude: "/analyze '${item}'"   # Uses 300s from command_timeouts
    - shell: "test ${item.path}"     # Uses 60s from command_timeouts
</code></pre>
<p><strong>Real example from tests</strong> (<code>tests/timeout_integration_test.rs:215-233</code>):</p>
<pre><code class="language-yaml">agent_timeout_secs: 600
timeout_config:
  timeout_policy: hybrid
  cleanup_grace_period_secs: 30
  timeout_action: dlq
  enable_monitoring: true
  command_timeouts:
    claude: 300
    shell: 60
    claude_0: 600
</code></pre>
<h4 id="timeout-policies"><a class="header" href="#timeout-policies">Timeout Policies</a></h4>
<p><strong>Source</strong>: <code>src/cook/execution/mapreduce/timeout.rs:79-88</code> - <code>TimeoutPolicy</code> enum</p>
<ul>
<li>
<p><strong><code>per_agent</code></strong> (default): Timeout applies to entire agent execution</p>
<ul>
<li>Agent must complete all commands within timeout</li>
<li>Best for workflows where total time matters</li>
</ul>
</li>
<li>
<p><strong><code>per_command</code></strong>: Timeout applies to each command individually</p>
<ul>
<li>Each command gets full timeout duration</li>
<li>Best for workflows with highly variable command durations</li>
</ul>
</li>
<li>
<p><strong><code>hybrid</code></strong>: Per-agent timeout with command-specific overrides</p>
<ul>
<li>Commands use <code>command_timeouts</code> if specified, otherwise agent timeout</li>
<li>Most flexible option</li>
</ul>
</li>
</ul>
<p><strong>Example: Per-command policy</strong></p>
<pre><code class="language-yaml">timeout_config:
  agent_timeout_secs: 100
  timeout_policy: per_command  # Each command gets 100 seconds
</code></pre>
<h4 id="timeout-actions"><a class="header" href="#timeout-actions">Timeout Actions</a></h4>
<p><strong>Source</strong>: <code>src/cook/execution/mapreduce/timeout.rs:91-102</code> - <code>TimeoutAction</code> enum</p>
<ul>
<li><strong><code>dlq</code></strong> (default): Send item to Dead Letter Queue for retry</li>
<li><strong><code>skip</code></strong>: Skip the item and continue with other items</li>
<li><strong><code>fail</code></strong>: Fail the entire MapReduce job</li>
<li><strong><code>graceful_terminate</code></strong>: Attempt graceful shutdown before force kill</li>
</ul>
<pre><code class="language-yaml">timeout_config:
  timeout_action: skip  # Skip timed-out items instead of retrying
</code></pre>
<h4 id="default-values-2"><a class="header" href="#default-values-2">Default Values</a></h4>
<p><strong>Source</strong>: <code>src/cook/execution/mapreduce/timeout.rs</code> - Default implementations</p>
<div class="table-wrapper"><table><thead><tr><th>Configuration</th><th>Default Value</th><th>Description</th></tr></thead><tbody>
<tr><td><code>agent_timeout_secs</code></td><td>600 (10 minutes)</td><td>Global agent timeout</td></tr>
<tr><td><code>cleanup_grace_period_secs</code></td><td>30 seconds</td><td>Time for cleanup after timeout</td></tr>
<tr><td><code>enable_monitoring</code></td><td>true</td><td>Track timeout metrics</td></tr>
<tr><td><code>timeout_policy</code></td><td><code>per_agent</code></td><td>Apply timeout to entire agent</td></tr>
<tr><td><code>timeout_action</code></td><td><code>dlq</code></td><td>Send timed-out items to DLQ</td></tr>
</tbody></table>
</div>
<h3 id="best-practices-13"><a class="header" href="#best-practices-13">Best Practices</a></h3>
<p><strong>Set Appropriate Timeouts:</strong></p>
<ul>
<li>Set timeouts high enough to complete under normal conditions</li>
<li>Consider worst-case scenarios (slow CI, cold caches, network latency)</li>
<li>Use shorter timeouts for quick operations to fail fast</li>
<li>Test timeout values in your environment before production use</li>
</ul>
<p><strong>MapReduce Timeout Strategy:</strong></p>
<ul>
<li>Start with default <code>per_agent</code> policy and adjust based on metrics</li>
<li>Use <code>hybrid</code> policy when some commands need more time than others</li>
<li>Set <code>cleanup_grace_period_secs</code> to allow proper resource cleanup</li>
<li>Choose <code>timeout_action</code> based on retry strategy:
<ul>
<li><code>dlq</code> for retriable operations</li>
<li><code>skip</code> for optional/best-effort work items</li>
<li><code>fail</code> for critical operations where partial completion is unacceptable</li>
</ul>
</li>
</ul>
<p><strong>Environment Variables for Flexibility:</strong></p>
<ul>
<li>Use environment variables in MapReduce workflows to parameterize timeouts</li>
<li>Define different timeout values for dev/staging/production</li>
<li>Document expected timeout ranges in workflow comments</li>
<li>Example: <code>AGENT_TIMEOUT=300 prodigy run workflow.yml</code> for faster iteration</li>
</ul>
<p><strong>Monitoring and Adjustment:</strong></p>
<ul>
<li>Enable <code>enable_monitoring: true</code> to track timeout patterns</li>
<li>Review timeout events in <code>.prodigy/events/</code> to identify patterns</li>
<li>Adjust timeouts based on actual execution times</li>
<li>Consider using <code>timeout_config.command_timeouts</code> for frequently timing-out commands</li>
</ul>
<h3 id="troubleshooting-10"><a class="header" href="#troubleshooting-10">Troubleshooting</a></h3>
<p><strong>Commands timing out unexpectedly:</strong></p>
<ol>
<li>Check event logs in <code>.prodigy/events/{repo_name}/{job_id}/</code> for timeout events</li>
<li>Verify timeout value is appropriate for the operation</li>
<li>For MapReduce workflows, check if <code>timeout_policy</code> is appropriate</li>
<li>Review <code>cleanup_grace_period_secs</code> if cleanup is slow</li>
</ol>
<p><strong>Items repeatedly sent to DLQ due to timeout:</strong></p>
<ol>
<li>Increase <code>agent_timeout_secs</code> or specific command timeout</li>
<li>Consider changing <code>timeout_action</code> to <code>skip</code> if items aren’t critical</li>
<li>Use <code>hybrid</code> policy with higher timeout for slow commands</li>
<li>Review work item complexity - may need to split items</li>
</ol>
<p><strong>Timeout not being enforced:</strong></p>
<ol>
<li>Verify timeout is set (defaults to no timeout for command-level)</li>
<li>Check that numeric timeout value is positive</li>
<li>For MapReduce, ensure <code>timeout_config</code> or <code>agent_timeout_secs</code> is specified</li>
<li>Review logs to confirm timeout monitoring is enabled</li>
</ol>
<h3 id="see-also-20"><a class="header" href="#see-also-20">See Also</a></h3>
<ul>
<li><a href="advanced/../mapreduce/index.html">MapReduce Documentation</a> - Overview of MapReduce workflows</li>
<li><a href="advanced/../mapreduce/dead-letter-queue-dlq.html">Dead Letter Queue</a> - Handling timed-out items</li>
<li><a href="advanced/../mapreduce/environment-variables-in-configuration.html">Environment Variables</a> - Parameterizing MapReduce workflows</li>
<li><a href="advanced/../mapreduce/performance-tuning.html">Performance Tuning</a> - Optimizing workflow execution times</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="implementation-validation"><a class="header" href="#implementation-validation">Implementation Validation</a></h2>
<p>Validate that implementations meet requirements using the <code>validate</code> field.</p>
<h3 id="basic-validation"><a class="header" href="#basic-validation">Basic Validation</a></h3>
<p>Run validation commands after a step completes:</p>
<pre><code class="language-yaml">- claude: "/implement-feature"
  validate:
    shell: "cargo test"
    threshold: 100  # Require 100% completion (default)
</code></pre>
<p><strong>Note</strong>: The <code>threshold</code> field defaults to <strong>100</strong> if not specified, requiring full implementation completion.</p>
<p><strong>Source</strong>: <a href="advanced/../../../src/cook/workflow/validation.rs">src/cook/workflow/validation.rs:280-282</a></p>
<h3 id="validation-with-claude"><a class="header" href="#validation-with-claude">Validation with Claude</a></h3>
<p>Use Claude to validate implementation quality:</p>
<pre><code class="language-yaml">- shell: "generate-code.sh"
  validate:
    claude: "/verify-implementation"
    threshold: 95
</code></pre>
<h3 id="multi-step-validation"><a class="header" href="#multi-step-validation">Multi-Step Validation</a></h3>
<p>Run multiple validation commands in sequence using the <code>commands</code> array:</p>
<pre><code class="language-yaml">- claude: "/refactor"
  validate:
    commands:
      - shell: "cargo test"
      - shell: "cargo clippy"
      - shell: "cargo fmt --check"
    threshold: 100
</code></pre>
<p><strong>Convenience Array Syntax</strong>: For simple cases, you can use an array format directly:</p>
<pre><code class="language-yaml">- claude: "/refactor"
  validate:
    - shell: "cargo test"
    - shell: "cargo clippy"
    - shell: "cargo fmt --check"
</code></pre>
<h3 id="validation-with-result-files"><a class="header" href="#validation-with-result-files">Validation with Result Files</a></h3>
<p>Read validation results from a file instead of stdout:</p>
<pre><code class="language-yaml">- claude: "/implement-feature"
  validate:
    shell: "run-validator.sh"
    result_file: "validation-results.json"
    threshold: 95
</code></pre>
<p><strong>When to Use result_file:</strong></p>
<p>The <code>result_file</code> option is useful when you need to separate validation output from command logs:</p>
<ul>
<li><strong>Complex JSON Output</strong>: Validation produces structured JSON that shouldn’t be mixed with logs</li>
<li><strong>Separate Concerns</strong>: Keep validation results separate from command stdout/stderr</li>
<li><strong>Additional Logging</strong>: Validation command produces diagnostic output alongside results</li>
<li><strong>Debugging</strong>: Preserve validation output in a file for later inspection</li>
</ul>
<p>The file must contain valid JSON matching the ValidationResult schema. When the validation command completes, Prodigy reads the specified file and parses it as JSON. If the file doesn’t exist or contains invalid JSON, the validation fails.</p>
<p><strong>Source</strong>: <a href="advanced/../../../src/cook/workflow/executor/validation.rs">src/cook/workflow/executor/validation.rs:700-715</a></p>
<h4 id="advanced-result-files-with-commands-array"><a class="header" href="#advanced-result-files-with-commands-array">Advanced: Result Files with Commands Array</a></h4>
<p>You can use <code>result_file</code> with the <code>commands</code> array for multi-step validation where the final result is written to a file:</p>
<pre><code class="language-yaml">- claude: "/implement-spec $ARG"
  validate:
    commands:
      - claude: "/prodigy-validate-spec $ARG --output .prodigy/validation-result.json"
    result_file: ".prodigy/validation-result.json"
    threshold: 100
    on_incomplete:
      claude: "/prodigy-complete-spec $ARG --gaps ${validation.gaps}"
      max_attempts: 5
      commit_required: true
</code></pre>
<p>In this pattern, the validation command writes its results to a JSON file, and Prodigy reads that file after all commands complete.</p>
<p><strong>Source</strong>: Real-world example from <a href="advanced/../../../workflows/implement.yml">workflows/implement.yml:6-16</a></p>
<h3 id="handling-incomplete-implementations"><a class="header" href="#handling-incomplete-implementations">Handling Incomplete Implementations</a></h3>
<p>Automatically remediate when validation fails to meet the threshold.</p>
<p><strong>Convenience Array Syntax</strong> - For simple remediation workflows:</p>
<pre><code class="language-yaml">- claude: "/implement-spec"
  validate:
    shell: "check-completeness.sh"
    threshold: 100
    on_incomplete:
      - claude: "/fill-gaps"
      - shell: "cargo fmt"
</code></pre>
<p><strong>Verbose Configuration</strong> - For complex cases requiring additional control:</p>
<pre><code class="language-yaml">- claude: "/implement-spec"
  validate:
    shell: "check-completeness.sh"
    threshold: 100
    on_incomplete:
      claude: "/fill-gaps"
      max_attempts: 2          # Default: 2 (not 3)
      fail_workflow: true      # Default: true
      commit_required: false   # Default: false
</code></pre>
<p><strong>Default Values</strong>:</p>
<ul>
<li><code>max_attempts</code>: <strong>2</strong> (maximum remediation attempts before giving up)</li>
<li><code>fail_workflow</code>: <strong>true</strong> (workflow fails if remediation doesn’t reach threshold)</li>
<li><code>commit_required</code>: <strong>false</strong> (remediation command doesn’t need to create a commit)</li>
</ul>
<p><strong>Source</strong>: <a href="advanced/../../../src/cook/workflow/validation.rs">src/cook/workflow/validation.rs:284-289</a></p>
<p>The <code>on_incomplete</code> configuration supports:</p>
<ul>
<li><code>claude</code>: Claude command to execute for gap-filling</li>
<li><code>shell</code>: Shell command to execute for gap-filling</li>
<li><code>commands</code>: Array of commands to execute in sequence</li>
<li><code>max_attempts</code>: Maximum remediation attempts (default: <strong>2</strong>)</li>
<li><code>fail_workflow</code>: Whether to fail workflow if remediation fails (default: <strong>true</strong>)</li>
<li><code>commit_required</code>: Whether remediation command should create a commit (default: <strong>false</strong>)</li>
</ul>
<p><strong>Source</strong>: <a href="advanced/../../../src/cook/workflow/validation.rs">src/cook/workflow/validation.rs:123-152</a></p>
<h3 id="timeout-configuration-1"><a class="header" href="#timeout-configuration-1">Timeout Configuration</a></h3>
<p>Set a timeout for validation commands to prevent hanging:</p>
<pre><code class="language-yaml">- claude: "/implement-feature"
  validate:
    shell: "long-running-test.sh"
    threshold: 100
    timeout: 300  # 5 minutes timeout
</code></pre>
<p>The <code>timeout</code> field specifies the maximum number of seconds the validation command can run. If the command exceeds this time, it’s terminated and the validation fails.</p>
<p><strong>Source</strong>: <a href="advanced/../../../src/cook/workflow/validation.rs">src/cook/workflow/validation.rs:37-39</a></p>
<h3 id="validationresult-schema"><a class="header" href="#validationresult-schema">ValidationResult Schema</a></h3>
<p>When using <code>result_file</code>, the JSON file must match this structure:</p>
<pre><code class="language-json">{
  "completion_percentage": 95.5,
  "status": "incomplete",
  "implemented": [
    "Feature A is fully implemented",
    "Feature B includes unit tests"
  ],
  "missing": [
    "Feature C lacks error handling",
    "Feature D needs integration tests"
  ],
  "gaps": {
    "error_handling": {
      "description": "Missing error handling in parser",
      "location": "src/parser.rs:45",
      "severity": "high",
      "suggested_fix": "Add Result&lt;T, E&gt; return type and handle parse errors"
    }
  }
}
</code></pre>
<p><strong>Fields</strong>:</p>
<ul>
<li><code>completion_percentage</code>: Float (0-100) indicating implementation completeness</li>
<li><code>status</code>: Enum - <code>"complete"</code>, <code>"incomplete"</code>, <code>"failed"</code>, or <code>"skipped"</code></li>
<li><code>implemented</code>: Array of strings describing completed requirements</li>
<li><code>missing</code>: Array of strings describing incomplete requirements</li>
<li><code>gaps</code>: Object mapping gap IDs to GapDetail objects with description, location, severity, and suggested_fix</li>
</ul>
<p><strong>Source</strong>: <a href="advanced/../../../src/cook/workflow/validation.rs">src/cook/workflow/validation.rs:216-239</a></p>
<h3 id="validation-patterns"><a class="header" href="#validation-patterns">Validation Patterns</a></h3>
<p><strong>Progressive Validation</strong> - Validate in stages:</p>
<pre><code class="language-yaml">- claude: "/implement-feature"
  validate:
    commands:
      - shell: "cargo check"     # Fast syntax check first
      - shell: "cargo test"      # Then run tests
      - shell: "cargo bench"     # Finally benchmarks
    threshold: 100
    timeout: 600  # 10 minute timeout for all commands
    on_incomplete:
      - claude: "/analyze-failures"
      - claude: "/fix-issues"
</code></pre>
<p><strong>Conditional Validation</strong> - Validate based on previous results:</p>
<pre><code class="language-yaml">- claude: "/optimize-code"
  id: "optimization"
  validate:
    shell: "benchmark.sh"
    threshold: 90

- shell: "verify-performance.sh"
  when: "${optimization.success}"
  validate:
    shell: "stress-test.sh"
    threshold: 100
</code></pre>
<p><strong>Complex Multi-Step Validation with Result Files</strong> - Real-world pattern from Prodigy’s debtmap workflow:</p>
<pre><code class="language-yaml">- claude: "/implement-changes"
  commit_required: true
  validate:
    commands:
      - shell: "just coverage-lcov"
      - shell: "debtmap analyze . --lcov target/coverage/lcov.info --output .prodigy/debtmap-after.json"
      - shell: "debtmap compare --before .prodigy/debtmap-before.json --after .prodigy/debtmap-after.json --output .prodigy/comparison.json"
      - claude: "/validate-improvement --comparison .prodigy/comparison.json --output .prodigy/validation.json"
    result_file: ".prodigy/validation.json"
    threshold: 75
    on_incomplete:
      commands:
        - claude: "/fix-remaining-gaps --validation .prodigy/validation.json"
          commit_required: true
        - shell: "just coverage-lcov"
        - shell: "debtmap analyze . --lcov target/coverage/lcov.info --output .prodigy/debtmap-after.json"
      max_attempts: 5
      fail_workflow: true
</code></pre>
<p><strong>Source</strong>: <a href="advanced/../../../workflows/debtmap.yml">workflows/debtmap.yml:26-42</a></p>
<p>This pattern demonstrates:</p>
<ul>
<li>Multiple validation commands executed in sequence</li>
<li>Reading results from a file after all commands complete</li>
<li>Multi-command remediation with commit requirements</li>
<li>Iterative validation and fixing</li>
</ul>
<h3 id="configuration-reference"><a class="header" href="#configuration-reference">Configuration Reference</a></h3>
<p>Complete list of validation configuration fields:</p>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr></thead><tbody>
<tr><td><code>shell</code></td><td>String</td><td>None</td><td>Shell command to run for validation</td></tr>
<tr><td><code>claude</code></td><td>String</td><td>None</td><td>Claude command to run for validation</td></tr>
<tr><td><code>commands</code></td><td>Array</td><td>None</td><td>Array of commands for multi-step validation</td></tr>
<tr><td><code>threshold</code></td><td>Number</td><td><strong>100</strong></td><td>Completion percentage required (0-100)</td></tr>
<tr><td><code>timeout</code></td><td>Number</td><td>None</td><td>Timeout in seconds for validation commands</td></tr>
<tr><td><code>result_file</code></td><td>String</td><td>None</td><td>File path to read validation results from</td></tr>
<tr><td><code>on_incomplete</code></td><td>Object</td><td>None</td><td>Configuration for handling validation failures</td></tr>
</tbody></table>
</div>
<p><strong>Source</strong>: <a href="advanced/../../../src/cook/workflow/validation.rs">src/cook/workflow/validation.rs:11-49</a></p>
<h3 id="best-practices-14"><a class="header" href="#best-practices-14">Best Practices</a></h3>
<ol>
<li><strong>Use result_file for structured output</strong>: When validation produces JSON results, write them to a file to keep logs clean</li>
<li><strong>Set appropriate timeouts</strong>: Long-running tests should have explicit timeouts to prevent hanging</li>
<li><strong>Start with high thresholds</strong>: Use threshold: 100 for critical workflows, lower for exploratory work</li>
<li><strong>Limit remediation attempts</strong>: Set <code>max_attempts: 2-5</code> to balance thoroughness with workflow time</li>
<li><strong>Require commits for fixes</strong>: Use <code>commit_required: true</code> in <code>on_incomplete</code> to verify fixes were actually made</li>
<li><strong>Progressive validation</strong>: Run fast checks first (syntax), then slower ones (tests, benchmarks)</li>
</ol>
<h3 id="troubleshooting-11"><a class="header" href="#troubleshooting-11">Troubleshooting</a></h3>
<p><strong>Validation always fails with 0% completion</strong>:</p>
<ul>
<li>Check that your validation command outputs valid JSON to stdout or <code>result_file</code></li>
<li>Verify the JSON structure matches the ValidationResult schema</li>
<li>Use <code>result_file</code> to debug what your validation command is producing</li>
</ul>
<p><strong>Validation times out</strong>:</p>
<ul>
<li>Increase the <code>timeout</code> value for long-running tests</li>
<li>Consider splitting validation into multiple steps with separate timeouts</li>
</ul>
<p><strong>on_incomplete doesn’t fix issues</strong>:</p>
<ul>
<li>Verify <code>max_attempts</code> is set high enough (default is 2)</li>
<li>Check that the remediation command has access to <code>${validation.gaps}</code> for context</li>
<li>Use <code>commit_required: true</code> to ensure fixes are actually committed</li>
</ul>
<p><strong>For more information on error handling</strong>, see <a href="advanced/../error-handling.html">Error Handling</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="parallel-iteration-with-foreach"><a class="header" href="#parallel-iteration-with-foreach">Parallel Iteration with Foreach</a></h2>
<p>Process multiple items in parallel using the <code>foreach</code> command.</p>
<p><strong>Source</strong>: Configuration defined in src/config/command.rs:191-211</p>
<h3 id="basic-foreach"><a class="header" href="#basic-foreach">Basic Foreach</a></h3>
<p>The <code>foreach</code> command uses a nested object structure where the <code>foreach</code> field specifies the input source:</p>
<pre><code class="language-yaml">- foreach:
    foreach: ["a", "b", "c"]
    do:
      - shell: "process ${item}"
</code></pre>
<p><strong>Input Formats</strong> (src/config/command.rs:193-194):</p>
<ul>
<li><strong>List</strong>: Static array of items: <code>["item1", "item2"]</code></li>
<li><strong>Command</strong>: Shell command that outputs items: <code>"find . -name '*.rs'"</code></li>
</ul>
<p><strong>Source</strong>: Example from src/cook/execution/foreach_tests.rs:16-21</p>
<h3 id="dynamic-item-lists"><a class="header" href="#dynamic-item-lists">Dynamic Item Lists</a></h3>
<p>Generate items dynamically from command output. Commands are executed via shell, and output is parsed line-by-line:</p>
<pre><code class="language-yaml">- foreach:
    foreach: "find . -name '*.rs'"
    do:
      - shell: "rustfmt ${item}"
</code></pre>
<p><strong>Command Output Parsing</strong> (src/cook/execution/foreach.rs:201-206):</p>
<ul>
<li>Output is split into lines</li>
<li>Empty lines are automatically filtered out</li>
<li>Each non-empty line becomes one item</li>
</ul>
<p><strong>Source</strong>: Example from src/cook/execution/foreach_tests.rs:59-64</p>
<h3 id="variables-available-in-foreach"><a class="header" href="#variables-available-in-foreach">Variables Available in Foreach</a></h3>
<p>The following variables are automatically available in <code>do</code> block commands (src/cook/execution/foreach.rs:223-225):</p>
<ul>
<li><code>${item}</code> - Current item value</li>
<li><code>${index}</code> - Zero-based index of current item (0, 1, 2, …)</li>
<li><code>${total}</code> - Total number of items being processed</li>
</ul>
<p><strong>Example with Variables:</strong></p>
<pre><code class="language-yaml">- foreach:
    foreach: ["test1.rs", "test2.rs", "test3.rs"]
    do:
      - shell: "echo Processing ${item} (${index}/${total})"
</code></pre>
<p>This would output:</p>
<pre><code>Processing test1.rs (0/3)
Processing test2.rs (1/3)
Processing test3.rs (2/3)
</code></pre>
<h3 id="parallel-execution"><a class="header" href="#parallel-execution">Parallel Execution</a></h3>
<p>Control parallelism with the <code>parallel</code> field (src/config/command.rs:196-198). It accepts both boolean and numeric values:</p>
<p><strong>Boolean - Default Parallelism:</strong></p>
<pre><code class="language-yaml">- foreach:
    foreach: "ls *.txt"
    parallel: true  # Default: 10 concurrent workers
    do:
      - shell: "analyze ${item}"
</code></pre>
<p><strong>Important</strong>: <code>parallel: true</code> uses a fixed default of 10 concurrent workers, not “all available cores” (src/cook/execution/foreach.rs:81).</p>
<p><strong>Number - Explicit Concurrency Limit:</strong></p>
<pre><code class="language-yaml">- foreach:
    foreach: "ls *.txt"
    parallel: 5  # Process 5 items concurrently
    do:
      - shell: "analyze ${item}"
</code></pre>
<p><strong>Source</strong>: Example from src/cook/execution/foreach_tests.rs:102-114</p>
<p><strong>Choosing Parallelism:</strong></p>
<ul>
<li>Use <code>parallel: false</code> (default) for sequential processing</li>
<li>Use <code>parallel: true</code> for moderate parallelism (10 workers)</li>
<li>Use <code>parallel: N</code> to specify exact concurrency level</li>
<li>Consider I/O limits, CPU cores, and rate limits when choosing N</li>
</ul>
<h3 id="error-handling-2"><a class="header" href="#error-handling-2">Error Handling</a></h3>
<p>Continue processing remaining items on failure:</p>
<pre><code class="language-yaml">- foreach:
    foreach: ["test1", "test2", "test3"]
    continue_on_error: true
    do:
      - shell: "run-test ${item}"
</code></pre>
<p><strong>Behavior</strong> (src/config/command.rs:205-206):</p>
<ul>
<li><code>continue_on_error: true</code> - Process all items even if some fail</li>
<li><code>continue_on_error: false</code> (default) - Stop on first failure</li>
<li>Failed items are tracked and reported in results</li>
</ul>
<h3 id="limiting-items"><a class="header" href="#limiting-items">Limiting Items</a></h3>
<p>Process only a subset of items:</p>
<pre><code class="language-yaml">- foreach:
    foreach: "find . -name '*.log'"
    max_items: 10  # Process first 10 items only
    do:
      - shell: "compress ${item}"
</code></pre>
<p><strong>Source</strong>: Field definition in src/config/command.rs:209-210</p>
<p>Useful for:</p>
<ul>
<li>Testing workflows on a small subset</li>
<li>Rate-limiting batch operations</li>
<li>Processing most recent items only</li>
</ul>
<h3 id="nested-commands"><a class="header" href="#nested-commands">Nested Commands</a></h3>
<p>Each item can execute multiple commands. Both <code>shell</code> and <code>claude</code> commands are supported in <code>do</code> blocks:</p>
<pre><code class="language-yaml">- foreach:
    foreach: "cargo metadata --format-version 1 | jq -r '.packages[].name'"
    do:
      - shell: "cargo build -p ${item}"
      - shell: "cargo test -p ${item}"
      - shell: "cargo doc -p ${item}"
</code></pre>
<p><strong>Command Types Supported</strong> (src/cook/execution/foreach.rs:286-375):</p>
<ul>
<li><code>shell</code> - Execute shell commands</li>
<li><code>claude</code> - Execute Claude commands</li>
</ul>
<p>Each command in the <code>do</code> block has access to the same variables (<code>${item}</code>, <code>${index}</code>, <code>${total}</code>).</p>
<h3 id="practical-use-cases"><a class="header" href="#practical-use-cases">Practical Use Cases</a></h3>
<p><strong>Process Multiple Directories:</strong></p>
<pre><code class="language-yaml">- foreach:
    foreach: ["frontend", "backend", "shared"]
    parallel: 3
    do:
      - shell: "cd ${item} &amp;&amp; npm install"
      - shell: "cd ${item} &amp;&amp; npm test"
</code></pre>
<p><strong>Batch File Processing:</strong></p>
<pre><code class="language-yaml">- foreach:
    foreach: "find src -name '*.rs'"
    parallel: 10
    continue_on_error: true
    do:
      - shell: "rustfmt ${item}"
      - shell: "cargo clippy --manifest-path=${item}"
</code></pre>
<h3 id="when-to-use-foreach-vs-mapreduce"><a class="header" href="#when-to-use-foreach-vs-mapreduce">When to Use Foreach vs MapReduce</a></h3>
<p><strong>Use Foreach when:</strong></p>
<ul>
<li>Simple iteration over items (&lt; 100 items)</li>
<li>All items processed in the same worktree</li>
<li>No need for checkpoint/resume</li>
<li>Lightweight operations</li>
</ul>
<p><strong>Use MapReduce when:</strong></p>
<ul>
<li>Processing many items (100+)</li>
<li>Need checkpoint and resume capability</li>
<li>Need isolated worktrees per item</li>
<li>Complex failure handling and retry logic</li>
<li>Dead letter queue for failed items</li>
</ul>
<p>See <a href="advanced/../mapreduce/index.html">MapReduce</a> for large-scale parallel processing.</p>
<h3 id="progress-tracking"><a class="header" href="#progress-tracking">Progress Tracking</a></h3>
<p>During execution, a progress bar displays (src/cook/execution/foreach.rs:88):</p>
<ul>
<li>Current item number</li>
<li>Total items</li>
<li>Processing rate</li>
<li>Estimated time remaining</li>
</ul>
<h3 id="see-also-21"><a class="header" href="#see-also-21">See Also</a></h3>
<ul>
<li><a href="advanced/../workflow-basics/command-types.html">Command Types</a> - All available command types</li>
<li><a href="advanced/../variables/index.html">Variables</a> - Variable interpolation system</li>
<li><a href="advanced/../mapreduce/index.html">MapReduce</a> - Large-scale parallel processing</li>
<li><a href="advanced/../environment/index.html">Environment Variables</a> - Using environment variables in workflows</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="goal-seeking-operations"><a class="header" href="#goal-seeking-operations">Goal-Seeking Operations</a></h2>
<p>Iteratively refine implementations until they meet validation criteria. Goal-seeking addresses the fundamental challenge that AI often fails on first attempts but succeeds with validation feedback and retry mechanisms.</p>
<h3 id="basic-goal-seek"><a class="header" href="#basic-goal-seek">Basic Goal Seek</a></h3>
<p>Define a goal and validation command using either <code>shell</code> or <code>claude</code>:</p>
<p><strong>Using Shell Command:</strong></p>
<pre><code class="language-yaml">- goal_seek:
    goal: "All tests pass"
    shell: "cargo fix"
    validate: "cargo test 2&gt;&amp;1 | grep -q 'test result: ok' &amp;&amp; echo 'score: 100' || echo 'score: 0'"
    threshold: 100
</code></pre>
<p><strong>Using Claude Command:</strong></p>
<pre><code class="language-yaml">- goal_seek:
    goal: "Code quality improved"
    claude: "/fix-issues"
    validate: "quality-check.sh"  # Must output score in parseable format
    threshold: 95
</code></pre>
<p><strong>Important</strong>: Validation commands must output a score in one of these formats:</p>
<ul>
<li><code>score: 85</code> (simple text)</li>
<li><code>85%</code> (percentage)</li>
<li><code>85/100</code> (ratio)</li>
<li><code>85 out of 100</code> (natural language)</li>
<li><code>{"score": 85, "gaps": ["list of issues"]}</code> (JSON with optional gaps)</li>
</ul>
<p><strong>Source</strong>: Score extraction patterns defined in src/cook/goal_seek/validator.rs:37-62</p>
<p>The goal-seeking operation will:</p>
<ol>
<li>Run the command (shell or claude)</li>
<li>Run the validation and extract numeric score (0-100)</li>
<li>Pass validation context to next attempt via environment variables</li>
<li>Retry if validation threshold not met</li>
<li>Stop when goal achieved, max attempts reached, or convergence detected</li>
</ol>
<h3 id="advanced-goal-seek-configuration"><a class="header" href="#advanced-goal-seek-configuration">Advanced Goal Seek Configuration</a></h3>
<p>Control iteration behavior and convergence detection:</p>
<pre><code class="language-yaml">- goal_seek:
    goal: "Code passes all quality checks"
    shell: "auto-fix.sh"
    validate: "quality-check.sh"
    threshold: 95
    max_attempts: 5
    timeout_seconds: 300
    fail_on_incomplete: true
</code></pre>
<p><strong>Configuration Fields</strong> (from src/cook/goal_seek/mod.rs:13-27):</p>
<ul>
<li><code>goal</code>: Description of what you’re trying to achieve</li>
<li><code>shell</code> or <code>claude</code>: The command to execute (use one or the other)</li>
<li><code>validate</code>: Shell command to validate progress (must output score 0-100)</li>
<li><code>threshold</code>: Minimum score to consider goal achieved (0-100)</li>
<li><code>max_attempts</code>: Maximum number of iterations (default: 3)</li>
<li><code>timeout_seconds</code>: Optional timeout for the entire goal-seeking operation</li>
<li><code>fail_on_incomplete</code>: Fail workflow if goal not achieved (default: true)</li>
</ul>
<h3 id="automatic-convergence-detection"><a class="header" href="#automatic-convergence-detection">Automatic Convergence Detection</a></h3>
<p>Goal-seeking automatically detects when no progress is being made and stops early to prevent wasted iterations.</p>
<p><strong>Convergence Criteria</strong> (src/cook/goal_seek/engine.rs:179-197):</p>
<ul>
<li>Triggers after 3+ attempts</li>
<li>Last 3 scores are within 2 points of each other</li>
<li>Returns <code>Converged</code> result with reason</li>
</ul>
<p><strong>Example</strong>: If attempts produce scores [82, 83, 82], the system detects convergence and stops instead of continuing to max_attempts.</p>
<p><strong>Benefits</strong>:</p>
<ul>
<li>Saves time and computational resources</li>
<li>Prevents infinite loops when stuck</li>
<li>Provides clear feedback about why iteration stopped</li>
</ul>
<pre><code class="language-yaml"># Example: Convergence will stop early if no improvement
- goal_seek:
    goal: "Improve test coverage"
    claude: "/add-tests"
    validate: "coverage-check.sh"  # Returns score: 0-100
    threshold: 90
    max_attempts: 10  # May stop earlier if converged
</code></pre>
<h3 id="validation-integration"><a class="header" href="#validation-integration">Validation Integration</a></h3>
<p>Goal-seeking integrates with the validation system through flexible score extraction.</p>
<p><strong>Validation Output Formats</strong> (src/cook/goal_seek/validator.rs:65-96):</p>
<p><strong>1. JSON Format with Score:</strong></p>
<pre><code class="language-json">{"score": 85}
</code></pre>
<p><strong>2. JSON Format with Score and Gaps:</strong></p>
<pre><code class="language-json">{
  "score": 75,
  "gaps": [
    "Missing test for user authentication",
    "No error handling tests",
    "Coverage below 80% in auth module"
  ]
}
</code></pre>
<p><strong>3. Simple Text Formats:</strong></p>
<ul>
<li><code>score: 85</code></li>
<li><code>85%</code></li>
<li><code>85/100</code></li>
<li><code>85 out of 100</code></li>
</ul>
<p><strong>Example Validation Command:</strong></p>
<pre><code class="language-yaml">- goal_seek:
    goal: "100% test coverage"
    claude: "/add-tests"
    validate: |
      # Extract coverage percentage and format as score
      cargo tarpaulin --print-summary 2&gt;/dev/null | \
        grep 'Coverage' | \
        sed 's/.*Coverage=\([0-9]*\).*/score: \1/'
    threshold: 100
    timeout_seconds: 600
    max_attempts: 10
</code></pre>
<p><strong>Real-World Example</strong> (from workflows/goal-seeking-examples.yml:6-14):</p>
<pre><code class="language-yaml">- goal_seek:
    goal: "Achieve 90% test coverage"
    claude: "/prodigy-coverage --improve"
    validate: "cargo tarpaulin --print-summary 2&gt;/dev/null | grep 'Coverage' | sed 's/.*Coverage=\\([0-9]*\\).*/score: \\1/'"
    threshold: 90
    max_attempts: 5
    timeout_seconds: 300
    fail_on_incomplete: true
  commit_required: true
</code></pre>
<p><strong>Validation Context Environment Variables</strong> (src/cook/goal_seek/engine.rs:128-153):</p>
<p>After the first attempt, refinement commands receive validation feedback via environment variables:</p>
<ul>
<li><code>PRODIGY_VALIDATION_SCORE</code>: Previous attempt’s numeric score (0-100)</li>
<li><code>PRODIGY_VALIDATION_OUTPUT</code>: Full text output from validation command</li>
<li><code>PRODIGY_VALIDATION_GAPS</code>: Parsed JSON gaps array (if validation returned JSON with gaps field)</li>
</ul>
<p>This allows Claude or shell scripts to understand what failed and make targeted improvements.</p>
<h3 id="progressive-refinement"><a class="header" href="#progressive-refinement">Progressive Refinement</a></h3>
<p>Use goal-seeking for incremental improvements through multi-stage workflows.</p>
<p><strong>Multi-Stage Example</strong> (from workflows/goal-seeking-examples.yml:99-129):</p>
<pre><code class="language-yaml"># Stage 1: Implement feature
- name: "Complete feature implementation"
  goal_seek:
    goal: "Implement user profile feature"
    claude: "/implement-feature user-profile"
    validate: "test -f src/features/user_profile.rs &amp;&amp; echo 'score: 100' || echo 'score: 0'"
    threshold: 100
    max_attempts: 2

# Stage 2: Add comprehensive tests
- name: "Add tests for user profile"
  goal_seek:
    goal: "Add tests for user profile feature"
    claude: "/add-tests src/features/user_profile.rs"
    validate: |
      test_count=$(grep -c "#\[test\]" src/features/user_profile.rs || echo 0)
      if [ "$test_count" -ge 5 ]; then
        echo "score: 100"
      else
        score=$((test_count * 20))
        echo "score: $score"
      fi
    threshold: 100
    max_attempts: 3

# Stage 3: Ensure tests pass
- name: "Make all tests pass"
  goal_seek:
    goal: "Make all user profile tests pass"
    claude: "/fix-tests user_profile"
    validate: "cargo test user_profile 2&gt;&amp;1 | grep -q 'test result: ok' &amp;&amp; echo 'score: 100' || echo 'score: 0'"
    threshold: 100
    max_attempts: 4
    fail_on_incomplete: true
</code></pre>
<p><strong>Progressive Quality Improvement:</strong></p>
<pre><code class="language-yaml"># First pass: Get to 80% quality
- goal_seek:
    goal: "Basic quality standards met"
    shell: "quick-fix.sh"
    validate: "quality-check.sh"  # Outputs score: 0-100
    threshold: 80
    max_attempts: 3

# Second pass: Polish to 100%
- goal_seek:
    goal: "Perfect quality"
    claude: "/polish-code"
    validate: "quality-check.sh"  # Same validator, higher threshold
    threshold: 100
    max_attempts: 5
    timeout_seconds: 600
</code></pre>
<p><strong>Benefits of Progressive Refinement:</strong></p>
<ul>
<li>Each stage has clear, achievable goals</li>
<li>Earlier stages provide foundation for later ones</li>
<li>Context from previous attempts helps Claude make targeted improvements</li>
<li>Convergence detection prevents wasted effort at each stage</li>
</ul>
<h3 id="error-handling-3"><a class="header" href="#error-handling-3">Error Handling</a></h3>
<p>Goal-seeking operations can terminate in several ways (src/cook/goal_seek/mod.rs:44-76):</p>
<p><strong>Result Types:</strong></p>
<ul>
<li><code>Success</code>: Threshold reached within max_attempts</li>
<li><code>MaxAttemptsReached</code>: Exhausted retries without reaching threshold</li>
<li><code>Timeout</code>: Time limit exceeded</li>
<li><code>Converged</code>: No improvement detected in last 3 attempts</li>
<li><code>Failed</code>: Command execution error</li>
</ul>
<p><strong>Handling Incomplete Goals:</strong></p>
<pre><code class="language-yaml">- goal_seek:
    goal: "All tests pass"
    shell: "cargo fix"
    validate: "cargo test 2&gt;&amp;1 | grep -q 'test result: ok' &amp;&amp; echo 'score: 100' || echo 'score: 0'"
    threshold: 100
    max_attempts: 3
    fail_on_incomplete: false  # Continue workflow even if goal not achieved

# Next step proceeds regardless of goal-seek result
- shell: "echo 'Continuing workflow despite incomplete goal'"
</code></pre>
<p><strong>Using Goal-Seeking in on_failure Handlers</strong> (from workflows/goal-seeking-examples.yml:76-95):</p>
<pre><code class="language-yaml">- shell: "cargo test"
  on_failure:
    goal_seek:
      goal: "Fix all failing tests"
      claude: "/debug-test-failures"
      validate: |
        cargo test 2&gt;&amp;1 | grep -q "test result: ok" &amp;&amp; echo "score: 100" || {
          passed=$(cargo test 2&gt;&amp;1 | grep -oP '\d+(?= passed)' | head -1)
          failed=$(cargo test 2&gt;&amp;1 | grep -oP '\d+(?= failed)' | head -1)
          total=$((passed + failed))
          if [ "$total" -gt 0 ]; then
            score=$((passed * 100 / total))
            echo "score: $score"
          else
            echo "score: 0"
          fi
        }
      threshold: 100
      max_attempts: 3
      fail_on_incomplete: true
</code></pre>
<p><strong>Note</strong>: Goal-seeking operations return <code>GoalSeekResult</code> variants, not shell exit codes. The workflow executor converts these to step results based on <code>fail_on_incomplete</code> configuration (src/cook/workflow/executor/commands.rs).</p>
<h3 id="best-practices-15"><a class="header" href="#best-practices-15">Best Practices</a></h3>
<p><strong>Set Realistic Thresholds:</strong></p>
<ul>
<li>Start with achievable thresholds (80-90%)</li>
<li>Increase gradually in multiple goal-seeking steps</li>
<li>Consider diminishing returns on higher thresholds</li>
<li>Remember: convergence detection will stop early if no progress</li>
</ul>
<p><strong>Provide Good Validation:</strong></p>
<ul>
<li>Validation should be fast and deterministic</li>
<li>Always output a numeric score (0-100) in a parseable format</li>
<li>Use JSON with gaps field for rich feedback to refinement commands</li>
<li>Test validation commands independently before using in goal-seeking</li>
</ul>
<p><strong>Write Effective Validation Commands:</strong></p>
<pre><code class="language-yaml"># Good: Explicit score output
validate: "cargo test 2&gt;&amp;1 | grep -q 'ok' &amp;&amp; echo 'score: 100' || echo 'score: 0'"

# Better: Proportional score based on actual results
validate: |
  passed=$(cargo test 2&gt;&amp;1 | grep -oP '\d+(?= passed)')
  total=$(cargo test 2&gt;&amp;1 | grep -oP '\d+ tests')
  score=$((passed * 100 / total))
  echo "score: $score"

# Best: JSON with actionable feedback
validate: |
  result=$(cargo test --format json)
  passed=$(echo "$result" | jq '.passed')
  total=$(echo "$result" | jq '.total')
  score=$((passed * 100 / total))
  gaps=$(echo "$result" | jq '.failures')
  echo "{\"score\": $score, \"gaps\": $gaps}"
</code></pre>
<p><strong>Limit Iterations:</strong></p>
<ul>
<li>Set reasonable max_attempts (3-10)</li>
<li>Use timeout_seconds for long-running operations (per-operation timeout)</li>
<li>Consider fail_on_incomplete based on criticality</li>
<li>Trust convergence detection to prevent wasted attempts</li>
</ul>
<p><strong>Leverage Validation Context:</strong></p>
<ul>
<li>Claude commands can access previous scores via <code>PRODIGY_VALIDATION_SCORE</code></li>
<li>Use gaps information (<code>PRODIGY_VALIDATION_GAPS</code>) for targeted fixes</li>
<li>Design validation to provide actionable feedback, not just scores</li>
</ul>
<p><strong>Troubleshooting Common Issues:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>Issue</th><th>Cause</th><th>Solution</th></tr></thead><tbody>
<tr><td>Validation always returns 0</td><td>Score not in parseable format</td><td>Test validation command, ensure it outputs <code>score: N</code> format</td></tr>
<tr><td>Convergence happens too early</td><td>Score variance within 2 points</td><td>Ensure validation is precise enough to detect small improvements</td></tr>
<tr><td>Timeout vs MaxAttemptsReached</td><td>timeout_seconds too short</td><td>Increase timeout or reduce max_attempts</td></tr>
<tr><td>Context not available to Claude</td><td>Environment variables not passed</td><td>Verify command executor supports env vars (src/cook/goal_seek/engine.rs:128-153)</td></tr>
</tbody></table>
</div>
<p><strong>See Also:</strong></p>
<ul>
<li><a href="advanced/implementation-validation.html">Implementation Validation</a> - Using goal-seeking for spec validation</li>
<li><a href="advanced/../error-handling.html">Error Handling</a> - Workflow-level error handling</li>
<li>Technical documentation: docs/goal-seeking.md in repository</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="best-practices-16"><a class="header" href="#best-practices-16">Best Practices</a></h2>
<h3 id="1-use-meaningful-variable-names"><a class="header" href="#1-use-meaningful-variable-names">1. Use Meaningful Variable Names</a></h3>
<p>Choose descriptive names for captured outputs to make workflows self-documenting:</p>
<pre><code class="language-yaml"># Good - Clear and descriptive
- shell: "cargo test --format json"
  capture_output: "test_results"
  capture_format: "json"

# Avoid - Cryptic and unclear
- shell: "cargo test --format json"
  capture_output: "x"
</code></pre>
<h3 id="2-set-appropriate-timeouts"><a class="header" href="#2-set-appropriate-timeouts">2. Set Appropriate Timeouts</a></h3>
<p>Protect workflows from hanging on long-running operations:</p>
<pre><code class="language-yaml"># Set timeouts for potentially long-running operations
- shell: "npm install"
  timeout: 300  # 5 minutes

- claude: "/analyze-large-codebase"
  timeout: 1800  # 30 minutes
</code></pre>
<p><strong>Guidelines:</strong></p>
<ul>
<li>Set timeouts high enough for normal completion</li>
<li>Consider worst-case scenarios (slow CI, cold caches)</li>
<li>Use environment variables for configurable timeouts</li>
</ul>
<h3 id="3-handle-failures-gracefully"><a class="header" href="#3-handle-failures-gracefully">3. Handle Failures Gracefully</a></h3>
<p>Provide automatic remediation for common failure scenarios using <code>on_failure</code> handlers:</p>
<pre><code class="language-yaml"># Provide automatic remediation
- shell: "cargo test"
  on_failure:
    claude: "/fix-failing-tests"
    max_retries: 2
    fail_workflow: false
</code></pre>
<p><strong>Source</strong>: <code>on_failure</code> configuration from src/cook/workflow/on_failure.rs:68-115</p>
<p><strong>Available Fields:</strong></p>
<ul>
<li><code>claude</code> or <code>shell</code>: Command to execute on failure</li>
<li><code>max_retries</code> (alias: <code>max_attempts</code>): Number of retry attempts</li>
<li><code>fail_workflow</code>: Whether to fail the workflow after handler execution (default: false)</li>
<li><code>retry_original</code>: Whether to retry the original command after handler</li>
</ul>
<p><strong>Consider:</strong></p>
<ul>
<li>Whether to fail the workflow or continue</li>
<li>How many retry attempts are reasonable</li>
<li>Use <a href="advanced/../retry-configuration/index.html">retry configuration</a> for advanced backoff strategies</li>
<li>See <a href="advanced/../error-handling.html">error handling</a> for comprehensive error management</li>
</ul>
<h3 id="4-validate-critical-changes-with-automatic-gap-filling"><a class="header" href="#4-validate-critical-changes-with-automatic-gap-filling">4. Validate Critical Changes with Automatic Gap Filling</a></h3>
<p>Ensure implementations meet requirements before proceeding, with automatic remediation for incomplete implementations:</p>
<pre><code class="language-yaml"># Ensure implementation meets requirements
- claude: "/implement-feature"
  validate:
    commands:
      - shell: "cargo test"
      - shell: "cargo clippy -- -D warnings"
    threshold: 100
    on_incomplete:
      claude: "/fix-issues"
      max_attempts: 3
      fail_workflow: true
      commit_required: true
</code></pre>
<p><strong>Source</strong>: Validation with <code>on_incomplete</code> from src/cook/workflow/validation.rs:122-152</p>
<p><strong>Key Features:</strong></p>
<ul>
<li><strong><code>threshold</code></strong>: Percentage (0-100) of validation criteria that must pass</li>
<li><strong><code>on_incomplete</code></strong>: Handler executed when validation score &lt; threshold</li>
<li><strong><code>max_attempts</code></strong>: Maximum retry attempts for gap filling (default: 2)</li>
<li><strong><code>fail_workflow</code></strong>: Whether to fail after max attempts (default: true)</li>
<li><strong><code>commit_required</code></strong>: Whether handler must create git commits</li>
</ul>
<p><strong>Multi-Command Recovery:</strong></p>
<pre><code class="language-yaml">validate:
  commands:
    - shell: "cargo test"
    - shell: "debtmap analyze . --output after.json"
  threshold: 100
  on_incomplete:
    commands:
      - claude: "/fix-test-failures"
        commit_required: true
      - shell: "cargo fmt"
      - shell: "cargo clippy --fix --allow-dirty"
    max_attempts: 5
    fail_workflow: false
</code></pre>
<p><strong>Best Practices:</strong></p>
<ul>
<li>Validate after significant changes</li>
<li>Use multiple validation steps for comprehensive coverage</li>
<li>Set appropriate thresholds based on criticality (100 for critical paths, 80-90 for development)</li>
<li>Use <code>on_incomplete</code> for automatic remediation instead of manual intervention</li>
<li>Enable <code>commit_required: true</code> when fixes should be preserved</li>
<li>See <a href="advanced/implementation-validation.html">implementation validation</a> for detailed examples</li>
</ul>
<h3 id="5-use-step-ids-for-complex-workflows"><a class="header" href="#5-use-step-ids-for-complex-workflows">5. Use Step IDs for Complex Workflows</a></h3>
<p>Make output references explicit in complex workflows:</p>
<pre><code class="language-yaml"># Make output references explicit
- shell: "git diff --stat"
  id: "git-changes"
  capture_output: "diff"

- claude: "/review-changes '${git-changes.output}'"
  id: "code-review"
</code></pre>
<p><strong>When to Use:</strong></p>
<ul>
<li>Multiple steps producing similar outputs</li>
<li>Complex conditional logic based on specific steps</li>
<li>Debugging specific step outputs</li>
<li>Combining step metadata (exit_code, success, duration)</li>
</ul>
<h3 id="6-leverage-parallel-execution"><a class="header" href="#6-leverage-parallel-execution">6. Leverage Parallel Execution</a></h3>
<p>Use <code>foreach</code> with parallelism for independent operations:</p>
<pre><code class="language-yaml">- foreach:
    foreach: ["frontend", "backend", "shared"]
    parallel: 3
    do:
      - shell: "cd ${item} &amp;&amp; npm install"
      - shell: "cd ${item} &amp;&amp; npm test"
</code></pre>
<p><strong>Guidelines:</strong></p>
<ul>
<li>Ensure items are truly independent (no shared state or file conflicts)</li>
<li>Test with sequential execution first (<code>parallel: 1</code>) to verify correctness</li>
<li>Set appropriate parallel limits based on resources (CPU cores, memory)</li>
<li>Use <code>continue_on_error</code> for fault tolerance</li>
<li>Consider timeout implications with parallel execution</li>
</ul>
<p><strong>Verifying Independence:</strong></p>
<pre><code class="language-yaml"># STEP 1: Test with sequential execution
- foreach:
    foreach: ${modules}
    parallel: 1  # Start with sequential
    do:
      - shell: "test-module.sh ${item}"

# STEP 2: After verifying correctness, increase parallelism
- foreach:
    foreach: ${modules}
    parallel: 5  # Scale up after validation
    do:
      - shell: "test-module.sh ${item}"
</code></pre>
<h3 id="7-structure-complex-conditionals"><a class="header" href="#7-structure-complex-conditionals">7. Structure Complex Conditionals</a></h3>
<p>Use comparison operators and logical operators effectively:</p>
<pre><code class="language-yaml"># Clear multi-condition logic
- shell: "deploy.sh"
  when: "${environment == 'production' &amp;&amp; tests_passed &amp;&amp; coverage &gt;= 80}"

# Explicit step references
- shell: "notify-team.sh"
  when: "${test-step.exit_code == 0 &amp;&amp; build-step.success}"
</code></pre>
<h3 id="8-use-git-context-variables-for-change-aware-workflows"><a class="header" href="#8-use-git-context-variables-for-change-aware-workflows">8. Use Git Context Variables for Change-Aware Workflows</a></h3>
<p>Leverage git context variables to make workflows respond intelligently to changes:</p>
<pre><code class="language-yaml"># Run tests only on changed files
- shell: "git diff --stat"
  id: "detect-changes"

# Selective test execution based on file changes
- shell: "cargo test"
  when: "${step.files_changed}" != ""

# Pass changed files to linter (using shell for space-to-newline conversion)
- shell: |
    changed_files=$(echo "${step.files_changed}" | tr ' ' '\n' | grep '\.rs$')
    if [ -n "$changed_files" ]; then
      echo "$changed_files" | xargs cargo clippy --
    fi
</code></pre>
<p><strong>Source</strong>: Git context tracking from src/cook/workflow/git_context.rs:86-116</p>
<p><strong>Available Variables:</strong></p>
<p><strong>Step-Level (Current Step):</strong></p>
<ul>
<li><code>${step.files_added}</code>: Files added in current step</li>
<li><code>${step.files_modified}</code>: Files modified in current step</li>
<li><code>${step.files_deleted}</code>: Files deleted in current step</li>
<li><code>${step.files_changed}</code>: All files changed (combined)</li>
<li><code>${step.commits}</code>: Commit SHAs from current step</li>
<li><code>${step.commit_count}</code>: Number of commits</li>
<li><code>${step.insertions}</code>: Lines added</li>
<li><code>${step.deletions}</code>: Lines deleted</li>
</ul>
<p><strong>Workflow-Level (Cumulative):</strong></p>
<ul>
<li><code>${workflow.files_added}</code>: All files added in workflow</li>
<li><code>${workflow.files_modified}</code>: All files modified in workflow</li>
<li><code>${workflow.files_deleted}</code>: All files deleted in workflow</li>
<li><code>${workflow.files_changed}</code>: All files changed in workflow</li>
<li><code>${workflow.commits}</code>: All commit SHAs</li>
<li><code>${workflow.commit_count}</code>: Total commits</li>
<li><code>${workflow.insertions}</code>: Total lines added</li>
<li><code>${workflow.deletions}</code>: Total lines deleted</li>
</ul>
<p><strong>Format Note</strong>: Variables are currently space-separated. Use shell commands for filtering and formatting:</p>
<pre><code class="language-yaml"># Filter by extension and convert to newlines
- shell: "echo ${step.files_changed} | tr ' ' '\n' | grep '\.rs$'"

# Convert to JSON array
- shell: "echo ${step.files_added} | tr ' ' '\n' | jq -R | jq -s"

# Filter and pass to command
- shell: |
    ts_files=$(echo "${step.files_changed}" | tr ' ' '\n' | grep '\.ts$' | tr '\n' ' ')
    if [ -n "$ts_files" ]; then
      prettier --write $ts_files
    fi
</code></pre>
<p>See <a href="advanced/../git-context-advanced.html">git context advanced</a> for detailed usage patterns.</p>
<h3 id="9-separate-concerns-with-step-level-environment"><a class="header" href="#9-separate-concerns-with-step-level-environment">9. Separate Concerns with Step-Level Environment</a></h3>
<p>Configure environment variables at the step level for isolation:</p>
<pre><code class="language-yaml">- shell: "npm test"
  env:
    NODE_ENV: "test"
    DEBUG: "*"
  working_dir: "./frontend"

- shell: "npm run build"
  env:
    NODE_ENV: "production"
  working_dir: "./frontend"
</code></pre>
<h3 id="10-document-complex-workflows"><a class="header" href="#10-document-complex-workflows">10. Document Complex Workflows</a></h3>
<p>Add comments to explain non-obvious logic:</p>
<pre><code class="language-yaml"># Run tests with coverage, requiring 80% threshold
# Falls back to basic tests if coverage tooling unavailable
- shell: "cargo tarpaulin --out json"
  timeout: 600
  capture_output: "coverage"
  capture_format: "json"
  on_failure:
    shell: "cargo test"  # Fallback without coverage
</code></pre>
<h3 id="11-design-idempotent-mapreduce-work-items"><a class="header" href="#11-design-idempotent-mapreduce-work-items">11. Design Idempotent MapReduce Work Items</a></h3>
<p>For MapReduce workflows, design work items that can be safely retried and processed independently:</p>
<pre><code class="language-yaml">name: tech-debt-elimination
mode: mapreduce

setup:
  - shell: "debtmap analyze . --output debt.json"

map:
  input: debt.json
  json_path: "$.items[*]"

  # Prevent duplicate processing
  distinct: "item.id"

  # Process high-priority items first
  filter: "item.severity == 'critical' || item.severity == 'high'"
  sort_by: "item.priority DESC"

  # Limit scope for initial run
  max_items: 20
  max_parallel: 5

  agent_template:
    - claude: "/fix-debt-item '${item.description}' --id ${item.id}"
      commit_required: true

    # Verify fix with tests
    - shell: "cargo test"
      on_failure:
        claude: "/debug-and-fix"
        max_retries: 2

reduce:
  - shell: "debtmap analyze . --output debt-after.json"
  - claude: "/compare-debt-reports --before debt.json --after debt-after.json"
</code></pre>
<p><strong>Source</strong>: MapReduce architecture from src/cook/mapreduce/orchestrator.rs and book/src/mapreduce-worktree-architecture.md</p>
<p><strong>Key Principles:</strong></p>
<ol>
<li>
<p><strong>Idempotency</strong>: Use <code>distinct</code> field to prevent duplicate processing</p>
<pre><code class="language-yaml">map:
  distinct: "item.id"  # Deduplicates based on this field
</code></pre>
</li>
<li>
<p><strong>Work Item Independence</strong>: Each item processes in isolated worktree</p>
<ul>
<li>No shared state between agents</li>
<li>Independent git histories</li>
<li>Isolated file system changes</li>
<li>See <a href="advanced/../mapreduce-worktree-architecture.html">MapReduce worktree architecture</a></li>
</ul>
</li>
<li>
<p><strong>Failure Isolation</strong>: Failed items go to Dead Letter Queue (DLQ)</p>
<pre><code class="language-yaml"># Default error policy (configurable)
error_policy:
  on_item_failure: dlq
  continue_on_failure: true
</code></pre>
</li>
<li>
<p><strong>Retry Strategy</strong>: Configure backoff for transient failures</p>
<pre><code class="language-yaml">map:
  retry_config:
    attempts: 5
    backoff: exponential
    max_delay: "30s"
    jitter: true  # Prevents thundering herd
</code></pre>
</li>
<li>
<p><strong>DLQ Management</strong>: Monitor and retry failed items</p>
<pre><code class="language-bash"># View failed items
prodigy dlq show &lt;job_id&gt;

# Retry with parallelism
prodigy dlq retry &lt;job_id&gt; --max-parallel 10

# View failure statistics
prodigy dlq stats &lt;job_id&gt;
</code></pre>
</li>
</ol>
<p><strong>Testing MapReduce Workflows:</strong></p>
<pre><code class="language-yaml"># Start with small scope
map:
  max_items: 5        # Test with 5 items first
  max_parallel: 1     # Sequential to verify correctness

# After validation, scale up
map:
  max_items: 100
  max_parallel: 10    # Increase parallelism
</code></pre>
<p><strong>Best Practices:</strong></p>
<ul>
<li>Design work items to be retryable without side effects</li>
<li>Avoid shared state or file dependencies between items</li>
<li>Use DLQ for automatic failure collection and retry</li>
<li>Test with <code>parallel: 1</code> before scaling to <code>parallel: N</code></li>
<li>Monitor DLQ statistics to identify systematic issues</li>
<li>Enable jitter in retry config to prevent concurrent retry storms</li>
<li>Use <code>distinct</code> to ensure exactly-once processing semantics</li>
</ul>
<p>See also:</p>
<ul>
<li><a href="advanced/../mapreduce/index.html">MapReduce overview</a></li>
<li><a href="advanced/../mapreduce/dead-letter-queue-dlq.html">Dead Letter Queue</a></li>
<li><a href="advanced/../retry-configuration/index.html">Retry configuration</a></li>
</ul>
<h3 id="12-monitor-resource-usage-in-parallel-workflows"><a class="header" href="#12-monitor-resource-usage-in-parallel-workflows">12. Monitor Resource Usage in Parallel Workflows</a></h3>
<p>When running parallel operations, monitor system resources to avoid overload:</p>
<pre><code class="language-yaml"># Conservative parallelism for resource-intensive tasks
- foreach:
    foreach: ${large_modules}
    parallel: 3  # Lower limit for CPU/memory intensive work
    do:
      - shell: "cargo build --release"

# Higher parallelism for I/O-bound tasks
- foreach:
    foreach: ${test_suites}
    parallel: 8  # Higher limit for I/O-bound operations
    do:
      - shell: "npm test"
</code></pre>
<p><strong>Guidelines:</strong></p>
<ul>
<li>CPU-bound: <code>parallel &lt;= CPU cores</code></li>
<li>I/O-bound: <code>parallel = 1.5-2x CPU cores</code></li>
<li>Memory-intensive: Calculate based on available RAM per process</li>
<li>Network-bound: Consider rate limits and connection pools</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="common-patterns-3"><a class="header" href="#common-patterns-3">Common Patterns</a></h2>
<h3 id="test-fix-verify-loop"><a class="header" href="#test-fix-verify-loop">Test-Fix-Verify Loop</a></h3>
<p>Automatically fix test failures and verify the fixes:</p>
<pre><code class="language-yaml">- shell: "cargo test"
  on_failure:
    claude: "/fix-tests"
    commit_required: true  # Ensure fixes create commits for audit trail
    on_success:
      shell: "cargo test --release"
</code></pre>
<p><strong>Source</strong>: Pattern structure from src/config/command.rs:320-401 (WorkflowStepCommand)</p>
<p>This pattern:</p>
<ol>
<li>Runs tests</li>
<li>If tests fail, uses Claude to fix them</li>
<li>Creates a commit for each fix (important for tracking changes)</li>
<li>If fixes succeed, runs tests again in release mode to verify</li>
</ol>
<p><strong>Best Practices:</strong></p>
<ul>
<li>Use <code>commit_required: true</code> for automated fixes to maintain audit trail</li>
<li>Consider adding timeout to prevent indefinite hanging</li>
<li>Capture test output for debugging: <code>capture_output: "test_results"</code></li>
</ul>
<h3 id="mapreduce-massive-parallel-processing"><a class="header" href="#mapreduce-massive-parallel-processing">MapReduce: Massive Parallel Processing</a></h3>
<p>For large-scale parallel processing, use MapReduce workflows instead of foreach:</p>
<pre><code class="language-yaml">name: parallel-analysis
mode: mapreduce

setup:
  - shell: "find src -name '*.rs' | jq -R -s 'split(\"\n\")[:-1] | map({file: .})' &gt; files.json"

map:
  input: "files.json"
  json_path: "$[*]"

  agent_template:
    - shell: "analyze-file ${item.file}"
      capture_output: "analysis"

    - shell: "echo '${analysis}' &gt; results/${item.file}.json"

  max_parallel: 10

reduce:
  - shell: "jq -s '.' results/*.json &gt; aggregated-results.json"
  - claude: "/summarize-analysis aggregated-results.json"
</code></pre>
<p><strong>Source</strong>: workflows/mapreduce-example.yml:1-39</p>
<p><strong>Use Cases:</strong></p>
<ul>
<li>Processing hundreds of files in parallel</li>
<li>Running tests across large codebases</li>
<li>Batch data processing with aggregation</li>
<li>Distributed code analysis</li>
</ul>
<p><strong>When to Use MapReduce vs Foreach:</strong></p>
<ul>
<li><strong>MapReduce</strong>: 50+ items, need isolation, resumable, complex aggregation</li>
<li><strong>Foreach</strong>: &lt;50 items, simple iteration, linear processing acceptable</li>
</ul>
<p>See the <a href="advanced/../mapreduce/index.html">MapReduce</a> chapter for comprehensive details.</p>
<h3 id="parallel-iteration-with-foreach-1"><a class="header" href="#parallel-iteration-with-foreach-1">Parallel Iteration with Foreach</a></h3>
<p>For simpler parallel iteration over small lists:</p>
<pre><code class="language-yaml">- foreach: "ls *.txt"
  parallel: 5
  do:
    - shell: "process-file ${item}"
      capture_output: "result_${item}"
</code></pre>
<p><strong>Source</strong>: src/config/command.rs:191-211 (ForeachConfig)</p>
<p><strong>Note</strong>: The foreach feature exists in the configuration but has limited examples. For robust parallel processing with work isolation and resume capabilities, prefer MapReduce patterns.</p>
<p><strong>Use Cases:</strong></p>
<ul>
<li>Processing small batches (&lt;50 items)</li>
<li>Simple transformations without complex aggregation</li>
<li>Quick parallel operations in standard workflows</li>
</ul>
<h3 id="gradual-quality-improvement"><a class="header" href="#gradual-quality-improvement">Gradual Quality Improvement</a></h3>
<p>Iteratively improve code quality until threshold is met:</p>
<pre><code class="language-yaml">- goal_seek:
    goal: "Code quality score above 90"
    shell: "auto-improve.sh"
    validate: "quality-check.sh"
    threshold: 90
    max_attempts: 5
  commit_required: true
</code></pre>
<p><strong>Source</strong>: src/cook/goal_seek/mod.rs:14-41 (GoalSeekConfig), workflows/goal-seeking-examples.yml:29-43</p>
<p><strong>How It Works:</strong></p>
<ol>
<li>Executes the improvement command (<code>auto-improve.sh</code>)</li>
<li>Runs validation command (<code>quality-check.sh</code>)</li>
<li>Validation must output <code>score: N</code> where N is 0-100</li>
<li>If score &gt;= threshold (90), goal is achieved</li>
<li>Otherwise, repeats up to max_attempts times</li>
<li>Creates commit when goal is achieved (due to <code>commit_required: true</code>)</li>
</ol>
<p><strong>Validation Output Format</strong> (src/cook/goal_seek/validator.rs:37-62):</p>
<p>Your validation command must output one of these formats:</p>
<pre><code class="language-bash">score: 85           # Primary format (recommended)
85%                 # Percentage format
85/100              # Fraction format
85 out of 100       # Verbose format
</code></pre>
<p>Or JSON format:</p>
<pre><code class="language-json">{
  "score": 85,
  "gaps": ["missing tests", "undocumented functions"]
}
</code></pre>
<p><strong>Real Example</strong> (workflows/goal-seeking-examples.yml:6-14):</p>
<pre><code class="language-yaml">- goal_seek:
    goal: "Achieve 90% test coverage"
    claude: "/prodigy-coverage --improve"
    validate: "cargo tarpaulin --print-summary 2&gt;/dev/null | grep 'Coverage' | sed 's/.*Coverage=\\([0-9]*\\).*/score: \\1/'"
    threshold: 90
    max_attempts: 5
    timeout_seconds: 300
  commit_required: true
</code></pre>
<h3 id="conditional-deployment"><a class="header" href="#conditional-deployment">Conditional Deployment</a></h3>
<p>Deploy only when all tests pass:</p>
<pre><code class="language-yaml">- shell: "cargo test --format json"
  capture_output: "test_results"
  capture_format: "json"  # Required to parse output as JSON for field access

- shell: "deploy.sh"
  when: "${test_results.passed == test_results.total}"
  on_success:
    shell: "notify-success.sh"
  on_failure:
    shell: "rollback.sh"
</code></pre>
<p><strong>Source</strong>: src/config/command.rs:WorkflowStepCommand.when field, src/config/capture.rs</p>
<p><strong>Pattern Components:</strong></p>
<ul>
<li>Capture test results as JSON</li>
<li><code>capture_format: "json"</code> is <strong>required</strong> to enable JSON field access in <code>when</code> clauses</li>
<li>Deploy only if all tests passed (conditional execution via <code>when</code>)</li>
<li>Send notification on success</li>
<li>Rollback on failure</li>
</ul>
<p><strong>Important</strong>: Without <code>capture_format: "json"</code>, the <code>when</code> clause cannot access object fields like <code>test_results.passed</code>. The output would be treated as a plain string.</p>
<h3 id="multi-stage-validation"><a class="header" href="#multi-stage-validation">Multi-Stage Validation</a></h3>
<p>Validate through multiple quality gates:</p>
<pre><code class="language-yaml">- claude: "/implement-feature"
  validate:
    result_file: ".prodigy/validation-result.json"
    threshold: 100
    on_incomplete:
      claude: "/fix-validation-gaps"
      max_attempts: 3
      commit_required: true
</code></pre>
<p><strong>Source</strong>: src/cook/workflow/validation.rs:11-49 (ValidationConfig), workflows/spec.yml:12-19</p>
<p><strong>How Validation Works:</strong></p>
<p>The validation command must write a JSON file with this schema (src/cook/workflow/validation.rs:216-239):</p>
<pre><code class="language-json">{
  "completion_percentage": 85.0,
  "status": "Incomplete",
  "implemented": ["feature_a", "feature_b"],
  "missing": ["feature_c"],
  "gaps": {
    "feature_c": {
      "description": "Missing error handling",
      "location": "src/main.rs:45",
      "severity": "High",
      "suggested_fix": "Add try-catch block"
    }
  }
}
</code></pre>
<p><strong>Required Fields:</strong></p>
<ul>
<li><code>completion_percentage</code>: 0-100 (compared against threshold)</li>
<li><code>status</code>: “Complete” | “Incomplete” | “Failed” | “Skipped”</li>
<li><code>gaps</code>: Object mapping gap IDs to GapDetail objects</li>
</ul>
<p><strong>If completion_percentage &lt; threshold:</strong></p>
<ul>
<li><code>on_incomplete</code> commands execute</li>
<li>Validation variables available: <code>${validation.gaps}</code>, <code>${validation.incomplete_specs}</code></li>
<li>Process repeats up to <code>max_attempts</code> times</li>
</ul>
<p><strong>Real Example</strong> (workflows/spec.yml:12-19):</p>
<pre><code class="language-yaml">validate:
  result_file: ".prodigy/spec-validation.json"
  threshold: 100
  on_incomplete:
    claude: "/prodigy-refine-specs ${validation.incomplete_specs} --gaps ${validation.gaps}"
    max_attempts: 5
    commit_required: true
</code></pre>
<h3 id="progressive-enhancement"><a class="header" href="#progressive-enhancement">Progressive Enhancement</a></h3>
<p>Build features incrementally with validation at each stage:</p>
<pre><code class="language-yaml"># Stage 1: Basic implementation
- claude: "/implement-basic-feature"
  validate:
    shell: "basic-tests.sh"
    threshold: 100

# Stage 2: Add edge case handling
- claude: "/add-edge-cases"
  validate:
    shell: "comprehensive-tests.sh"
    threshold: 100

# Stage 3: Optimize performance
- claude: "/optimize-performance"
  validate:
    shell: "benchmark.sh"
    threshold: 95
</code></pre>
<h3 id="fail-fast-with-early-validation"><a class="header" href="#fail-fast-with-early-validation">Fail-Fast with Early Validation</a></h3>
<p>Validate quickly before expensive operations:</p>
<pre><code class="language-yaml"># Quick syntax check first
- shell: "cargo check"
  on_failure:
    fail_workflow: true

# Then run expensive tests
- shell: "cargo test"
  timeout: 600

# Finally run benchmarks
- shell: "cargo bench"
  timeout: 1800
</code></pre>
<h3 id="multi-environment-workflow"><a class="header" href="#multi-environment-workflow">Multi-Environment Workflow</a></h3>
<p>Different behavior based on environment:</p>
<pre><code class="language-yaml">- shell: "run-tests.sh"
  env:
    ENV: "${ENVIRONMENT}"
  capture_output: "test_results"

- shell: "deploy.sh"
  when: "${ENVIRONMENT == 'production' &amp;&amp; test_results.exit_code == 0}"
  env:
    DEPLOY_TARGET: "production"

- shell: "deploy-staging.sh"
  when: "${ENVIRONMENT == 'staging' &amp;&amp; test_results.exit_code == 0}"
  env:
    DEPLOY_TARGET: "staging"
</code></pre>
<h3 id="retry-with-backoff"><a class="header" href="#retry-with-backoff">Retry with Backoff</a></h3>
<p>Retry failed operations with different strategies:</p>
<pre><code class="language-yaml">- shell: "flaky-operation.sh"
  on_failure:
    shell: "flaky-operation.sh --retry"
    on_failure:
      shell: "flaky-operation.sh --force"
      max_attempts: 3
</code></pre>
<h3 id="data-pipeline"><a class="header" href="#data-pipeline">Data Pipeline</a></h3>
<p>Transform data through multiple stages:</p>
<pre><code class="language-yaml">- shell: "extract-data.sh"
  capture_output: "raw_data"

- shell: "transform-data.sh '${raw_data}'"
  capture_output: "transformed_data"
  capture_format: "json"

- shell: "validate-data.sh '${transformed_data}'"
  validate:
    shell: "schema-check.sh"
    threshold: 100

- shell: "load-data.sh '${transformed_data}'"
</code></pre>
<h3 id="feature-flag-workflow"><a class="header" href="#feature-flag-workflow">Feature Flag Workflow</a></h3>
<p>Enable features conditionally:</p>
<pre><code class="language-yaml">- shell: "check-feature-flag.sh new-feature"
  capture_output: "feature_enabled"
  capture_format: "boolean"

- claude: "/implement-new-feature"
  when: "${feature_enabled}"

- shell: "run-tests.sh"
  env:
    FEATURE_NEW: "${feature_enabled}"
</code></pre>
<h3 id="environment-profile-pattern"><a class="header" href="#environment-profile-pattern">Environment Profile Pattern</a></h3>
<p>Use environment profiles for multi-environment workflows:</p>
<pre><code class="language-yaml">env:
  DATABASE_URL:
    default: "postgres://localhost/dev"
    staging: "postgres://staging-server/db"
    prod: "postgres://prod-server/db"

  API_KEY:
    secret: true
    value: "${API_KEY_FROM_ENV}"

- shell: "migrate-database"
  env:
    DB_URL: "${DATABASE_URL}"

- shell: "deploy-app"
  when: "${profile == 'prod'}"
</code></pre>
<p><strong>Source</strong>: Environment variables specification (Spec 120), src/config/environment.rs</p>
<p><strong>Key Features:</strong></p>
<ul>
<li><strong>Profile-specific values</strong>: Different configurations per environment (dev/staging/prod)</li>
<li><strong>Secret masking</strong>: Mark sensitive values with <code>secret: true</code> to mask in logs</li>
<li><strong>Activation</strong>: Run with <code>prodigy run workflow.yml --profile prod</code></li>
</ul>
<p><strong>Use Cases:</strong></p>
<ul>
<li>Multi-environment deployments</li>
<li>Secrets management</li>
<li>Environment-specific behavior</li>
</ul>
<p>See <a href="advanced/../configuration/environment-variables.html">Environment Variables</a> for comprehensive details.</p>
<h3 id="circuit-breaker-and-dlq-retry"><a class="header" href="#circuit-breaker-and-dlq-retry">Circuit Breaker and DLQ Retry</a></h3>
<p>Handle failures gracefully with Dead Letter Queue retry:</p>
<pre><code class="language-yaml"># Initial MapReduce workflow with automatic DLQ for failures
name: resilient-processing
mode: mapreduce

map:
  input: "items.json"
  json_path: "$[*]"

  agent_template:
    - shell: "process-item ${item.id}"
      timeout: 60
      on_failure:
        # Failures automatically go to DLQ
        shell: "log-failure ${item.id}"

  max_parallel: 10
</code></pre>
<p><strong>After initial run, retry failed items:</strong></p>
<pre><code class="language-bash"># Retry all failed items from the job
prodigy dlq retry &lt;job_id&gt;

# Retry with custom parallelism
prodigy dlq retry &lt;job_id&gt; --max-parallel 5

# Dry run to see what would be retried
prodigy dlq retry &lt;job_id&gt; --dry-run
</code></pre>
<p><strong>Source</strong>: MapReduce DLQ implementation, src/mapreduce/dlq/mod.rs</p>
<p><strong>How It Works:</strong></p>
<ol>
<li>Failed work items automatically go to Dead Letter Queue</li>
<li>DLQ stores failure reason, timestamp, and retry count</li>
<li>Use <code>prodigy dlq retry</code> to reprocess failed items</li>
<li>Supports partial success (some items succeed, others remain in DLQ)</li>
</ol>
<p><strong>Use Cases:</strong></p>
<ul>
<li>Handling transient failures (network issues, timeouts)</li>
<li>Incremental retry of failed operations</li>
<li>Production resilience patterns</li>
</ul>
<p>See <a href="advanced/../mapreduce/dead-letter-queue-dlq.html">Dead Letter Queue</a> for comprehensive details.</p>
<h2 id="see-also-22"><a class="header" href="#see-also-22">See Also</a></h2>
<ul>
<li><a href="advanced/goal-seeking-operations.html">Goal-Seeking Operations</a> - Deep dive into goal_seek patterns</li>
<li><a href="advanced/implementation-validation.html">Implementation Validation</a> - Validation command details</li>
<li><a href="advanced/../mapreduce/index.html">MapReduce</a> - Comprehensive parallel processing guide</li>
<li><a href="advanced/../configuration/environment-variables.html">Environment Variables</a> - Environment configuration</li>
<li><a href="advanced/../error-handling.html">Error Handling</a> - Comprehensive error handling strategies</li>
<li><a href="advanced/../mapreduce/dead-letter-queue-dlq.html">Dead Letter Queue</a> - DLQ and retry patterns</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="advanced-git-context"><a class="header" href="#advanced-git-context">Advanced Git Context</a></h1>
<p>This chapter covers automatic git tracking and git context variables in Prodigy workflows. Learn how to access file changes, commits, and modification statistics, and how to filter and format this data using shell commands.</p>
<blockquote>
<p><strong>⚠️ Current Implementation Status</strong></p>
<p>Git context variables are currently provided as <strong>space-separated strings only</strong>. Advanced features like pattern filtering (<code>:*.rs</code>) and format modifiers (<code>:json</code>, <code>:lines</code>) are <strong>not yet implemented</strong> in the variable interpolation system, though the underlying infrastructure exists.</p>
<p><strong>For filtering and formatting</strong>, use shell post-processing commands like <code>grep</code>, <code>tr</code>, <code>jq</code>, and <code>xargs</code>. See <a href="git-context-advanced.html#shell-based-filtering-and-formatting">Shell-Based Filtering and Formatting</a> for practical examples.</p>
</blockquote>
<h2 id="overview-7"><a class="header" href="#overview-7">Overview</a></h2>
<p>Prodigy automatically tracks git changes throughout workflow execution and exposes them through variables. No configuration is needed—git context variables are available out-of-the-box in any git repository. You can access file changes, commits, and modification statistics at both the step and workflow level.</p>
<p><strong>What you get:</strong></p>
<ul>
<li>Automatic tracking of all git changes during workflow execution</li>
<li>Variables for step-level changes (current command) and workflow-level changes (cumulative)</li>
<li>Simple space-separated format ready for shell commands</li>
<li>Full integration with MapReduce workflows</li>
</ul>
<h2 id="how-git-tracking-works"><a class="header" href="#how-git-tracking-works">How Git Tracking Works</a></h2>
<h3 id="automatic-tracking"><a class="header" href="#automatic-tracking">Automatic Tracking</a></h3>
<p>Git context is automatically tracked when you run workflows in a git repository:</p>
<ul>
<li><strong>GitChangeTracker</strong> is initialized at workflow start (src/cook/workflow/git_context.rs)</li>
<li>Each step’s changes are tracked between <code>begin_step</code> and <code>complete_step</code> calls</li>
<li>Variables are pre-formatted as space-separated strings and added to the interpolation context</li>
<li>No YAML configuration needed—tracking happens transparently</li>
</ul>
<p><strong>Technical Details</strong> (src/cook/workflow/executor/context.rs:96-172):</p>
<p>When preparing the interpolation context for each command, git variables are added like this:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Variables are pre-formatted as space-separated strings
context.set("step.files_added", Value::String(changes.files_added.join(" ")));
context.set("step.files_modified", Value::String(changes.files_modified.join(" ")));
// ... etc for all git context variables
<span class="boring">}</span></code></pre></pre>
<p>This means custom formatting must be done using shell commands after variable interpolation.</p>
<h3 id="when-tracking-is-active"><a class="header" href="#when-tracking-is-active">When Tracking is Active</a></h3>
<p>Git tracking is active in:</p>
<ul>
<li>Regular workflows running in git repositories</li>
<li>MapReduce setup, map, and reduce phases</li>
<li>Child worktrees created for map agents</li>
</ul>
<p>Git tracking is <strong>not</strong> active in:</p>
<ul>
<li>Non-git repositories</li>
<li>Workflows without git integration</li>
</ul>
<h2 id="git-context-variables-1"><a class="header" href="#git-context-variables-1">Git Context Variables</a></h2>
<h3 id="step-level-variables"><a class="header" href="#step-level-variables">Step-Level Variables</a></h3>
<p>Track changes made during the current step:</p>
<pre><code class="language-yaml"># Access files changed in this step
- shell: "echo Changed: ${step.files_changed}"
- shell: "echo Added: ${step.files_added}"
- shell: "echo Modified: ${step.files_modified}"
- shell: "echo Deleted: ${step.files_deleted}"

# Access commit information
- shell: "echo Commits: ${step.commits}"
- shell: "echo Commit count: ${step.commit_count}"

# Access modification statistics
- shell: "echo Insertions: ${step.insertions}"
- shell: "echo Deletions: ${step.deletions}"
</code></pre>
<h3 id="workflow-level-variables"><a class="header" href="#workflow-level-variables">Workflow-Level Variables</a></h3>
<p>Track cumulative changes across all steps:</p>
<pre><code class="language-yaml"># Access all files changed in workflow
- shell: "echo Changed: ${workflow.files_changed}"
- shell: "echo Added: ${workflow.files_added}"
- shell: "echo Modified: ${workflow.files_modified}"
- shell: "echo Deleted: ${workflow.files_deleted}"

# Access all commits
- shell: "echo Commits: ${workflow.commits}"
- shell: "echo Commit count: ${workflow.commit_count}"

# Access total modifications
- shell: "echo Insertions: ${workflow.insertions}"
- shell: "echo Deletions: ${workflow.deletions}"
</code></pre>
<h3 id="variable-reference"><a class="header" href="#variable-reference">Variable Reference</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Variable</th><th>Scope</th><th>Description</th></tr></thead><tbody>
<tr><td><code>step.files_added</code></td><td>Step</td><td>Files added in current step</td></tr>
<tr><td><code>step.files_modified</code></td><td>Step</td><td>Files modified in current step</td></tr>
<tr><td><code>step.files_deleted</code></td><td>Step</td><td>Files deleted in current step</td></tr>
<tr><td><code>step.files_changed</code></td><td>Step</td><td>All files changed (added + modified + deleted)</td></tr>
<tr><td><code>step.commits</code></td><td>Step</td><td>Commit SHAs from current step</td></tr>
<tr><td><code>step.commit_count</code></td><td>Step</td><td>Number of commits in current step</td></tr>
<tr><td><code>step.insertions</code></td><td>Step</td><td>Lines added in current step</td></tr>
<tr><td><code>step.deletions</code></td><td>Step</td><td>Lines deleted in current step</td></tr>
<tr><td><code>workflow.files_added</code></td><td>Workflow</td><td>All files added in workflow</td></tr>
<tr><td><code>workflow.files_modified</code></td><td>Workflow</td><td>All files modified in workflow</td></tr>
<tr><td><code>workflow.files_deleted</code></td><td>Workflow</td><td>All files deleted in workflow</td></tr>
<tr><td><code>workflow.files_changed</code></td><td>Workflow</td><td>All files changed in workflow</td></tr>
<tr><td><code>workflow.commits</code></td><td>Workflow</td><td>All commit SHAs in workflow</td></tr>
<tr><td><code>workflow.commit_count</code></td><td>Workflow</td><td>Total commits in workflow</td></tr>
<tr><td><code>workflow.insertions</code></td><td>Workflow</td><td>Total lines added in workflow</td></tr>
<tr><td><code>workflow.deletions</code></td><td>Workflow</td><td>Total lines deleted in workflow</td></tr>
</tbody></table>
</div>
<h2 id="shell-based-filtering-and-formatting"><a class="header" href="#shell-based-filtering-and-formatting">Shell-Based Filtering and Formatting</a></h2>
<p>Since git context variables are provided as space-separated strings, all filtering and formatting must be done using shell commands. This section shows practical patterns for common tasks.</p>
<h3 id="default-format-space-separated"><a class="header" href="#default-format-space-separated">Default Format (Space-Separated)</a></h3>
<p>Git context variables are always formatted as space-separated strings:</p>
<pre><code class="language-yaml">- shell: "echo ${step.files_changed}"
# Output: src/main.rs src/lib.rs tests/test.rs
</code></pre>
<p>This format works well with most shell commands:</p>
<pre><code class="language-yaml"># Pass directly to commands
- shell: "cargo fmt ${step.files_changed}"
- shell: "git add ${workflow.files_modified}"

# Use in loops
- shell: |
    for file in ${step.files_added}; do
      echo "Processing $file"
    done
</code></pre>
<h3 id="filtering-by-file-extension"><a class="header" href="#filtering-by-file-extension">Filtering by File Extension</a></h3>
<p>Use <code>grep</code> to filter files by extension or pattern:</p>
<pre><code class="language-yaml"># Only Rust files
- shell: |
    rust_files=$(echo "${step.files_changed}" | tr ' ' '\n' | grep '\.rs$')
    echo "$rust_files"
# Output:
# src/main.rs
# src/lib.rs

# Only files in src/ directory
- shell: |
    src_files=$(echo "${step.files_changed}" | tr ' ' '\n' | grep '^src/')
    echo "$src_files"

# Multiple extensions (Rust or TOML)
- shell: |
    filtered=$(echo "${step.files_modified}" | tr ' ' '\n' | grep -E '\.(rs|toml)$')
    echo "$filtered"

# Pass filtered files to a command
- shell: |
    rust_files=$(echo "${step.files_changed}" | tr ' ' '\n' | grep '\.rs$' | tr '\n' ' ')
    if [ -n "$rust_files" ]; then
      cargo fmt $rust_files
    fi
</code></pre>
<h3 id="converting-to-json-format"><a class="header" href="#converting-to-json-format">Converting to JSON Format</a></h3>
<p>Use <code>jq</code> to convert space-separated files to JSON arrays:</p>
<pre><code class="language-yaml"># Convert to JSON array
- shell: "echo ${step.files_added} | tr ' ' '\n' | jq -R | jq -s"
# Output: ["src/main.rs","src/lib.rs","tests/test.rs"]

# Filter AND convert to JSON
- shell: |
    echo "${step.files_changed}" | tr ' ' '\n' | grep '\.rs$' | jq -R | jq -s
# Output: ["src/main.rs","src/lib.rs"]

# Pretty-print JSON
- shell: |
    echo "${workflow.files_modified}" | tr ' ' '\n' | jq -R | jq -s '.'
</code></pre>
<h3 id="converting-to-newline-separated-format"><a class="header" href="#converting-to-newline-separated-format">Converting to Newline-Separated Format</a></h3>
<p>Use <code>tr</code> to convert space-separated to newline-separated:</p>
<pre><code class="language-yaml"># One file per line
- shell: "echo ${step.files_changed} | tr ' ' '\n'"
# Output:
# src/main.rs
# src/lib.rs
# tests/test.rs

# Useful with xargs for parallel processing
- shell: |
    echo "${workflow.files_modified}" | tr ' ' '\n' | xargs -I {} cp {} backup/

# Count files
- shell: "echo ${step.files_added} | tr ' ' '\n' | wc -l"
</code></pre>
<h3 id="converting-to-csv-format"><a class="header" href="#converting-to-csv-format">Converting to CSV Format</a></h3>
<p>Use <code>tr</code> to convert to comma-separated values:</p>
<pre><code class="language-yaml"># Comma-separated
- shell: "echo ${step.files_added} | tr ' ' ','"
# Output: src/main.rs,src/lib.rs,tests/test.rs

# CSV with filtering
- shell: |
    echo "${step.files_changed}" | tr ' ' '\n' | grep '\.md$' | tr '\n' ',' | sed 's/,$//'
# Output: README.md,CHANGELOG.md
</code></pre>
<h3 id="combining-filtering-and-formatting"><a class="header" href="#combining-filtering-and-formatting">Combining Filtering and Formatting</a></h3>
<p>Practical examples combining multiple operations:</p>
<pre><code class="language-yaml"># Get Rust files as JSON
- shell: |
    echo "${step.files_changed}" | tr ' ' '\n' | grep '\.rs$' | jq -R | jq -s

# Get source files as comma-separated list
- shell: |
    echo "${workflow.files_modified}" | tr ' ' '\n' | grep '^src/' | tr '\n' ',' | sed 's/,$//'

# Count files by extension
- shell: |
    echo "${workflow.files_changed}" | tr ' ' '\n' | sed 's/.*\.//' | sort | uniq -c
# Output:
#    5 md
#    12 rs
#    3 toml
</code></pre>
<h2 id="use-cases"><a class="header" href="#use-cases">Use Cases</a></h2>
<h3 id="code-review-workflows"><a class="header" href="#code-review-workflows">Code Review Workflows</a></h3>
<p>Review only source code changes using shell filtering:</p>
<pre><code class="language-yaml"># Filter to only Rust source files before review
- shell: |
    rust_files=$(echo "${step.files_changed}" | tr ' ' '\n' | grep '^src/.*\.rs$' | tr '\n' ' ')
    if [ -n "$rust_files" ]; then
      echo "Rust files changed: $rust_files"
    fi

# Pass filtered files to Claude for review
- shell: |
    src_changes=$(echo "${step.files_changed}" | tr ' ' '\n' | grep '^src/')
    if [ -n "$src_changes" ]; then
      echo "$src_changes" &gt; /tmp/review-files.txt
      # Then use /tmp/review-files.txt in your review command
    fi

- shell: "echo Reviewing ${step.commit_count} commits"
</code></pre>
<h3 id="documentation-updates"><a class="header" href="#documentation-updates">Documentation Updates</a></h3>
<p>Work with documentation changes using filtering:</p>
<pre><code class="language-yaml"># Find markdown files that changed
- shell: |
    md_files=$(echo "${workflow.files_changed}" | tr ' ' '\n' | grep '\.md$' | tr '\n' ' ')
    if [ -n "$md_files" ]; then
      echo "Documentation files changed: $md_files"
      markdownlint $md_files
    fi

# List changed docs in newline format
- shell: |
    echo "${workflow.files_modified}" | tr ' ' '\n' | grep '\.md$'

# Check if any docs were updated
- shell: |
    doc_count=$(echo "${workflow.files_changed}" | tr ' ' '\n' | grep '\.md$' | wc -l)
    if [ "$doc_count" -gt 0 ]; then
      echo "Documentation was updated ($doc_count files)"
    fi
</code></pre>
<h3 id="test-verification"><a class="header" href="#test-verification">Test Verification</a></h3>
<p>Focus on test-related changes:</p>
<pre><code class="language-yaml"># Run tests for changed test files
- shell: |
    test_files=$(echo "${step.files_changed}" | tr ' ' '\n' | grep '_test\.rs$' | tr '\n' ' ')
    if [ -n "$test_files" ]; then
      cargo test $test_files
    fi

# Verify test coverage for new files in tests/
- shell: |
    new_tests=$(echo "${step.files_added}" | tr ' ' '\n' | grep '^tests/')
    if [ -n "$new_tests" ]; then
      echo "New test files added:"
      echo "$new_tests"
      # Run coverage analysis
    fi
</code></pre>
<h3 id="conditional-execution-2"><a class="header" href="#conditional-execution-2">Conditional Execution</a></h3>
<p>Use git context with shell conditions:</p>
<pre><code class="language-yaml"># Only run clippy if Rust files changed
- shell: |
    has_rust=$(echo "${step.files_changed}" | tr ' ' '\n' | grep '\.rs$')
    if [ -n "$has_rust" ]; then
      cargo clippy
    fi

# Run different linters based on file types
- shell: |
    if echo "${workflow.files_changed}" | tr ' ' '\n' | grep -q '\.rs$'; then
      cargo fmt --check
    fi

    if echo "${workflow.files_changed}" | tr ' ' '\n' | grep -q '\.md$'; then
      markdownlint **/*.md
    fi

# Check commit count
- shell: |
    if [ "${step.commit_count}" -gt 1 ]; then
      echo "Multiple commits detected (${step.commit_count})"
    fi
</code></pre>
<h3 id="mapreduce-workflows-2"><a class="header" href="#mapreduce-workflows-2">MapReduce Workflows</a></h3>
<p>Git context works across MapReduce phases:</p>
<pre><code class="language-yaml">name: review-changes
mode: mapreduce

setup:
  # Workflow-level tracking starts here
  - shell: "git diff main --name-only &gt; changed-files.txt"
  - shell: "echo Setup modified: ${step.files_changed}"

map:
  input: "changed-files.txt"
  agent_template:
    # Each agent has its own step tracking
    - claude: "/review ${item}"
    - shell: "echo Agent changed: ${step.files_changed}"

reduce:
  # Access workflow-level changes from all agents
  - shell: "echo Total changes: ${workflow.files_changed}"
  - shell: "echo Total commits: ${workflow.commit_count}"
</code></pre>
<h2 id="best-practices-17"><a class="header" href="#best-practices-17">Best Practices</a></h2>
<ul>
<li><strong>Use Shell Filtering</strong>: Filter variables to only relevant files using <code>grep</code>, <code>tr</code>, and other shell utilities</li>
<li><strong>Choose Appropriate Format</strong>: Convert to JSON with <code>jq</code>, newlines with <code>tr</code>, or CSV for different use cases</li>
<li><strong>Scope Appropriately</strong>: Use <code>step.*</code> for current changes, <code>workflow.*</code> for cumulative tracking</li>
<li><strong>Handle Empty Results</strong>: Always check if filtered results are non-empty before using them</li>
<li><strong>Test Your Filters</strong>: Debug with <code>echo</code> commands to verify filtering works as expected</li>
<li><strong>Document Intent</strong>: Add comments explaining complex shell filtering pipelines</li>
<li><strong>Combine Operations</strong>: Chain <code>tr</code>, <code>grep</code>, and <code>jq</code> for powerful filtering and formatting</li>
</ul>
<h2 id="performance-considerations-2"><a class="header" href="#performance-considerations-2">Performance Considerations</a></h2>
<ul>
<li>Git operations are performed once per step and cached (src/cook/workflow/git_context.rs)</li>
<li>Variables are pre-formatted when added to the interpolation context</li>
<li>Shell filtering happens at runtime, so complex filters may add overhead</li>
<li>Workflow-level tracking maintains cumulative state without re-scanning git history</li>
<li>Variable resolution is fast since values are pre-computed strings</li>
</ul>
<h2 id="troubleshooting-12"><a class="header" href="#troubleshooting-12">Troubleshooting</a></h2>
<h3 id="filter-not-matching-any-files"><a class="header" href="#filter-not-matching-any-files">Filter Not Matching Any Files</a></h3>
<p><strong>Issue</strong>: Your grep filter doesn’t match any files</p>
<pre><code class="language-yaml"># Debug: Echo the unfiltered variable first
- shell: "echo All files: ${step.files_changed}"
- shell: |
    filtered=$(echo "${step.files_changed}" | tr ' ' '\n' | grep '\.rs$')
    echo "Filtered: $filtered"
</code></pre>
<p><strong>What happens</strong>: When a filter matches no files, the variable is empty. This is expected behavior.</p>
<p><strong>Solution</strong>: Always check if filtered results are non-empty:</p>
<pre><code class="language-yaml">- shell: |
    rust_files=$(echo "${step.files_changed}" | tr ' ' '\n' | grep '\.rs$' | tr '\n' ' ')
    if [ -n "$rust_files" ]; then
      cargo fmt $rust_files
    else
      echo "No Rust files changed"
    fi
</code></pre>
<h3 id="empty-git-context-variables"><a class="header" href="#empty-git-context-variables">Empty Git Context Variables</a></h3>
<p><strong>Issue</strong>: Git context variables are empty</p>
<p><strong>Possible causes:</strong></p>
<ul>
<li>Not running in a git repository</li>
<li>No commits have been made in the current step</li>
<li>Git tracking not initialized</li>
</ul>
<p><strong>Solution</strong>: Verify git tracking is active:</p>
<pre><code class="language-yaml"># Check if variables are populated
- shell: |
    echo "Step files changed: ${step.files_changed}"
    echo "Workflow files changed: ${workflow.files_changed}"
    echo "Commit count: ${step.commit_count}"
</code></pre>
<p>If all are empty, check:</p>
<ol>
<li>Are you in a git repository? (<code>git status</code>)</li>
<li>Has the step made any commits yet?</li>
<li>Is git tracking active for this workflow type?</li>
</ol>
<h3 id="pattern-syntax-not-working"><a class="header" href="#pattern-syntax-not-working">Pattern Syntax Not Working</a></h3>
<p><strong>Issue</strong>: Trying to use <code>:*.rs</code> or <code>:json</code> modifiers produces errors or unexpected results</p>
<p><strong>Cause</strong>: Pattern filtering and format modifiers are <strong>not implemented</strong> in variable interpolation. Git context variables are always space-separated strings.</p>
<p><strong>What you tried</strong> (doesn’t work):</p>
<pre><code class="language-yaml"># These do NOT work - modifiers not implemented
- shell: "echo ${step.files_changed:*.rs}"
- shell: "echo ${step.files_added:json}"
- shell: "echo ${workflow.files_modified:lines}"
</code></pre>
<p><strong>Solution</strong>: Use shell commands for all filtering and formatting:</p>
<pre><code class="language-yaml"># Filter with grep
- shell: "echo ${step.files_changed} | tr ' ' '\n' | grep '\.rs$'"

# Format as JSON
- shell: "echo ${step.files_added} | tr ' ' '\n' | jq -R | jq -s"

# Format as newlines
- shell: "echo ${workflow.files_modified} | tr ' ' '\n'"
</code></pre>
<p>See <a href="git-context-advanced.html#shell-based-filtering-and-formatting">Shell-Based Filtering and Formatting</a> for complete examples.</p>
<h3 id="variables-not-interpolating"><a class="header" href="#variables-not-interpolating">Variables Not Interpolating</a></h3>
<p><strong>Issue</strong>: Variables appear as literal strings like <code>${step.files_changed}</code></p>
<p><strong>Possible causes:</strong></p>
<ul>
<li>Variable name misspelled</li>
<li>Using unsupported variable</li>
<li>YAML quoting issues</li>
</ul>
<p><strong>Solution</strong>: Verify the variable name and use proper quoting:</p>
<pre><code class="language-yaml"># Correct syntax
- shell: "echo ${step.files_changed}"
- shell: |
    echo "${workflow.files_modified}"
</code></pre>
<h3 id="shell-filtering-complexity"><a class="header" href="#shell-filtering-complexity">Shell Filtering Complexity</a></h3>
<p><strong>Issue</strong>: Shell filtering pipelines are getting too complex</p>
<p><strong>Solution</strong>: Extract complex filtering to separate shell scripts:</p>
<pre><code class="language-yaml"># Create a helper script
- shell: |
    cat &gt; /tmp/filter-rust.sh &lt;&lt;'EOF'
    #!/bin/bash
    echo "$1" | tr ' ' '\n' | grep '\.rs$' | tr '\n' ' '
    EOF
    chmod +x /tmp/filter-rust.sh

# Use the helper
- shell: |
    rust_files=$(/tmp/filter-rust.sh "${step.files_changed}")
    if [ -n "$rust_files" ]; then
      cargo clippy $rust_files
    fi
</code></pre>
<h2 id="future-features"><a class="header" href="#future-features">Future Features</a></h2>
<p>The git context infrastructure includes methods that are not yet exposed to workflows. These are planned for future releases:</p>
<h3 id="pattern-filtering-planned"><a class="header" href="#pattern-filtering-planned">Pattern Filtering (Planned)</a></h3>
<p>The <code>GitChangeTracker::resolve_variable()</code> method (src/cook/workflow/git_context.rs:489-505) supports pattern filtering, but it’s not currently called during workflow execution.</p>
<p><strong>Planned syntax</strong>:</p>
<pre><code class="language-yaml"># Not yet implemented - planned for future release
- shell: "echo ${step.files_changed:*.rs}"
- shell: "echo ${workflow.files_modified:src/**/*.rs}"
</code></pre>
<p>Currently variables are pre-formatted as space-separated strings during interpolation context creation (src/cook/workflow/executor/context.rs:106-172).</p>
<h3 id="format-modifiers-planned"><a class="header" href="#format-modifiers-planned">Format Modifiers (Planned)</a></h3>
<p>The <code>GitChangeTracker::format_file_list()</code> method (src/cook/workflow/git_context.rs:477-486) supports JSON, newline, and CSV formats, but it’s not used during variable resolution.</p>
<p><strong>Planned syntax</strong>:</p>
<pre><code class="language-yaml"># Not yet implemented - planned for future release
- shell: "echo ${step.files_added:json}"
- shell: "echo ${workflow.files_changed:lines}"
- shell: "echo ${step.files_modified:csv}"
</code></pre>
<h3 id="implementation-note"><a class="header" href="#implementation-note">Implementation Note</a></h3>
<p>To enable these features, the interpolation engine would need to support custom resolvers that call <code>git_tracker.resolve_variable()</code> instead of using pre-formatted string values. This would allow runtime formatting and filtering based on variable modifier syntax.</p>
<p><strong>Until then</strong>, use shell post-processing as documented in <a href="git-context-advanced.html#shell-based-filtering-and-formatting">Shell-Based Filtering and Formatting</a>.</p>
<h2 id="see-also-23"><a class="header" href="#see-also-23">See Also</a></h2>
<ul>
<li><a href="variables.html">Variables and Interpolation</a> - Basic variable usage and interpolation syntax</li>
<li><a href="workflow-basics.html">Workflow Basics</a> - Git integration fundamentals and workflow structure</li>
<li><a href="mapreduce/index.html">MapReduce Workflows</a> - Using git context in parallel jobs</li>
<li><a href="conditionals.html">Conditional Execution</a> - Using git context with <code>when:</code> conditions</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="workflow-composition"><a class="header" href="#workflow-composition">Workflow Composition</a></h1>
<p>Prodigy provides powerful composition features that enable building complex workflows from reusable components. This chapter covers importing workflows, using templates, defining parameters, and composing workflows through inheritance.</p>
<blockquote>
<p><strong>⚠️ Implementation Status</strong></p>
<p>Workflow composition is currently in <strong>phased implementation</strong>. The core composition engine and template system are fully implemented and tested, but integration with workflow execution varies by feature:</p>
<p><strong>✅ What Works Today:</strong></p>
<ul>
<li>Template management via <code>prodigy template</code> CLI commands (register, list, show, delete, etc.)</li>
<li>Programmatic workflow composition using <code>WorkflowComposer</code> API</li>
<li>Parameter validation with type checking</li>
<li>Template registry storage and retrieval (<code>~/.prodigy/templates/</code>)</li>
</ul>
<p><strong>⏳ Limited Integration:</strong></p>
<ul>
<li>Using imports, extends, and templates in <code>prodigy run</code> workflows (detection works, execution integration is limited)</li>
<li>Composable workflow detection and parsing (functional but not extensively tested end-to-end)</li>
</ul>
<p><strong>❌ Not Yet Implemented:</strong></p>
<ul>
<li>Sub-workflow execution (types defined, executor is placeholder)</li>
<li>MapReduce workflow composition</li>
<li><code>prodigy compose</code> command</li>
</ul>
<p>See the <a href="composition/index.html#implementation-roadmap">Implementation Roadmap</a> section below for details.</p>
</blockquote>
<h2 id="overview-8"><a class="header" href="#overview-8">Overview</a></h2>
<p>Workflow composition allows you to:</p>
<ul>
<li><strong>Import</strong> shared workflow configurations from other files</li>
<li><strong>Extend</strong> base workflows to inherit common configurations</li>
<li><strong>Use templates</strong> from a registry for standardized patterns</li>
<li><strong>Define parameters</strong> with type validation for flexible workflows</li>
<li><strong>Execute sub-workflows</strong> in parallel or sequentially (planned)</li>
<li><strong>Set defaults</strong> for common parameter values</li>
</ul>
<p>These features promote code reuse, maintainability, and consistency across your automation workflows.</p>
<h3 id="when-to-use-composition"><a class="header" href="#when-to-use-composition">When to Use Composition</a></h3>
<p>Composition features are most valuable when:</p>
<ol>
<li><strong>Multiple projects share common workflow patterns</strong> - Standardize CI/CD, deployment, or testing workflows across teams</li>
<li><strong>Workflows need environment-specific parameterization</strong> - Same workflow logic with different configurations for dev/staging/prod</li>
<li><strong>Building a library of reusable components</strong> - Create organizational workflow templates for consistent practices</li>
</ol>
<p>For simple, project-specific workflows, direct YAML without composition is often clearer and easier to maintain.</p>
<h2 id="workflow-imports"><a class="header" href="#workflow-imports">Workflow Imports</a></h2>
<p>Import external workflow files to reuse configurations and share common patterns across multiple workflows. Imports allow you to reference workflows from other files and incorporate them into your current workflow with optional aliasing and selective field imports.</p>
<blockquote>
<p><strong>📝 Note on Usage</strong></p>
<p>The examples below show composition syntax in workflow YAML files. While the core composition logic is fully implemented, integration with <code>prodigy run</code> is limited. For production use today, the recommended approach is using the <a href="composition/index.html#template-system-cli">Template System</a> via <code>prodigy template</code> commands.</p>
<p>The syntax shown is validated and supported by the composition engine but may have limited end-to-end testing in workflow execution. See <a href="composition/index.html#implementation-roadmap">Implementation Roadmap</a> for current status.</p>
</blockquote>
<h3 id="basic-import-syntax"><a class="header" href="#basic-import-syntax">Basic Import Syntax</a></h3>
<pre><code class="language-yaml">name: my-workflow
mode: standard

imports:
  # Simple import - loads entire workflow
  - path: "workflows/common-setup.yml"

  # Import with alias for namespacing
  - path: "workflows/deployment.yml"
    alias: "prod-deploy"

  # Selective import - only import specific workflows
  - path: "workflows/utilities.yml"
    selective:
      - "test-runner"
      - "linter"
</code></pre>
<h3 id="import-fields"><a class="header" href="#import-fields">Import Fields</a></h3>
<p>Each import can specify (defined in <code>WorkflowImport</code> struct, src/cook/workflow/composition/mod.rs:52-65):</p>
<ul>
<li><strong>path</strong> (required): Relative or absolute path to workflow file</li>
<li><strong>alias</strong> (optional): Namespace alias for imported workflows</li>
<li><strong>selective</strong> (optional): List of specific workflow names to import</li>
</ul>
<p><strong>Source</strong>: <code>WorkflowImport</code> struct in src/cook/workflow/composition/mod.rs:52-65
<strong>Test example</strong>: tests/workflow_composition_test.rs:95-106 shows import usage with both alias and selective fields</p>
<h3 id="how-imports-work"><a class="header" href="#how-imports-work">How Imports Work</a></h3>
<p>When a workflow is imported:</p>
<ol>
<li>The external file is loaded and parsed</li>
<li>If an alias is specified, imported content is namespaced under that alias</li>
<li>If selective is specified, only named workflows are included</li>
<li>Imported workflows are merged into the current workflow’s configuration</li>
<li>Circular dependencies are detected and prevented</li>
</ol>
<p><strong>Implementation</strong>: Import processing in src/cook/workflow/composition/composer.rs:98-133 (<code>process_imports</code> function)
<strong>Circular dependency detection</strong>: src/cook/workflow/composition/composer.rs:56 and validation in <code>validate_composition</code> (lines 259-273)</p>
<h3 id="use-cases-1"><a class="header" href="#use-cases-1">Use Cases</a></h3>
<p><strong>Shared Setup Steps:</strong></p>
<pre><code class="language-yaml"># common-setup.yml
setup:
  - shell: "npm install"
  - shell: "cargo build"

# main-workflow.yml
imports:
  - path: "common-setup.yml"

name: integration-tests
mode: standard
# Inherits setup steps from common-setup.yml
</code></pre>
<p><strong>Namespace Isolation:</strong></p>
<pre><code class="language-yaml">imports:
  - path: "prod-workflows.yml"
    alias: "production"
  - path: "staging-workflows.yml"
    alias: "staging"

# Reference as ${production.deploy} vs ${staging.deploy}
</code></pre>
<p><strong>Selective Imports:</strong></p>
<pre><code class="language-yaml"># Only import specific utilities, not entire file
imports:
  - path: "workflows/all-utilities.yml"
    selective:
      - "lint"
      - "format"
      - "test"
</code></pre>
<p>For more advanced composition patterns, see the <a href="composition/template-system.html">Template System</a> and <a href="composition/workflow-extension-inheritance.html">Workflow Extension</a> sections.</p>
<h2 id="template-system-cli"><a class="header" href="#template-system-cli">Template System CLI</a></h2>
<p>While full workflow composition integration is in progress, the template management system is <strong>fully functional</strong> and ready for production use. Templates provide a practical way to reuse workflow patterns today.</p>
<h3 id="managing-templates"><a class="header" href="#managing-templates">Managing Templates</a></h3>
<p>The <code>prodigy template</code> commands provide complete template lifecycle management:</p>
<p><strong>Register a template:</strong></p>
<pre><code class="language-bash">prodigy template register workflow.yml --name my-template \
  --description "CI pipeline for Rust projects" \
  --version 1.0.0 \
  --tags rust,ci,testing \
  --author "team@example.com"
</code></pre>
<p><strong>List available templates:</strong></p>
<pre><code class="language-bash"># List all templates
prodigy template list

# Long format with details
prodigy template list --long

# Filter by tag
prodigy template list --tag rust
</code></pre>
<p><strong>Show template details:</strong></p>
<pre><code class="language-bash">prodigy template show my-template
</code></pre>
<p><strong>Search templates:</strong></p>
<pre><code class="language-bash">prodigy template search "rust ci"
</code></pre>
<p><strong>Delete a template:</strong></p>
<pre><code class="language-bash">prodigy template delete my-template
</code></pre>
<p><strong>Validate template syntax:</strong></p>
<pre><code class="language-bash">prodigy template validate workflow.yml
</code></pre>
<p><strong>Initialize from template:</strong></p>
<pre><code class="language-bash">prodigy template init my-template --output new-workflow.yml
</code></pre>
<h3 id="template-storage"><a class="header" href="#template-storage">Template Storage</a></h3>
<p>Templates are stored in <code>~/.prodigy/templates/</code> with the following structure:</p>
<pre><code>~/.prodigy/templates/
├── my-template.yml
├── ci-pipeline.yml
├── deployment.yml
└── metadata/
    ├── my-template.json
    ├── ci-pipeline.json
    └── deployment.json
</code></pre>
<p><strong>Implementation</strong>: See <a href="composition/template-system.html">Template System</a> section for detailed template syntax and usage patterns.</p>
<p><strong>Source</strong>: Template CLI implementation in <code>src/cli/template.rs</code> (388 lines), wired in <code>src/cli/router.rs:199-234</code></p>
<h2 id="implementation-roadmap"><a class="header" href="#implementation-roadmap">Implementation Roadmap</a></h2>
<p>This section clarifies what’s implemented, what’s in progress, and what’s planned for workflow composition features.</p>
<h3 id="phase-1-core-composition-engine--complete"><a class="header" href="#phase-1-core-composition-engine--complete">Phase 1: Core Composition Engine (✅ Complete)</a></h3>
<p>The foundational composition system is fully implemented and tested:</p>
<p><strong>Core Types and Logic:</strong></p>
<ul>
<li><code>WorkflowComposer</code> - Main composition orchestration (<code>src/cook/workflow/composition/composer.rs</code>, 986 lines)</li>
<li><code>TemplateRegistry</code> - Template storage and retrieval (<code>src/cook/workflow/composition/registry.rs</code>, 779 lines)</li>
<li><code>ComposableWorkflow</code> - Type system with validation (<code>src/cook/workflow/composition/mod.rs</code>, 334 lines)</li>
<li>Parameter validation with type checking</li>
<li>Circular dependency detection</li>
<li>Template parameter interpolation</li>
</ul>
<p><strong>Quality Metrics:</strong></p>
<ul>
<li>2,300+ lines of core composition code</li>
<li>100+ unit tests</li>
<li>Zero <code>unwrap()</code> calls in production code</li>
<li>Full async/await support with tokio</li>
<li>Comprehensive error handling with <code>Result&lt;T&gt;</code></li>
</ul>
<p><strong>Test Coverage:</strong></p>
<ul>
<li><code>tests/workflow_composition_test.rs</code> - Integration tests with real workflows</li>
<li>Unit tests in each composition module</li>
<li>Parameter validation edge cases</li>
<li>Import circular dependency scenarios</li>
</ul>
<h3 id="phase-2-cli-and-template-management--complete"><a class="header" href="#phase-2-cli-and-template-management--complete">Phase 2: CLI and Template Management (✅ Complete)</a></h3>
<p>Template management commands are fully functional and production-ready:</p>
<p><strong>Template CLI Commands</strong> (<code>src/cli/template.rs</code>, 388 lines):</p>
<ul>
<li>✅ <code>prodigy template register</code> - Register templates with metadata</li>
<li>✅ <code>prodigy template list</code> - List templates with filtering</li>
<li>✅ <code>prodigy template show</code> - Display template details</li>
<li>✅ <code>prodigy template delete</code> - Remove templates</li>
<li>✅ <code>prodigy template search</code> - Search by name/tags</li>
<li>✅ <code>prodigy template validate</code> - Validate template syntax</li>
<li>✅ <code>prodigy template init</code> - Initialize from template</li>
</ul>
<p><strong>Template Storage:</strong></p>
<ul>
<li>File-based storage in <code>~/.prodigy/templates/</code></li>
<li>Metadata tracking (version, author, tags, timestamps)</li>
<li>Template caching for performance</li>
</ul>
<p><strong>CLI Integration:</strong></p>
<ul>
<li>Commands wired in <code>src/cli/router.rs:199-234</code></li>
<li>Argument parsing in <code>src/cli/args.rs:333-907</code></li>
<li>Proper error handling and user feedback</li>
</ul>
<h3 id="phase-3-workflow-execution-integration--partial"><a class="header" href="#phase-3-workflow-execution-integration--partial">Phase 3: Workflow Execution Integration (⏳ Partial)</a></h3>
<p>Integration with workflow execution has limited implementation:</p>
<p><strong>What’s Implemented:</strong></p>
<ul>
<li>Composable workflow detection (<code>src/cook/workflow/composer_integration.rs:43-90</code>)</li>
<li>Workflow parsing and conversion to <code>WorkflowConfig</code></li>
<li>Integration point in workflow loading (<code>src/cook/mod.rs:438-442</code>)</li>
<li>Parameter passing via <code>--param</code> and <code>--param-file</code> flags</li>
</ul>
<p><strong>What’s Limited:</strong></p>
<ul>
<li>End-to-end testing of composition in <code>prodigy run</code> workflows</li>
<li>MapReduce workflow composition (not implemented)</li>
<li>Sub-workflow execution (executor is placeholder, <code>src/cook/workflow/composition/sub_workflow.rs:233-239</code>)</li>
</ul>
<p><strong>Detection Logic:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/cook/workflow/composer_integration.rs
pub fn is_composable_workflow(yaml: &amp;str) -&gt; bool {
    // Detects: imports, template, extends, parameters
    yaml.contains("imports:")
        || yaml.contains("template:")
        || yaml.contains("extends:")
        || yaml.contains("parameters:")
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Integration Point:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/cook/mod.rs:438-442
if composer_integration::is_composable_workflow(&amp;content) {
    let composable = composer_integration::parse_composable_workflow(&amp;content)?;
    return Ok(composable.into());  // Converts to WorkflowConfig
}
<span class="boring">}</span></code></pre></pre>
<h3 id="phase-4-advanced-features--not-implemented"><a class="header" href="#phase-4-advanced-features--not-implemented">Phase 4: Advanced Features (❌ Not Implemented)</a></h3>
<p>Features planned but not yet started:</p>
<ul>
<li><strong>Sub-Workflow Execution</strong>: Types defined, executor needs implementation</li>
<li><strong>MapReduce Composition</strong>: Composition in MapReduce agent templates</li>
<li><strong><code>prodigy compose</code> Command</strong>: Dedicated composition command for testing</li>
<li><strong>URL-based Templates</strong>: Load templates from remote URLs</li>
<li><strong>Template Inheritance</strong>: Templates extending other templates</li>
<li><strong>Template Override Application</strong>: Override fields during composition (structure exists, application logic TODO)</li>
</ul>
<h3 id="current-recommendations"><a class="header" href="#current-recommendations">Current Recommendations</a></h3>
<p><strong>For Production Use Today:</strong></p>
<ol>
<li><strong>Use <code>prodigy template</code> commands</strong> for managing reusable workflows</li>
<li><strong>Register templates</strong> in <code>~/.prodigy/templates/</code> for your team</li>
<li><strong>Use template parameters</strong> for environment-specific configuration</li>
<li><strong>Keep workflows simple</strong> unless you need heavy parameterization</li>
</ol>
<p><strong>For Experimentation:</strong></p>
<ol>
<li><strong>Try composable workflow syntax</strong> in YAML files - detection and parsing work</li>
<li><strong>Report issues</strong> if composition doesn’t work as expected</li>
<li><strong>Contribute tests</strong> for end-to-end composition scenarios</li>
<li><strong>Review Specs 131-133</strong> for implementation progress tracking</li>
</ol>
<p><strong>What to Avoid:</strong></p>
<ol>
<li>❌ Don’t rely on sub-workflow execution (not implemented)</li>
<li>❌ Don’t use composition in MapReduce workflows (not supported)</li>
<li>❌ Don’t expect URL-based template loading (returns error)</li>
<li>❌ Don’t assume template override fields are applied (TODO)</li>
</ol>
<h3 id="contributing"><a class="header" href="#contributing">Contributing</a></h3>
<p>The composition system has excellent code quality and test coverage, making it approachable for contributions:</p>
<p><strong>Good First Issues:</strong></p>
<ul>
<li>Implement sub-workflow executor (placeholder exists at <code>src/cook/workflow/composition/sub_workflow.rs:233-239</code>)</li>
<li>Add end-to-end integration tests for composition in workflows</li>
<li>Implement template override application (<code>apply_overrides</code> function)</li>
<li>Add support for URL-based template loading</li>
</ul>
<p><strong>Code Quality Standards:</strong></p>
<ul>
<li>No <code>unwrap()</code> in production code (use <code>?</code> operator with <code>Result</code>)</li>
<li>Comprehensive error messages with context</li>
<li>Unit tests for all new functionality</li>
<li>Integration tests for user-facing features</li>
</ul>
<p><strong>Source References:</strong></p>
<ul>
<li><strong>Specs</strong>: Look for Spec 131-133 in project documentation</li>
<li><strong>Core Implementation</strong>: <code>src/cook/workflow/composition/</code></li>
<li><strong>CLI Integration</strong>: <code>src/cli/template.rs</code>, <code>src/cli/router.rs</code></li>
<li><strong>Tests</strong>: <code>tests/workflow_composition_test.rs</code></li>
</ul>
<h2 id="additional-topics-6"><a class="header" href="#additional-topics-6">Additional Topics</a></h2>
<p>See also:</p>
<ul>
<li><a href="composition/workflow-extension-inheritance.html">Workflow Extension (Inheritance)</a></li>
<li><a href="composition/template-system.html">Template System</a></li>
<li><a href="composition/parameter-definitions.html">Parameter Definitions</a></li>
<li><a href="composition/default-values.html">Default Values</a></li>
<li><a href="composition/sub-workflows.html">Sub-Workflows</a></li>
<li><a href="composition/composition-metadata.html">Composition Metadata</a></li>
<li><a href="composition/complete-examples.html">Complete Examples</a></li>
<li><a href="composition/best-practices.html">Best Practices</a></li>
<li><a href="composition/troubleshooting.html">Troubleshooting</a></li>
<li><a href="composition/related-chapters.html">Related Chapters</a></li>
<li><a href="composition/further-reading.html">Further Reading</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="workflow-extension-inheritance"><a class="header" href="#workflow-extension-inheritance">Workflow Extension (Inheritance)</a></h2>
<p>Extend a base workflow to inherit its configuration. Child workflows override parent values, allowing you to customize specific aspects while maintaining common configuration. This enables environment-specific variations and layered configuration management.</p>
<h3 id="basic-extension-syntax"><a class="header" href="#basic-extension-syntax">Basic Extension Syntax</a></h3>
<pre><code class="language-yaml"># production.yml
name: production-deployment
mode: standard

# Inherit from base workflow
extends: "base-deployment.yml"

# Override specific values
env:
  ENVIRONMENT: "production"
  REPLICAS: "5"
</code></pre>
<h3 id="how-extension-works"><a class="header" href="#how-extension-works">How Extension Works</a></h3>
<p>When a workflow extends a base workflow:</p>
<ol>
<li><strong>Base workflow is loaded</strong> from the specified path</li>
<li><strong>Child values override parent</strong> for matching keys</li>
<li><strong>Parent values are preserved</strong> where child doesn’t override</li>
<li><strong>Merging is deep</strong> - nested objects merge recursively</li>
<li><strong>Arrays are replaced</strong> - child arrays replace parent arrays entirely</li>
</ol>
<h3 id="extension-vs-imports"><a class="header" href="#extension-vs-imports">Extension vs Imports</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Feature</th><th>Extension (<code>extends</code>)</th><th>Imports</th></tr></thead><tbody>
<tr><td><strong>Purpose</strong></td><td>Inherit and customize base workflow</td><td>Reuse workflow components</td></tr>
<tr><td><strong>Relationship</strong></td><td>Parent-child hierarchy</td><td>Modular composition</td></tr>
<tr><td><strong>Override behavior</strong></td><td>Child overrides parent</td><td>Imports merge with main</td></tr>
<tr><td><strong>Use case</strong></td><td>Environment variations</td><td>Shared utilities</td></tr>
</tbody></table>
</div>
<h3 id="multi-environment-example"><a class="header" href="#multi-environment-example">Multi-Environment Example</a></h3>
<p><strong>base-deployment.yml</strong> (shared configuration):</p>
<pre><code class="language-yaml">name: base-deployment
mode: standard

env:
  APP_NAME: "my-service"
  REPLICAS: "1"
  LOG_LEVEL: "info"

commands:
  - shell: "docker build -t ${APP_NAME}:${VERSION} ."
  - shell: "kubectl apply -f k8s/${ENVIRONMENT}/deployment.yml"
  - shell: "kubectl scale deployment ${APP_NAME} --replicas=${REPLICAS}"
</code></pre>
<p><strong>dev.yml</strong> (development environment):</p>
<pre><code class="language-yaml">name: dev-deployment
extends: "base-deployment.yml"

env:
  ENVIRONMENT: "dev"
  REPLICAS: "1"
  LOG_LEVEL: "debug"

# Inherits all commands from base
</code></pre>
<p><strong>staging.yml</strong> (staging environment):</p>
<pre><code class="language-yaml">name: staging-deployment
extends: "base-deployment.yml"

env:
  ENVIRONMENT: "staging"
  REPLICAS: "3"
  LOG_LEVEL: "info"

# Additional staging-specific commands
commands:
  - shell: "run-smoke-tests.sh"
</code></pre>
<p><strong>production.yml</strong> (production environment):</p>
<pre><code class="language-yaml">name: production-deployment
extends: "base-deployment.yml"

env:
  ENVIRONMENT: "production"
  REPLICAS: "5"
  LOG_LEVEL: "warn"
  ENABLE_MONITORING: "true"

# Additional production safeguards
commands:
  - shell: "verify-release-notes.sh"
  - shell: "notify-team 'Deploying to production'"
</code></pre>
<h3 id="merge-behavior"><a class="header" href="#merge-behavior">Merge Behavior</a></h3>
<p><strong>Scalar Values</strong> - Child replaces parent:</p>
<pre><code class="language-yaml"># base.yml
timeout: 300

# child.yml
extends: "base.yml"
timeout: 600  # Replaces 300 with 600
</code></pre>
<p><strong>Objects</strong> - Deep merge:</p>
<pre><code class="language-yaml"># base.yml
env:
  APP_NAME: "service"
  LOG_LEVEL: "info"

# child.yml
extends: "base.yml"
env:
  LOG_LEVEL: "debug"  # Overrides
  NEW_VAR: "value"     # Adds

# Result:
env:
  APP_NAME: "service"     # From base
  LOG_LEVEL: "debug"      # Overridden
  NEW_VAR: "value"        # Added
</code></pre>
<p><strong>Environment Variables</strong> - Deep merge with selective override:</p>
<pre><code class="language-yaml"># base.yml
env:
  APP_NAME: "service"
  LOG_LEVEL: "info"
  REPLICAS: "3"

# child.yml
extends: "base.yml"
env:
  LOG_LEVEL: "debug"  # Overrides parent value
  NEW_VAR: "value"     # Adds new variable

# Result:
env:
  APP_NAME: "service"     # Preserved from parent
  LOG_LEVEL: "debug"      # Overridden by child
  REPLICAS: "3"           # Preserved from parent
  NEW_VAR: "value"        # Added by child
</code></pre>
<p><strong>Source</strong>: Environment variables inherit the object deep merge behavior (src/cook/workflow/composition/composer.rs:325-355)</p>
<p><strong>Arrays (Commands)</strong> - Child replaces parent:</p>
<pre><code class="language-yaml"># base.yml
commands:
  - shell: "step1"
  - shell: "step2"

# child.yml
extends: "base.yml"
commands:
  - shell: "custom-step"  # Completely replaces base commands

# Result: Only custom-step runs
</code></pre>
<p><strong>Source</strong>: In inheritance mode (<code>extends</code>), non-empty child command arrays replace parent arrays entirely (src/cook/workflow/composition/composer.rs:335-337)</p>
<p><strong>Note</strong>: This behavior differs from imports, where <code>merge_workflows</code> extends arrays instead of replacing them (src/cook/workflow/composition/composer.rs:275-323). See <a href="composition/index.html#workflow-imports">Workflow Imports</a> for comparison.</p>
<h3 id="layered-extension"><a class="header" href="#layered-extension">Layered Extension</a></h3>
<p>Workflows can extend workflows that themselves extend other workflows:</p>
<pre><code class="language-yaml"># base.yml
name: base-config
timeout: 300

# intermediate.yml
extends: "base.yml"
timeout: 600
max_parallel: 5

# final.yml
extends: "intermediate.yml"
max_parallel: 10

# Result: timeout=600 (from intermediate), max_parallel=10 (from final)
</code></pre>
<h3 id="path-resolution"><a class="header" href="#path-resolution">Path Resolution</a></h3>
<p>When resolving base workflow paths, Prodigy searches the following locations in order:</p>
<ol>
<li><code>./bases/{name}.yml</code></li>
<li><code>./templates/{name}.yml</code></li>
<li><code>./workflows/{name}.yml</code></li>
<li><code>./{name}.yml</code> (current directory)</li>
</ol>
<p><strong>Source</strong>: Path resolution implementation in src/cook/workflow/composition/composer.rs:625-642</p>
<p>Extension paths can be:</p>
<ul>
<li><strong>Relative</strong>: Resolved from workflow file’s directory</li>
<li><strong>Absolute</strong>: Full filesystem path</li>
<li><strong>Registry</strong>: Future support for template registry paths</li>
</ul>
<pre><code class="language-yaml"># Name-based lookup (searches standard directories)
extends: "base-deployment"  # Searches bases/, templates/, workflows/, ./

# Relative path
extends: "../shared/base.yml"

# Absolute path
extends: "/etc/prodigy/workflows/base.yml"
</code></pre>
<p><strong>Search Order Example:</strong></p>
<pre><code class="language-bash"># For extends: "ci-base"
# Prodigy searches:
./bases/ci-base.yml       # First
./templates/ci-base.yml   # Second
./workflows/ci-base.yml   # Third
./ci-base.yml             # Fourth (current directory)
# Error if not found in any location
</code></pre>
<h3 id="use-cases-2"><a class="header" href="#use-cases-2">Use Cases</a></h3>
<p><strong>Environment-Specific Deployments:</strong></p>
<ul>
<li>Share common deployment steps</li>
<li>Override environment variables per environment</li>
<li>Customize resource limits (replicas, memory, CPU)</li>
</ul>
<p><strong>Testing Variations:</strong></p>
<pre><code class="language-yaml"># base-test.yml
name: base-test
commands:
  - shell: "cargo build"
  - shell: "cargo test"

# integration-test.yml
extends: "base-test.yml"
env:
  DATABASE_URL: "postgres://localhost/test"
commands:
  - shell: "setup-test-db.sh"
  # Runs instead of base commands

# unit-test.yml
extends: "base-test.yml"
env:
  RUST_TEST_THREADS: "1"
</code></pre>
<p><strong>Progressive Configuration:</strong></p>
<ul>
<li>Start with minimal base config</li>
<li>Add complexity in child workflows</li>
<li>Keep each layer focused on specific concerns</li>
</ul>
<h3 id="circular-dependency-protection"><a class="header" href="#circular-dependency-protection">Circular Dependency Protection</a></h3>
<p>Prodigy detects and prevents circular dependencies:</p>
<pre><code class="language-yaml"># workflow-a.yml
extends: "workflow-b.yml"

# workflow-b.yml
extends: "workflow-a.yml"

# Error: Circular dependency detected
</code></pre>
<h3 id="complete-example-9"><a class="header" href="#complete-example-9">Complete Example</a></h3>
<p><strong>base-ci.yml:</strong></p>
<pre><code class="language-yaml">name: base-ci
mode: standard

env:
  RUST_BACKTRACE: "1"

commands:
  - shell: "cargo fmt --check"
  - shell: "cargo clippy"
  - shell: "cargo test"
</code></pre>
<p><strong>pr-ci.yml</strong> (runs on pull requests):</p>
<pre><code class="language-yaml">name: pr-ci
extends: "base-ci.yml"

env:
  CARGO_INCREMENTAL: "0"  # Faster CI builds

# Inherits format, clippy, test from base
</code></pre>
<p><strong>release-ci.yml</strong> (runs on release):</p>
<pre><code class="language-yaml">name: release-ci
extends: "base-ci.yml"

env:
  CARGO_INCREMENTAL: "0"

commands:
  - shell: "cargo build --release"
  - shell: "cargo test --release"
  - shell: "cargo publish --dry-run"
</code></pre>
<h3 id="debugging-extensions"><a class="header" href="#debugging-extensions">Debugging Extensions</a></h3>
<p><strong>Current Method</strong> - Enable verbose logging to see composition details:</p>
<pre><code class="language-bash"># Use -vvv for trace-level logging showing composition process
prodigy run workflow.yml -vvv

# Combine with --dry-run to preview without execution
prodigy run workflow.yml --dry-run -vvv
</code></pre>
<p>Verbose logging output includes:</p>
<ul>
<li>Base workflow loading events</li>
<li>Inheritance chain resolution</li>
<li>Merge operations for each configuration section</li>
<li>Circular dependency checks</li>
<li>Path resolution steps</li>
</ul>
<p><strong>Source</strong>: Composition events logged via <code>tracing::debug!()</code> macros throughout src/cook/workflow/composition/composer.rs</p>
<p><strong>Planned Feature</strong> - Dedicated composition inspection flag (not yet implemented):</p>
<pre><code class="language-bash"># Future: --show-composition flag (Spec 131-133)
prodigy run workflow.yml --dry-run --show-composition
</code></pre>
<p>This will display structured composition metadata including:</p>
<ul>
<li>Sources and dependency types</li>
<li>Complete inheritance chain</li>
<li>Applied parameters and templates</li>
<li>Resolved paths for all dependencies</li>
</ul>
<p>See <a href="composition/composition-metadata.html">Composition Metadata</a> for details on the metadata structure.</p>
<h3 id="implementation-status"><a class="header" href="#implementation-status">Implementation Status</a></h3>
<ul>
<li>✅ Base workflow loading</li>
<li>✅ Deep merge of child and parent configurations</li>
<li>✅ Circular dependency detection</li>
<li>✅ Path resolution (relative and absolute)</li>
<li>✅ Composition metadata tracking</li>
<li>✅ Workflow caching - Files are cached during composition to improve performance and avoid redundant file system reads (src/cook/workflow/composition/composer.rs:662-698)</li>
</ul>
<h3 id="related-topics-1"><a class="header" href="#related-topics-1">Related Topics</a></h3>
<ul>
<li><a href="composition/index.html#workflow-imports">Workflow Imports</a> - Modular composition</li>
<li><a href="composition/template-system.html">Template System</a> - Parameterized workflows</li>
<li><a href="composition/composition-metadata.html">Composition Metadata</a> - Inspect composition details</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="template-system"><a class="header" href="#template-system">Template System</a></h2>
<p>Templates provide reusable workflow patterns that can be instantiated with different parameters. Templates can be stored in a registry or loaded from files, enabling standardized workflows across teams and projects.</p>
<p><strong>Source</strong>: Implemented in src/cook/workflow/composition/mod.rs and src/cook/workflow/composition/registry.rs</p>
<h3 id="template-basics"><a class="header" href="#template-basics">Template Basics</a></h3>
<p>A template is a reusable workflow definition that can be parameterized and instantiated multiple times with different values. Templates support:</p>
<ul>
<li>Registry-based or file-based storage</li>
<li>Parameter substitution</li>
<li>Field overrides</li>
<li>Metadata and versioning</li>
</ul>
<h3 id="template-configuration"><a class="header" href="#template-configuration">Template Configuration</a></h3>
<p>Templates are defined using the <code>WorkflowTemplate</code> struct (src/cook/workflow/composition/mod.rs:67-83):</p>
<pre><code class="language-yaml">template:
  # Template name (for identification)
  name: "standard-ci"

  # Template source (see Template Sources below)
  source: "template-name"  # or { file: "path.yml" }

  # Parameter values to pass to template
  with:
    param1: "value1"
    param2: "value2"

  # Override specific template fields
  override:
    timeout: 600
    max_parallel: 5
</code></pre>
<p><strong>Source</strong>: Field definitions from <code>WorkflowTemplate</code> struct in src/cook/workflow/composition/mod.rs:67-83</p>
<h3 id="template-sources"><a class="header" href="#template-sources">Template Sources</a></h3>
<p>Prodigy uses an untagged enum for <code>TemplateSource</code> (src/cook/workflow/composition/mod.rs:85-95), which means the YAML format varies based on the source type:</p>
<h4 id="registry-lookup-string"><a class="header" href="#registry-lookup-string">Registry Lookup (String)</a></h4>
<p>Load a template by name from the template registry:</p>
<pre><code class="language-yaml">template:
  name: "ci-pipeline"
  source: "standard-ci"  # Simple string = registry lookup
  with:
    project_name: "my-project"
</code></pre>
<p><strong>Source</strong>: <code>TemplateSource::Registry(String)</code> variant in src/cook/workflow/composition/mod.rs:92</p>
<p><strong>How it works</strong>: When the source is a plain string, Prodigy looks up the template in the registry at <code>~/.prodigy/templates/</code> or <code>.prodigy/templates/</code>. Example from test: tests/workflow_composition_test.rs:125-133</p>
<h4 id="file-path"><a class="header" href="#file-path">File Path</a></h4>
<p>Load a template from a file:</p>
<pre><code class="language-yaml">template:
  name: "deployment"
  source:
    file: "templates/k8s-deploy.yml"  # File path in struct format
  with:
    environment: "production"
</code></pre>
<p><strong>Source</strong>: <code>TemplateSource::File(PathBuf)</code> variant in src/cook/workflow/composition/mod.rs:90</p>
<p><strong>How it works</strong>: When the source uses a <code>file</code> field, Prodigy loads the template from the specified file path. Paths can be relative (to workflow file) or absolute.</p>
<h4 id="url-planned"><a class="header" href="#url-planned">URL (Planned)</a></h4>
<p>Load a template from a remote URL:</p>
<pre><code class="language-yaml">template:
  name: "remote-template"
  source: "https://templates.example.com/ci.yml"  # String starting with https://
  with:
    config: "production"
</code></pre>
<p><strong>Source</strong>: <code>TemplateSource::Url(String)</code> variant in src/cook/workflow/composition/mod.rs:94</p>
<p><strong>Status</strong>: Currently returns an error. Planned for future implementation. See src/cook/workflow/composition/composer.rs for URL handling code.</p>
<h3 id="template-registry"><a class="header" href="#template-registry">Template Registry</a></h3>
<p>The template registry stores reusable workflow templates. Templates can be stored in two locations (similar to git worktrees):</p>
<p><strong>Registry Locations:</strong></p>
<pre><code># Local (project-specific)
.prodigy/templates/
├── project-ci.yml
├── custom-deployment.yml
└── ...

# Global (user-wide)
~/.prodigy/templates/
├── standard-ci.yml
├── deployment-pipeline.yml
├── test-suite.yml
└── ...
</code></pre>
<p><strong>Implementation</strong>: <code>FileTemplateStorage</code> in src/cook/workflow/composition/registry.rs:26-29 uses a configurable base directory (defaults to “templates”).</p>
<h4 id="programmatic-registration"><a class="header" href="#programmatic-registration">Programmatic Registration</a></h4>
<p>Register templates using the API (src/cook/workflow/composition/registry.rs:41-73):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use prodigy::cook::workflow::composition::registry::TemplateRegistry;

let registry = TemplateRegistry::new();

// Basic registration
registry
    .register_template("ci-pipeline".to_string(), template)
    .await?;

// Registration with metadata
registry
    .register_template_with_metadata(
        "deployment".to_string(),
        template,
        metadata
    )
    .await?;
<span class="boring">}</span></code></pre></pre>
<p><strong>Source</strong>: Example from tests/workflow_composition_test.rs:177-199</p>
<h4 id="manual-registry-management"><a class="header" href="#manual-registry-management">Manual Registry Management</a></h4>
<pre><code class="language-bash"># Copy template to local registry
cp my-template.yml .prodigy/templates/my-template.yml

# Copy to global registry (user-wide)
cp my-template.yml ~/.prodigy/templates/my-template.yml

# Templates are automatically discovered by filename
</code></pre>
<h3 id="template-parameters"><a class="header" href="#template-parameters">Template Parameters</a></h3>
<p>Templates can define parameters that are substituted when the template is instantiated. See <a href="composition/parameter-definitions.html">Parameter Definitions</a> for detailed parameter syntax and validation.</p>
<p><strong>Template with Parameters:</strong></p>
<pre><code class="language-yaml"># templates/deployment.yml
name: deployment-template

parameters:
  required:
    - environment
    - version
  optional:
    - timeout

commands:
  - shell: "deploy --env ${environment} --version ${version}"
  - shell: "verify-deployment ${environment}"
</code></pre>
<p><strong>Using Parameterized Template:</strong></p>
<pre><code class="language-yaml">template:
  source:
    file: "templates/deployment.yml"
  with:
    environment: "staging"
    version: "2.0.0"
    timeout: "300"
</code></pre>
<h3 id="template-metadata"><a class="header" href="#template-metadata">Template Metadata</a></h3>
<p>Templates can include metadata for better organization and discovery (src/cook/workflow/composition/registry.rs:475-508):</p>
<p><strong>Metadata Fields:</strong></p>
<ul>
<li><code>description</code>: Human-readable description</li>
<li><code>author</code>: Template author</li>
<li><code>version</code>: Semantic version string</li>
<li><code>tags</code>: List of categorization tags</li>
<li><code>created_at</code>: Creation timestamp</li>
<li><code>updated_at</code>: Last modification timestamp</li>
</ul>
<p><strong>Metadata Storage:</strong>
Templates registered with metadata store an additional <code>.meta.json</code> file alongside the template YAML. For example, <code>standard-ci.yml</code> has metadata in <code>standard-ci.yml.meta.json</code>.</p>
<p><strong>Source</strong>: <code>TemplateMetadata</code> struct in src/cook/workflow/composition/registry.rs:475-495</p>
<h3 id="template-discovery"><a class="header" href="#template-discovery">Template Discovery</a></h3>
<p>The registry provides search and listing capabilities (src/cook/workflow/composition/registry.rs:156-168):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// List all templates
let templates = registry.list().await?;

// Search by tags
let ci_templates = registry
    .search_by_tags(&amp;["ci".to_string(), "testing".to_string()])
    .await?;

// Get specific template
let template = registry.get("standard-ci").await?;

// Delete template
registry.delete("old-template").await?;
<span class="boring">}</span></code></pre></pre>
<p><strong>Source</strong>: Methods in <code>TemplateRegistry</code> implementation</p>
<h3 id="template-caching"><a class="header" href="#template-caching">Template Caching</a></h3>
<p>Prodigy caches loaded templates to improve performance:</p>
<ul>
<li>Templates are loaded once and reused across workflow executions</li>
<li>Registry templates are cached until the registry is updated</li>
<li>In-memory cache stored in <code>Arc&lt;RwLock&lt;HashMap&gt;&gt;</code> (src/cook/workflow/composition/registry.rs:14)</li>
</ul>
<p><strong>File Change Detection</strong>: The documentation previously claimed file-based templates are re-read on file changes, but this is not currently implemented in the caching layer. Templates are cached for the lifetime of the registry instance.</p>
<h3 id="template-override"><a class="header" href="#template-override">Template Override</a></h3>
<p>The <code>override</code> field allows you to override specific template fields without modifying the template file:</p>
<pre><code class="language-yaml">template:
  source: "standard-workflow"
  override:
    timeout: 1200  # Override default timeout
    max_parallel: 10  # Override concurrency limit
</code></pre>
<p><strong>Implementation Status</strong>: The <code>override_field</code> is defined in the <code>WorkflowTemplate</code> struct (src/cook/workflow/composition/mod.rs:80-83) and is properly deserialized, but the application logic in <code>apply_overrides()</code> is not yet implemented. The field is validated and stored but not applied during workflow composition.</p>
<p><strong>Source</strong>: See <code>override_field</code> in src/cook/workflow/composition/mod.rs:81-82</p>
<h3 id="use-cases-3"><a class="header" href="#use-cases-3">Use Cases</a></h3>
<p><strong>Standardized CI/CD Pipelines:</strong></p>
<pre><code class="language-yaml"># Use company-wide CI template
template:
  source: "company-ci-pipeline"
  with:
    project_type: "rust"
    test_coverage: "80"
    deploy_targets: ["staging", "production"]
</code></pre>
<p><strong>Environment-Specific Deployments:</strong></p>
<pre><code class="language-yaml"># Reuse deployment template with different params
template:
  source:
    file: "templates/k8s-deploy.yml"
  with:
    cluster: "us-west-2"
    namespace: "production"
    replicas: "5"
</code></pre>
<p><strong>Testing Workflow Variations:</strong></p>
<pre><code class="language-yaml"># Test different configurations using same template
template:
  source:
    file: "templates/integration-tests.yml"
  with:
    database: "postgres"
    cache: "redis"
    message_queue: "rabbitmq"
</code></pre>
<h3 id="implementation-status-1"><a class="header" href="#implementation-status-1">Implementation Status</a></h3>
<ul>
<li>✅ File-based template loading</li>
<li>✅ Registry template storage and retrieval</li>
<li>✅ Template parameter validation</li>
<li>✅ Template caching (in-memory)</li>
<li>✅ Template metadata and versioning</li>
<li>✅ Template search and discovery</li>
<li>✅ Programmatic registration API</li>
<li>⏳ URL-based template loading (returns error, planned for future)</li>
<li>⏳ Template override application (field exists but not applied in compose())</li>
<li>⏳ File modification detection for cache invalidation</li>
</ul>
<h3 id="related-topics-2"><a class="header" href="#related-topics-2">Related Topics</a></h3>
<ul>
<li><a href="composition/parameter-definitions.html">Parameter Definitions</a> - Define and validate template parameters</li>
<li><a href="composition/workflow-extension-inheritance.html">Workflow Extension</a> - Inherit from base workflows</li>
<li><a href="composition/default-values.html">Default Values</a> - Set default parameter values</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="parameter-definitions"><a class="header" href="#parameter-definitions">Parameter Definitions</a></h2>
<p>Define parameters with type validation to create flexible, reusable workflows. Parameters enable workflows and templates to accept inputs with enforced types, default values, and validation rules.</p>
<h3 id="basic-parameter-definition"><a class="header" href="#basic-parameter-definition">Basic Parameter Definition</a></h3>
<pre><code class="language-yaml">name: deployment-workflow

parameters:
  # Required parameters (must be provided)
  required:
    - environment
    - version

  # Optional parameters (have defaults or can be omitted)
  optional:
    - debug_mode
    - timeout
</code></pre>
<h3 id="parameter-structure"><a class="header" href="#parameter-structure">Parameter Structure</a></h3>
<p>Parameters are organized into <code>required</code> and <code>optional</code> arrays. Each parameter specifies its type, description, and validation rules:</p>
<pre><code class="language-yaml">parameters:
  required:
    - name: environment
      type: String
      description: "Target environment for deployment"
      validation: "matches('^(dev|staging|prod)$')"

    - name: version
      type: String
      description: "Application version to deploy"

  optional:
    - name: port
      type: Number
      description: "Server port number"
      default: 8080

    - name: enable_ssl
      type: Boolean
      description: "Enable SSL/TLS"
      default: true

    - name: allowed_hosts
      type: Array
      description: "List of allowed hostnames"
      default: ["localhost"]

    - name: config
      type: Object
      description: "Configuration object"
      default: {"timeout": 30}

    - name: data
      type: Any
      description: "Free-form data of any type"
</code></pre>
<p><strong>Source</strong>: <code>ParameterDefinitions</code> structure in src/cook/workflow/composition/mod.rs:97-107</p>
<h3 id="parameter-types"><a class="header" href="#parameter-types">Parameter Types</a></h3>
<p>Prodigy supports six parameter types with validation (defined in src/cook/workflow/composition/mod.rs:131-141):</p>
<div class="table-wrapper"><table><thead><tr><th>Type</th><th>Description</th><th>Example Values</th></tr></thead><tbody>
<tr><td><code>String</code></td><td>Text values</td><td><code>"production"</code>, <code>"v1.2.3"</code></td></tr>
<tr><td><code>Number</code></td><td>Integer or float</td><td><code>42</code>, <code>3.14</code>, <code>-100</code></td></tr>
<tr><td><code>Boolean</code></td><td>True or false</td><td><code>true</code>, <code>false</code></td></tr>
<tr><td><code>Array</code></td><td>List of values</td><td><code>[1, 2, 3]</code>, <code>["a", "b"]</code></td></tr>
<tr><td><code>Object</code></td><td>Key-value map</td><td><code>{"key": "value"}</code></td></tr>
<tr><td><code>Any</code></td><td>Any JSON value</td><td>Any valid JSON</td></tr>
</tbody></table>
</div>
<p><strong>Source</strong>: <code>ParameterType</code> enum in src/cook/workflow/composition/mod.rs:131-141</p>
<p><strong>Type Validation:</strong></p>
<ul>
<li>Type checking is enforced when parameters are provided (src/cook/workflow/composition/mod.rs:226-280)</li>
<li>Mismatched types cause workflow validation errors</li>
<li><code>Any</code> type accepts any value without validation</li>
<li>Validation logic in <code>validate_parameters</code> function</li>
</ul>
<p><strong>Test example</strong>: tests/workflow_composition_test.rs:49-79 demonstrates parameter validation with String type</p>
<h3 id="default-values-3"><a class="header" href="#default-values-3">Default Values</a></h3>
<p>Parameters can specify default values used when no value is provided. Defaults can be set at two levels:</p>
<p><strong>Parameter-Level Defaults</strong> (in parameter definition):</p>
<pre><code class="language-yaml">parameters:
  optional:
    - name: timeout
      type: Number
      description: "Operation timeout in seconds"
      default: 300

    - name: log_level
      type: String
      description: "Logging verbosity"
      default: "info"

    - name: retry_enabled
      type: Boolean
      description: "Enable retry logic"
      default: true
</code></pre>
<p><strong>Workflow-Level Defaults</strong> (applies to all sub-workflows):</p>
<pre><code class="language-yaml">name: parent-workflow

defaults:
  environment: "development"
  debug_mode: true
  timeout: 600

parameters:
  optional:
    - name: environment
      type: String
      default: "production"  # Overridden by workflow defaults

    - name: timeout
      type: Number
      default: 300  # Overridden by workflow defaults
</code></pre>
<p><strong>Source</strong>: <code>defaults</code> field in src/cook/workflow/composition/mod.rs:204, <code>default</code> field in src/cook/workflow/composition/mod.rs:123-124</p>
<h3 id="validation-expressions"><a class="header" href="#validation-expressions">Validation Expressions</a></h3>
<p>The <code>validation</code> field allows custom validation logic:</p>
<pre><code class="language-yaml">parameters:
  definitions:
    email:
      type: String
      validation: "matches('^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$')"

    priority:
      type: Number
      validation: "value &gt;= 1 &amp;&amp; value &lt;= 10"

    status:
      type: String
      validation: "in(['pending', 'active', 'completed'])"
</code></pre>
<p><em>Note: Validation expressions are currently stored and validated for syntax, but custom expression evaluation is not yet implemented. Type validation is fully functional.</em></p>
<h3 id="providing-parameter-values"><a class="header" href="#providing-parameter-values">Providing Parameter Values</a></h3>
<p><strong>Via Command Line:</strong></p>
<pre><code class="language-bash"># Individual parameters (with automatic type inference)
prodigy run workflow.yml --param environment=production --param timeout=600

# From JSON file
prodigy run workflow.yml --param-file params.json
</code></pre>
<p><strong>Automatic Type Inference:</strong></p>
<p>When using <code>--param</code> flags, Prodigy automatically infers parameter types:</p>
<ul>
<li><strong>Numbers</strong>: <code>--param port=8080</code> → parsed as Number (i64 or f64)</li>
<li><strong>Booleans</strong>: <code>--param debug=true</code> → parsed as Boolean</li>
<li><strong>Strings</strong>: <code>--param name=app</code> → parsed as String (default if no other type matches)</li>
</ul>
<pre><code class="language-bash"># These are automatically typed correctly:
prodigy run workflow.yml \
  --param port=8080 \           # Number
  --param timeout=30.5 \        # Number (float)
  --param debug=true \          # Boolean
  --param environment=prod      # String
</code></pre>
<p><strong>Source</strong>: <code>parse_param_value</code> function in src/cli/params.rs:51-72</p>
<p><strong>params.json:</strong></p>
<pre><code class="language-json">{
  "environment": "staging",
  "version": "2.1.0",
  "debug_mode": false,
  "timeout": 300
}
</code></pre>
<p><strong>Parameter Precedence:</strong></p>
<ol>
<li>CLI <code>--param</code> flags (highest priority)</li>
<li><code>--param-file</code> values</li>
<li>Workflow <code>defaults</code> values</li>
<li>Parameter <code>default</code> values (lowest priority)</li>
</ol>
<h3 id="using-parameters-in-workflows"><a class="header" href="#using-parameters-in-workflows">Using Parameters in Workflows</a></h3>
<p>Parameters are interpolated into commands using the standard variable interpolation system with <code>${param_name}</code> syntax. This is the same syntax used for all workflow variables (captured outputs, environment variables, etc.).</p>
<pre><code class="language-yaml">parameters:
  required:
    - name: app_name
      type: String
    - name: deploy_env
      type: String

commands:
  - shell: "echo Deploying ${app_name} to ${deploy_env}"
  - shell: "kubectl apply -f k8s/${deploy_env}/deployment.yml"
  - claude: "/deploy ${app_name} --environment ${deploy_env}"
</code></pre>
<p>Parameters are resolved during variable interpolation before command execution, making them available everywhere workflow variables are supported.</p>
<p><strong>Source</strong>: Variable interpolation system in src/cook/workflow/variables.rs</p>
<h3 id="complete-example-10"><a class="header" href="#complete-example-10">Complete Example</a></h3>
<pre><code class="language-yaml">name: database-migration
mode: standard

parameters:
  required:
    - name: database_url
      type: String
      description: "Database connection string"
      validation: "matches('^postgres://')"

    - name: migration_version
      type: String
      description: "Target migration version"

  optional:
    - name: dry_run
      type: Boolean
      description: "Run in dry-run mode"
      default: false

    - name: timeout
      type: Number
      description: "Migration timeout in seconds"
      default: 300

commands:
  - shell: "echo Running migration to ${migration_version}"
  - shell: |
      migrate --database-url ${database_url} \
              --target ${migration_version} \
              --timeout ${timeout} \
              $( [ "${dry_run}" = "true" ] &amp;&amp; echo "--dry-run" )
</code></pre>
<p><strong>Run with parameters:</strong></p>
<pre><code class="language-bash">prodigy run migration.yml \
  --param database_url="postgres://localhost/mydb" \
  --param migration_version="20250109_001" \
  --param dry_run=true
</code></pre>
<h3 id="parameter-validation-errors"><a class="header" href="#parameter-validation-errors">Parameter Validation Errors</a></h3>
<p>When validation fails, Prodigy provides clear error messages:</p>
<pre><code>Error: Parameter validation failed
  - 'environment': Expected String, got Number
  - 'port': Value 99999 exceeds valid range
  - 'config': Required parameter not provided
</code></pre>
<h3 id="implementation-status-2"><a class="header" href="#implementation-status-2">Implementation Status</a></h3>
<ul>
<li>✅ Parameter type definitions (all 6 types)</li>
<li>✅ Type validation enforcement</li>
<li>✅ Default value support</li>
<li>✅ Required/optional parameter tracking</li>
<li>✅ CLI parameter passing (–param, –param-file)</li>
<li>✅ Parameter precedence handling</li>
<li>⏳ Custom validation expression evaluation (field stored, not evaluated)</li>
</ul>
<h3 id="related-topics-3"><a class="header" href="#related-topics-3">Related Topics</a></h3>
<ul>
<li><a href="composition/template-system.html">Template System</a> - Use parameters in templates</li>
<li><a href="composition/default-values.html">Default Values</a> - Set workflow-level defaults</li>
<li><a href="composition/parameter-definitions.html#providing-parameter-values">Providing Parameter Values</a> - Command-line parameter usage</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="default-values-4"><a class="header" href="#default-values-4">Default Values</a></h2>
<p>Set default parameter values and environment variables at the workflow level. Defaults reduce required parameters and simplify workflow usage by providing sensible fallback values.</p>
<h3 id="basic-syntax-2"><a class="header" href="#basic-syntax-2">Basic Syntax</a></h3>
<pre><code class="language-yaml">name: my-workflow
mode: standard

defaults:
  timeout: 300
  retry_count: 3
  verbose: false
  environment: "development"
  log_level: "info"
</code></pre>
<h3 id="defaults-field-structure"><a class="header" href="#defaults-field-structure">Defaults Field Structure</a></h3>
<p>The <code>defaults</code> field is a HashMap&lt;String, Value&gt; that accepts any JSON-compatible values:</p>
<pre><code class="language-yaml">defaults:
  # String values
  environment: "staging"
  log_level: "debug"

  # Number values
  timeout: 600
  max_retries: 5

  # Boolean values
  dry_run: false
  enable_cache: true

  # Array values
  allowed_regions: ["us-west-2", "us-east-1"]

  # Object values
  database_config:
    host: "localhost"
    port: 5432
    pool_size: 10
</code></pre>
<h3 id="parameter-precedence"><a class="header" href="#parameter-precedence">Parameter Precedence</a></h3>
<p>When multiple sources provide values for the same parameter, they are resolved in this order:</p>
<ol>
<li><strong>CLI <code>--param</code> flags</strong> (highest priority) - Always override all other sources</li>
<li><strong>Parameter <code>default</code> values</strong> - Defined in parameter definitions</li>
<li><strong>Workflow <code>defaults</code> values</strong> (lowest priority) - Only used if parameter has no default</li>
</ol>
<p><strong>Source</strong>: Parameter precedence implemented in src/cook/workflow/composition/composer.rs:245-254 and src/cook/workflow/composer_integration.rs:68-72</p>
<h3 id="example-with-precedence"><a class="header" href="#example-with-precedence">Example with Precedence</a></h3>
<pre><code class="language-yaml"># workflow.yml
defaults:
  environment: "development"
  timeout: 300
  log_level: "info"

parameters:
  definitions:
    environment:
      type: String
      # No parameter default - uses workflow default "development"

    timeout:
      type: Number
      default: 600  # Parameter default overrides workflow default (600, not 300)

    log_level:
      type: String
      # No parameter default - uses workflow default "info"
</code></pre>
<p><strong>Behavior without CLI flags:</strong></p>
<pre><code class="language-bash"># Uses: environment="development", timeout=600, log_level="info"
# Note: timeout uses parameter default (600), others use workflow defaults
prodigy run workflow.yml
</code></pre>
<p><strong>CLI override:</strong></p>
<pre><code class="language-bash"># Final values: environment="production", timeout=900, log_level="debug"
# CLI flags override all defaults
prodigy run workflow.yml \
  --param environment=production \
  --param timeout=900 \
  --param log_level=debug
</code></pre>
<p><strong>Partial CLI override:</strong></p>
<pre><code class="language-bash"># environment="production" (CLI), timeout=600 (param default), log_level="info" (workflow default)
prodigy run workflow.yml --param environment=production
</code></pre>
<h3 id="cli-parameter-overrides"><a class="header" href="#cli-parameter-overrides">CLI Parameter Overrides</a></h3>
<p>Defaults can be overridden using CLI <code>--param</code> flags, providing flexibility for different execution contexts:</p>
<pre><code class="language-yaml"># workflow.yml
defaults:
  timeout: 300
  environment: "development"
  log_level: "info"
</code></pre>
<p><strong>Override specific defaults:</strong></p>
<pre><code class="language-bash"># Override timeout to 900 seconds for long-running operation
prodigy run workflow.yml --param timeout=900

# Change environment to production
prodigy run workflow.yml --param environment=production

# Override multiple defaults
prodigy run workflow.yml \
  --param environment=staging \
  --param log_level=debug
</code></pre>
<p><strong>Source</strong>: CLI parameter handling in src/cook/workflow/composer_integration.rs:68-72 shows that CLI params are merged with workflow defaults, with CLI params taking precedence.</p>
<h3 id="defaults-with-parameters"><a class="header" href="#defaults-with-parameters">Defaults with Parameters</a></h3>
<p>Defaults simplify parameter requirements:</p>
<p><strong>Without defaults:</strong></p>
<pre><code class="language-yaml">parameters:
  required:
    - environment
    - timeout
    - log_level
    - retry_count

# Users must provide all 4 parameters
prodigy run workflow.yml \
  --param environment=staging \
  --param timeout=600 \
  --param log_level=info \
  --param retry_count=3
</code></pre>
<p><strong>With defaults:</strong></p>
<pre><code class="language-yaml">defaults:
  environment: "development"
  timeout: 300
  log_level: "info"
  retry_count: 3

parameters:
  required:
    - environment  # Still required but has default

# Users can run without parameters (uses defaults)
prodigy run workflow.yml

# Or override specific values
prodigy run workflow.yml --param environment=production
</code></pre>
<h3 id="defaults-for-environment-variables"><a class="header" href="#defaults-for-environment-variables">Defaults for Environment Variables</a></h3>
<p>Use defaults to set common environment variables:</p>
<pre><code class="language-yaml">defaults:
  RUST_BACKTRACE: "1"
  CARGO_INCREMENTAL: "0"
  DATABASE_URL: "postgres://localhost/dev"
  REDIS_URL: "redis://localhost:6379"

commands:
  # Commands use default environment variables
  - shell: "cargo test"
  - shell: "redis-cli -u $REDIS_URL ping"
</code></pre>
<h3 id="template-integration"><a class="header" href="#template-integration">Template Integration</a></h3>
<p>Templates can use defaults for parameterization:</p>
<pre><code class="language-yaml"># template.yml
name: deployment-template

defaults:
  replicas: "3"
  environment: "staging"

parameters:
  required:
    - app_name

commands:
  - shell: "kubectl scale deployment ${app_name} --replicas=${replicas}"
  - shell: "kubectl set env deployment/${app_name} ENV=${environment}"
</code></pre>
<p><strong>Using template:</strong></p>
<pre><code class="language-yaml">template:
  source:
    file: "template.yml"
  with:
    app_name: "my-service"
    # Uses defaults: replicas=3, environment=staging
</code></pre>
<h3 id="implementation-status-3"><a class="header" href="#implementation-status-3">Implementation Status</a></h3>
<p>All default value features are fully implemented and functional:</p>
<ul>
<li>✅ Defaults field parsing and storage</li>
<li>✅ Defaults validation during composition</li>
<li>✅ Integration into composition flow</li>
<li>✅ Merge logic with environment variables (composer.rs:230-242)</li>
<li>✅ Merge logic with parameter definitions (composer.rs:245-254)</li>
<li>✅ CLI parameter override support (composer_integration.rs:68-72)</li>
</ul>
<p>The <code>apply_defaults</code> function at src/cook/workflow/composition/composer.rs:217-257 handles:</p>
<ol>
<li>Applying defaults to environment variables (only if not already set)</li>
<li>Applying defaults to parameter definitions (only if parameter has no default value)</li>
<li>Type conversion for environment variable values (strings, numbers, booleans)</li>
</ol>
<h3 id="best-practices-18"><a class="header" href="#best-practices-18">Best Practices</a></h3>
<ol>
<li><strong>Provide sensible defaults</strong> - Choose values that work for most use cases</li>
<li><strong>Document defaults</strong> - Add comments explaining default choices</li>
<li><strong>Use defaults for non-sensitive data</strong> - Secrets should be passed via CLI/env</li>
<li><strong>Keep defaults simple</strong> - Complex objects can be hard to override partially</li>
</ol>
<h3 id="related-topics-4"><a class="header" href="#related-topics-4">Related Topics</a></h3>
<ul>
<li><a href="composition/parameter-definitions.html">Parameter Definitions</a> - Define parameters with types</li>
<li><a href="composition/template-system.html">Template System</a> - Use defaults in templates</li>
<li><a href="composition/workflow-extension-inheritance.html">Workflow Extension</a> - Inherit defaults from base workflows</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="sub-workflows"><a class="header" href="#sub-workflows">Sub-Workflows</a></h2>
<p>Execute child workflows as part of a parent workflow. Sub-workflows can run in parallel and have their own parameters and outputs, enabling modular workflow design and reusable validation/test pipelines.</p>
<h3 id="basic-sub-workflow-syntax"><a class="header" href="#basic-sub-workflow-syntax">Basic Sub-Workflow Syntax</a></h3>
<pre><code class="language-yaml">name: deployment-pipeline
mode: standard

sub_workflows:
  lint-and-test:
    source: "workflows/quality-checks.yml"

  build:
    source: "workflows/build.yml"
    parameters:
      environment: "production"

  deploy:
    source: "workflows/deploy.yml"
    inputs:
      build_artifact: "${build.artifact_path}"
</code></pre>
<h3 id="sub-workflow-configuration"><a class="header" href="#sub-workflow-configuration">Sub-Workflow Configuration</a></h3>
<p>Each sub-workflow is defined as a key-value pair in a HashMap, where the key is the sub-workflow name (defined in <code>SubWorkflow</code> struct, src/cook/workflow/composition/sub_workflow.rs:13-66):</p>
<pre><code class="language-yaml">sub_workflows:
  validation:  # Key is the sub-workflow name
    source: "path/to/workflow.yml"  # Required: workflow file path

    parameters:                       # Optional: parameter values
      env: "staging"
      timeout: 600

    inputs:                           # Optional: input from parent context
      commit_sha: "${git.commit}"
      branch: "${git.branch}"

    outputs:                          # Optional: extract values from sub-workflow
      - "test_coverage"
      - "artifact_url"

    parallel: false                   # Optional: run in parallel (default: false)

    continue_on_error: false          # Optional: continue if sub-workflow fails

    timeout: 1800                     # Optional: sub-workflow timeout (seconds)

    working_dir: "./sub-project"      # Optional: working directory for sub-workflow
                                       # Note: Parsed but not yet applied (implementation in progress)
</code></pre>
<p><strong>Source</strong>: <code>SubWorkflow</code> struct in src/cook/workflow/composition/sub_workflow.rs:13-66
<strong>Validation</strong>: Sub-workflow validation in src/cook/workflow/composition/composer.rs:381-395
<strong>Test example</strong>: tests/workflow_composition_test.rs:152-173 demonstrates sub-workflow configuration</p>
<h3 id="parent-child-context-isolation"><a class="header" href="#parent-child-context-isolation">Parent-Child Context Isolation</a></h3>
<p>Sub-workflows execute in isolated contexts with clean variable scopes (src/cook/workflow/composition/sub_workflow.rs:242-262):</p>
<ul>
<li><strong>Empty variable scope</strong>: Sub-workflow starts with cleared variables (<code>sub_context.variables.clear()</code>)</li>
<li><strong>Explicit input passing</strong>: Use <code>inputs</code> to copy specific parent variables to child context</li>
<li><strong>No variable leakage</strong>: Sub-workflow variables don’t leak back to parent</li>
<li><strong>Output extraction</strong>: Use <code>outputs</code> to explicitly capture child results</li>
<li><strong>Independent git state</strong>: Sub-workflows can operate in different directories</li>
</ul>
<p>The isolation process:</p>
<ol>
<li>Clone parent context</li>
<li>Clear all variables in cloned context</li>
<li>Copy only specified <code>inputs</code> from parent to child</li>
<li>Execute sub-workflow in isolated context</li>
<li>Extract only specified <code>outputs</code> back to parent</li>
</ol>
<h3 id="output-variable-extraction"><a class="header" href="#output-variable-extraction">Output Variable Extraction</a></h3>
<p>Capture values from sub-workflow execution:</p>
<pre><code class="language-yaml"># parent-workflow.yml
sub_workflows:
  build:  # Sub-workflow name
    source: "workflows/build.yml"
    outputs:
      - "docker_image_tag"
      - "artifact_sha256"

commands:
  # Access outputs using ${sub-workflow-name.output-variable}
  - shell: "echo Deploying ${build.docker_image_tag}"
  - shell: "verify-checksum ${build.artifact_sha256}"
</code></pre>
<h3 id="parallel-execution-1"><a class="header" href="#parallel-execution-1">Parallel Execution</a></h3>
<p>Run multiple sub-workflows concurrently using <code>tokio::spawn</code> for concurrent task execution (src/cook/workflow/composition/sub_workflow.rs:179-226):</p>
<pre><code class="language-yaml">sub_workflows:
  # These run in parallel
  unit-tests:
    source: "workflows/unit-tests.yml"
    parallel: true

  integration-tests:
    source: "workflows/integration-tests.yml"
    parallel: true

  e2e-tests:
    source: "workflows/e2e-tests.yml"
    parallel: true

# Parent waits for all parallel sub-workflows before continuing
commands:
  - shell: "echo All tests completed"
</code></pre>
<p><strong>Execution behavior:</strong></p>
<ul>
<li>Each parallel sub-workflow spawns as a separate async task (<code>tokio::spawn</code>)</li>
<li>Parent workflow waits for all parallel tasks to complete via <code>join</code></li>
<li>All outputs are merged back to parent context after completion</li>
<li>If any parallel sub-workflow fails, execution stops (unless <code>continue_on_error: true</code>)</li>
</ul>
<h3 id="error-handling-4"><a class="header" href="#error-handling-4">Error Handling</a></h3>
<p>Control behavior when sub-workflows fail:</p>
<pre><code class="language-yaml">sub_workflows:
  # Critical step - fail parent if this fails
  security-scan:
    source: "workflows/security-scan.yml"
    continue_on_error: false  # Default behavior

  # Optional step - parent continues even if this fails
  performance-test:
    source: "workflows/perf-test.yml"
    continue_on_error: true
</code></pre>
<h3 id="modular-pipeline-example"><a class="header" href="#modular-pipeline-example">Modular Pipeline Example</a></h3>
<p><strong>parent-pipeline.yml:</strong></p>
<pre><code class="language-yaml">name: ci-cd-pipeline
mode: standard

sub_workflows:
  # Step 1: Validation (sequential)
  validate:
    source: "workflows/validation.yml"
    outputs:
      - "validation_passed"

  # Step 2: Tests (parallel)
  unit-tests:
    source: "workflows/unit-tests.yml"
    parallel: true

  integration-tests:
    source: "workflows/integration-tests.yml"
    parallel: true

  # Step 3: Build (sequential, after tests)
  build:
    source: "workflows/build.yml"
    parameters:
      optimization_level: "3"
    outputs:
      - "artifact_path"

  # Step 4: Deploy (sequential, uses build output)
  deploy:
    source: "workflows/deploy.yml"
    inputs:
      artifact: "${build.artifact_path}"
      environment: "production"
</code></pre>
<p><strong>validation.yml</strong> (reusable sub-workflow):</p>
<pre><code class="language-yaml">name: validation
mode: standard

commands:
  - shell: "cargo fmt --check"
  - shell: "cargo clippy -- -D warnings"
  - shell: "echo validation_passed=true &gt;&gt; $PRODIGY_OUTPUT"
</code></pre>
<h3 id="working-directory-isolation"><a class="header" href="#working-directory-isolation">Working Directory Isolation</a></h3>
<p>Sub-workflows can specify different working directories (parsed but not yet applied - see implementation note in src/cook/workflow/composition/sub_workflow.rs:104-107):</p>
<pre><code class="language-yaml">sub_workflows:
  # Backend tests in backend/
  backend-tests:
    source: "workflows/rust-tests.yml"
    working_dir: "./backend"  # Parsed but not yet applied

  # Frontend tests in frontend/
  frontend-tests:
    source: "workflows/js-tests.yml"
    working_dir: "./frontend"  # Parsed but not yet applied
</code></pre>
<blockquote>
<p><strong>Note</strong>: The <code>working_dir</code> field is parsed and validated but not yet applied during execution. The <code>WorkflowContext</code> struct needs a <code>working_directory</code> field to enable this feature. Currently, all sub-workflows execute in the parent’s working directory.</p>
</blockquote>
<h3 id="timeout-configuration-2"><a class="header" href="#timeout-configuration-2">Timeout Configuration</a></h3>
<p>Set execution time limits:</p>
<pre><code class="language-yaml">sub_workflows:
  quick-tests:
    source: "workflows/smoke-tests.yml"
    timeout: 120  # 2 minutes

  comprehensive-tests:
    source: "workflows/full-suite.yml"
    timeout: 3600  # 1 hour
</code></pre>
<h3 id="use-cases-4"><a class="header" href="#use-cases-4">Use Cases</a></h3>
<p><strong>Modular Testing:</strong></p>
<ul>
<li>Separate unit, integration, and e2e tests into sub-workflows</li>
<li>Run test suites in parallel for faster feedback</li>
<li>Reuse test workflows across multiple projects</li>
</ul>
<p><strong>Multi-Language Projects:</strong></p>
<ul>
<li>Separate workflows for each language/component</li>
<li>Independent validation for microservices</li>
<li>Coordinated deployment of multiple services</li>
</ul>
<p><strong>Reusable Validation:</strong></p>
<ul>
<li>Shared linting/formatting workflows</li>
<li>Common security scanning workflows</li>
<li>Standardized compliance checks</li>
</ul>
<p><strong>Environment-Specific Pipelines:</strong></p>
<pre><code class="language-yaml">sub_workflows:
  # Different deployment sub-workflows per environment
  deploy-staging:
    source: "workflows/deploy.yml"
    parameters:
      environment: "staging"
      replicas: "2"

  deploy-production:
    source: "workflows/deploy.yml"
    parameters:
      environment: "production"
      replicas: "5"
</code></pre>
<h3 id="complete-example-11"><a class="header" href="#complete-example-11">Complete Example</a></h3>
<pre><code class="language-yaml">name: monorepo-ci
mode: standard

sub_workflows:
  # Validate everything first
  validate:
    source: "shared/validate.yml"

  # Test all services in parallel
  api-tests:
    source: "services/api/test.yml"
    working_dir: "./services/api"
    parallel: true
    outputs:
      - "coverage"

  worker-tests:
    source: "services/worker/test.yml"
    working_dir: "./services/worker"
    parallel: true
    outputs:
      - "coverage"

  frontend-tests:
    source: "apps/frontend/test.yml"
    working_dir: "./apps/frontend"
    parallel: true
    outputs:
      - "coverage"

# After all sub-workflows complete
commands:
  - shell: "echo API coverage: ${api-tests.coverage}%"
  - shell: "echo Worker coverage: ${worker-tests.coverage}%"
  - shell: "echo Frontend coverage: ${frontend-tests.coverage}%"
  - shell: "generate-combined-coverage-report.sh"
</code></pre>
<h3 id="sub-workflow-result"><a class="header" href="#sub-workflow-result">Sub-Workflow Result</a></h3>
<p>Each sub-workflow execution produces a <code>SubWorkflowResult</code> (src/cook/workflow/composition/sub_workflow.rs:48-65):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>SubWorkflowResult {
    success: bool,                // Execution success
    outputs: HashMap&lt;String, Value&gt;, // Extracted output variables
    duration: Duration,           // Execution time
    error: Option&lt;String&gt;,        // Error message if failed
    logs: Vec&lt;String&gt;,            // Sub-workflow execution logs
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Note</strong>: The sub-workflow name is tracked separately as the HashMap key in the parent workflow’s <code>sub_workflows</code> field, not as a field within <code>SubWorkflowResult</code>.</p>
<h3 id="implementation-status-4"><a class="header" href="#implementation-status-4">Implementation Status</a></h3>
<ul>
<li>✅ Sub-workflow configuration parsing</li>
<li>✅ Sub-workflow validation (<code>validate_sub_workflows</code>)</li>
<li>✅ Parameter and input definitions</li>
<li>✅ Output extraction structure</li>
<li>✅ Parallel execution configuration</li>
<li>✅ Error handling options (continue_on_error)</li>
<li>✅ Timeout and working directory settings</li>
<li>✅ SubWorkflowExecutor structure</li>
<li>⏳ Executor integration with main workflow runtime (in progress)</li>
</ul>
<p><em>Note: Sub-workflow definitions are fully validated and composed, but execution integration with the main workflow orchestrator is currently in development.</em></p>
<h3 id="related-topics-5"><a class="header" href="#related-topics-5">Related Topics</a></h3>
<ul>
<li><a href="composition/index.html#workflow-imports">Workflow Imports</a> - Import shared configurations</li>
<li><a href="composition/template-system.html">Template System</a> - Parameterized workflows</li>
<li><a href="composition/parameter-definitions.html">Parameter Definitions</a> - Define sub-workflow parameters</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="composition-metadata"><a class="header" href="#composition-metadata">Composition Metadata</a></h2>
<p>Prodigy tracks metadata about workflow composition for debugging and dependency analysis. This metadata provides visibility into how workflows are composed, what dependencies exist, and when composition occurred.</p>
<blockquote>
<p><strong>Implementation Status</strong>: The composition metadata types and dependency tracking are fully implemented in the core composition system. These features are accessible programmatically via the WorkflowComposer API. CLI integration for viewing composition metadata in workflows is under development (Spec 131-133).</p>
</blockquote>
<h3 id="compositionmetadata-structure"><a class="header" href="#compositionmetadata-structure">CompositionMetadata Structure</a></h3>
<p>Every composed workflow includes metadata tracking all composition operations:</p>
<p><strong>Source</strong>: <code>src/cook/workflow/composition/mod.rs:153-170</code></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Metadata about workflow composition
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CompositionMetadata {
    /// Source files involved in composition
    pub sources: Vec&lt;PathBuf&gt;,

    /// Templates used
    pub templates: Vec&lt;String&gt;,

    /// Parameters applied
    pub parameters: HashMap&lt;String, Value&gt;,

    /// Composition timestamp
    pub composed_at: chrono::DateTime&lt;chrono::Utc&gt;,

    /// Dependency graph
    pub dependencies: Vec&lt;DependencyInfo&gt;,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Field Details</strong>:</p>
<ul>
<li><code>sources</code>: File paths of all workflow files involved in composition (as <code>PathBuf</code> objects)</li>
<li><code>templates</code>: Template names/sources used during composition</li>
<li><code>parameters</code>: Final parameter values applied to the workflow (as <code>serde_json::Value</code>)</li>
<li><code>composed_at</code>: ISO 8601 timestamp when composition occurred (UTC timezone)</li>
<li><code>dependencies</code>: Complete dependency graph with all imports, extends, templates, and sub-workflows</li>
</ul>
<h3 id="dependency-tracking"><a class="header" href="#dependency-tracking">Dependency Tracking</a></h3>
<p>Each dependency includes detailed information:</p>
<p><strong>Source</strong>: <code>src/cook/workflow/composition/mod.rs:172-183</code></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Information about workflow dependencies
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DependencyInfo {
    /// Source of the dependency
    pub source: PathBuf,

    /// Type of dependency
    pub dep_type: DependencyType,

    /// Resolved path or name
    pub resolved: String,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Field Details</strong>:</p>
<ul>
<li><code>source</code>: Source file path of the dependency (as <code>PathBuf</code>)</li>
<li><code>dep_type</code>: Type of dependency (Import, Extends, Template, or SubWorkflow)</li>
<li><code>resolved</code>: Resolved file path or template name (as <code>String</code>)</li>
</ul>
<h3 id="dependency-types"><a class="header" href="#dependency-types">Dependency Types</a></h3>
<p>Prodigy tracks four types of dependencies:</p>
<p><strong>Source</strong>: <code>src/cook/workflow/composition/mod.rs:185-193</code></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Type of workflow dependency
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum DependencyType {
    Import,
    Extends,
    Template,
    SubWorkflow,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Import Dependencies:</strong></p>
<pre><code class="language-yaml">imports:
  - path: "shared/utilities.yml"

# Creates DependencyInfo:
# dep_type: DependencyType::Import
# source: PathBuf::from("shared/utilities.yml")
# resolved: "/full/path/to/shared/utilities.yml"
</code></pre>
<p><strong>Extends Dependencies:</strong></p>
<pre><code class="language-yaml">extends: "base-config.yml"

# Creates DependencyInfo:
# dep_type: DependencyType::Extends
# source: PathBuf::from("base-config.yml")
# resolved: "/full/path/to/base-config.yml"
</code></pre>
<p><strong>Template Dependencies:</strong></p>
<pre><code class="language-yaml">template:
  source:
    registry: "ci-pipeline"

# Creates DependencyInfo:
# dep_type: DependencyType::Template
# source: PathBuf::from("registry:ci-pipeline")
# resolved: "~/.prodigy/templates/ci-pipeline.yml"
</code></pre>
<p><strong>SubWorkflow Dependencies:</strong></p>
<pre><code class="language-yaml">sub_workflows:
  - name: "tests"
    source: "workflows/test.yml"

# Creates DependencyInfo:
# dep_type: DependencyType::SubWorkflow
# source: PathBuf::from("workflows/test.yml")
# resolved: "/full/path/to/workflows/test.yml"
</code></pre>
<h3 id="viewing-composition-metadata"><a class="header" href="#viewing-composition-metadata">Viewing Composition Metadata</a></h3>
<blockquote>
<p><strong>Note</strong>: CLI commands for viewing composition metadata in workflow execution are under development. Currently, metadata can be accessed programmatically via the WorkflowComposer API (see <a href="composition/composition-metadata.html#programmatic-access">Programmatic Access</a> below).</p>
</blockquote>
<p><strong>Future CLI Usage</strong> (planned):</p>
<pre><code class="language-bash"># Show composition metadata (planned feature)
prodigy run workflow.yml --dry-run --show-composition
</code></pre>
<p><strong>Expected Output:</strong></p>
<pre><code>Composition Metadata:
  Composed at: 2025-01-11T20:00:00Z

  Sources (3):
    - /path/to/workflow.yml
    - /path/to/base-config.yml
    - /path/to/shared/utilities.yml

  Templates (1):
    - registry:ci-pipeline

  Dependencies (3):
    [Import] shared/utilities.yml -&gt; /path/to/shared/utilities.yml
    [Extends] base-config.yml -&gt; /path/to/base-config.yml
    [Template] registry:ci-pipeline -&gt; ~/.prodigy/templates/ci-pipeline.yml

  Parameters (2):
    environment: "production"
    timeout: 600
</code></pre>
<h3 id="programmatic-access"><a class="header" href="#programmatic-access">Programmatic Access</a></h3>
<p>Access metadata in code using the WorkflowComposer API:</p>
<p><strong>Source</strong>: <code>src/cook/workflow/composition/composer.rs:21-37</code></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use prodigy::cook::workflow::composition::{WorkflowComposer, TemplateRegistry};
use std::collections::HashMap;
use std::sync::Arc;
use std::path::Path;

// Create composer with template registry
let registry = Arc::new(TemplateRegistry::new());
let composer = WorkflowComposer::new(registry);

// Compose workflow with parameters
let params = HashMap::new();
let composed = composer.compose(Path::new("workflow.yml"), params).await?;

// Access metadata
let metadata = &amp;composed.metadata;

// Inspect dependencies
for dep in &amp;metadata.dependencies {
    println!("{:?}: {} -&gt; {}",
        dep.dep_type,
        dep.source.display(),
        dep.resolved
    );
}

// Check composition timestamp
println!("Composed at: {}", metadata.composed_at);

// View final parameters
for (name, value) in &amp;metadata.parameters {
    println!("Parameter {}: {:?}", name, value);
}

// List source files
println!("Sources:");
for source in &amp;metadata.sources {
    println!("  - {}", source.display());
}

// List templates used
println!("Templates:");
for template in &amp;metadata.templates {
    println!("  - {}", template);
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Real-World Example</strong> (from <code>src/cook/workflow/composition/composer.rs:45-51</code>):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Metadata is created during composition
let mut metadata = CompositionMetadata {
    sources: vec![source.to_path_buf()],
    templates: Vec::new(),
    parameters: params.clone(),
    composed_at: chrono::Utc::now(),
    dependencies: Vec::new(),
};
<span class="boring">}</span></code></pre></pre>
<h3 id="dependency-graph-visualization"><a class="header" href="#dependency-graph-visualization">Dependency Graph Visualization</a></h3>
<p>Metadata enables dependency visualization:</p>
<pre><code>workflow.yml
├─ [Extends] base-config.yml
│  └─ [Import] shared/setup.yml
├─ [Import] shared/utilities.yml
└─ [Template] registry:ci-pipeline
   └─ [SubWorkflow] workflows/test.yml
</code></pre>
<h3 id="use-cases-5"><a class="header" href="#use-cases-5">Use Cases</a></h3>
<p><strong>Debugging Composition Issues:</strong></p>
<ul>
<li>Verify which files were loaded</li>
<li>Check parameter resolution order</li>
<li>Identify circular dependencies</li>
<li>Trace inheritance chains</li>
</ul>
<p><strong>Dependency Analysis:</strong></p>
<ul>
<li>Find all workflows using a template</li>
<li>Identify shared imports</li>
<li>Map workflow relationships</li>
<li>Audit composition complexity</li>
</ul>
<p><strong>Change Impact Assessment:</strong></p>
<pre><code class="language-bash"># Before changing base-config.yml, find all dependents
grep -r "extends.*base-config" workflows/

# View composition metadata programmatically
# (CLI integration for --show-composition is under development)
</code></pre>
<p><strong>Compliance and Auditing:</strong></p>
<ul>
<li>Track template versions used</li>
<li>Record composition timestamps</li>
<li>Document parameter sources</li>
<li>Verify configuration origins</li>
</ul>
<h3 id="metadata-in-composed-workflows"><a class="header" href="#metadata-in-composed-workflows">Metadata in Composed Workflows</a></h3>
<p>Composed workflows carry metadata through the composition process:</p>
<p><strong>Source</strong>: <code>src/cook/workflow/composition/mod.rs:143-151</code></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ComposedWorkflow structure
pub struct ComposedWorkflow {
    /// The composed workflow
    pub workflow: ComposableWorkflow,

    /// Metadata about the composition
    pub metadata: CompositionMetadata,
}

// Access metadata from composed workflow
let composed = composer.compose(source, params).await?;
println!("This workflow was composed from {} sources",
    composed.metadata.sources.len());
<span class="boring">}</span></code></pre></pre>
<h3 id="circular-dependency-detection"><a class="header" href="#circular-dependency-detection">Circular Dependency Detection</a></h3>
<p>Metadata enables circular dependency detection:</p>
<pre><code class="language-yaml"># workflow-a.yml
extends: "workflow-b.yml"

# workflow-b.yml
extends: "workflow-a.yml"
</code></pre>
<p><strong>Detection:</strong></p>
<pre><code>Error: Circular dependency detected
  workflow-a.yml -&gt; workflow-b.yml -&gt; workflow-a.yml

Dependency chain:
  1. workflow-a.yml (extends workflow-b.yml)
  2. workflow-b.yml (extends workflow-a.yml) &lt;- Circular!
</code></pre>
<h3 id="parameter-tracking"><a class="header" href="#parameter-tracking">Parameter Tracking</a></h3>
<p>Metadata tracks final parameter values applied during composition:</p>
<pre><code class="language-yaml"># workflow.yml
parameters:
  definitions:
    environment:
      type: string
      required: true
    timeout:
      type: integer
      default: 600
</code></pre>
<p><strong>Metadata captures:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>metadata.parameters = {
    "environment": "production",
    "timeout": 600,
}
<span class="boring">}</span></code></pre></pre>
<p>The <code>parameters</code> field in <code>CompositionMetadata</code> stores the final resolved parameter values as a <code>HashMap&lt;String, Value&gt;</code>. This enables reproducibility and debugging of composed workflows.</p>
<h3 id="caching-and-performance"><a class="header" href="#caching-and-performance">Caching and Performance</a></h3>
<p>Composition metadata enables future caching optimizations:</p>
<ul>
<li><code>composed_at</code> timestamp can be used for cache invalidation</li>
<li><code>sources</code> list enables dependency-based cache busting</li>
<li><code>dependencies</code> graph supports incremental composition</li>
<li><code>parameters</code> hash can detect identical compositions</li>
</ul>
<blockquote>
<p><strong>Note</strong>: Workflow caching is a planned feature. Currently, metadata is generated fresh on each composition.</p>
</blockquote>
<h3 id="data-structure-properties"><a class="header" href="#data-structure-properties">Data Structure Properties</a></h3>
<p>CompositionMetadata uses standard Rust types for broad compatibility:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CompositionMetadata {
    // All fields use standard types
    pub sources: Vec&lt;PathBuf&gt;,           // Cloneable
    pub templates: Vec&lt;String&gt;,          // Cloneable
    pub parameters: HashMap&lt;String, Value&gt;,  // Cloneable
    pub composed_at: chrono::DateTime&lt;chrono::Utc&gt;,  // Copy
    pub dependencies: Vec&lt;DependencyInfo&gt;,   // Cloneable
}
<span class="boring">}</span></code></pre></pre>
<p>The struct derives <code>Clone</code>, making it easy to share metadata across components without requiring explicit synchronization primitives.</p>
<h3 id="related-topics-6"><a class="header" href="#related-topics-6">Related Topics</a></h3>
<ul>
<li><a href="composition/template-system.html">Template System</a> - Template caching and loading</li>
<li><a href="composition/workflow-extension-inheritance.html">Workflow Extension</a> - Inheritance tracking</li>
<li><a href="composition/best-practices.html">Best Practices</a> - Using metadata for debugging</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="complete-examples-1"><a class="header" href="#complete-examples-1">Complete Examples</a></h2>
<p>This section provides end-to-end examples demonstrating multiple composition features working together.</p>
<h3 id="example-1-multi-environment-cicd-pipeline"><a class="header" href="#example-1-multi-environment-cicd-pipeline">Example 1: Multi-Environment CI/CD Pipeline</a></h3>
<p>This example uses templates, parameters, and inheritance for environment-specific deployments.</p>
<p><strong>base-ci-template.yml</strong> (template in registry):</p>
<pre><code class="language-yaml">name: ci-pipeline-template
mode: standard

parameters:
  required:
    - name: environment
      type: string
      description: "Deployment environment"
      validation: "matches('^(dev|staging|prod)$')"

  optional:
    - name: replicas
      type: number
      description: "Number of service replicas"
      default: 1

    - name: run_tests
      type: boolean
      description: "Whether to run test suite"
      default: true

defaults:
  timeout: 600
  log_level: "info"

commands:
  - shell: "echo Deploying to ${environment} with ${replicas} replicas"
  - shell: "cargo build --release"
  - shell: |
      if [ "${run_tests}" = "true" ]; then
        cargo test --release
      fi
  - shell: "kubectl apply -f k8s/${environment}/deployment.yml"
  - shell: "kubectl scale deployment app --replicas=${replicas}"
</code></pre>
<p><strong>Source</strong>: Template and parameter structure from src/cook/workflow/composition/mod.rs:75-129</p>
<p><strong>dev-deployment.yml</strong>:</p>
<pre><code class="language-yaml">name: dev-deployment
mode: standard

template:
  source: "ci-pipeline-template"  # Registry name
  with:
    environment: "dev"
    replicas: 1
    run_tests: false  # Skip tests in dev for speed
</code></pre>
<p><strong>Source</strong>: TemplateSource is an untagged enum accepting string values (src/cook/workflow/composition/mod.rs:86-95)</p>
<p><strong>prod-deployment.yml</strong>:</p>
<pre><code class="language-yaml">name: prod-deployment
mode: standard

template:
  source: "ci-pipeline-template"  # Registry name
  with:
    environment: "prod"
    replicas: 5
    run_tests: true  # Always test before prod

# Add production-specific safeguards
commands:
  - shell: "verify-release-notes.sh"
  - shell: "notify-team 'Production deployment starting'"
</code></pre>
<h3 id="example-2-modular-monorepo-testing"><a class="header" href="#example-2-modular-monorepo-testing">Example 2: Modular Monorepo Testing</a></h3>
<p>Uses sub-workflows and imports for testing multiple services in parallel.</p>
<p><strong>shared/common-setup.yml</strong>:</p>
<pre><code class="language-yaml">name: common-setup

commands:
  - shell: "git fetch origin"
  - shell: "npm install"
  - shell: "cargo build"
</code></pre>
<p><strong>monorepo-test.yml</strong>:</p>
<pre><code class="language-yaml">name: monorepo-test
mode: standard

imports:
  - path: "shared/common-setup.yml"

workflows:
  # Test all services in parallel using workflow map (not array)
  api-tests:
    source: "services/api/test.yml"
    working_dir: "./services/api"
    parallel: true
    outputs:
      - "coverage"
      - "test_count"

  worker-tests:
    source: "services/worker/test.yml"
    working_dir: "./services/worker"
    parallel: true
    outputs:
      - "coverage"
      - "test_count"

  frontend-tests:
    source: "apps/frontend/test.yml"
    working_dir: "./apps/frontend"
    parallel: true
    outputs:
      - "coverage"
      - "test_count"

commands:
  - shell: |
      echo "Test Results:"
      echo "  API: ${api-tests.test_count} tests, ${api-tests.coverage}% coverage"
      echo "  Worker: ${worker-tests.test_count} tests, ${worker-tests.coverage}% coverage"
      echo "  Frontend: ${frontend-tests.test_count} tests, ${frontend-tests.coverage}% coverage"
  - shell: "generate-combined-report.sh"
</code></pre>
<p><strong>Source</strong>: Sub-workflows use HashMap, not array (src/cook/workflow/composition/mod.rs:49 and src/cook/workflow/composition/sub_workflow.rs:14-45)</p>
<h3 id="example-3-layered-configuration-with-extends"><a class="header" href="#example-3-layered-configuration-with-extends">Example 3: Layered Configuration with Extends</a></h3>
<p>Uses inheritance to create environment-specific variations of a base workflow.</p>
<p><strong>base-config.yml</strong>:</p>
<pre><code class="language-yaml">name: base-config
mode: standard

defaults:
  log_level: "info"
  timeout: 300

env:
  APP_NAME: "my-service"
  DATABASE_POOL_SIZE: "10"

commands:
  - shell: "cargo fmt --check"
  - shell: "cargo clippy"
  - shell: "cargo test"
  - shell: "cargo build"
</code></pre>
<p><strong>staging-config.yml</strong>:</p>
<pre><code class="language-yaml">name: staging-config
extends: "base-config.yml"

defaults:
  log_level: "debug"
  timeout: 600

env:
  DATABASE_POOL_SIZE: "20"
  ENABLE_DEBUG_ENDPOINTS: "true"

# Inherits all commands from base, adds staging-specific
commands:
  - shell: "run-integration-tests.sh"
  - shell: "deploy-to-staging.sh"
</code></pre>
<p><strong>production-config.yml</strong>:</p>
<pre><code class="language-yaml">name: production-config
extends: "base-config.yml"

defaults:
  log_level: "warn"
  timeout: 900

env:
  DATABASE_POOL_SIZE: "50"
  ENABLE_MONITORING: "true"
  RATE_LIMIT_ENABLED: "true"

commands:
  - shell: "verify-security-scan.sh"
  - shell: "cargo build --release"
  - shell: "run-smoke-tests.sh"
  - shell: "deploy-to-production.sh"
  - shell: "notify-deployment-complete.sh"
</code></pre>
<h3 id="example-4-complex-composition-with-multiple-features"><a class="header" href="#example-4-complex-composition-with-multiple-features">Example 4: Complex Composition with Multiple Features</a></h3>
<p>Combines imports, extends, template, parameters, and sub-workflows.</p>
<p><strong>templates/microservice-ci.yml</strong>:</p>
<pre><code class="language-yaml">name: microservice-ci-template
mode: standard

parameters:
  required:
    - name: service_name
      type: string
      description: "Name of the microservice"

    - name: language
      type: string
      description: "Programming language"
      validation: "matches('^(rust|typescript|python)$')"

  optional:
    - name: test_timeout
      type: number
      description: "Test timeout in seconds"
      default: 300

defaults:
  coverage_threshold: "80"

workflows:
  lint:
    source: "workflows/${language}/lint.yml"
    working_dir: "./services/${service_name}"

  test:
    source: "workflows/${language}/test.yml"
    working_dir: "./services/${service_name}"
    timeout: "${test_timeout}"
    outputs:
      - "coverage"

commands:
  - shell: "echo Testing ${service_name} (${language})"
  - shell: |
      if [ "${test.coverage}" -lt "${coverage_threshold}" ]; then
        echo "Coverage ${test.coverage}% below threshold ${coverage_threshold}%"
        exit 1
      fi
</code></pre>
<p><strong>service-api-ci.yml</strong> (uses the template):</p>
<pre><code class="language-yaml">name: api-service-ci
mode: standard

imports:
  - path: "shared/docker-utils.yml"

template:
  source: "templates/microservice-ci.yml"  # File path
  with:
    service_name: "api"
    language: "rust"
    test_timeout: 600

commands:
  - shell: "docker build -t api:latest ./services/api"
  - shell: "docker push api:latest"
</code></pre>
<h3 id="example-5-workflow-registry-pattern"><a class="header" href="#example-5-workflow-registry-pattern">Example 5: Workflow Registry Pattern</a></h3>
<p>Demonstrates using a template registry for standardized workflows across teams.</p>
<p><strong>Setup Registry</strong>:</p>
<p>Templates can be stored in two locations:</p>
<ul>
<li>Global: <code>~/.prodigy/templates/</code> (available to all projects)</li>
<li>Local: <code>.prodigy/templates/</code> (project-specific)</li>
</ul>
<pre><code class="language-bash"># Global templates (one-time setup)
mkdir -p ~/.prodigy/templates
cp standard-ci.yml ~/.prodigy/templates/
cp security-scan.yml ~/.prodigy/templates/

# Or use project-local templates
mkdir -p .prodigy/templates
cp deployment.yml .prodigy/templates/
</code></pre>
<p><strong>Source</strong>: TemplateRegistry and FileTemplateStorage (src/cook/workflow/composition/registry.rs)</p>
<p><strong>team-workflow.yml</strong>:</p>
<pre><code class="language-yaml">name: team-workflow
mode: standard

# Use registry template (string value, not nested object)
template:
  source: "standard-ci"  # Registry name
  with:
    project_type: "rust"

# Import additional registry workflows
imports:
  - path: "~/.prodigy/templates/security-scan.yml"

# Extend with team-specific configuration
commands:
  - shell: "run-team-specific-tests.sh"
</code></pre>
<h3 id="example-6-progressive-composition"><a class="header" href="#example-6-progressive-composition">Example 6: Progressive Composition</a></h3>
<p>Builds complexity through layers of composition.</p>
<p><strong>Layer 1 - Base</strong> (minimal.yml):</p>
<pre><code class="language-yaml">name: minimal
mode: standard
defaults:
  timeout: 300
commands:
  - shell: "cargo build"
</code></pre>
<p><strong>Layer 2 - Add Testing</strong> (with-tests.yml):</p>
<pre><code class="language-yaml">name: with-tests
mode: standard
extends: "minimal.yml"
commands:
  - shell: "cargo test"
</code></pre>
<p><strong>Layer 3 - Add Linting</strong> (with-quality.yml):</p>
<pre><code class="language-yaml">name: with-quality
mode: standard
extends: "with-tests.yml"
commands:
  - shell: "cargo fmt --check"
  - shell: "cargo clippy"
</code></pre>
<p><strong>Layer 4 - Full CI</strong> (full-ci.yml):</p>
<pre><code class="language-yaml">name: full-ci
mode: standard
extends: "with-quality.yml"

parameters:
  required:
    - name: deploy_env
      type: string
      description: "Deployment environment"

commands:
  - shell: "cargo build --release"
  - shell: "deploy-to-${deploy_env}.sh"
</code></pre>
<h3 id="running-the-examples"><a class="header" href="#running-the-examples">Running the Examples</a></h3>
<pre><code class="language-bash"># Example 1: Template-based deployment
prodigy run dev-deployment.yml
prodigy run prod-deployment.yml --param replicas=10

# Example 2: Monorepo testing
prodigy run monorepo-test.yml

# Example 3: Environment-specific configs
prodigy run staging-config.yml
prodigy run production-config.yml

# Example 4: Complex composition
prodigy run service-api-ci.yml

# Example 5: Registry templates
prodigy run team-workflow.yml

# Example 6: Progressive composition
prodigy run full-ci.yml --param deploy_env=staging
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="best-practices-19"><a class="header" href="#best-practices-19">Best Practices</a></h2>
<p>Guidelines for effective workflow organization, MapReduce design, and environment management in Prodigy.</p>
<blockquote>
<p><strong>Implementation Status Note</strong>: Some composition features (imports, extends, templates) are implemented in the codebase but not yet integrated with the CLI. This document focuses on features available in current releases.</p>
</blockquote>
<h3 id="when-to-use-each-feature"><a class="header" href="#when-to-use-each-feature">When to Use Each Feature</a></h3>
<p><strong>MapReduce Workflows</strong> - Use when:</p>
<ul>
<li>Processing multiple items in parallel (lint checks, test files, code analysis)</li>
<li>Distributing work across isolated git worktrees</li>
<li>Need automatic retry and DLQ for failed items</li>
<li>Want checkpoint-based resume for long-running operations</li>
<li>See: <a href="composition/../mapreduce/index.html">MapReduce chapter</a></li>
</ul>
<p><strong>Sub-Workflows</strong> (workflows field) - Use when:</p>
<ul>
<li>Breaking complex workflows into smaller, reusable pieces</li>
<li>Running independent validation/test workflows</li>
<li>Isolating different workflow stages</li>
<li>Sharing workflow logic between projects</li>
<li>Source: src/cook/workflow/composition/sub_workflow.rs:14-46</li>
</ul>
<p><strong>Environment Variables</strong> - Use when:</p>
<ul>
<li>Parameterizing workflows for different environments (dev/staging/prod)</li>
<li>Managing secrets and sensitive configuration</li>
<li>Configuring per-command overrides</li>
<li>Using profiles for different deployment contexts</li>
<li>See: <a href="composition/../environment/index.html">Environment chapter</a></li>
</ul>
<p><strong>Goal-Seeking Operations</strong> - Use when:</p>
<ul>
<li>Iteratively refining code until tests pass</li>
<li>Fixing lint/clippy errors automatically</li>
<li>Improving code quality metrics</li>
<li>Need validation-driven refinement</li>
<li>See: <a href="composition/../advanced/goal-seeking-operations.html">Goal-Seeking chapter</a></li>
</ul>
<h3 id="workflow-organization"><a class="header" href="#workflow-organization">Workflow Organization</a></h3>
<p><strong>Real-World Directory Structure</strong> (from Prodigy itself):</p>
<pre><code>workflows/
├── book-docs-drift.yml           # MapReduce: documentation maintenance
├── debtmap-reduce.yml            # MapReduce: technical debt analysis
├── documentation-maintain.yml    # MapReduce: multi-chapter updates
├── fix-files-mapreduce.yml       # MapReduce: batch file processing
├── implement-with-tests.yml      # Standard: TDD workflow
├── coverage.yml                  # Standard: test coverage check
├── environment-example.yml       # Example: env var configuration
├── mapreduce-env-example.yml     # Example: MapReduce with env vars
├── data/                         # Work item JSON files
│   ├── chapters.json
│   ├── test-files.json
│   └── components.json
└── tests/                        # Test workflows
    └── minimal-mapreduce.yml
</code></pre>
<p><strong>Organization Patterns:</strong></p>
<ol>
<li><strong>MapReduce Workflows</strong> - Named with clear purpose, store work items in <code>data/</code></li>
<li><strong>Standard Workflows</strong> - Single-file workflows for linear processes</li>
<li><strong>Environment Examples</strong> - Demonstrate environment variable usage</li>
<li><strong>Test Workflows</strong> - Minimal examples for testing features</li>
</ol>
<h3 id="environment-variable-naming-conventions"><a class="header" href="#environment-variable-naming-conventions">Environment Variable Naming Conventions</a></h3>
<p><strong>Good names</strong> (from workflows/mapreduce-env-example.yml:7-20):</p>
<pre><code class="language-yaml">env:
  # Configuration
  PROJECT_NAME: "example-project"
  PROJECT_CONFIG: "config.yml"
  FEATURES_PATH: "features"

  # Output settings
  OUTPUT_DIR: "output"
  REPORT_FORMAT: "json"

  # Workflow behavior
  MAX_RETRIES: "3"
  TIMEOUT_SECONDS: "300"
  DEBUG_MODE: "false"
</code></pre>
<p><strong>Avoid:</strong></p>
<pre><code class="language-yaml">env:
  ENV: "prod"          # Too brief, conflicts with common shell var
  COUNT: "3"           # Unclear what it counts
  FLAG: "true"         # Doesn't indicate purpose
  PATH: "/usr/bin"     # Overwrites critical shell variable
</code></pre>
<p><strong>Best Practices:</strong></p>
<ul>
<li>Use UPPER_CASE for environment variables</li>
<li>Prefix project-specific vars (e.g., <code>MYAPP_*</code>)</li>
<li>Be descriptive: <code>DEPLOYMENT_ENVIRONMENT</code> not <code>ENV</code></li>
<li>Avoid common shell variable names (PATH, HOME, USER)</li>
<li>See: <a href="composition/../environment/index.html">Environment Variables chapter</a></li>
</ul>
<h3 id="mapreduce-work-item-design"><a class="header" href="#mapreduce-work-item-design">MapReduce Work Item Design</a></h3>
<p><strong>Granularity Best Practices:</strong></p>
<p>Good granularity:</p>
<pre><code class="language-json">{
  "items": [
    {"file": "src/main.rs", "task": "lint"},
    {"file": "src/lib.rs", "task": "lint"},
    {"file": "tests/integration.rs", "task": "lint"}
  ]
}
</code></pre>
<p>Too coarse (loses parallelism):</p>
<pre><code class="language-json">{
  "items": [
    {"files": ["src/**/*.rs"], "task": "lint-all"}
  ]
}
</code></pre>
<p>Too fine (overhead dominates):</p>
<pre><code class="language-json">{
  "items": [
    {"line": 1, "file": "src/main.rs", "task": "lint-line"},
    {"line": 2, "file": "src/main.rs", "task": "lint-line"}
  ]
}
</code></pre>
<p><strong>Idempotency:</strong></p>
<ul>
<li>Design work items so they can be retried safely</li>
<li>Avoid operations that fail on re-execution (file creation, append operations)</li>
<li>Use <code>git add</code> not <code>git commit</code> in agents (reduce phase commits)</li>
<li>See: <a href="composition/../mapreduce/index.html">MapReduce chapter</a></li>
</ul>
<p><strong>Performance Considerations:</strong></p>
<p><strong>Parallel Execution:</strong></p>
<ul>
<li>Set <code>max_parallel</code> based on available resources (default: 5)</li>
<li>For I/O-bound tasks: Higher parallelism (10-20)</li>
<li>For CPU-bound tasks: Match CPU count</li>
<li>For API calls: Respect rate limits</li>
<li>Source: workflows/book-docs-drift.yml:59 uses <code>max_parallel: 3</code></li>
</ul>
<p><strong>Checkpoint Frequency:</strong></p>
<ul>
<li>Checkpoints save after configurable number of items</li>
<li>More frequent = faster resume but more I/O overhead</li>
<li>Less frequent = slower resume but better performance</li>
<li>See: <a href="composition/../mapreduce/checkpoint-and-resume.html">Checkpoint and Resume</a></li>
</ul>
<h3 id="testing-and-validation"><a class="header" href="#testing-and-validation">Testing and Validation</a></h3>
<p><strong>Validate Workflow Syntax:</strong></p>
<pre><code class="language-bash"># Validate YAML format and detect issues
prodigy validate workflow.yml

# Dry-run without executing
prodigy run workflow.yml --dry-run
</code></pre>
<p><strong>Test with Different Environments:</strong></p>
<pre><code class="language-bash"># Test with development profile
prodigy run workflow.yml --profile development

# Test with production profile
prodigy run workflow.yml --profile production
</code></pre>
<p><strong>Test MapReduce with Minimal Items:</strong></p>
<pre><code class="language-bash"># Create test work items file
echo '{"items": [{"id": "test1"}, {"id": "test2"}]}' &gt; test-items.json

# Run MapReduce with minimal parallelism
prodigy run workflow.yml  # Uses max_parallel from workflow
</code></pre>
<h3 id="debugging-workflows"><a class="header" href="#debugging-workflows">Debugging Workflows</a></h3>
<p><strong>Verbose Output Levels:</strong></p>
<pre><code class="language-bash"># Standard output (clean, progress only)
prodigy run workflow.yml

# Verbose: Show Claude streaming output
prodigy run workflow.yml -v

# Debug: Show debug logs
prodigy run workflow.yml -vv

# Trace: Show trace-level logs
prodigy run workflow.yml -vvv
</code></pre>
<p><strong>View Execution Logs:</strong></p>
<pre><code class="language-bash"># View Claude JSON logs for debugging
prodigy logs --latest

# View with summary
prodigy logs --latest --summary

# Follow log in real-time
prodigy logs --latest --tail
</code></pre>
<p><strong>Inspect MapReduce Job State:</strong></p>
<pre><code class="language-bash"># View job progress
prodigy progress &lt;job_id&gt;

# View job events
prodigy events &lt;job_id&gt;

# Check failed items
prodigy dlq list &lt;job_id&gt;

# Analyze failure patterns
prodigy dlq analyze &lt;job_id&gt;
</code></pre>
<h3 id="security-best-practices-2"><a class="header" href="#security-best-practices-2">Security Best Practices</a></h3>
<p><strong>Secrets Management:</strong></p>
<p><strong>DO: Use environment variables and secret masking</strong> (from workflows/mapreduce-env-example.yml:22-26):</p>
<pre><code class="language-yaml">env:
  PROJECT_NAME: "example-project"

secrets:
  API_TOKEN:
    provider: env
    key: "GITHUB_TOKEN"  # Reads from environment, masked in logs
</code></pre>
<p><strong>DON’T: Store secrets in workflow files:</strong></p>
<pre><code class="language-yaml">env:
  API_KEY: "sk-abc123"  # Visible in version control!
</code></pre>
<p><strong>Pass secrets via environment:</strong></p>
<pre><code class="language-bash"># Set in shell
export GITHUB_TOKEN="ghp_abc123"

# Run workflow (secret is masked in logs)
prodigy run workflow.yml
</code></pre>
<p><strong>Environment Profiles for Security:</strong></p>
<pre><code class="language-yaml">profiles:
  development:
    API_URL: "http://localhost:3000"  # Safe for dev

  production:
    API_URL: "https://api.prod.com"   # Protected endpoint
</code></pre>
<p>See: <a href="composition/../environment/secrets-management.html">Secrets Management</a></p>
<h3 id="dlq-and-error-recovery"><a class="header" href="#dlq-and-error-recovery">DLQ and Error Recovery</a></h3>
<p><strong>When MapReduce Items Fail:</strong></p>
<ol>
<li><strong>Check the DLQ:</strong></li>
</ol>
<pre><code class="language-bash"># List failed items
prodigy dlq list &lt;job_id&gt;

# Inspect specific failure
prodigy dlq inspect &lt;job_id&gt; &lt;item_id&gt;

# View failure patterns
prodigy dlq analyze &lt;job_id&gt;
</code></pre>
<ol start="2">
<li><strong>Fix the Root Cause:</strong></li>
</ol>
<ul>
<li>Update workflow to handle edge cases</li>
<li>Fix Claude command logic</li>
<li>Adjust work item format</li>
</ul>
<ol start="3">
<li><strong>Retry Failed Items:</strong></li>
</ol>
<pre><code class="language-bash"># Retry all failed items
prodigy dlq retry &lt;job_id&gt;

# Retry with custom parallelism
prodigy dlq retry &lt;job_id&gt; --max-parallel 10

# Dry run to preview
prodigy dlq retry &lt;job_id&gt; --dry-run
</code></pre>
<p>See: <a href="composition/../mapreduce/dead-letter-queue-dlq.html">Dead Letter Queue chapter</a></p>
<h3 id="workflow-documentation"><a class="header" href="#workflow-documentation">Workflow Documentation</a></h3>
<p><strong>Document Workflow Purpose:</strong></p>
<pre><code class="language-yaml"># Book Documentation Drift Detection
# Purpose: Analyzes codebase features and updates documentation automatically
# Runs: On-demand or via CI when features change
# Duration: ~30 minutes for full book
# Output: Updated markdown files in book/src/

name: prodigy-book-docs-drift-detection
mode: mapreduce

env:
  PROJECT_NAME: "Prodigy"
  BOOK_DIR: "book"
</code></pre>
<p><strong>Include Usage Examples:</strong></p>
<pre><code class="language-yaml"># Run with default settings:
#   prodigy run book-docs-drift.yml
#
# Run with custom parallel limit:
#   prodigy run book-docs-drift.yml
#   (Edit MAX_PARALLEL in env section)
#
# Resume interrupted run:
#   prodigy resume &lt;session_id&gt;
</code></pre>
<p><strong>Document Work Item Format:</strong></p>
<pre><code class="language-yaml"># Work items structure (for future contributors):
# {
#   "items": [
#     {
#       "id": "chapter-id",
#       "file": "book/src/chapter.md",
#       "features": ["feature1", "feature2"],
#       "validation": "Check completeness"
#     }
#   ]
# }
</code></pre>
<h3 id="workflow-maintenance"><a class="header" href="#workflow-maintenance">Workflow Maintenance</a></h3>
<p><strong>Version Control:</strong></p>
<ul>
<li>Commit workflows to git</li>
<li>Use descriptive commit messages for workflow changes</li>
<li>Tag workflow versions for major changes</li>
<li>Review workflow changes in PRs</li>
</ul>
<p><strong>Regular Updates:</strong></p>
<ul>
<li>Keep work item JSON files up to date</li>
<li>Update environment variable defaults</li>
<li>Review and update max_parallel settings</li>
<li>Clean up obsolete workflows</li>
</ul>
<p><strong>Monitoring:</strong></p>
<ul>
<li>Check DLQ regularly for patterns</li>
<li>Monitor workflow execution times</li>
<li>Review checkpoint sizes</li>
<li>Track resource usage (parallelism impact)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="troubleshooting-13"><a class="header" href="#troubleshooting-13">Troubleshooting</a></h2>
<p>Common issues with workflow composition and their solutions.</p>
<h3 id="circular-dependency-errors"><a class="header" href="#circular-dependency-errors">Circular Dependency Errors</a></h3>
<p><strong>Error:</strong></p>
<pre><code>Error: Circular dependency detected in workflow composition
</code></pre>
<p><strong>Cause:</strong> Workflow inheritance or imports form a cycle.</p>
<p><strong>Solution:</strong></p>
<pre><code class="language-yaml"># Bad: Circular dependency
# workflow-a.yml extends workflow-b.yml
# workflow-b.yml extends workflow-a.yml

# Good: Use common base
# base.yml (no extends)
# workflow-a.yml extends base.yml
# workflow-b.yml extends base.yml
</code></pre>
<p><strong>Debugging:</strong></p>
<p>The error message doesn’t include the dependency chain. To diagnose which workflows are involved:</p>
<pre><code class="language-bash"># Use verbose mode to see composition steps
prodigy run workflow.yml --dry-run -vv

# Manually trace the chain
# 1. Check what the workflow extends
grep "^extends:" workflow-a.yml

# 2. Check what that workflow extends
grep "^extends:" workflow-b.yml

# 3. Continue until you find the cycle
</code></pre>
<p><strong>Source:</strong> Error generated in src/cook/workflow/composition/composer.rs:727</p>
<h3 id="template-not-found-in-registry"><a class="header" href="#template-not-found-in-registry">Template Not Found in Registry</a></h3>
<p><strong>Error:</strong></p>
<pre><code>Error: Template 'ci-pipeline' not found in registry
</code></pre>
<p><strong>Cause:</strong> Template doesn’t exist in the template search path.</p>
<p><strong>Solutions:</strong></p>
<ol>
<li><strong>Verify template locations (searched in order):</strong></li>
</ol>
<p>Prodigy searches for templates in this priority order:</p>
<pre><code class="language-bash"># 1. Global templates (highest priority, shared across repos)
ls ~/.prodigy/templates/ci-pipeline.yml

# 2. Project-local templates
ls .prodigy/templates/ci-pipeline.yml

# 3. Legacy project-local templates
ls templates/ci-pipeline.yml
</code></pre>
<p><strong>Source:</strong> Template search path defined in src/cook/workflow/composer_integration.rs:94-110</p>
<ol start="2">
<li><strong>Check template name:</strong></li>
</ol>
<pre><code class="language-yaml"># Ensure template file name matches reference
template:
  source:
    registry: "ci-pipeline"  # Looks for ci-pipeline.yml
</code></pre>
<ol start="3">
<li><strong>Add template to registry:</strong></li>
</ol>
<pre><code class="language-bash"># Add to global registry (available to all projects)
cp my-template.yml ~/.prodigy/templates/

# Or add to project-local registry
cp my-template.yml .prodigy/templates/
</code></pre>
<ol start="4">
<li><strong>Use file-based template instead:</strong></li>
</ol>
<pre><code class="language-yaml">template:
  source:
    file: "path/to/template.yml"
</code></pre>
<p><strong>See also:</strong> <a href="composition/template-system.html">Template System</a> for more details on template sources.</p>
<h3 id="parameter-validation-failures"><a class="header" href="#parameter-validation-failures">Parameter Validation Failures</a></h3>
<p><strong>Error:</strong></p>
<pre><code>Error: Parameter validation failed
  - 'environment': Expected String, got Number
</code></pre>
<p><strong>Cause:</strong> Parameter type mismatch.</p>
<p><strong>Solutions:</strong></p>
<ol>
<li><strong>Check parameter type:</strong></li>
</ol>
<pre><code class="language-yaml">parameters:
  definitions:
    environment:
      type: String  # Must pass string value

# Pass correct type
prodigy run workflow.yml --param environment="production"  # String
# Not: --param environment=123  # Number
</code></pre>
<ol start="2">
<li><strong>Verify validation expression:</strong></li>
</ol>
<pre><code class="language-yaml">parameters:
  definitions:
    environment:
      type: String
      validation: "matches('^(dev|staging|prod)$')"

# Must match regex pattern
prodigy run workflow.yml --param environment="staging"  # OK
# Not: --param environment="test"  # Fails validation
</code></pre>
<ol start="3">
<li><strong>Check required vs optional:</strong></li>
</ol>
<pre><code class="language-yaml">parameters:
  required:
    - environment  # Must provide

# Error if missing:
prodigy run workflow.yml  # Fails

# Solution:
prodigy run workflow.yml --param environment="dev"
</code></pre>
<p><strong>See also:</strong> <a href="composition/parameter-definitions.html">Parameter Definitions</a> for complete parameter validation reference.</p>
<h3 id="import-path-resolution-errors"><a class="header" href="#import-path-resolution-errors">Import Path Resolution Errors</a></h3>
<p><strong>Error:</strong></p>
<pre><code>Error: Failed to load import: shared/utilities.yml
  No such file or directory
</code></pre>
<p><strong>Cause:</strong> Import path doesn’t exist or is incorrect.</p>
<p><strong>Solutions:</strong></p>
<ol>
<li><strong>Use absolute path:</strong></li>
</ol>
<pre><code class="language-yaml">imports:
  - path: "/full/path/to/shared/utilities.yml"
</code></pre>
<ol start="2">
<li><strong>Verify relative path:</strong></li>
</ol>
<pre><code class="language-bash"># From workflow file directory
ls shared/utilities.yml

# If in different location:
imports:
  - path: "../shared/utilities.yml"  # Go up one level
</code></pre>
<ol start="3">
<li><strong>Check current directory:</strong></li>
</ol>
<pre><code class="language-bash"># Run from correct directory
cd /path/to/workflows
prodigy run my-workflow.yml
</code></pre>
<h3 id="type-mismatch-errors"><a class="header" href="#type-mismatch-errors">Type Mismatch Errors</a></h3>
<p><strong>Error:</strong></p>
<pre><code>Error: Type mismatch for parameter 'timeout'
  Expected Number, got String "300"
</code></pre>
<p><strong>Cause:</strong> Parameter value type doesn’t match definition.</p>
<p><strong>Solutions:</strong></p>
<ol>
<li><strong>Pass correct type:</strong></li>
</ol>
<pre><code class="language-bash"># Number type - no quotes
prodigy run workflow.yml --param timeout=300

# String type - use quotes
prodigy run workflow.yml --param environment="production"

# Boolean type - no quotes
prodigy run workflow.yml --param enable_debug=true
</code></pre>
<ol start="2">
<li><strong>Check parameter file format:</strong></li>
</ol>
<pre><code class="language-json">{
  "timeout": 300,        // Number (no quotes)
  "environment": "prod", // String (quotes)
  "debug": true          // Boolean (no quotes)
}
</code></pre>
<h3 id="base-workflow-resolution-failures"><a class="header" href="#base-workflow-resolution-failures">Base Workflow Resolution Failures</a></h3>
<p><strong>Error:</strong></p>
<pre><code>Error: Failed to resolve base workflow: base-config.yml
</code></pre>
<p><strong>Cause:</strong> Extended workflow file not found.</p>
<p><strong>Solutions:</strong></p>
<ol>
<li><strong>Verify extends path:</strong></li>
</ol>
<pre><code class="language-yaml"># Relative to current workflow file
extends: "base-config.yml"         # Same directory
extends: "../base/config.yml"      # Parent directory
extends: "shared/base-config.yml"  # Subdirectory
</code></pre>
<ol start="2">
<li><strong>Use absolute path:</strong></li>
</ol>
<pre><code class="language-yaml">extends: "/full/path/to/base-config.yml"
</code></pre>
<ol start="3">
<li><strong>Check file exists:</strong></li>
</ol>
<pre><code class="language-bash">ls -la base-config.yml
</code></pre>
<h3 id="parameter-substitution-issues"><a class="header" href="#parameter-substitution-issues">Parameter Substitution Issues</a></h3>
<p><strong>Error:</strong></p>
<pre><code>Workflow runs but ${param} appears literally in output
</code></pre>
<p><strong>Cause:</strong> Parameter not found in the parameters map, or incorrect syntax.</p>
<p><strong>Status:</strong> Parameter substitution is fully implemented and works in all command types.</p>
<p><strong>Source:</strong> Implemented in src/cook/workflow/composition/composer.rs:760-877 (supports Simple, Structured, WorkflowStep, and SimpleObject command types)</p>
<p><strong>Solutions:</strong></p>
<ol>
<li><strong>Verify parameter is defined:</strong></li>
</ol>
<pre><code class="language-yaml">parameters:
  definitions:
    environment:
      type: String

# Then use in commands
commands:
  - shell: "echo Deploying to ${environment}"
  - claude: "/deploy ${environment}"
</code></pre>
<ol start="2">
<li><strong>Check parameter is provided:</strong></li>
</ol>
<pre><code class="language-bash"># Parameter must be passed at runtime
prodigy run workflow.yml --param environment="production"
</code></pre>
<ol start="3">
<li><strong>Use correct syntax:</strong></li>
</ol>
<pre><code class="language-yaml"># Correct: ${param_name}
- shell: "process ${target_file}"

# Incorrect: $param_name (shell variable, not Prodigy parameter)
- shell: "process $target_file"
</code></pre>
<ol start="4">
<li><strong>Parameter substitution works in all command types:</strong></li>
</ol>
<pre><code class="language-yaml">commands:
  # Simple string commands
  - "shell: process ${file}"

  # Structured commands
  - name: "process"
    args: ["${file}", "${output}"]

  # WorkflowStep format
  - shell: "test ${file}"
    id: "test-${file}"

  # SimpleObject format
  - name: "build"
    args: ["${target}"]
</code></pre>
<p><strong>Supported value types:</strong></p>
<ul>
<li>Strings: Used as-is</li>
<li>Numbers: Converted to string representation</li>
<li>Booleans: Converted to “true” or “false”</li>
<li>Arrays/Objects: Serialized as JSON</li>
<li>Null: Becomes empty string</li>
</ul>
<p><strong>See also:</strong> <a href="composition/parameter-definitions.html">Parameter Definitions</a> for parameter syntax reference.</p>
<h3 id="sub-workflow-execution-issues"><a class="header" href="#sub-workflow-execution-issues">Sub-Workflow Execution Issues</a></h3>
<p><strong>Error:</strong></p>
<pre><code>Sub-workflows defined but not executing as expected
</code></pre>
<p><strong>Status:</strong> Sub-workflow execution is fully implemented via SubWorkflowExecutor.</p>
<p><strong>Source:</strong> Implemented in src/cook/workflow/composition/sub_workflow.rs:67-176</p>
<p><strong>Common Issues:</strong></p>
<ol>
<li><strong>Sub-workflow file path incorrect:</strong></li>
</ol>
<pre><code class="language-yaml"># Verify the source path exists
workflows:
  build:
    source: "workflows/build.yml"  # Must exist relative to current file
</code></pre>
<pre><code class="language-bash"># Check file exists
ls workflows/build.yml
</code></pre>
<ol start="2">
<li><strong>Parameter type mismatch:</strong></li>
</ol>
<pre><code class="language-yaml"># Sub-workflow parameters must match defined types
workflows:
  deploy:
    source: "deploy.yml"
    parameters:
      timeout: 300        # Number, not "300"
      environment: "prod" # String with quotes
</code></pre>
<ol start="3">
<li><strong>Input/output mapping errors:</strong></li>
</ol>
<pre><code class="language-yaml"># Input variables must exist in parent context
workflows:
  test:
    source: "test.yml"
    inputs:
      target_file: "build_output"  # Parent var 'build_output' must exist
    outputs:
      - "test_result"  # Will be available in parent after execution
</code></pre>
<ol start="4">
<li><strong>Timeout too short:</strong></li>
</ol>
<pre><code class="language-yaml">workflows:
  long_running:
    source: "build.yml"
    timeout: 60  # Seconds - may be too short

# Increase if sub-workflow times out
workflows:
  long_running:
    source: "build.yml"
    timeout: 600  # 10 minutes
</code></pre>
<p><strong>Supported features:</strong></p>
<ul>
<li>Parameter passing (JSON values)</li>
<li>Input/output variable mapping</li>
<li>Context isolation (sub-workflow has clean context)</li>
<li>Error handling with <code>continue_on_error</code> flag</li>
<li>Timeout support</li>
<li>Parallel execution with <code>parallel: true</code></li>
</ul>
<p><strong>See also:</strong> <a href="composition/sub-workflows.html">Sub-Workflows</a> for complete usage guide.</p>
<h3 id="default-values-not-applied"><a class="header" href="#default-values-not-applied">Default Values Not Applied</a></h3>
<p><strong>Error:</strong></p>
<pre><code>Parameters require values even though defaults are set
</code></pre>
<p><strong>Status:</strong> Default values are fully applied through the apply_defaults method.</p>
<p><strong>Source:</strong> Implemented in src/cook/workflow/composition/composer.rs:217-257</p>
<p><strong>How defaults work:</strong></p>
<ol>
<li><strong>Workflow-level defaults are applied to environment variables:</strong></li>
</ol>
<pre><code class="language-yaml">defaults:
  TIMEOUT: "300"
  ENVIRONMENT: "dev"

# These become available as env vars in all commands
commands:
  - shell: "echo Timeout: $TIMEOUT"  # Uses default
</code></pre>
<ol start="2">
<li><strong>Parameter-level defaults work differently:</strong></li>
</ol>
<pre><code class="language-yaml">parameters:
  definitions:
    timeout:
      type: Number
      default: 300  # Used if not provided at runtime

# Run without providing timeout
prodigy run workflow.yml  # Uses default 300
</code></pre>
<ol start="3">
<li><strong>Precedence order (highest to lowest):</strong>
<ul>
<li>Explicitly provided parameter values (–param)</li>
<li>Parameter definition defaults</li>
<li>Workflow-level defaults</li>
<li>No value (error if required parameter)</li>
</ul>
</li>
</ol>
<p><strong>Common mistakes:</strong></p>
<ol>
<li><strong>Workflow defaults don’t set parameter values:</strong></li>
</ol>
<pre><code class="language-yaml"># This does NOT work as expected
defaults:
  timeout: 300  # Sets env var TIMEOUT, not parameter 'timeout'

parameters:
  required:
    - timeout  # Still required!

# Solution: Use parameter default instead
parameters:
  definitions:
    timeout:
      type: Number
      default: 300  # Now parameter has a default
</code></pre>
<ol start="2">
<li><strong>Existing values are not overwritten:</strong></li>
</ol>
<pre><code class="language-yaml"># If a value is already set, defaults don't override
parameters:
  definitions:
    timeout:
      type: Number
      default: 300  # Only used if not already set

# This overrides the default
prodigy run workflow.yml --param timeout=600
</code></pre>
<p><strong>See also:</strong> <a href="composition/default-values.html">Default Values</a> for complete default value semantics.</p>
<h3 id="url-template-source-errors"><a class="header" href="#url-template-source-errors">URL Template Source Errors</a></h3>
<p><strong>Error:</strong></p>
<pre><code>Error: URL template sources are not yet implemented
</code></pre>
<p><strong>Cause:</strong> URL-based template loading is planned but not implemented.</p>
<p><strong>Solutions:</strong></p>
<ol>
<li><strong>Download template to file:</strong></li>
</ol>
<pre><code class="language-bash">curl https://example.com/template.yml &gt; /tmp/template.yml
</code></pre>
<ol start="2">
<li><strong>Use file-based template:</strong></li>
</ol>
<pre><code class="language-yaml">template:
  source:
    file: "/tmp/template.yml"
</code></pre>
<ol start="3">
<li><strong>Add to registry:</strong></li>
</ol>
<pre><code class="language-bash">curl https://example.com/template.yml &gt; ~/.prodigy/templates/template.yml
</code></pre>
<pre><code class="language-yaml">template:
  source:
    registry: "template"
</code></pre>
<h3 id="debugging-strategies"><a class="header" href="#debugging-strategies">Debugging Strategies</a></h3>
<p><strong>Enable Verbose Logging:</strong></p>
<pre><code class="language-bash"># Show composition steps
prodigy run workflow.yml -v

# Show detailed debug output
prodigy run workflow.yml -vv

# Show trace-level output (includes full composition details)
prodigy run workflow.yml -vvv
</code></pre>
<p><strong>Dry-Run Validation:</strong></p>
<pre><code class="language-bash"># Validate composition without execution
prodigy run workflow.yml --dry-run

# Combine with verbose mode to see composition steps
prodigy run workflow.yml --dry-run -vv
</code></pre>
<p><strong>Isolate Composition Layers:</strong></p>
<pre><code class="language-bash"># Test base workflow alone
prodigy run base-config.yml --dry-run

# Add one composition feature at a time
# 1. Test with imports only
# 2. Add extends
# 3. Add template
# 4. Add parameters
</code></pre>
<p><strong>Check File Permissions:</strong></p>
<pre><code class="language-bash"># Verify read access
ls -la workflow.yml base-config.yml

# Check registry permissions
ls -la ~/.prodigy/templates/
ls -la .prodigy/templates/
</code></pre>
<p><strong>Verify JSON Syntax:</strong></p>
<pre><code class="language-bash"># Validate param file
jq . params.json

# Check for syntax errors
cat params.json | jq empty
</code></pre>
<p><strong>Trace Parameter Substitution:</strong></p>
<pre><code class="language-bash"># Use verbose mode to see parameter values
prodigy run workflow.yml --param environment="prod" -vv

# Check which parameters are being substituted
</code></pre>
<p><strong>Debug Sub-Workflow Execution:</strong></p>
<pre><code class="language-bash"># Test sub-workflow independently first
prodigy run workflows/build.yml --dry-run

# Then test from parent workflow
prodigy run main.yml -vv  # See sub-workflow execution logs
</code></pre>
<h3 id="getting-help-2"><a class="header" href="#getting-help-2">Getting Help</a></h3>
<p>If issues persist:</p>
<ol>
<li><strong>Check implementation status</strong> in relevant subsection docs</li>
<li><strong>Review error context</strong> in error messages</li>
<li><strong>Use verbose mode</strong> (-vv or -vvv) to understand what’s happening</li>
<li><strong>Test components independently</strong> (base workflows, sub-workflows, templates)</li>
<li><strong>File issue</strong> with minimal reproduction case</li>
<li><strong>Include:</strong>
<ul>
<li>Workflow files</li>
<li>Command used</li>
<li>Full error output</li>
<li>Prodigy version</li>
<li>Output from verbose mode (-vv)</li>
</ul>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h2 id="related-chapters-1"><a class="header" href="#related-chapters-1">Related Chapters</a></h2>
<h3 id="foundation"><a class="header" href="#foundation">Foundation</a></h3>
<ul>
<li><a href="composition/../workflow-basics.html">Workflow Basics</a> - Fundamental workflow concepts and structure</li>
<li><a href="composition/../commands.html">Commands</a> - Command types, execution, and error handling</li>
<li><a href="composition/../variables/index.html">Variables</a> - Variable interpolation, substitution, and scoping</li>
</ul>
<h3 id="advanced-topics-2"><a class="header" href="#advanced-topics-2">Advanced Topics</a></h3>
<ul>
<li><a href="composition/../environment.html">Environment Variables</a> - Environment variable configuration and profiles</li>
<li><a href="composition/../error-handling.html">Error Handling</a> - Error handling patterns and retry strategies</li>
<li><a href="composition/../examples.html">Examples</a> - Practical workflow examples demonstrating composition patterns</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="further-reading-1"><a class="header" href="#further-reading-1">Further Reading</a></h2>
<p>This section provides additional resources for deepening your understanding of Prodigy’s workflow composition system, including source code references, related documentation, and external resources.</p>
<h3 id="source-code-implementation"><a class="header" href="#source-code-implementation">Source Code Implementation</a></h3>
<p>For developers interested in understanding the implementation details, explore these core modules:</p>
<p><strong>Core Composition Engine</strong> (<code>src/cook/workflow/composition/</code>)</p>
<ul>
<li>The main composition implementation directory containing the template registry, dependency resolution logic, and workflow composition engine</li>
<li>Entry point: <code>mod.rs</code> defines the public API and core types (<code>ComposableWorkflow</code>, <code>CompositionMetadata</code>, <code>ParameterDefinitions</code>)</li>
</ul>
<p><strong>Template Registry</strong> (<code>src/cook/workflow/composition/registry.rs</code>)</p>
<ul>
<li>Implements template storage, retrieval, and management</li>
<li>Provides the <code>TemplateRegistry</code> struct with methods for registering, loading, and validating workflow templates</li>
<li>Includes file-based storage implementation with template versioning support</li>
<li>Referenced in: <a href="composition/template-system.html">Template System</a></li>
</ul>
<p><strong>Dependency Resolution</strong> (<code>src/cook/workflow/composition/composer.rs</code>)</p>
<ul>
<li>Implements the <code>WorkflowComposer</code> that handles workflow composition from multiple sources</li>
<li>Resolves dependencies between workflows, templates, and imports</li>
<li>Performs parameter interpolation and validation</li>
<li>Handles circular dependency detection and resolution order</li>
<li>Referenced in: <a href="composition/workflow-extension-inheritance.html">Workflow Extension &amp; Inheritance</a></li>
</ul>
<p><strong>Sub-Workflow Execution</strong> (<code>src/cook/workflow/composition/sub_workflow.rs</code>)</p>
<ul>
<li>Defines types and structures for sub-workflow execution (implementation in progress)</li>
<li>Includes <code>SubWorkflow</code> definition with execution modes (parallel/sequential)</li>
<li>Referenced in: <a href="composition/sub-workflows.html">Sub-Workflows</a></li>
</ul>
<p><strong>Integration Tests</strong> (<code>tests/workflow_composition_test.rs</code>)</p>
<ul>
<li>Comprehensive test suite covering workflow composition, template registration, parameter validation, and dependency resolution</li>
<li>Provides practical examples of API usage for:
<ul>
<li>Basic workflow composition</li>
<li>Parameter definitions with type checking</li>
<li>Template registry operations</li>
<li>Import and extension workflows</li>
<li>Circular dependency detection</li>
</ul>
</li>
<li>Excellent starting point for understanding how to use the composition API programmatically</li>
</ul>
<h3 id="related-subsections"><a class="header" href="#related-subsections">Related Subsections</a></h3>
<p>To explore specific aspects of workflow composition in depth:</p>
<ul>
<li><strong><a href="composition/template-system.html">Template System</a></strong> - Learn about creating and using reusable workflow templates</li>
<li><strong><a href="composition/workflow-extension-inheritance.html">Workflow Extension &amp; Inheritance</a></strong> - Understand how to extend base workflows and inherit configurations</li>
<li><strong><a href="composition/parameter-definitions.html">Parameter Definitions</a></strong> - Master parameter type validation and default values</li>
<li><strong><a href="composition/sub-workflows.html">Sub-Workflows</a></strong> - Execute workflows within workflows for complex orchestration patterns</li>
<li><strong><a href="composition/default-values.html">Default Values</a></strong> - Configure sensible defaults for parameters and settings</li>
<li><strong><a href="composition/composition-metadata.html">Composition Metadata</a></strong> - Track composition sources and dependencies</li>
<li><strong><a href="composition/best-practices.html">Best Practices</a></strong> - Follow established patterns for maintainable workflow composition</li>
<li><strong><a href="composition/troubleshooting.html">Troubleshooting</a></strong> - Resolve common composition issues and errors</li>
<li><strong><a href="composition/complete-examples.html">Complete Examples</a></strong> - See real-world examples of workflow composition in action</li>
</ul>
<h3 id="related-chapters-2"><a class="header" href="#related-chapters-2">Related Chapters</a></h3>
<p>Workflow composition integrates with other Prodigy features:</p>
<ul>
<li><strong><a href="composition/../configuration/index.html">Configuration</a></strong> - Global and project-level configuration for composition behavior</li>
<li><strong><a href="composition/../environment/index.html">Environment Variables</a></strong> - Use environment variables in composed workflows</li>
<li><strong><a href="composition/../variables/index.html">Variables</a></strong> - Variable interpolation and capture in composed workflows</li>
<li><strong><a href="composition/../workflow-basics/index.html">Workflow Basics</a></strong> - Foundation for understanding workflow structure</li>
</ul>
<h3 id="implementation-specifications"><a class="header" href="#implementation-specifications">Implementation Specifications</a></h3>
<p>The workflow composition system is implemented according to these specifications:</p>
<ul>
<li><strong>Spec 131</strong>: Template Registry and Storage</li>
<li><strong>Spec 132</strong>: Workflow Composition Engine and Dependency Resolution</li>
<li><strong>Spec 133</strong>: Sub-Workflow Execution Framework</li>
</ul>
<p>These specs are referenced in the main codebase and track the implementation progress of composition features. See the <a href="composition/index.html#implementation-status">Implementation Status</a> section for current progress.</p>
<h3 id="external-resources-1"><a class="header" href="#external-resources-1">External Resources</a></h3>
<p>For broader context on workflow composition and orchestration:</p>
<p><strong>YAML Best Practices</strong></p>
<ul>
<li><a href="https://yaml.org/spec/">YAML Specification</a> - Official YAML 1.2 specification</li>
<li><a href="https://yaml.org/">YAML Ain’t Markup Language</a> - YAML homepage with tutorials and examples</li>
<li><a href="https://www.cloudbees.com/blog/yaml-tutorial-everything-you-need-get-started">CloudBees YAML Best Practices</a> - Practical tips for writing maintainable YAML</li>
</ul>
<p><strong>Workflow Orchestration Patterns</strong></p>
<ul>
<li><a href="http://www.workflowpatterns.com/">Workflow Patterns</a> - Academic resource on workflow patterns and control flow</li>
<li><a href="https://airflow.apache.org/docs/apache-airflow/stable/concepts/index.html">Airflow Concepts</a> - DAG composition and dependencies (similar concepts to Prodigy)</li>
<li><a href="https://docs.github.com/en/actions/using-workflows/reusing-workflows">GitHub Actions Reusable Workflows</a> - Another approach to workflow composition</li>
</ul>
<p><strong>Template Design Principles</strong></p>
<ul>
<li><a href="https://jinja.palletsprojects.com/en/stable/templates/">Jinja2 Template Designer Documentation</a> - Similar template concepts (parameter passing, inheritance)</li>
<li><a href="https://helm.sh/docs/chart_template_guide/">Helm Templates Guide</a> - Template best practices from Kubernetes ecosystem</li>
<li><a href="https://json-schema.org/">JSON Schema</a> - Parameter validation patterns (similar to Prodigy’s parameter definitions)</li>
</ul>
<p><strong>Configuration Management</strong></p>
<ul>
<li><a href="https://12factor.net/config">The Twelve-Factor App: Config</a> - Principles for configuration management</li>
<li><a href="https://gohugohq.com/partials/yaml-vs-toml-vs-json/">TOML vs YAML vs JSON</a> - Comparison of configuration formats</li>
</ul>
<h3 id="learning-path"><a class="header" href="#learning-path">Learning Path</a></h3>
<p><strong>For Beginners:</strong></p>
<ol>
<li>Start with <a href="composition/../workflow-basics/index.html">Workflow Basics</a> to understand fundamental workflow structure</li>
<li>Read <a href="composition/template-system.html">Template System</a> to learn about creating reusable components</li>
<li>Explore <a href="composition/complete-examples.html">Complete Examples</a> to see composition in practice</li>
<li>Review the integration tests (<code>tests/workflow_composition_test.rs</code>) for API usage examples</li>
</ol>
<p><strong>For Advanced Users:</strong></p>
<ol>
<li>Study <a href="composition/workflow-extension-inheritance.html">Workflow Extension &amp; Inheritance</a> for complex composition patterns</li>
<li>Master <a href="composition/parameter-definitions.html">Parameter Definitions</a> with advanced type validation</li>
<li>Explore the source code in <code>src/cook/workflow/composition/</code> to understand implementation details</li>
<li>Read <a href="composition/best-practices.html">Best Practices</a> for production-ready composition strategies</li>
<li>Contribute: Examine Spec 131-133 and help complete sub-workflow execution implementation</li>
</ol>
<p><strong>For Contributors:</strong></p>
<ol>
<li>Review the composition source code (<code>src/cook/workflow/composition/</code>)</li>
<li>Study existing tests in <code>tests/workflow_composition_test.rs</code></li>
<li>Check the <a href="composition/index.html#implementation-status">Implementation Status</a> for areas needing work</li>
<li>Follow the patterns established in <code>composer.rs</code> and <code>registry.rs</code></li>
<li>Add test coverage for any new composition features</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="retry-configuration"><a class="header" href="#retry-configuration">Retry Configuration</a></h1>
<p>Prodigy provides sophisticated retry mechanisms with multiple backoff strategies to handle transient failures gracefully. The retry system supports both command-level and workflow-level configurations with fine-grained control over retry behavior.</p>
<h2 id="overview-9"><a class="header" href="#overview-9">Overview</a></h2>
<p>Prodigy has two retry systems that work together:</p>
<ol>
<li><strong>Enhanced Retry System</strong> - Rich, configurable retry with multiple backoff strategies, jitter, circuit breakers, and conditional retry (from <code>src/cook/retry_v2.rs</code>)</li>
<li><strong>Workflow-Level Retry</strong> - Simpler retry configuration for workflow-level error policies (from <code>src/cook/workflow/error_policy.rs</code>)</li>
</ol>
<p>This chapter focuses on the enhanced retry system which provides comprehensive retry capabilities. Circuit breakers prevent cascading failures by temporarily stopping retries when a threshold of consecutive failures is reached.</p>
<h3 id="when-to-use-each-retry-system"><a class="header" href="#when-to-use-each-retry-system">When to Use Each Retry System</a></h3>
<p><strong>Use Enhanced Retry (retry_v2) for:</strong></p>
<ul>
<li>Individual command execution failures (API calls, shell commands, file operations)</li>
<li>Operations needing fine-grained control over backoff strategies</li>
<li>Situations requiring conditional retry based on error types</li>
<li>Commands where jitter is needed to prevent thundering herd</li>
<li>External API calls with rate limiting</li>
<li>Operations benefiting from circuit breakers</li>
</ul>
<p><strong>Use Workflow-Level Retry (error_policy) for:</strong></p>
<ul>
<li>MapReduce work item failures</li>
<li>Workflow-wide error handling policies</li>
<li>Bulk operations requiring Dead Letter Queue (DLQ) integration</li>
<li>Scenarios needing failure thresholds and batch error collection</li>
<li>When you want to retry entire work items rather than individual commands</li>
</ul>
<p>For a detailed comparison with examples, see <a href="retry-configuration/workflow-level-vs-command-level-retry.html">Workflow-Level vs Command-Level Retry</a>.</p>
<h2 id="retryconfig-structure"><a class="header" href="#retryconfig-structure">RetryConfig Structure</a></h2>
<p>This table documents the enhanced retry system (<code>retry_v2::RetryConfig</code>). For workflow-level retry configuration, see the <a href="retry-configuration/workflow-level-vs-command-level-retry.html">Workflow-Level vs Command-Level Retry</a> subsection.</p>
<p>The <code>RetryConfig</code> struct controls retry behavior with the following fields:</p>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr></thead><tbody>
<tr><td><code>attempts</code></td><td><code>u32</code></td><td><code>3</code></td><td>Maximum number of retry attempts</td></tr>
<tr><td><code>backoff</code></td><td><code>BackoffStrategy</code></td><td><code>Exponential (base: 2.0)</code></td><td>Strategy for calculating delays between retries</td></tr>
<tr><td><code>initial_delay</code></td><td><code>Duration</code></td><td><code>1s</code></td><td>Initial delay before first retry</td></tr>
<tr><td><code>max_delay</code></td><td><code>Duration</code></td><td><code>30s</code></td><td>Maximum delay between any two retries</td></tr>
<tr><td><code>jitter</code></td><td><code>bool</code></td><td><code>false</code></td><td>Whether to add randomness to delays</td></tr>
<tr><td><code>jitter_factor</code></td><td><code>f64</code></td><td><code>0.3</code></td><td>Amount of jitter (0.0 to 1.0)</td></tr>
<tr><td><code>retry_on</code></td><td><code>Vec&lt;ErrorMatcher&gt;</code></td><td><code>[]</code></td><td>Retry only on specific error types (empty = retry all)</td></tr>
<tr><td><code>retry_budget</code></td><td><code>Option&lt;Duration&gt;</code></td><td><code>None</code></td><td>Maximum total time for all retry attempts</td></tr>
<tr><td><code>on_failure</code></td><td><code>FailureAction</code></td><td><code>Stop</code></td><td>Action to take after all retries exhausted</td></tr>
</tbody></table>
</div>
<p><strong>Source</strong>: RetryConfig struct defined in <code>src/cook/retry_v2.rs:14-52</code></p>
<h3 id="yaml-configuration-syntax"><a class="header" href="#yaml-configuration-syntax">YAML Configuration Syntax</a></h3>
<p>The RetryConfig fields map to YAML workflow syntax as follows:</p>
<pre><code class="language-yaml">commands:
  - shell: "your-command-here"
    retry_config:
      attempts: 5                    # RetryConfig.attempts (u32)
      backoff:
        type: exponential            # BackoffStrategy::Exponential
        base: 2.0                    # exponential base multiplier
      initial_delay: "1s"            # RetryConfig.initial_delay (humantime format)
      max_delay: "30s"               # RetryConfig.max_delay (humantime format)
      jitter: true                   # RetryConfig.jitter (bool)
      jitter_factor: 0.3             # RetryConfig.jitter_factor (0.0-1.0)
      retry_on:                      # RetryConfig.retry_on (Vec&lt;ErrorMatcher&gt;)
        - network
        - timeout
        - server_error
      retry_budget: "5m"             # RetryConfig.retry_budget (Optional&lt;Duration&gt;)
      on_failure: stop               # RetryConfig.on_failure (FailureAction)
</code></pre>
<p><strong>Alternative Backoff Strategies</strong>:</p>
<pre><code class="language-yaml"># Fixed delay
backoff: fixed

# Linear backoff
backoff:
  type: linear
  increment: "2s"

# Fibonacci backoff
backoff: fibonacci

# Custom delay sequence
backoff:
  type: custom
  delays: ["1s", "2s", "5s", "10s"]
</code></pre>
<p><strong>Note</strong>: Field names use snake_case in YAML but map to the exact struct fields in <code>src/cook/retry_v2.rs:14-52</code>. Duration values use humantime format (e.g., “1s”, “30s”, “5m”).</p>
<p>For complete working examples, see <a href="retry-configuration/complete-examples.html">Complete Examples</a>.</p>
<h3 id="circuit-breakers"><a class="header" href="#circuit-breakers">Circuit Breakers</a></h3>
<p>Circuit breakers are configured separately via <code>RetryExecutor</code>, <strong>not as part of RetryConfig</strong>. Circuit breakers provide fail-fast behavior when downstream systems are consistently failing, preventing resource exhaustion from repeated failed retries.</p>
<p><strong>Configuration</strong> (programmatic):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let executor = RetryExecutor::new(retry_config)
    .with_circuit_breaker(
        5,                          // failure_threshold: open after 5 consecutive failures
        Duration::from_secs(30)     // recovery_timeout: attempt recovery after 30 seconds
    );
<span class="boring">}</span></code></pre></pre>
<p><strong>Source</strong>: <code>src/cook/retry_v2.rs:184-188</code> (with_circuit_breaker method), <code>src/cook/retry_v2.rs:325-397</code> (CircuitBreaker implementation)</p>
<p><strong>Circuit States</strong>:</p>
<ul>
<li><strong>Closed</strong>: Normal operation, retries are attempted</li>
<li><strong>Open</strong>: Circuit tripped, requests fail immediately without retry</li>
<li><strong>HalfOpen</strong>: Testing recovery, limited requests allowed</li>
</ul>
<p>See <a href="retry-configuration/best-practices.html">Best Practices</a> for guidance on combining retry with circuit breakers for high-reliability systems.</p>
<h2 id="additional-topics-7"><a class="header" href="#additional-topics-7">Additional Topics</a></h2>
<p>See also:</p>
<ul>
<li><a href="retry-configuration/basic-retry-configuration.html">Basic Retry Configuration</a></li>
<li><a href="retry-configuration/backoff-strategies.html">Backoff Strategies</a></li>
<li><a href="retry-configuration/backoff-strategy-comparison.html">Backoff Strategy Comparison</a></li>
<li><a href="retry-configuration/jitter-for-distributed-systems.html">Jitter for Distributed Systems</a></li>
<li><a href="retry-configuration/conditional-retry-with-error-matchers.html">Conditional Retry with Error Matchers</a></li>
<li><a href="retry-configuration/retry-budget.html">Retry Budget</a></li>
<li><a href="retry-configuration/failure-actions.html">Failure Actions</a></li>
<li><a href="retry-configuration/complete-examples.html">Complete Examples</a></li>
<li><a href="retry-configuration/workflow-level-vs-command-level-retry.html">Workflow-Level vs Command-Level Retry</a></li>
<li><a href="retry-configuration/retry-metrics-and-observability.html">Retry Metrics and Observability</a></li>
<li><a href="retry-configuration/best-practices.html">Best Practices</a></li>
<li><a href="retry-configuration/troubleshooting.html">Troubleshooting</a></li>
<li><a href="retry-configuration/related-topics.html">Related Topics</a></li>
<li><a href="retry-configuration/implementation-references.html">Implementation References</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="basic-retry-configuration"><a class="header" href="#basic-retry-configuration">Basic Retry Configuration</a></h2>
<p>The enhanced retry system (<code>retry_v2</code>) provides command-level retry capabilities with sophisticated backoff strategies and error handling. This subsection covers the basic retry configuration options.</p>
<blockquote>
<p><strong>Note</strong>: This documents the enhanced retry system (<code>retry_v2::RetryConfig</code>). For workflow-level retry configuration, see <a href="retry-configuration/./workflow-level-vs-command-level-retry.html">Workflow-Level vs Command-Level Retry</a>.</p>
</blockquote>
<h3 id="where-retry-config-is-used"><a class="header" href="#where-retry-config-is-used">Where Retry Config is Used</a></h3>
<p>Retry configuration can be applied at the command level in your workflow YAML files. The <code>retry_config</code> field is available on individual commands to control how that specific command is retried on failure.</p>
<p><strong>Source</strong>: <code>src/cook/retry_v2.rs:14-52</code> (RetryConfig struct definition)</p>
<h3 id="basic-configuration-example"><a class="header" href="#basic-configuration-example">Basic Configuration Example</a></h3>
<p>The simplest retry configuration uses just the attempts field:</p>
<pre><code class="language-yaml">commands:
  - shell: "curl https://api.example.com/data"
    retry_config:
      attempts: 3
</code></pre>
<p>This configuration will:</p>
<ul>
<li>Retry the command up to 3 times on failure</li>
<li>Use exponential backoff with base 2.0 (default)</li>
<li>Start with 1 second initial delay (default)</li>
<li>Cap delays at 30 seconds maximum (default)</li>
<li>Retry on all error types (default when <code>retry_on</code> is empty)</li>
</ul>
<h3 id="complete-basic-configuration"><a class="header" href="#complete-basic-configuration">Complete Basic Configuration</a></h3>
<p>For more control, you can specify all basic retry parameters:</p>
<pre><code class="language-yaml">commands:
  - shell: "make test"
    retry_config:
      attempts: 5              # Maximum retry attempts (default: 3)
      initial_delay: "2s"      # Initial delay between retries (default: 1s)
      max_delay: "60s"         # Maximum delay cap (default: 30s)
      backoff: exponential     # Backoff strategy (default: exponential)
</code></pre>
<p><strong>Field Reference</strong> (from <code>src/cook/retry_v2.rs:54-68</code>):</p>
<ul>
<li><code>attempts: u32</code> - Maximum number of retry attempts (default: 3)</li>
<li><code>initial_delay: Duration</code> - Starting delay between retries (default: 1 second)</li>
<li><code>max_delay: Duration</code> - Maximum delay ceiling (default: 30 seconds)</li>
<li><code>backoff: BackoffStrategy</code> - Strategy for calculating delays (default: Exponential { base: 2.0 })</li>
</ul>
<h3 id="default-behavior"><a class="header" href="#default-behavior">Default Behavior</a></h3>
<p>When <code>retry_config</code> is omitted entirely, commands run <strong>without retry</strong>. When <code>retry_config</code> is present but fields are omitted, defaults from <code>RetryConfig::default()</code> apply:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Default values (src/cook/retry_v2.rs:54-68)
RetryConfig {
    attempts: 3,
    backoff: BackoffStrategy::Exponential { base: 2.0 },
    initial_delay: Duration::from_secs(1),
    max_delay: Duration::from_secs(30),
    jitter: false,
    jitter_factor: 0.3,
    retry_on: Vec::new(),  // Empty = retry all errors
    retry_budget: None,
    on_failure: FailureAction::Stop,
}
<span class="boring">}</span></code></pre></pre>
<h3 id="relationship-to-command-execution"><a class="header" href="#relationship-to-command-execution">Relationship to Command Execution</a></h3>
<p>When a command with <code>retry_config</code> fails, the <code>RetryExecutor</code> orchestrates the retry logic:</p>
<ol>
<li><strong>Execute Command</strong>: Run the shell/Claude command</li>
<li><strong>Check Result</strong>: If successful, return immediately</li>
<li><strong>Check Retry Budget</strong>: If set, ensure budget not exceeded</li>
<li><strong>Calculate Delay</strong>: Use backoff strategy to determine next delay</li>
<li><strong>Wait</strong>: Sleep for calculated delay (with optional jitter)</li>
<li><strong>Retry</strong>: Execute command again</li>
<li><strong>Repeat</strong>: Continue until success, max attempts reached, or budget exhausted</li>
</ol>
<p><strong>Source</strong>: <code>src/cook/retry_v2.rs:191-262</code> (RetryExecutor::execute_with_retry)</p>
<h3 id="integration-with-retryconfig-struct"><a class="header" href="#integration-with-retryconfig-struct">Integration with RetryConfig Struct</a></h3>
<p>All YAML retry configuration maps directly to the <code>RetryConfig</code> struct fields:</p>
<div class="table-wrapper"><table><thead><tr><th>YAML Field</th><th>Rust Field</th><th>Type</th><th>Default</th></tr></thead><tbody>
<tr><td><code>attempts</code></td><td><code>attempts</code></td><td><code>u32</code></td><td>3</td></tr>
<tr><td><code>initial_delay</code></td><td><code>initial_delay</code></td><td><code>Duration</code></td><td>1s</td></tr>
<tr><td><code>max_delay</code></td><td><code>max_delay</code></td><td><code>Duration</code></td><td>30s</td></tr>
<tr><td><code>backoff</code></td><td><code>backoff</code></td><td><code>BackoffStrategy</code></td><td>Exponential</td></tr>
<tr><td><code>jitter</code></td><td><code>jitter</code></td><td><code>bool</code></td><td>false</td></tr>
<tr><td><code>jitter_factor</code></td><td><code>jitter_factor</code></td><td><code>f64</code></td><td>0.3</td></tr>
<tr><td><code>retry_on</code></td><td><code>retry_on</code></td><td><code>Vec&lt;ErrorMatcher&gt;</code></td><td>[] (all)</td></tr>
<tr><td><code>retry_budget</code></td><td><code>retry_budget</code></td><td><code>Option&lt;Duration&gt;</code></td><td>None</td></tr>
<tr><td><code>on_failure</code></td><td><code>on_failure</code></td><td><code>FailureAction</code></td><td>Stop</td></tr>
</tbody></table>
</div>
<p><strong>Source</strong>: <code>src/cook/retry_v2.rs:14-52</code></p>
<h3 id="minimal-vs-full-configuration"><a class="header" href="#minimal-vs-full-configuration">Minimal vs Full Configuration</a></h3>
<p><strong>Minimal</strong> (use defaults for most fields):</p>
<pre><code class="language-yaml">retry_config:
  attempts: 5
</code></pre>
<p><strong>Full</strong> (explicit control over all parameters):</p>
<pre><code class="language-yaml">retry_config:
  attempts: 10
  backoff: fibonacci
  initial_delay: "500ms"
  max_delay: "2m"
  jitter: true
  jitter_factor: 0.5
  retry_budget: "10m"
  retry_on:
    - network
    - timeout
  on_failure: continue
</code></pre>
<h3 id="see-also-24"><a class="header" href="#see-also-24">See Also</a></h3>
<ul>
<li><a href="retry-configuration/./backoff-strategies.html">Backoff Strategies</a> - Detailed backoff strategy documentation</li>
<li><a href="retry-configuration/./conditional-retry-with-error-matchers.html">Conditional Retry with Error Matchers</a> - Selective retry with <code>retry_on</code></li>
<li><a href="retry-configuration/./failure-actions.html">Failure Actions</a> - What happens after final failure</li>
<li><a href="retry-configuration/./jitter-for-distributed-systems.html">Jitter for Distributed Systems</a> - Preventing thundering herd</li>
<li><a href="retry-configuration/./retry-budget.html">Retry Budget</a> - Time-based retry limits</li>
<li><a href="retry-configuration/./complete-examples.html">Complete Examples</a> - Full workflow examples</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="backoff-strategies-1"><a class="header" href="#backoff-strategies-1">Backoff Strategies</a></h2>
<blockquote>
<p><strong>Note</strong>: This subsection documents the <strong>enhanced retry system</strong> (<code>retry_v2::RetryConfig</code>) used for command-level retry configuration. For workflow-level retry (MapReduce error policies), see <a href="retry-configuration/./workflow-level-vs-command-level-retry.html">Workflow-Level vs Command-Level Retry</a>. The enhanced system provides more sophisticated backoff options and features.</p>
</blockquote>
<p>Prodigy supports five backoff strategies for controlling delay between retries. Backoff strategies determine how the delay between retry attempts increases over time, helping to avoid overwhelming systems while maximizing chances of success.</p>
<p>All backoff strategies use <code>initial_delay</code> as the base delay and respect the <code>max_delay</code> cap. Delays are calculated per attempt and can be combined with <a href="retry-configuration/jitter-for-distributed-systems.html">jitter</a> to avoid thundering herd problems.</p>
<p><strong>Source</strong>: BackoffStrategy enum defined in <code>src/cook/retry_v2.rs:70-98</code></p>
<p><strong>Default Strategy</strong>: If no backoff strategy is specified, Prodigy uses <strong>Exponential</strong> backoff with a base of 2.0 (src/cook/retry_v2.rs:92-98).</p>
<h3 id="fixed-backoff-1"><a class="header" href="#fixed-backoff-1">Fixed Backoff</a></h3>
<p>Fixed backoff uses a constant delay between all retry attempts. This is the simplest strategy and works well when you want predictable, consistent retry timing.</p>
<p><strong>Delay Pattern</strong>: Same delay for every attempt</p>
<ul>
<li>Attempt 1: <code>initial_delay</code></li>
<li>Attempt 2: <code>initial_delay</code></li>
<li>Attempt 3: <code>initial_delay</code></li>
</ul>
<p><strong>YAML Configuration</strong>:</p>
<pre><code class="language-yaml">map:
  agent_template:
    - shell: "flaky-command"
      retry_config:
        attempts: 5
        initial_delay: "2s"
        backoff: fixed
</code></pre>
<blockquote>
<p><strong>Note</strong>: <code>backoff: fixed</code> is the shorthand for the Fixed unit variant. Equivalent to <code>backoff: { fixed: null }</code> (src/cook/retry_v2.rs:75).</p>
</blockquote>
<p><strong>When to Use</strong>:</p>
<ul>
<li>Simple retry scenarios where timing is not critical</li>
<li>Testing and development environments</li>
<li>When you want predictable retry intervals</li>
</ul>
<h3 id="linear-backoff-1"><a class="header" href="#linear-backoff-1">Linear Backoff</a></h3>
<p>Linear backoff increases the delay by a fixed increment for each retry attempt. This provides gradual backoff that’s easy to reason about.</p>
<p><strong>Delay Pattern</strong>: Increases by constant increment</p>
<ul>
<li>Attempt 1: <code>initial_delay</code></li>
<li>Attempt 2: <code>initial_delay + increment</code></li>
<li>Attempt 3: <code>initial_delay + 2 * increment</code></li>
</ul>
<p><strong>YAML Configuration</strong>:</p>
<pre><code class="language-yaml">map:
  agent_template:
    - shell: "database-query"
      retry_config:
        attempts: 5
        initial_delay: "1s"
        backoff:
          linear:
            increment: "2s"
</code></pre>
<p><strong>Example Timeline</strong> (initial_delay=1s, increment=2s):</p>
<ul>
<li>Attempt 1: 1s</li>
<li>Attempt 2: 3s (1s + 2s)</li>
<li>Attempt 3: 5s (1s + 4s)</li>
<li>Attempt 4: 7s (1s + 6s)</li>
</ul>
<p><strong>When to Use</strong>:</p>
<ul>
<li>Moderate load situations</li>
<li>When you need predictable but increasing delays</li>
<li>API rate limiting scenarios with linear cooldown</li>
</ul>
<h3 id="exponential-backoff"><a class="header" href="#exponential-backoff">Exponential Backoff</a></h3>
<p>Exponential backoff multiplies the delay by a base factor for each retry, causing delays to grow rapidly. This is the <strong>default strategy</strong> and is recommended for most retry scenarios.</p>
<p><strong>Delay Pattern</strong>: Multiplies by base^(attempt-1)</p>
<ul>
<li>Attempt 1: <code>initial_delay * base^0</code></li>
<li>Attempt 2: <code>initial_delay * base^1</code></li>
<li>Attempt 3: <code>initial_delay * base^2</code></li>
</ul>
<p><strong>YAML Configuration</strong> (default base=2.0):</p>
<pre><code class="language-yaml">map:
  agent_template:
    - shell: "network-call"
      retry_config:
        attempts: 5
        initial_delay: "1s"
        max_delay: "60s"
        # Uses exponential backoff with base 2.0 by default
</code></pre>
<p><strong>YAML Configuration</strong> (custom base):</p>
<pre><code class="language-yaml">map:
  agent_template:
    - shell: "api-request"
      retry_config:
        attempts: 5
        initial_delay: "1s"
        max_delay: "60s"
        backoff:
          exponential:
            base: 3.0  # More aggressive backoff
</code></pre>
<p><strong>Example Timeline</strong> (initial_delay=1s, base=2.0):</p>
<ul>
<li>Attempt 1: 1s (1 * 2^0)</li>
<li>Attempt 2: 2s (1 * 2^1)</li>
<li>Attempt 3: 4s (1 * 2^2)</li>
<li>Attempt 4: 8s (1 * 2^3)</li>
<li>Attempt 5: 16s (1 * 2^4)</li>
</ul>
<p><strong>When to Use</strong>:</p>
<ul>
<li>Network requests and API calls (default choice)</li>
<li>Situations where quick retries might make things worse</li>
<li>Distributed systems with cascading failures</li>
<li>When you want rapid backoff to give systems time to recover</li>
</ul>
<h3 id="fibonacci-backoff-1"><a class="header" href="#fibonacci-backoff-1">Fibonacci Backoff</a></h3>
<p>Fibonacci backoff uses the Fibonacci sequence (1, 1, 2, 3, 5, 8, 13…) to calculate delays. This provides a middle ground between linear and exponential backoff, growing quickly but not as aggressively as exponential.</p>
<p><strong>Delay Pattern</strong>: Uses Fibonacci sequence multiplier</p>
<ul>
<li>Attempt 1: <code>initial_delay * 1</code></li>
<li>Attempt 2: <code>initial_delay * 1</code></li>
<li>Attempt 3: <code>initial_delay * 2</code></li>
<li>Attempt 4: <code>initial_delay * 3</code></li>
<li>Attempt 5: <code>initial_delay * 5</code></li>
</ul>
<p><strong>YAML Configuration</strong>:</p>
<pre><code class="language-yaml">map:
  agent_template:
    - shell: "distributed-operation"
      retry_config:
        attempts: 6
        initial_delay: "1s"
        max_delay: "30s"
        backoff: fibonacci
</code></pre>
<blockquote>
<p><strong>Note</strong>: <code>backoff: fibonacci</code> is the shorthand for the Fibonacci unit variant, similar to Fixed (src/cook/retry_v2.rs:87).</p>
</blockquote>
<p><strong>Example Timeline</strong> (initial_delay=1s):</p>
<ul>
<li>Attempt 1: 1s (fib(1) = 1)</li>
<li>Attempt 2: 1s (fib(2) = 1)</li>
<li>Attempt 3: 2s (fib(3) = 2)</li>
<li>Attempt 4: 3s (fib(4) = 3)</li>
<li>Attempt 5: 5s (fib(5) = 5)</li>
<li>Attempt 6: 8s (fib(6) = 8)</li>
</ul>
<p><strong>When to Use</strong>:</p>
<ul>
<li>Distributed systems where you want balanced backoff</li>
<li>Situations where exponential is too aggressive</li>
<li>When you want retry intervals that grow naturally but moderately</li>
</ul>
<h3 id="custom-backoff"><a class="header" href="#custom-backoff">Custom Backoff</a></h3>
<p>Custom backoff allows you to specify an explicit sequence of delays for complete control over retry timing. If the retry attempt exceeds the number of delays specified, the <code>max_delay</code> is used.</p>
<p><strong>Delay Pattern</strong>: Uses explicit delay list</p>
<ul>
<li>Delays are used in order from the array</li>
<li>If attempts exceed delays array length, uses <code>max_delay</code></li>
</ul>
<p><strong>YAML Configuration</strong>:</p>
<pre><code class="language-yaml">map:
  agent_template:
    - shell: "custom-retry-operation"
      retry_config:
        attempts: 5
        max_delay: "60s"
        backoff:
          custom:
            delays:
              - secs: 1
                nanos: 0
              - secs: 3
                nanos: 0
              - secs: 7
                nanos: 0
              - secs: 15
                nanos: 0
              # Attempt 5 would use max_delay (60s)
</code></pre>
<blockquote>
<p><strong>Note</strong>: Custom backoff delays use Duration struct format (<code>{secs: N, nanos: 0}</code>) instead of humantime strings like “1s”. This is because <code>Vec&lt;Duration&gt;</code> doesn’t have the <code>humantime_serde</code> annotation (src/cook/retry_v2.rs:89).</p>
</blockquote>
<p><strong>Example Timeline</strong>:</p>
<ul>
<li>Attempt 1: 1s (delays[0])</li>
<li>Attempt 2: 3s (delays[1])</li>
<li>Attempt 3: 7s (delays[2])</li>
<li>Attempt 4: 15s (delays[3])</li>
<li>Attempt 5: 60s (max_delay, delays[4] doesn’t exist)</li>
</ul>
<p><strong>When to Use</strong>:</p>
<ul>
<li>When you need precise control over retry timing</li>
<li>Integration with third-party APIs with specific retry requirements</li>
<li>Complex retry scenarios with non-standard delay patterns</li>
<li>Testing specific timing scenarios</li>
</ul>
<p><strong>Edge Cases</strong>:</p>
<ul>
<li>Empty delays array: Falls back to <code>max_delay</code> for all attempts</li>
<li>Fewer delays than attempts: Uses <code>max_delay</code> for remaining attempts</li>
</ul>
<h2 id="integration-with-retryconfig"><a class="header" href="#integration-with-retryconfig">Integration with RetryConfig</a></h2>
<p>Backoff strategies work together with other retry configuration options:</p>
<p><strong>Complete Example</strong>:</p>
<pre><code class="language-yaml">map:
  input: "work-items.json"
  json_path: "$.items[*]"

  agent_template:
    - shell: "process-item ${item.id}"
      retry_config:
        attempts: 5
        initial_delay: "2s"      # Base delay for backoff calculation
        max_delay: "60s"         # Cap on calculated delays
        backoff:
          exponential:
            base: 2.0
        jitter: true             # Add randomization (±25% by default)
        jitter_factor: 0.25
        retry_on:
          - timeout              # Built-in timeout matcher
          - pattern: "connection refused"  # Custom pattern for specific errors
</code></pre>
<p><strong>How It Works Together</strong>:</p>
<ol>
<li>Backoff strategy calculates base delay using <code>initial_delay</code></li>
<li>Calculated delay is capped at <code>max_delay</code></li>
<li>If <code>jitter: true</code>, randomization is applied (±<code>jitter_factor</code> percentage)</li>
<li>Final delay is applied before next retry attempt</li>
<li>If <code>retry_on</code> is specified, error must match one of the matchers to trigger retry</li>
</ol>
<blockquote>
<p><strong>Error Matcher Syntax</strong>: The <code>retry_on</code> field uses <code>ErrorMatcher</code> enum variants. Built-in matchers (<code>timeout</code>, <code>network</code>, <code>server_error</code>, <code>rate_limit</code>) use lowercase names. Custom patterns use <code>pattern: "regex"</code> syntax. See <a href="retry-configuration/conditional-retry-with-error-matchers.html">Conditional Retry with Error Matchers</a> for complete documentation.</p>
</blockquote>
<p>See also:</p>
<ul>
<li><a href="retry-configuration/basic-retry-configuration.html">Basic Retry Configuration</a> - Overall retry configuration options</li>
<li><a href="retry-configuration/backoff-strategy-comparison.html">Backoff Strategy Comparison</a> - Visual comparison of strategies</li>
<li><a href="retry-configuration/jitter-for-distributed-systems.html">Jitter for Distributed Systems</a> - How jitter prevents thundering herd</li>
<li><a href="retry-configuration/retry-metrics-and-observability.html">Retry Metrics and Observability</a> - Track retry performance</li>
<li><a href="retry-configuration/best-practices.html">Best Practices</a> - When to use which strategy</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="backoff-strategy-comparison"><a class="header" href="#backoff-strategy-comparison">Backoff Strategy Comparison</a></h2>
<p>This comparison assumes <code>initial_delay = 1s</code> for all strategies. Actual delays depend on your retry configuration. All strategies are capped by <code>max_delay</code> (default 30s) as enforced in <code>src/cook/retry_v2.rs:304</code>.</p>
<p><strong>Source</strong>: Delay calculations from <code>src/cook/retry_v2.rs:284-305</code> (calculate_delay method)</p>
<h3 id="strategy-comparison-table"><a class="header" href="#strategy-comparison-table">Strategy Comparison Table</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Strategy</th><th>Attempt 1</th><th>Attempt 2</th><th>Attempt 3</th><th>Attempt 4</th><th>Attempt 5</th><th>Best For</th></tr></thead><tbody>
<tr><td><strong>Fixed (2s)</strong></td><td>2s</td><td>2s</td><td>2s</td><td>2s</td><td>2s</td><td>Predictable timing, simple errors without exponential backpressure</td></tr>
<tr><td><strong>Linear (+2s)</strong></td><td>1s</td><td>3s</td><td>5s</td><td>7s</td><td>9s</td><td>Moderate load reduction, gradual backoff without aggressive delays</td></tr>
<tr><td><strong>Exponential (base 2.0)</strong></td><td>1s</td><td>2s</td><td>4s</td><td>8s</td><td>16s</td><td><strong>Recommended default</strong> - Aggressive backoff for most failures, best for temporary outages</td></tr>
<tr><td><strong>Fibonacci</strong></td><td>1s</td><td>1s</td><td>2s</td><td>3s</td><td>5s</td><td>Gentler than exponential, good for rate limits or distributed systems</td></tr>
<tr><td><strong>Custom</strong></td><td>user-defined</td><td>user-defined</td><td>user-defined</td><td>user-defined</td><td>user-defined</td><td>Specific delay patterns, custom business logic</td></tr>
</tbody></table>
</div>
<p><strong>Note</strong>: Exponential is the default backoff strategy (see <code>src/cook/retry_v2.rs:92-97</code>)</p>
<h3 id="strategy-formulas"><a class="header" href="#strategy-formulas">Strategy Formulas</a></h3>
<p>Each strategy calculates delay using a different formula:</p>
<p><strong>Fixed</strong> (src/cook/retry_v2.rs:286):</p>
<pre><code>delay = initial_delay
</code></pre>
<p>Always returns the same delay, regardless of attempt number.</p>
<p><strong>Linear</strong> (src/cook/retry_v2.rs:287-289):</p>
<pre><code>delay = initial_delay + increment * (attempt - 1)
</code></pre>
<p>Example: With <code>initial_delay = 1s</code> and <code>increment = 2s</code>:</p>
<ul>
<li>Attempt 1: 1s + 2s × (1-1) = 1s</li>
<li>Attempt 2: 1s + 2s × (2-1) = 3s</li>
<li>Attempt 3: 1s + 2s × (3-1) = 5s</li>
</ul>
<p><strong>Exponential</strong> (src/cook/retry_v2.rs:290-292):</p>
<pre><code>delay = initial_delay * base^(attempt - 1)
</code></pre>
<p>Example: With <code>initial_delay = 1s</code> and <code>base = 2.0</code>:</p>
<ul>
<li>Attempt 1: 1s × 2^(1-1) = 1s × 1 = 1s</li>
<li>Attempt 2: 1s × 2^(2-1) = 1s × 2 = 2s</li>
<li>Attempt 3: 1s × 2^(3-1) = 1s × 4 = 4s</li>
</ul>
<p>The <code>base</code> parameter is configurable (default: 2.0). Common values:</p>
<ul>
<li><code>base = 2.0</code>: Aggressive backoff (doubles each retry)</li>
<li><code>base = 1.5</code>: Gentler exponential growth</li>
<li><code>base = 3.0</code>: Very aggressive backoff</li>
</ul>
<p><strong>Fibonacci</strong> (src/cook/retry_v2.rs:294-297):</p>
<pre><code>delay = initial_delay * fibonacci(attempt)
</code></pre>
<p>Example: With <code>initial_delay = 1s</code>:</p>
<ul>
<li>Attempt 1: 1s × fibonacci(1) = 1s × 1 = 1s</li>
<li>Attempt 2: 1s × fibonacci(2) = 1s × 1 = 1s</li>
<li>Attempt 3: 1s × fibonacci(3) = 1s × 2 = 2s</li>
<li>Attempt 4: 1s × fibonacci(4) = 1s × 3 = 3s</li>
<li>Attempt 5: 1s × fibonacci(5) = 1s × 5 = 5s</li>
</ul>
<p>Fibonacci sequence: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34…</p>
<p><strong>Custom</strong> (src/cook/retry_v2.rs:298-301):</p>
<pre><code>delay = delays[attempt - 1] or max_delay if out of bounds
</code></pre>
<p>Allows you to specify exact delays for each attempt. If retry exceeds the array length, <code>max_delay</code> is used.</p>
<h3 id="max-delay-cap"><a class="header" href="#max-delay-cap">Max Delay Cap</a></h3>
<p><strong>All strategies are capped by <code>max_delay</code></strong> (src/cook/retry_v2.rs:304):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>base_delay.min(self.config.max_delay)
<span class="boring">}</span></code></pre></pre>
<p>This prevents unbounded delays in exponential/fibonacci strategies. Default <code>max_delay = 30s</code> (src/cook/retry_v2.rs:30).</p>
<p>Example with <code>max_delay = 30s</code> and Exponential (base 2.0):</p>
<ul>
<li>Attempt 6: Would be 32s → capped to 30s</li>
<li>Attempt 7+: All capped to 30s</li>
</ul>
<h3 id="jitter-support"><a class="header" href="#jitter-support">Jitter Support</a></h3>
<p><strong>All strategies support optional jitter</strong> to prevent thundering herd problems (src/cook/retry_v2.rs:308-317).</p>
<p>When <code>jitter = true</code> (default <code>jitter_factor = 0.3</code>), delays are randomized:</p>
<pre><code>jitter_range = delay * jitter_factor
actual_delay = delay + random(-jitter_range/2, +jitter_range/2)
</code></pre>
<p>Example with 1s delay and 30% jitter (factor 0.3):</p>
<ul>
<li>jitter_range = 1s × 0.3 = 0.3s</li>
<li>actual_delay = random(0.85s, 1.15s)</li>
</ul>
<p>See <a href="retry-configuration/jitter-for-distributed-systems.html">Jitter for Distributed Systems</a> for when to use jitter.</p>
<h3 id="yaml-configuration-examples"><a class="header" href="#yaml-configuration-examples">YAML Configuration Examples</a></h3>
<p><strong>Fixed Delay</strong>:</p>
<pre><code class="language-yaml">retry_config:
  backoff: fixed
  initial_delay: "2s"  # Always wait 2s between retries
</code></pre>
<p><strong>Linear Backoff</strong>:</p>
<pre><code class="language-yaml">retry_config:
  backoff:
    type: linear
    increment: "2s"
  initial_delay: "1s"  # 1s, 3s, 5s, 7s, 9s...
</code></pre>
<p><strong>Exponential Backoff</strong> (default):</p>
<pre><code class="language-yaml">retry_config:
  backoff:
    type: exponential
    base: 2.0  # Optional: default is 2.0
  initial_delay: "1s"  # 1s, 2s, 4s, 8s, 16s...
  max_delay: "30s"     # Cap at 30s
</code></pre>
<p><strong>Fibonacci Backoff</strong>:</p>
<pre><code class="language-yaml">retry_config:
  backoff: fibonacci
  initial_delay: "1s"  # 1s, 1s, 2s, 3s, 5s, 8s...
</code></pre>
<p><strong>Custom Delays</strong>:</p>
<pre><code class="language-yaml">retry_config:
  backoff:
    type: custom
    delays: ["1s", "2s", "5s", "10s", "30s"]
  max_delay: "60s"  # Fallback if attempts exceed array length
</code></pre>
<p><strong>With Jitter</strong> (any strategy):</p>
<pre><code class="language-yaml">retry_config:
  backoff: exponential
  initial_delay: "1s"
  jitter: true         # Enable randomization
  jitter_factor: 0.3   # 30% jitter (±15%)
</code></pre>
<h3 id="choosing-the-right-strategy"><a class="header" href="#choosing-the-right-strategy">Choosing the Right Strategy</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Use Case</th><th>Recommended Strategy</th><th>Reasoning</th></tr></thead><tbody>
<tr><td><strong>Temporary service outage</strong></td><td>Exponential (base 2.0)</td><td>Quickly backs off to avoid overwhelming recovering service</td></tr>
<tr><td><strong>Rate limiting (429 errors)</strong></td><td>Fibonacci or Linear</td><td>Gentler backoff respects rate limits without excessive delays</td></tr>
<tr><td><strong>Network flakiness</strong></td><td>Exponential + Jitter</td><td>Aggressive backoff with jitter prevents thundering herd</td></tr>
<tr><td><strong>Predictable timing needs</strong></td><td>Fixed</td><td>Consistent delay for deterministic behavior</td></tr>
<tr><td><strong>Gradual load shedding</strong></td><td>Linear</td><td>Steady increase allows system to recover gradually</td></tr>
<tr><td><strong>Custom business logic</strong></td><td>Custom</td><td>Full control over delay pattern (e.g., comply with API retry-after headers)</td></tr>
<tr><td><strong>Distributed systems</strong></td><td>Fibonacci + Jitter</td><td>Balances quick retries with avoiding cascade failures</td></tr>
</tbody></table>
</div>
<p><strong>Default recommendation</strong>: Exponential with <code>base = 2.0</code> is the default for good reason - it works well for most transient failures while avoiding excessive load on failing systems.</p>
<h3 id="performance-comparison"><a class="header" href="#performance-comparison">Performance Comparison</a></h3>
<p>Time to reach <code>max_delay = 30s</code> with <code>initial_delay = 1s</code>:</p>
<div class="table-wrapper"><table><thead><tr><th>Strategy</th><th>Attempts to Max</th><th>Total Time (5 attempts)</th></tr></thead><tbody>
<tr><td>Fixed (2s)</td><td>Never (always 2s)</td><td>10s</td></tr>
<tr><td>Linear (+2s)</td><td>15 attempts</td><td>25s (1+3+5+7+9)</td></tr>
<tr><td>Exponential (base 2.0)</td><td>5 attempts</td><td>31s (1+2+4+8+16)</td></tr>
<tr><td>Fibonacci</td><td>9 attempts</td><td>12s (1+1+2+3+5)</td></tr>
</tbody></table>
</div>
<p><strong>Key insight</strong>: Exponential reaches max delay fastest, making it most aggressive. Fibonacci is gentler, making it better for gradual recovery scenarios.</p>
<h2 id="see-also-25"><a class="header" href="#see-also-25">See Also</a></h2>
<ul>
<li><a href="retry-configuration/backoff-strategies.html">Backoff Strategies</a> - Detailed explanation of each strategy</li>
<li><a href="retry-configuration/basic-retry-configuration.html">Basic Retry Configuration</a> - How to configure retry in workflows</li>
<li><a href="retry-configuration/jitter-for-distributed-systems.html">Jitter for Distributed Systems</a> - When and how to use jitter</li>
<li><a href="retry-configuration/complete-examples.html">Complete Examples</a> - Full workflow examples with retry configuration</li>
<li><a href="retry-configuration/best-practices.html">Best Practices</a> - Guidelines for effective retry strategies</li>
</ul>
<p><strong>Source References</strong>:</p>
<ul>
<li>BackoffStrategy enum: <code>src/cook/retry_v2.rs:70-90</code></li>
<li>Delay calculation: <code>src/cook/retry_v2.rs:284-305</code></li>
<li>Jitter application: <code>src/cook/retry_v2.rs:308-317</code></li>
<li>Default values: <code>src/cook/retry_v2.rs:26-31</code> (initial_delay, max_delay)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="jitter-for-distributed-systems"><a class="header" href="#jitter-for-distributed-systems">Jitter for Distributed Systems</a></h2>
<p>Jitter adds randomness to retry delays to prevent the “thundering herd” problem where many clients retry at the same time. By introducing controlled randomness, jitter helps distribute retry attempts over time rather than having all clients retry simultaneously after a failure.</p>
<h3 id="configuration-2"><a class="header" href="#configuration-2">Configuration</a></h3>
<pre><code class="language-yaml">retry:
  attempts: 5
  backoff:
    exponential:
      base: 2.0
  initial_delay: 10s
  jitter: true           # Enable jitter (default: false)
  jitter_factor: 0.3     # Jitter factor (default: 0.3 when jitter is enabled)
</code></pre>
<p><strong>Configuration options:</strong></p>
<ul>
<li><code>jitter</code>: Boolean flag to enable/disable jitter (default: <code>false</code>)</li>
<li><code>jitter_factor</code>: Controls the randomness range as a fraction of the base delay (default: <code>0.3</code>)</li>
</ul>
<p><strong>Source</strong>: Configuration structure defined in <code>src/cook/retry_v2.rs:455-457</code> (default_jitter_factor)</p>
<h3 id="how-jitter-works"><a class="header" href="#how-jitter-works">How Jitter Works</a></h3>
<p>The jitter calculation follows this formula:</p>
<pre><code>jitter_range = base_delay * jitter_factor
jitter = random(-jitter_range / 2, +jitter_range / 2)
final_delay = base_delay + jitter
</code></pre>
<p><strong>Source</strong>: Implementation in <code>src/cook/retry_v2.rs:308-317</code> (apply_jitter method)</p>
<p>The implementation uses Rust’s <code>random_range</code> function to generate a uniformly distributed random value within the inclusive range. The jitter can be:</p>
<ul>
<li><strong>Negative</strong>: Reduces the delay below the base value</li>
<li><strong>Positive</strong>: Increases the delay above the base value</li>
</ul>
<p>This bidirectional randomness ensures retry attempts are spread across time.</p>
<h3 id="example-calculations"><a class="header" href="#example-calculations">Example Calculations</a></h3>
<p><strong>With default <code>jitter_factor: 0.3</code>:</strong></p>
<p>For a 10 second base delay:</p>
<ol>
<li><code>jitter_range = 10s * 0.3 = 3s</code></li>
<li><code>jitter = random(-1.5s, +1.5s)</code></li>
<li><code>final_delay = 10s + jitter</code> → <strong>Range: 8.5s to 11.5s</strong></li>
</ol>
<p><strong>With <code>jitter_factor: 0.5</code>:</strong></p>
<p>For a 10 second base delay:</p>
<ol>
<li><code>jitter_range = 10s * 0.5 = 5s</code></li>
<li><code>jitter = random(-2.5s, +2.5s)</code></li>
<li><code>final_delay = 10s + jitter</code> → <strong>Range: 7.5s to 12.5s</strong></li>
</ol>
<p>For a 20 second base delay:</p>
<ol>
<li><code>jitter_range = 20s * 0.5 = 10s</code></li>
<li><code>jitter = random(-5s, +5s)</code></li>
<li><code>final_delay = 20s + jitter</code> → <strong>Range: 15s to 25s</strong></li>
</ol>
<p><strong>Source</strong>: Test validation in <code>src/cook/retry_v2.rs:654-658</code></p>
<h3 id="interaction-with-max-delay"><a class="header" href="#interaction-with-max-delay">Interaction with Max Delay</a></h3>
<p>Jitter is applied <strong>after</strong> backoff calculation and max_delay capping. The execution order is:</p>
<ol>
<li>Calculate base delay using backoff strategy</li>
<li>Apply <code>max_delay</code> cap if configured</li>
<li>Apply jitter to the capped delay</li>
</ol>
<p>This means jittered delays can still exceed <code>max_delay</code> temporarily, but only by the jitter amount. If you need strict delay caps, consider using a lower <code>jitter_factor</code>.</p>
<p><strong>Source</strong>: Execution order in <code>src/cook/retry_v2.rs:234-235</code> (calculate_delay followed by apply_jitter)</p>
<h3 id="when-to-use-jitter"><a class="header" href="#when-to-use-jitter">When to Use Jitter</a></h3>
<p><strong>Recommended use cases:</strong></p>
<ul>
<li><strong>Multiple clients accessing the same service</strong>: Prevents all clients from retrying simultaneously after a service outage</li>
<li><strong>Distributed systems with many workers</strong>: Spreads retry load across worker nodes</li>
<li><strong>Rate-limited APIs</strong>: Reduces the chance of hitting rate limits from synchronized retries</li>
<li><strong>Preventing synchronized retry storms</strong>: Breaks up “thundering herd” patterns that can overwhelm recovering services</li>
</ul>
<p><strong>Adjusting jitter_factor:</strong></p>
<ul>
<li><strong>Low values (0.1 - 0.2)</strong>: Tight clustering with minor randomization, suitable for stable systems</li>
<li><strong>Medium values (0.3 - 0.5)</strong>: Balanced spread, recommended for most distributed systems (default: 0.3)</li>
<li><strong>High values (0.6 - 1.0)</strong>: Wide distribution, useful for highly contended resources or aggressive load spreading</li>
</ul>
<h3 id="performance-impact"><a class="header" href="#performance-impact">Performance Impact</a></h3>
<p>Jitter adds minimal computational overhead (a single random number generation per retry). The randomization happens in-memory and does not require external resources.</p>
<h3 id="complete-example-with-multiple-backoff-strategies"><a class="header" href="#complete-example-with-multiple-backoff-strategies">Complete Example with Multiple Backoff Strategies</a></h3>
<p><strong>Exponential backoff with jitter:</strong></p>
<pre><code class="language-yaml">retry:
  attempts: 5
  backoff:
    exponential:
      base: 2.0
  initial_delay: 1s
  max_delay: 30s
  jitter: true
  jitter_factor: 0.3
</code></pre>
<p>Delay sequence (without jitter): 1s, 2s, 4s, 8s, 16s
Delay sequence (with jitter): ~0.85s-1.15s, ~1.7s-2.3s, ~3.4s-4.6s, ~6.8s-9.2s, ~13.6s-18.4s</p>
<p><strong>Linear backoff with jitter:</strong></p>
<pre><code class="language-yaml">retry:
  attempts: 5
  backoff:
    linear:
      increment: 2s
  initial_delay: 5s
  jitter: true
  jitter_factor: 0.4
</code></pre>
<p>Delay sequence (without jitter): 5s, 7s, 9s, 11s, 13s
Delay sequence (with jitter): ~4s-6s, ~5.6s-8.4s, ~7.2s-10.8s, ~8.8s-13.2s, ~10.4s-15.6s</p>
<h3 id="see-also-26"><a class="header" href="#see-also-26">See Also</a></h3>
<ul>
<li><a href="retry-configuration/backoff-strategies.html">Backoff Strategies</a> - Overview of different backoff strategies</li>
<li><a href="retry-configuration/basic-retry-configuration.html">Basic Retry Configuration</a> - Core retry configuration options</li>
<li><a href="retry-configuration/best-practices.html">Best Practices</a> - Recommendations for retry configuration in production systems</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="conditional-retry-with-error-matchers"><a class="header" href="#conditional-retry-with-error-matchers">Conditional Retry with Error Matchers</a></h2>
<p>By default, Prodigy retries all errors when <code>retry_on</code> is empty. Use the <code>retry_on</code> field to retry only specific error types, allowing fine-grained control over which failures should trigger retries.</p>
<p><strong>Source</strong>: <code>src/cook/retry_v2.rs:100-151</code> (ErrorMatcher enum definition)</p>
<p><strong>Case Sensitivity Behavior</strong>:</p>
<ul>
<li><strong>Built-in matchers</strong> (Network, Timeout, ServerError, RateLimit): <strong>Case-insensitive</strong> - error messages are normalized to lowercase before matching</li>
<li><strong>Pattern matcher</strong>: <strong>Case-sensitive by default</strong> - matches against original error message case (src/cook/retry_v2.rs:142-148)
<ul>
<li>Use regex flag <code>(?i)</code> for case-insensitive pattern matching</li>
<li>Example: <code>pattern: '(?i)database locked'</code> matches “Database Locked”, “DATABASE LOCKED”, etc.</li>
</ul>
</li>
</ul>
<h3 id="available-error-matchers"><a class="header" href="#available-error-matchers">Available Error Matchers</a></h3>
<p>The <code>ErrorMatcher</code> enum provides five built-in matchers for common error categories:</p>
<h4 id="1-network-errors"><a class="header" href="#1-network-errors">1. Network Errors</a></h4>
<p>Matches network connectivity issues:</p>
<pre><code class="language-yaml">retry_config:
  attempts: 5
  retry_on:
    - network
</code></pre>
<p><strong>Matches</strong> (case-insensitive):</p>
<ul>
<li>“network”</li>
<li>“connection”</li>
<li>“refused”</li>
<li>“unreachable”</li>
</ul>
<p><strong>Source</strong>: <code>src/cook/retry_v2.rs:128-132</code></p>
<p><strong>Use Case</strong>: Retrying HTTP requests, database connections, or API calls that fail due to network issues.</p>
<h4 id="2-timeout-errors"><a class="header" href="#2-timeout-errors">2. Timeout Errors</a></h4>
<p>Matches timeout-related failures:</p>
<pre><code class="language-yaml">retry_config:
  attempts: 3
  retry_on:
    - timeout
</code></pre>
<p><strong>Matches</strong> (case-insensitive):</p>
<ul>
<li>“timeout”</li>
<li>“timed out”</li>
</ul>
<p><strong>Source</strong>: <code>src/cook/retry_v2.rs:133-137</code></p>
<p><strong>Use Case</strong>: Retrying slow external services or operations with strict time limits.</p>
<h4 id="3-server-errors"><a class="header" href="#3-server-errors">3. Server Errors</a></h4>
<p>Matches HTTP 5xx server errors:</p>
<pre><code class="language-yaml">retry_config:
  attempts: 4
  retry_on:
    - server_error
</code></pre>
<p><strong>Matches</strong> (case-insensitive):</p>
<ul>
<li>“500”</li>
<li>“502”</li>
<li>“503”</li>
<li>“504”</li>
<li>“server error”</li>
</ul>
<p><strong>Source</strong>: <code>src/cook/retry_v2.rs:138-142</code></p>
<p><strong>Use Case</strong>: Retrying API requests during transient server failures or deployments.</p>
<h4 id="4-rate-limit-errors"><a class="header" href="#4-rate-limit-errors">4. Rate Limit Errors</a></h4>
<p>Matches rate limiting responses:</p>
<pre><code class="language-yaml">retry_config:
  attempts: 10
  initial_delay: "60s"
  retry_on:
    - rate_limit
</code></pre>
<p><strong>Matches</strong> (case-insensitive):</p>
<ul>
<li>“rate limit”</li>
<li>“429”</li>
<li>“too many requests”</li>
</ul>
<p><strong>Source</strong>: <code>src/cook/retry_v2.rs:143-147</code></p>
<p><strong>Use Case</strong>: Retrying API calls with exponential backoff when hitting rate limits.</p>
<h4 id="5-custom-pattern-matching"><a class="header" href="#5-custom-pattern-matching">5. Custom Pattern Matching</a></h4>
<p>Match specific error messages using regex patterns:</p>
<pre><code class="language-yaml">retry_config:
  attempts: 3
  retry_on:
    - pattern: "(?i)database locked"    # Case-insensitive: matches any case
    - pattern: "SQLITE_BUSY"            # Case-sensitive: exact match only
    - pattern: "(?i)temporary failure"  # Case-insensitive
</code></pre>
<p><strong>Pattern Syntax</strong>:</p>
<ul>
<li>Regex patterns matched against original error message (case-sensitive by default)</li>
<li>Use <code>(?i)</code> flag at start of pattern for case-insensitive matching</li>
<li>Invalid regex patterns return false (no match)</li>
</ul>
<p><strong>Source</strong>: <code>src/cook/retry_v2.rs:142-148</code> (Pattern variant implementation)</p>
<p><strong>Use Case</strong>: Matching application-specific error messages or database-specific errors.</p>
<p><strong>Case Sensitivity Examples</strong>:</p>
<ul>
<li><code>pattern: "SQLITE_BUSY"</code> - Only matches “SQLITE_BUSY” (not “sqlite_busy”)</li>
<li><code>pattern: "(?i)SQLITE_BUSY"</code> - Matches “SQLITE_BUSY”, “sqlite_busy”, “Sqlite_Busy”, etc.</li>
<li><code>pattern: "(?i)database.*locked"</code> - Case-insensitive regex with wildcards</li>
</ul>
<h3 id="combining-multiple-matchers"><a class="header" href="#combining-multiple-matchers">Combining Multiple Matchers</a></h3>
<p>You can specify multiple error matchers to retry on any of them:</p>
<pre><code class="language-yaml">retry_config:
  attempts: 5
  backoff: exponential
  initial_delay: "2s"
  retry_on:
    - network
    - timeout
    - server_error
</code></pre>
<p>This configuration retries if the error matches <strong>any</strong> of:</p>
<ul>
<li>Network errors</li>
<li>Timeout errors</li>
<li>Server errors (5xx)</li>
</ul>
<p><strong>Behavior</strong>: Matchers are evaluated with OR logic - if any matcher matches, the error is retryable.</p>
<h3 id="empty-retry_on-retry-all-errors"><a class="header" href="#empty-retry_on-retry-all-errors">Empty retry_on (Retry All Errors)</a></h3>
<p>When <code>retry_on</code> is empty or omitted, <strong>all errors trigger retry</strong>:</p>
<pre><code class="language-yaml">retry_config:
  attempts: 3
  # retry_on is empty - retries all errors
</code></pre>
<p><strong>Source</strong>: <code>src/cook/retry_v2.rs:42-43</code> (retry_on field with default <code>Vec::new()</code>)</p>
<p>This is equivalent to having no error filtering - every failure triggers the retry logic.</p>
<h3 id="selective-retry-example"><a class="header" href="#selective-retry-example">Selective Retry Example</a></h3>
<p>Only retry transient network and timeout issues, but fail immediately on other errors:</p>
<pre><code class="language-yaml">commands:
  - shell: "curl -f https://api.example.com/data"
    retry_config:
      attempts: 5
      backoff: exponential
      initial_delay: "1s"
      max_delay: "30s"
      retry_on:
        - network
        - timeout
</code></pre>
<p><strong>Behavior</strong>:</p>
<ul>
<li>If curl fails with “connection refused” → Retry</li>
<li>If curl fails with “timeout” → Retry</li>
<li>If curl fails with “404 Not Found” → <strong>Fail immediately</strong> (no retry)</li>
<li>If curl fails with “401 Unauthorized” → <strong>Fail immediately</strong> (no retry)</li>
</ul>
<h3 id="regex-pattern-syntax-and-case-sensitivity"><a class="header" href="#regex-pattern-syntax-and-case-sensitivity">Regex Pattern Syntax and Case Sensitivity</a></h3>
<p>Understanding case sensitivity is critical when using Pattern matchers:</p>
<div class="table-wrapper"><table><thead><tr><th>Matcher Type</th><th>Case Sensitivity</th><th>Normalization</th></tr></thead><tbody>
<tr><td>Network</td><td>Case-insensitive</td><td>Error converted to lowercase</td></tr>
<tr><td>Timeout</td><td>Case-insensitive</td><td>Error converted to lowercase</td></tr>
<tr><td>ServerError</td><td>Case-insensitive</td><td>Error converted to lowercase</td></tr>
<tr><td>RateLimit</td><td>Case-insensitive</td><td>Error converted to lowercase</td></tr>
<tr><td>Pattern</td><td><strong>Case-sensitive by default</strong></td><td>No normalization (original case)</td></tr>
</tbody></table>
</div>
<p><strong>Making Pattern Matching Case-Insensitive</strong>:</p>
<p>Use the <code>(?i)</code> flag at the start of your regex pattern:</p>
<pre><code class="language-yaml">retry_on:
  # Case-insensitive patterns (recommended)
  - pattern: "(?i)connection refused"  # Matches any case variation
  - pattern: "(?i)database.*locked"    # Case-insensitive with wildcards

  # Case-sensitive patterns (use with caution)
  - pattern: "SQLITE_BUSY"             # Only matches exact case
  - pattern: "ERROR: Authentication"   # Must match exact case
</code></pre>
<p><strong>Invalid Regex Handling</strong>:</p>
<p>If a pattern contains invalid regex syntax, it returns <code>false</code> (no match):</p>
<pre><code class="language-yaml">retry_on:
  - pattern: "[invalid(regex"  # Invalid syntax → returns false → no retry
</code></pre>
<p><strong>Source</strong>: src/cook/retry_v2.rs:142-148</p>
<h3 id="advanced-pattern-matching"><a class="header" href="#advanced-pattern-matching">Advanced Pattern Matching</a></h3>
<p>Use regex patterns for precise error matching:</p>
<pre><code class="language-yaml">retry_config:
  attempts: 3
  retry_on:
    # Case-insensitive patterns (recommended for flexible matching)
    - pattern: "(?i)SQLite.*database is locked"
    - pattern: "(?i)deadlock detected"

    # Case-sensitive pattern (exact match required)
    - pattern: "SQLITE_BUSY"
</code></pre>
<p><strong>Pattern Matching Logic</strong> (src/cook/retry_v2.rs:116-150):</p>
<ol>
<li>Each matcher’s <code>matches()</code> method is called with the error message</li>
<li>Built-in matchers normalize error to lowercase before checking</li>
<li>Pattern matcher applies regex to <strong>original case</strong> of error message</li>
<li>Invalid regex patterns return false (no match, no retry)</li>
<li>If any matcher returns true, error is retryable</li>
</ol>
<h3 id="implementation-details-2"><a class="header" href="#implementation-details-2">Implementation Details</a></h3>
<p>The matching logic is implemented in <code>ErrorMatcher::matches()</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Simplified implementation (src/cook/retry_v2.rs:116-150)
impl ErrorMatcher {
    pub fn matches(&amp;self, error_msg: &amp;str) -&gt; bool {
        let error_lower = error_msg.to_lowercase();
        match self {
            Self::Network =&gt; {
                // Case-insensitive matching via lowercase normalization
                error_lower.contains("network")
                    || error_lower.contains("connection")
                    || error_lower.contains("refused")
                    || error_lower.contains("unreachable")
            }
            Self::Timeout =&gt; {
                error_lower.contains("timeout") || error_lower.contains("timed out")
            }
            Self::ServerError =&gt; {
                error_lower.contains("500")
                    || error_lower.contains("502")
                    || error_lower.contains("503")
                    || error_lower.contains("504")
                    || error_lower.contains("server error")
            }
            Self::RateLimit =&gt; {
                error_lower.contains("rate limit")
                    || error_lower.contains("429")
                    || error_lower.contains("too many requests")
            }
            Self::Pattern(pattern) =&gt; {
                // Case-sensitive by default - matches against original error_msg
                // Use (?i) flag in pattern for case-insensitive matching
                if let Ok(re) = regex::Regex::new(pattern) {
                    re.is_match(error_msg)  // Uses original case, not error_lower
                } else {
                    false  // Invalid regex = no match
                }
            }
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="testing-error-matchers"><a class="header" href="#testing-error-matchers">Testing Error Matchers</a></h3>
<p>The retry_v2 module includes tests for built-in matchers (src/cook/retry_v2.rs:463-502):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[test]
fn test_error_matcher_network() {
    let matcher = ErrorMatcher::Network;
    assert!(matcher.matches("Connection refused"));
    assert!(matcher.matches("Network unreachable"));
    assert!(matcher.matches("connection timeout"));  // Case-insensitive
    assert!(!matcher.matches("Syntax error"));
}

#[test]
fn test_error_matcher_timeout() {
    let matcher = ErrorMatcher::Timeout;
    assert!(matcher.matches("Operation timeout"));
    assert!(matcher.matches("Request timed out"));
    assert!(!matcher.matches("Network error"));
}

#[test]
fn test_error_matcher_rate_limit() {
    let matcher = ErrorMatcher::RateLimit;
    assert!(matcher.matches("Rate limit exceeded"));
    assert!(matcher.matches("Error 429"));
    assert!(matcher.matches("Too many requests"));
    assert!(!matcher.matches("Server error"));
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Note</strong>: Pattern matcher tests are not yet implemented in the test suite. The above tests cover Network, Timeout, and RateLimit matchers only.</p>
<h3 id="see-also-27"><a class="header" href="#see-also-27">See Also</a></h3>
<ul>
<li><a href="retry-configuration/./basic-retry-configuration.html">Basic Retry Configuration</a> - Core retry configuration</li>
<li><a href="retry-configuration/./failure-actions.html">Failure Actions</a> - What happens when all retries are exhausted</li>
<li><a href="retry-configuration/./best-practices.html">Best Practices</a> - When to use selective retry</li>
<li><a href="retry-configuration/./complete-examples.html">Complete Examples</a> - Full workflow examples with error matchers</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="retry-budget"><a class="header" href="#retry-budget">Retry Budget</a></h2>
<p>A retry budget provides a time-based upper bound on retry operations, preventing workflows from hanging indefinitely even when attempt counts are high. The retry budget limits the cumulative <strong>delay time</strong> (not total execution time) spent on retries.</p>
<p><strong>Source</strong>: <code>src/cook/retry_v2.rs:47</code> (retry_budget field in RetryConfig struct)</p>
<h3 id="configuration-3"><a class="header" href="#configuration-3">Configuration</a></h3>
<p>The <code>retry_budget</code> field accepts human-readable duration formats using the <code>humantime_serde</code> parser:</p>
<pre><code class="language-yaml">retry_config:
  attempts: 10
  retry_budget: "5m"        # 5 minutes
  backoff:
    exponential:
      base: 2.0
  initial_delay: "1s"
</code></pre>
<p><strong>Supported Duration Formats</strong>:</p>
<ul>
<li>Seconds: <code>"30s"</code>, <code>"300s"</code></li>
<li>Minutes: <code>"5m"</code>, <code>"10m"</code></li>
<li>Hours: <code>"1h"</code>, <code>"2h"</code></li>
<li>Combined: <code>"1h30m"</code>, <code>"2m30s"</code></li>
</ul>
<p><strong>Source</strong>: Duration field with <code>humantime_serde</code> annotation at <code>src/cook/retry_v2.rs:47</code></p>
<h3 id="how-it-works"><a class="header" href="#how-it-works">How It Works</a></h3>
<p>Prodigy uses <strong>two complementary mechanisms</strong> to enforce retry budgets:</p>
<h4 id="1-duration-based-enforcement-active-execution"><a class="header" href="#1-duration-based-enforcement-active-execution">1. Duration-Based Enforcement (Active Execution)</a></h4>
<p>During active command execution, the <code>RetryExecutor</code> tracks cumulative delay time and checks the budget before each retry:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/cook/retry_v2.rs:236-244
if let Some(budget) = self.config.retry_budget {
    if total_delay + jittered_delay &gt; budget {
        warn!("Retry budget exhausted for {}", context);
        return Err(anyhow!("Retry budget exhausted"));
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Tracks <code>total_delay</code>: cumulative duration spent waiting between retries</li>
<li>Before each retry, checks: <code>total_delay + next_delay &gt; budget</code></li>
<li>If the next retry would exceed the budget, stops immediately</li>
<li>Returns error: <code>"Retry budget exhausted"</code></li>
</ul>
<p><strong>Source</strong>: <code>src/cook/retry_v2.rs:236-244</code></p>
<h4 id="2-timestamp-based-enforcement-stateful-tracking"><a class="header" href="#2-timestamp-based-enforcement-stateful-tracking">2. Timestamp-Based Enforcement (Stateful Tracking)</a></h4>
<p>For checkpoint/resume scenarios, the <code>RetryStateManager</code> uses expiration timestamps:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/cook/retry_state.rs:299-301
retry_budget_expires_at: config
    .retry_budget
    .map(|budget| Utc::now() + ChronoDuration::from_std(budget).unwrap())
<span class="boring">}</span></code></pre></pre>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Calculates expiration: <code>retry_budget_expires_at = now + budget_duration</code></li>
<li>Set at <strong>first failure</strong> (not at command start)</li>
<li>Before each retry attempt, checks: <code>Utc::now() &gt;= retry_budget_expires_at</code></li>
<li>Prevents retries even after workflow interruption and resume</li>
</ul>
<p><strong>Source</strong>: <code>src/cook/retry_state.rs:299-301, 342-346</code></p>
<h3 id="interaction-with-attempts-and-backoff"><a class="header" href="#interaction-with-attempts-and-backoff">Interaction with Attempts and Backoff</a></h3>
<p><strong>Critical behavior</strong>: Retries stop when <strong>EITHER</strong> the attempts limit <strong>OR</strong> the retry budget is exceeded, whichever comes first.</p>
<p><strong>Example 1: Budget Limits Before Attempts</strong></p>
<pre><code class="language-yaml">retry_config:
  attempts: 100           # High attempt limit
  retry_budget: "2m"      # Budget will be hit first
  backoff:
    exponential:
      base: 2.0
  initial_delay: "1s"
</code></pre>
<p>With exponential backoff (1s, 2s, 4s, 8s, 16s, 32s, 64s…), delays sum to ~2 minutes after 7-8 attempts. The budget stops retries at <strong>~8 attempts</strong> even though 100 are allowed.</p>
<p><strong>Example 2: Attempts Limit Before Budget</strong></p>
<pre><code class="language-yaml">retry_config:
  attempts: 3             # Attempts will be hit first
  retry_budget: "10m"     # Budget won't be reached
  backoff: fixed
  initial_delay: "5s"
</code></pre>
<p>Total delay: 5s + 5s + 5s = 15 seconds, well under the 10-minute budget. Stops after <strong>3 attempts</strong>.</p>
<p><strong>Source</strong>: Dual checking logic in <code>src/cook/retry_v2.rs:236-244</code> and <code>src/cook/retry_state.rs:337-347</code></p>
<h3 id="what-time-is-counted"><a class="header" href="#what-time-is-counted">What Time is Counted?</a></h3>
<p><strong>Included in Budget</strong>:</p>
<ul>
<li>✅ Backoff delay time (waiting between retries)</li>
<li>✅ Jitter-adjusted delays (if jitter is enabled)</li>
</ul>
<p><strong>NOT Included in Budget</strong>:</p>
<ul>
<li>❌ Command execution time</li>
<li>❌ Time for successful operations</li>
<li>❌ Time before first failure</li>
</ul>
<p><strong>Example Timeline</strong>:</p>
<pre><code>Command Start → Execute (30s) → Fail
                ↓ retry_budget timer starts
                Wait 1s (counted) → Execute (30s, NOT counted) → Fail
                Wait 2s (counted) → Execute (30s, NOT counted) → Fail
                Wait 4s (counted) → Execute (30s, NOT counted) → Success
Total Time: 30+1+30+2+30+4+30 = 127 seconds
Budget Used: 1+2+4 = 7 seconds
</code></pre>
<p>If <code>retry_budget: "5s"</code>, the workflow would fail at the third retry (1+2+4 = 7s &gt; 5s budget).</p>
<h3 id="best-practices-20"><a class="header" href="#best-practices-20">Best Practices</a></h3>
<h4 id="choosing-appropriate-retry-budget-values"><a class="header" href="#choosing-appropriate-retry-budget-values">Choosing Appropriate Retry Budget Values</a></h4>
<p>Consider these factors when setting retry budgets:</p>
<p><strong>1. Workflow Timeout Requirements</strong></p>
<ul>
<li>If your CI/CD pipeline times out at 10 minutes, set <code>retry_budget</code> to ensure retries complete within that window</li>
<li>Leave buffer for command execution time (budget only counts delays)</li>
</ul>
<p><strong>2. Balance Between Attempts and Time</strong></p>
<ul>
<li>High attempts + exponential backoff = potential for very long delays</li>
<li>Use retry budget as a safety net: <code>attempts: 50</code> with <code>retry_budget: "5m"</code> gives flexibility but prevents runaway retries</li>
</ul>
<p><strong>3. Typical Operation Duration</strong></p>
<ul>
<li>If commands take 2 minutes each, budget of “30s” with 5 attempts means max total time: 30s (delays) + 10 minutes (5 × 2min execution) = 10.5 minutes</li>
<li>Account for execution time separately from budget</li>
</ul>
<p><strong>4. User Experience Expectations</strong></p>
<ul>
<li>Interactive workflows: Short budgets (1-2 minutes) for faster feedback</li>
<li>Background jobs: Longer budgets (10-30 minutes) for better reliability</li>
</ul>
<h4 id="example-configurations"><a class="header" href="#example-configurations">Example Configurations</a></h4>
<p><strong>Fast-Failing Web Requests</strong>:</p>
<pre><code class="language-yaml">retry_config:
  attempts: 10
  retry_budget: "1m"
  backoff:
    exponential:
      base: 2.0
  initial_delay: "500ms"
  max_delay: "10s"
</code></pre>
<p><strong>Resilient Background Processing</strong>:</p>
<pre><code class="language-yaml">retry_config:
  attempts: 20
  retry_budget: "10m"
  backoff: fibonacci
  initial_delay: "2s"
  max_delay: "60s"
</code></pre>
<p><strong>CI/CD with Strict Timeout</strong>:</p>
<pre><code class="language-yaml">retry_config:
  attempts: 100          # Generous attempts
  retry_budget: "8m"     # But hard 8-minute budget (CI times out at 10m)
  backoff:
    exponential:
      base: 2.0
  initial_delay: "1s"
  max_delay: "30s"
</code></pre>
<h3 id="error-messages-and-debugging"><a class="header" href="#error-messages-and-debugging">Error Messages and Debugging</a></h3>
<p>When the retry budget is exceeded, the command fails with the last error encountered:</p>
<p><strong>Error Message</strong>:</p>
<pre><code>Error: Retry budget exhausted
</code></pre>
<p><strong>Debug Logs</strong> (from <code>src/cook/retry_state.rs:344</code>):</p>
<pre><code>DEBUG Command &lt;command_id&gt; retry budget expired
</code></pre>
<p><strong>What Happens</strong>:</p>
<ol>
<li>Retry budget check fails before next retry</li>
<li>Command returns error immediately (no final retry attempt)</li>
<li>If <code>on_failure</code> is configured, fallback action is triggered</li>
<li>Otherwise, workflow fails with “Retry budget exhausted” error</li>
</ol>
<h3 id="troubleshooting-14"><a class="header" href="#troubleshooting-14">Troubleshooting</a></h3>
<p><strong>Issue: Budget Seems to Be Ignored</strong></p>
<p>Remember that the budget counts <strong>delay time</strong>, not <strong>total time</strong>. If your commands take a long time to execute, total workflow time can significantly exceed the budget.</p>
<p><strong>Solution</strong>: If you need a hard total time limit, use a timeout mechanism at the workflow level, not just retry budget.</p>
<p><strong>Issue: Retries Stop Earlier Than Expected</strong></p>
<p>Check if jitter is enabled. Jitter adds randomization to delays, which can cause the budget to be reached sooner than calculated base delays would suggest.</p>
<p><strong>Solution</strong>: Account for jitter factor when calculating expected budget consumption: <code>max_delay = base_delay * (1 + jitter_factor)</code>.</p>
<h3 id="real-world-example-1"><a class="header" href="#real-world-example-1">Real-World Example</a></h3>
<p>From test suite (<code>src/cook/retry_v2.rs:727-747</code>):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_retry_budget() {
    let config = RetryConfig {
        attempts: 10,                                  // Allow many attempts
        initial_delay: Duration::from_millis(50),
        retry_budget: Some(Duration::from_millis(100)), // But cap at 100ms delay
        ..Default::default()
    };
    let executor = RetryExecutor::new(config);

    let start = Instant::now();
    let result = executor
        .execute_with_retry(
            || async { Err::&lt;i32, _&gt;(anyhow!("Persistent failure")) },
            "test",
        )
        .await;

    assert!(result.is_err());
    assert!(start.elapsed() &lt; Duration::from_millis(200)); // Budget enforced
}
<span class="boring">}</span></code></pre></pre>
<p>This test demonstrates:</p>
<ul>
<li>10 attempts allowed, but budget limits actual retries</li>
<li>Total delay time capped at 100ms despite potentially more attempts</li>
<li>Workflow completes quickly (&lt; 200ms) instead of running all 10 attempts</li>
</ul>
<h3 id="see-also-28"><a class="header" href="#see-also-28">See Also</a></h3>
<ul>
<li><a href="retry-configuration/./basic-retry-configuration.html">Basic Retry Configuration</a> - Understanding retry fundamentals</li>
<li><a href="retry-configuration/./backoff-strategies.html">Backoff Strategies</a> - How delays are calculated</li>
<li><a href="retry-configuration/./troubleshooting.html">Troubleshooting</a> - Debugging retry budget issues</li>
<li><a href="retry-configuration/./complete-examples.html">Complete Examples</a> - Full workflow examples with retry budgets</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="failure-actions"><a class="header" href="#failure-actions">Failure Actions</a></h2>
<blockquote>
<p><strong>IMPORTANT</strong>: This subsection documents the <code>on_failure</code> mechanism for <strong>workflow shell commands</strong>, which uses <code>TestDebugConfig</code> for automatic debugging and retry. This is distinct from the theoretical <code>retry_config.on_failure</code> field shown in the parent chapter overview, which is defined in code but not yet integrated into command execution.</p>
</blockquote>
<p>Configure automatic debugging and retry behavior when shell commands fail using the <code>on_failure</code> field. This mechanism allows Claude to automatically diagnose and fix test failures, build errors, and other command failures.</p>
<p><strong>Source</strong>: <code>src/config/command.rs:168-183</code> (TestDebugConfig struct), <code>src/config/command.rs:372</code> (WorkflowStepCommand.on_failure field)</p>
<h2 id="what-is-testdebugconfig"><a class="header" href="#what-is-testdebugconfig">What is TestDebugConfig?</a></h2>
<p>The <code>on_failure</code> field in workflow commands accepts a <code>TestDebugConfig</code> object that specifies:</p>
<ul>
<li>A Claude command to run when the shell command fails</li>
<li>How many times to retry the debug-fix cycle</li>
<li>Whether to fail the entire workflow if debugging doesn’t fix the issue</li>
<li>Whether the debug command should create git commits</li>
</ul>
<p>This enables <strong>self-healing workflows</strong> where Claude automatically attempts to fix failures before escalating them.</p>
<p><strong>Source</strong>: <code>src/config/command.rs:168-183</code></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct TestDebugConfig {
    /// Claude command to run on test failure
    pub claude: String,

    /// Maximum number of retry attempts (default: 3)
    pub max_attempts: u32,

    /// Whether to fail the workflow if max attempts reached (default: false)
    pub fail_workflow: bool,

    /// Whether the debug command should create commits (default: true)
    pub commit_required: bool,
}
<span class="boring">}</span></code></pre></pre>
<h2 id="basic-syntax-3"><a class="header" href="#basic-syntax-3">Basic Syntax</a></h2>
<p>The simplest form specifies just the Claude command to run on failure:</p>
<pre><code class="language-yaml">- shell: "cargo test"
  on_failure:
    claude: "/prodigy-debug-test-failure --output ${shell.output}"
</code></pre>
<p><strong>Execution Flow</strong>:</p>
<ol>
<li>Run <code>cargo test</code></li>
<li>If it fails → Capture output in <code>${shell.output}</code></li>
<li>Run <code>/prodigy-debug-test-failure --output ${shell.output}</code></li>
<li>Claude analyzes failure and attempts fix</li>
<li>If Claude creates commits → Re-run <code>cargo test</code></li>
<li>Repeat up to <code>max_attempts</code> times (default: 3)</li>
</ol>
<p><strong>Source</strong>: Example from <code>workflows/documentation-drift.yml:47-53</code></p>
<h2 id="configuration-options"><a class="header" href="#configuration-options">Configuration Options</a></h2>
<h3 id="full-configuration"><a class="header" href="#full-configuration">Full Configuration</a></h3>
<p>All <code>on_failure</code> options:</p>
<pre><code class="language-yaml">- shell: "cargo test --doc"
  on_failure:
    claude: "/prodigy-debug-test-failure --output ${shell.output}"
    max_attempts: 3           # Maximum debug-fix-retest cycles (default: 3)
    fail_workflow: false      # Continue workflow even if can't fix (default: false)
    commit_required: true     # Debug command should create commits (default: true)
</code></pre>
<p><strong>Source</strong>: Example from <code>workflows/documentation-drift.yml:47-53</code></p>
<h3 id="max_attempts"><a class="header" href="#max_attempts">max_attempts</a></h3>
<p>Controls how many debug-fix-retest cycles to attempt:</p>
<pre><code class="language-yaml">- shell: "just fmt-check &amp;&amp; just lint"
  on_failure:
    claude: "/prodigy-lint ${shell.output}"
    max_attempts: 5           # Try up to 5 times to fix linting issues
</code></pre>
<p><strong>Default</strong>: 3 attempts (defined in <code>src/config/command.rs:173</code>)</p>
<p><strong>Use Cases</strong>:</p>
<ul>
<li><strong>1 attempt</strong>: Quick fixes only, don’t waste time on hard problems</li>
<li><strong>3 attempts</strong> (default): Reasonable for most test failures</li>
<li><strong>5+ attempts</strong>: Complex issues that might need multiple iterations</li>
</ul>
<p><strong>Source</strong>: <code>workflows/implement.yml:27-30</code></p>
<h3 id="fail_workflow"><a class="header" href="#fail_workflow">fail_workflow</a></h3>
<p>Controls whether to stop the entire workflow if debugging can’t fix the issue:</p>
<pre><code class="language-yaml"># CRITICAL: Must succeed - fail workflow if can't fix
- shell: "cargo test --lib"
  on_failure:
    claude: "/prodigy-debug-test-failure --output ${shell.output}"
    max_attempts: 3
    fail_workflow: true       # Stop workflow if tests still fail after 3 attempts
</code></pre>
<pre><code class="language-yaml"># NON-CRITICAL: Best effort - continue even if can't fix
- shell: "cargo test --doc"
  on_failure:
    claude: "/prodigy-fix-doc-tests --output ${shell.output}"
    max_attempts: 2
    fail_workflow: false      # Continue workflow even if doc tests fail
</code></pre>
<p><strong>Default</strong>: <code>false</code> - workflow continues even if debugging doesn’t fix the issue (defined in <code>src/config/command.rs:177</code>)</p>
<p><strong>Source</strong>: Examples from <code>workflows/coverage-with-test-debug.yml:14-23</code></p>
<h3 id="commit_required-1"><a class="header" href="#commit_required-1">commit_required</a></h3>
<p>Controls whether the debug command must create git commits:</p>
<pre><code class="language-yaml">- shell: "cargo clippy -- -D warnings"
  on_failure:
    claude: "/prodigy-lint ${shell.output}"
    commit_required: true     # Must commit fixes (default)
</code></pre>
<p><strong>Default</strong>: <code>true</code> - debug command must create commits (defined in <code>src/config/command.rs:181</code>)</p>
<p><strong>When to set to false</strong>: Rare - only if the debug command doesn’t modify code (e.g., just logs diagnostic info)</p>
<p><strong>Source</strong>: Field definition <code>src/config/command.rs:180-182</code></p>
<h2 id="real-world-examples-3"><a class="header" href="#real-world-examples-3">Real-World Examples</a></h2>
<h3 id="example-1-test-debugging-with-multiple-attempts"><a class="header" href="#example-1-test-debugging-with-multiple-attempts">Example 1: Test Debugging with Multiple Attempts</a></h3>
<p>From <code>workflows/implement.yml</code>:</p>
<pre><code class="language-yaml">- shell: "cargo test"
  timeout: 600
  on_failure:
    claude: "/prodigy-debug-test-failure --spec $ARG --output ${shell.output}"
    max_attempts: 5
    fail_workflow: false      # Continue even if tests can't be fixed
</code></pre>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Run tests with 10-minute timeout</li>
<li>On failure, Claude analyzes output and fixes issues</li>
<li>Try up to 5 debug-fix-test cycles</li>
<li>If still failing after 5 attempts, continue workflow anyway</li>
<li>Each fix creates a commit</li>
</ul>
<p><strong>Source</strong>: <code>workflows/implement.yml:19-24</code></p>
<h3 id="example-2-critical-vs-non-critical-commands"><a class="header" href="#example-2-critical-vs-non-critical-commands">Example 2: Critical vs Non-Critical Commands</a></h3>
<p>From <code>workflows/coverage-with-test-debug.yml</code>:</p>
<pre><code class="language-yaml"># CRITICAL: Library tests must pass
- shell: "cargo test --lib"
  on_failure:
    claude: "/prodigy-debug-test-failure --spec ${coverage.spec} --output ${shell.output}"
    max_attempts: 3
    fail_workflow: true       # STOP if can't fix

# NON-CRITICAL: Doc tests are best-effort
- shell: "cargo test --doc"
  on_failure:
    claude: "/prodigy-fix-doc-tests --output ${shell.output}"
    max_attempts: 2
    fail_workflow: false      # CONTINUE if can't fix
</code></pre>
<p><strong>Source</strong>: <code>workflows/coverage-with-test-debug.yml:13-24</code></p>
<h3 id="example-3-chaining-with-on_success"><a class="header" href="#example-3-chaining-with-on_success">Example 3: Chaining with on_success</a></h3>
<p>From <code>workflows/implement-with-tests.yml</code>:</p>
<pre><code class="language-yaml">- shell: "cargo test"
  on_failure:
    # If tests fail, debug and fix them
    claude: "/prodigy-debug-test-failures '${test_output}'"
    commit_required: true
    on_success:
      # After fixing, verify tests now pass
      - shell: "cargo test"
        on_failure:
          # If STILL failing, try deeper analysis
          claude: "/prodigy-fix-test-failures '${shell.output}' --deep-analysis"
          commit_required: true
</code></pre>
<p><strong>Nested Retry Logic</strong>:</p>
<ol>
<li>Run tests</li>
<li>If fail → Debug and fix</li>
<li>If fix succeeded (commit created) → Re-run tests</li>
<li>If tests STILL fail → Try deeper analysis</li>
<li>Verify second fix works</li>
</ol>
<p><strong>Source</strong>: <code>workflows/implement-with-tests.yml:27-39</code></p>
<h3 id="example-4-linting-after-implementation"><a class="header" href="#example-4-linting-after-implementation">Example 4: Linting After Implementation</a></h3>
<p>From <code>workflows/documentation-drift.yml</code>:</p>
<pre><code class="language-yaml">- shell: "just fmt-check &amp;&amp; just lint"
  on_failure:
    claude: "/prodigy-lint ${shell.output}"
    commit_required: true
    max_attempts: 3
    fail_workflow: false      # Non-blocking - formatting issues shouldn't stop workflow
</code></pre>
<p><strong>Use Case</strong>: After implementing features, ensure code is properly formatted and linted, but don’t block if there are style issues.</p>
<p><strong>Source</strong>: <code>workflows/documentation-drift.yml:56-61</code></p>
<h2 id="how-it-works-execution-flow"><a class="header" href="#how-it-works-execution-flow">How It Works: Execution Flow</a></h2>
<h3 id="retry-loop"><a class="header" href="#retry-loop">Retry Loop</a></h3>
<p>When a shell command with <code>on_failure</code> fails:</p>
<ol>
<li><strong>Initial Failure</strong>: Shell command exits with non-zero code</li>
<li><strong>Capture Output</strong>: Stderr/stdout saved to <code>${shell.output}</code></li>
<li><strong>First Debug Attempt</strong>:
<ul>
<li>Run Claude command with failure output</li>
<li>Claude analyzes error and makes fixes</li>
<li>If <code>commit_required=true</code>, check for git commits</li>
</ul>
</li>
<li><strong>Retry Original Command</strong>:
<ul>
<li>If commits were created → Re-run original shell command</li>
<li>If no commits → Debug didn’t fix anything, stop retrying</li>
</ul>
</li>
<li><strong>Repeat</strong>: Continue debug-fix-retry cycle up to <code>max_attempts</code></li>
<li><strong>Final Result</strong>:
<ul>
<li>If any attempt succeeded → Command passes</li>
<li>If all attempts failed and <code>fail_workflow=true</code> → Workflow stops</li>
<li>If all attempts failed and <code>fail_workflow=false</code> → Workflow continues</li>
</ul>
</li>
</ol>
<h3 id="commit-requirement-logic"><a class="header" href="#commit-requirement-logic">Commit Requirement Logic</a></h3>
<p>The <code>commit_required</code> field interacts with the retry loop:</p>
<ul>
<li>
<p><strong><code>commit_required=true</code></strong> (default): Only retry if Claude created commits</p>
<ul>
<li>Rationale: If Claude didn’t commit, it didn’t find a fix</li>
<li>Prevents infinite loops where Claude can’t solve the problem</li>
</ul>
</li>
<li>
<p><strong><code>commit_required=false</code></strong>: Retry even if no commits</p>
<ul>
<li>Rare use case: Debug command doesn’t modify code (logging, diagnostics)</li>
</ul>
</li>
</ul>
<p><strong>Source</strong>: Conceptual flow based on <code>src/config/command.rs:168-183</code> definition and actual usage in workflows</p>
<h2 id="limitations-and-gotchas"><a class="header" href="#limitations-and-gotchas">Limitations and Gotchas</a></h2>
<h3 id="1-only-works-for-shell-commands"><a class="header" href="#1-only-works-for-shell-commands">1. Only Works for Shell Commands</a></h3>
<p>The <code>on_failure</code> field is only available for <code>shell:</code> commands, NOT for <code>claude:</code> commands:</p>
<pre><code class="language-yaml"># ✓ WORKS - shell command with on_failure
- shell: "cargo test"
  on_failure:
    claude: "/debug-test"

# ✗ DOES NOT WORK - claude command doesn't support on_failure
- claude: "/implement-feature"
  on_failure:              # This field is ignored!
    claude: "/fix-issue"
</code></pre>
<p><strong>Source</strong>: <code>src/config/command.rs:320-400</code> - WorkflowStepCommand has separate <code>shell</code> and <code>claude</code> fields, <code>on_failure</code> applies only to shell commands</p>
<h3 id="2-not-the-same-as-retry_config"><a class="header" href="#2-not-the-same-as-retry_config">2. Not the Same as retry_config</a></h3>
<p><strong>IMPORTANT</strong>: The <code>on_failure: TestDebugConfig</code> documented here is NOT the same as the theoretical <code>retry_config.on_failure: FailureAction</code> mentioned in the parent chapter.</p>
<pre><code class="language-yaml"># ✓ ACTUAL SYNTAX (what this subsection documents)
- shell: "cargo test"
  on_failure:
    claude: "/debug"
    max_attempts: 3

# ✗ DOES NOT WORK (theoretical syntax, not implemented)
- shell: "cargo test"
  retry_config:
    on_failure: stop        # This field exists in code but is NOT wired into execution
</code></pre>
<p>The <code>retry_config</code> field is <strong>not defined in WorkflowStepCommand</strong> (see <code>src/config/command.rs:320-400</code>). The <code>retry_v2::FailureAction</code> enum exists in <code>src/cook/retry_v2.rs:157-165</code> but is not integrated into command execution.</p>
<h3 id="3-testdebugconfig-is-not-retry-configuration"><a class="header" href="#3-testdebugconfig-is-not-retry-configuration">3. TestDebugConfig is Not Retry Configuration</a></h3>
<p>Despite the name and retry-like behavior, <code>TestDebugConfig</code> is a <strong>debugging mechanism</strong>, not a retry mechanism:</p>
<ul>
<li><strong>Retry</strong>: Re-run the same command without changes (for transient failures)</li>
<li><strong>Debug</strong>: Run a DIFFERENT command (Claude) to diagnose and fix the issue, THEN re-run</li>
</ul>
<p>The <code>TestDebugConfig</code> mechanism assumes failures are due to code issues that need fixing, not transient errors.</p>
<h3 id="4-max-attempts-includes-initial-failure"><a class="header" href="#4-max-attempts-includes-initial-failure">4. Max Attempts Includes Initial Failure</a></h3>
<p>If <code>max_attempts: 3</code>, the execution pattern is:</p>
<ol>
<li>Initial run (fails)</li>
<li>Debug attempt #1 → Retry</li>
<li>Debug attempt #2 → Retry</li>
<li>Debug attempt #3 → Retry</li>
</ol>
<p>So the original command runs up to <strong>4 times total</strong> (1 initial + 3 debug-retry cycles).</p>
<h2 id="best-practices-21"><a class="header" href="#best-practices-21">Best Practices</a></h2>
<h3 id="use-descriptive-claude-commands"><a class="header" href="#use-descriptive-claude-commands">Use Descriptive Claude Commands</a></h3>
<p>Good:</p>
<pre><code class="language-yaml">on_failure:
  claude: "/prodigy-debug-test-failure --spec ${spec_id} --output ${shell.output}"
</code></pre>
<p>Bad:</p>
<pre><code class="language-yaml">on_failure:
  claude: "/fix"              # Too generic - Claude won't know what to fix
</code></pre>
<h3 id="set-fail_workflow-based-on-criticality"><a class="header" href="#set-fail_workflow-based-on-criticality">Set fail_workflow Based on Criticality</a></h3>
<p><strong>Critical Operations</strong> (must succeed):</p>
<pre><code class="language-yaml">- shell: "cargo build --release"
  on_failure:
    claude: "/debug-build-failure"
    fail_workflow: true       # Stop if build can't be fixed
</code></pre>
<p><strong>Optional Operations</strong> (best effort):</p>
<pre><code class="language-yaml">- shell: "cargo bench"
  on_failure:
    claude: "/fix-bench-issues"
    fail_workflow: false      # Continue even if benchmarks fail
</code></pre>
<h3 id="use-appropriate-max_attempts"><a class="header" href="#use-appropriate-max_attempts">Use Appropriate max_attempts</a></h3>
<ul>
<li><strong>Quick fixes</strong> (formatting, simple errors): <code>max_attempts: 1-2</code></li>
<li><strong>Test debugging</strong> (moderate complexity): <code>max_attempts: 3</code> (default)</li>
<li><strong>Complex issues</strong> (integration tests, build issues): <code>max_attempts: 5</code></li>
<li><strong>Don’t use</strong>: <code>max_attempts: 10+</code> - if Claude can’t fix it in 5 tries, it won’t fix it in 10</li>
</ul>
<h3 id="pass-failure-output-to-claude"><a class="header" href="#pass-failure-output-to-claude">Pass Failure Output to Claude</a></h3>
<p>Always pass <code>${shell.output}</code> to give Claude the error context:</p>
<pre><code class="language-yaml">on_failure:
  claude: "/debug-test-failure --output ${shell.output}"
</code></pre>
<p>Without the output, Claude has to guess what went wrong.</p>
<h2 id="comparison-with-other-retry-mechanisms"><a class="header" href="#comparison-with-other-retry-mechanisms">Comparison with Other Retry Mechanisms</a></h2>
<p>Prodigy has multiple mechanisms that might be confused:</p>
<div class="table-wrapper"><table><thead><tr><th>Mechanism</th><th>Purpose</th><th>Scope</th><th>When to Use</th></tr></thead><tbody>
<tr><td><code>on_failure: TestDebugConfig</code></td><td>Auto-debug and fix code issues</td><td>Shell commands</td><td>Test failures, build errors that need code fixes</td></tr>
<tr><td><code>retry_config</code> (theoretical)</td><td>Retry with backoff for transient errors</td><td>Commands (not implemented)</td><td>Network errors, timeouts (when implemented)</td></tr>
<tr><td>Workflow-level retry</td><td>Retry entire work items</td><td>MapReduce jobs</td><td>Bulk operation failures, DLQ retry</td></tr>
<tr><td><code>on_success</code> chaining</td><td>Sequential command execution</td><td>Any command</td><td>Multi-step validation, verification after fixes</td></tr>
</tbody></table>
</div>
<p><strong>This subsection documents only</strong>: <code>on_failure: TestDebugConfig</code></p>
<p>For workflow-level retry, see <a href="retry-configuration/./workflow-level-vs-command-level-retry.html">Workflow-Level vs Command-Level Retry</a>.</p>
<h2 id="migration-from-deprecated-test-syntax"><a class="header" href="#migration-from-deprecated-test-syntax">Migration from Deprecated test: Syntax</a></h2>
<p>The <code>test:</code> command type is deprecated. Migrate to <code>shell:</code> with <code>on_failure:</code>:</p>
<p><strong>Old (Deprecated)</strong>:</p>
<pre><code class="language-yaml">- test:
    command: "cargo test"
    on_failure:
      claude: "/debug"
</code></pre>
<p><strong>New (Current)</strong>:</p>
<pre><code class="language-yaml">- shell: "cargo test"
  on_failure:
    claude: "/debug"
</code></pre>
<p><strong>Source</strong>: Deprecation warning in <code>src/config/command.rs:446-455</code></p>
<h2 id="see-also-29"><a class="header" href="#see-also-29">See Also</a></h2>
<ul>
<li><a href="retry-configuration/./basic-retry-configuration.html">Basic Retry Configuration</a> - Retry configuration basics</li>
<li><a href="retry-configuration/./conditional-retry-with-error-matchers.html">Conditional Retry with Error Matchers</a> - Selective retry by error type</li>
<li><a href="retry-configuration/./best-practices.html">Best Practices</a> - When to use each failure handling mechanism</li>
<li><a href="retry-configuration/./complete-examples.html">Complete Examples</a> - Full workflow examples</li>
<li><a href="retry-configuration/./workflow-level-vs-command-level-retry.html">Workflow-Level vs Command-Level Retry</a> - Comparison of retry systems</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="complete-examples-2"><a class="header" href="#complete-examples-2">Complete Examples</a></h2>
<p>This section provides complete, runnable YAML workflow examples demonstrating various retry configurations.</p>
<p><strong>Source</strong>: Examples based on test patterns from src/cook/retry_v2.rs:463-748</p>
<h3 id="example-1-basic-retry-with-exponential-backoff"><a class="header" href="#example-1-basic-retry-with-exponential-backoff">Example 1: Basic Retry with Exponential Backoff</a></h3>
<p>Simple API call with standard exponential backoff:</p>
<pre><code class="language-yaml">name: fetch-api-data
mode: standard

commands:
  - shell: "curl -f https://api.example.com/data"
    retry_config:
      attempts: 5
      backoff: exponential
      initial_delay: "1s"
      max_delay: "30s"
</code></pre>
<p><strong>When it’s useful</strong>:</p>
<ul>
<li>External API calls</li>
<li>Network-dependent operations</li>
<li>Transient failure recovery</li>
</ul>
<p><strong>Retry sequence</strong>:</p>
<ul>
<li>Attempt 1: Immediate</li>
<li>Attempt 2: ~2s delay</li>
<li>Attempt 3: ~4s delay</li>
<li>Attempt 4: ~8s delay</li>
<li>Attempt 5: ~16s delay</li>
</ul>
<h3 id="example-2-exponential-backoff-with-jitter-distributed-systems"><a class="header" href="#example-2-exponential-backoff-with-jitter-distributed-systems">Example 2: Exponential Backoff with Jitter (Distributed Systems)</a></h3>
<p>Multiple parallel agents with jitter to prevent thundering herd:</p>
<pre><code class="language-yaml">name: parallel-processing
mode: mapreduce

map:
  input: "items.json"
  json_path: "$.items[*]"
  max_parallel: 10

  agent_template:
    - shell: "process-item ${item.id}"
      retry_config:
        attempts: 5
        backoff: exponential
        initial_delay: "1s"
        max_delay: "30s"
        jitter: true          # Critical for parallel agents
        jitter_factor: 0.3    # 30% randomization
</code></pre>
<p><strong>Why jitter matters</strong>: Without jitter, all 10 parallel agents would retry at exactly the same time, overwhelming the recovering service.</p>
<p><strong>Source</strong>: Jitter implementation in src/cook/retry_v2.rs:308-317</p>
<h3 id="example-3-conditional-retry-with-error-matchers"><a class="header" href="#example-3-conditional-retry-with-error-matchers">Example 3: Conditional Retry with Error Matchers</a></h3>
<p>Only retry transient errors, fail fast on permanent errors:</p>
<pre><code class="language-yaml">name: selective-retry
mode: standard

commands:
  - shell: "curl -f https://api.example.com/resource"
    retry_config:
      attempts: 5
      backoff: exponential
      initial_delay: "1s"
      max_delay: "30s"
      retry_on:
        - network        # Connection issues
        - timeout        # Slow responses
        - server_error   # 5xx errors
</code></pre>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Retries: Network errors, timeouts, 500/502/503/504</li>
<li>Fails immediately: 404, 401, 400 (permanent errors)</li>
</ul>
<p><strong>Source</strong>: ErrorMatcher enum in src/cook/retry_v2.rs:100-151</p>
<h3 id="example-4-retry-budget-to-prevent-infinite-loops"><a class="header" href="#example-4-retry-budget-to-prevent-infinite-loops">Example 4: Retry Budget to Prevent Infinite Loops</a></h3>
<p>High retry attempts with time-based cap:</p>
<pre><code class="language-yaml">name: budget-limited-retry
mode: standard

commands:
  - shell: "long-running-operation"
    retry_config:
      attempts: 100          # High attempt count
      backoff: fibonacci
      initial_delay: "1s"
      max_delay: "60s"
      retry_budget: "10m"    # But never exceed 10 minutes total
</code></pre>
<p><strong>Why</strong>: Prevents endless retries while allowing many attempts for operations that typically succeed eventually.</p>
<p><strong>Source</strong>: retry_budget field in src/cook/retry_v2.rs:46-47, tests at lines 675-708</p>
<h3 id="example-5-fallback-on-failure"><a class="header" href="#example-5-fallback-on-failure">Example 5: Fallback on Failure</a></h3>
<p>Use cached data when API fails:</p>
<pre><code class="language-yaml">name: fallback-example
mode: standard

commands:
  - shell: "curl -f https://api.example.com/live-data"
    retry_config:
      attempts: 3
      backoff: exponential
      initial_delay: "2s"
      max_delay: "10s"
      retry_on:
        - network
        - timeout
      on_failure:
        fallback:
          command: "cat /cache/data.json"

  # Continue processing with either live or cached data
  - shell: "process-data data.json"
</code></pre>
<p><strong>Execution flow</strong>:</p>
<ol>
<li>Try to fetch live data (3 attempts with exponential backoff)</li>
<li>If all attempts fail → Use cached data</li>
<li>Continue with processing</li>
</ol>
<p><strong>Source</strong>: FailureAction::Fallback in src/cook/retry_v2.rs:164</p>
<h3 id="example-6-continue-on-failure-non-critical-operations"><a class="header" href="#example-6-continue-on-failure-non-critical-operations">Example 6: Continue on Failure (Non-Critical Operations)</a></h3>
<p>Allow workflow to continue even if optional operations fail:</p>
<pre><code class="language-yaml">name: mixed-criticality
mode: standard

commands:
  # Critical: must succeed
  - shell: "cargo build"
    retry_config:
      attempts: 3
      on_failure: stop

  # Optional: nice to have but not critical
  - shell: "notify-slack 'Build started'"
    retry_config:
      attempts: 2
      initial_delay: "5s"
      on_failure: continue    # Don't fail workflow if notification fails

  # Critical: must succeed
  - shell: "cargo test"
    retry_config:
      attempts: 3
      on_failure: stop
</code></pre>
<p><strong>Use case</strong>: Separating critical operations from best-effort operations.</p>
<p><strong>Source</strong>: FailureAction::Continue in src/cook/retry_v2.rs:162</p>
<h3 id="example-7-rate-limit-handling"><a class="header" href="#example-7-rate-limit-handling">Example 7: Rate Limit Handling</a></h3>
<p>Handle API rate limits with long delays:</p>
<pre><code class="language-yaml">name: rate-limit-aware
mode: standard

commands:
  - shell: "api-call.sh"
    retry_config:
      attempts: 10
      backoff: exponential
      initial_delay: "60s"    # Start with 1-minute delay
      max_delay: "10m"        # Cap at 10 minutes
      retry_on:
        - rate_limit          # Only retry on 429 errors
</code></pre>
<p><strong>Why</strong>: Rate limits often require longer delays than network errors.</p>
<p><strong>Source</strong>: ErrorMatcher::RateLimit in src/cook/retry_v2.rs:143-147</p>
<h3 id="example-8-custom-pattern-matching"><a class="header" href="#example-8-custom-pattern-matching">Example 8: Custom Pattern Matching</a></h3>
<p>Retry database-specific errors:</p>
<pre><code class="language-yaml">name: database-retry
mode: standard

commands:
  - shell: "sqlite3 db.sqlite 'INSERT INTO ...'"
    retry_config:
      attempts: 5
      backoff: linear
      initial_delay: "100ms"
      retry_on:
        - pattern: "database.*locked"
        - pattern: "SQLITE_BUSY"
        - pattern: "cannot commit.*in progress"
</code></pre>
<p><strong>Source</strong>: ErrorMatcher::Pattern in src/cook/retry_v2.rs:113</p>
<h3 id="example-9-fibonacci-backoff-for-gradual-recovery"><a class="header" href="#example-9-fibonacci-backoff-for-gradual-recovery">Example 9: Fibonacci Backoff for Gradual Recovery</a></h3>
<p>Gentler backoff curve for services needing recovery time:</p>
<pre><code class="language-yaml">name: fibonacci-backoff-example
mode: standard

commands:
  - shell: "connect-to-recovering-service.sh"
    retry_config:
      attempts: 8
      backoff: fibonacci
      initial_delay: "1s"
      max_delay: "60s"
</code></pre>
<p><strong>Delay sequence</strong>: 1s, 2s, 3s, 5s, 8s, 13s, 21s, 34s</p>
<p><strong>Why Fibonacci</strong>: Grows slower than exponential, giving services more time to recover without aggressive backoff.</p>
<p><strong>Source</strong>: Fibonacci calculation in src/cook/retry_v2.rs:424-440</p>
<h3 id="example-10-linear-backoff-for-predictable-delays"><a class="header" href="#example-10-linear-backoff-for-predictable-delays">Example 10: Linear Backoff for Predictable Delays</a></h3>
<p>Testing or debugging with consistent delays:</p>
<pre><code class="language-yaml">name: linear-backoff-example
mode: standard

commands:
  - shell: "test-operation.sh"
    retry_config:
      attempts: 5
      backoff:
        linear:
          increment: "3s"
      initial_delay: "1s"
</code></pre>
<p><strong>Delay sequence</strong>: 1s, 4s, 7s, 10s, 13s (initial + n * increment)</p>
<p><strong>Source</strong>: BackoffStrategy::Linear in src/cook/retry_v2.rs:77-80</p>
<h3 id="example-11-fixed-delay-for-polling"><a class="header" href="#example-11-fixed-delay-for-polling">Example 11: Fixed Delay for Polling</a></h3>
<p>Consistent polling interval:</p>
<pre><code class="language-yaml">name: polling-example
mode: standard

commands:
  - shell: "check-job-status.sh"
    retry_config:
      attempts: 20
      backoff: fixed
      initial_delay: "5s"
</code></pre>
<p><strong>Delay sequence</strong>: 5s between every attempt</p>
<p><strong>Use case</strong>: Status polling, health checks</p>
<p><strong>Source</strong>: BackoffStrategy::Fixed in src/cook/retry_v2.rs:75</p>
<h3 id="example-12-complex-multi-command-workflow"><a class="header" href="#example-12-complex-multi-command-workflow">Example 12: Complex Multi-Command Workflow</a></h3>
<p>Real-world example combining multiple retry strategies:</p>
<pre><code class="language-yaml">name: deployment-workflow
mode: standard

commands:
  # Step 1: Build (critical, retry network issues)
  - shell: "cargo build --release"
    retry_config:
      attempts: 3
      backoff: exponential
      retry_on:
        - network
      on_failure: stop

  # Step 2: Run tests (critical, no retry on real failures)
  - shell: "cargo test"
    retry_config:
      attempts: 2
      initial_delay: "5s"
      retry_on:
        - pattern: "temporary.*failure"
      on_failure: stop

  # Step 3: Upload artifacts (retry with backoff)
  - shell: "upload-to-s3.sh artifacts/"
    retry_config:
      attempts: 5
      backoff: exponential
      initial_delay: "2s"
      max_delay: "60s"
      jitter: true
      retry_on:
        - network
        - timeout
        - server_error
      on_failure:
        fallback:
          command: "save-to-local-backup.sh artifacts/"

  # Step 4: Notify (optional, don't block on failure)
  - shell: "notify-deployment.sh"
    retry_config:
      attempts: 2
      initial_delay: "5s"
      on_failure: continue

  # Step 5: Health check (retry with fixed delay)
  - shell: "health-check.sh"
    retry_config:
      attempts: 10
      backoff: fixed
      initial_delay: "10s"
      on_failure: stop
</code></pre>
<h3 id="example-13-mapreduce-with-dlq-and-retry"><a class="header" href="#example-13-mapreduce-with-dlq-and-retry">Example 13: MapReduce with DLQ and Retry</a></h3>
<p>MapReduce workflow with error handling:</p>
<pre><code class="language-yaml">name: mapreduce-with-retry
mode: mapreduce

error_policy:
  on_item_failure: dlq        # Send failures to Dead Letter Queue
  continue_on_failure: true   # Keep processing other items
  max_failures: 5             # Stop if more than 5 items fail

map:
  input: "work-items.json"
  json_path: "$.items[*]"
  max_parallel: 10

  agent_template:
    - shell: "process-item ${item.id}"
      retry_config:
        attempts: 3
        backoff: exponential
        initial_delay: "1s"
        max_delay: "30s"
        jitter: true          # Important for parallel agents
        retry_on:
          - network
          - timeout
        on_failure: stop      # Let DLQ handle final failures

reduce:
  - shell: "aggregate-results ${map.results}"
</code></pre>
<p><strong>Error handling flow</strong>:</p>
<ol>
<li>Each work item is retried up to 3 times per agent</li>
<li>If all retries fail → Item goes to DLQ</li>
<li>Processing continues for other items</li>
<li>After map phase, retry DLQ items with: <code>prodigy dlq retry &lt;job_id&gt;</code></li>
</ol>
<p><strong>Source</strong>: Workflow-level retry in src/cook/workflow/error_policy.rs:90-129</p>
<h3 id="testing-your-retry-configuration"><a class="header" href="#testing-your-retry-configuration">Testing Your Retry Configuration</a></h3>
<p>Validate retry behavior with controlled failures:</p>
<pre><code class="language-yaml">name: test-retry-behavior
mode: standard

commands:
  # Use a script that fails N times then succeeds
  - shell: "./fail-then-succeed.sh 2"  # Fails 2 times, succeeds on 3rd
    retry_config:
      attempts: 5
      backoff: exponential
      initial_delay: "1s"
</code></pre>
<p><strong>fail-then-succeed.sh</strong> example:</p>
<pre><code class="language-bash">#!/bin/bash
FAIL_COUNT=${1:-2}
STATE_FILE="/tmp/retry-test-$$"

if [ ! -f "$STATE_FILE" ]; then
  echo "0" &gt; "$STATE_FILE"
fi

CURRENT=$(cat "$STATE_FILE")
NEXT=$((CURRENT + 1))
echo "$NEXT" &gt; "$STATE_FILE"

if [ "$NEXT" -le "$FAIL_COUNT" ]; then
  echo "Attempt $NEXT: Simulated failure"
  exit 1
else
  echo "Attempt $NEXT: Success!"
  rm "$STATE_FILE"
  exit 0
fi
</code></pre>
<h3 id="see-also-30"><a class="header" href="#see-also-30">See Also</a></h3>
<ul>
<li><a href="retry-configuration/./basic-retry-configuration.html">Basic Retry Configuration</a> - Configuration options</li>
<li><a href="retry-configuration/./backoff-strategies.html">Backoff Strategies</a> - Detailed backoff strategy documentation</li>
<li><a href="retry-configuration/./conditional-retry-with-error-matchers.html">Conditional Retry with Error Matchers</a> - Selective retry</li>
<li><a href="retry-configuration/./failure-actions.html">Failure Actions</a> - Handling final failures</li>
<li><a href="retry-configuration/./best-practices.html">Best Practices</a> - When to use each pattern</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="workflow-level-vs-command-level-retry"><a class="header" href="#workflow-level-vs-command-level-retry">Workflow-Level vs Command-Level Retry</a></h2>
<p>Prodigy has two distinct retry systems that serve different purposes. Understanding when to use each is critical for effective error handling.</p>
<h3 id="overview-10"><a class="header" href="#overview-10">Overview</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Feature</th><th>Command-Level (retry_v2)</th><th>Workflow-Level (error_policy)</th></tr></thead><tbody>
<tr><td><strong>Scope</strong></td><td>Individual command execution</td><td>Work item failure in MapReduce</td></tr>
<tr><td><strong>Location</strong></td><td><code>retry_config</code> on commands</td><td><code>error_policy</code> + <code>retry_config</code> in workflow</td></tr>
<tr><td><strong>Implementation</strong></td><td><code>src/cook/retry_v2.rs</code></td><td><code>src/cook/workflow/error_policy.rs</code></td></tr>
<tr><td><strong>Use Case</strong></td><td>Retry transient command failures</td><td>Retry failed MapReduce work items</td></tr>
<tr><td><strong>Features</strong></td><td>Backoff, jitter, error matchers, circuit breakers</td><td>DLQ integration, failure thresholds, batch collection</td></tr>
</tbody></table>
</div>
<h3 id="command-level-retry-retry_v2"><a class="header" href="#command-level-retry-retry_v2">Command-Level Retry (retry_v2)</a></h3>
<p>The <strong>enhanced retry system</strong> (<code>retry_v2</code>) provides sophisticated retry capabilities for individual command execution.</p>
<p><strong>Source</strong>: <code>src/cook/retry_v2.rs</code></p>
<h4 id="key-features"><a class="header" href="#key-features">Key Features</a></h4>
<ol>
<li>
<p><strong>Multiple Backoff Strategies</strong>:</p>
<ul>
<li>Exponential (default)</li>
<li>Linear</li>
<li>Fibonacci</li>
<li>Fixed</li>
<li>Custom</li>
</ul>
</li>
<li>
<p><strong>Error Matchers</strong>:</p>
<ul>
<li>Selective retry based on error type</li>
<li>Built-in matchers: Network, Timeout, ServerError, RateLimit</li>
<li>Custom regex patterns</li>
</ul>
</li>
<li>
<p><strong>Jitter Support</strong>:</p>
<ul>
<li>Prevents thundering herd in distributed systems</li>
<li>Configurable jitter factor (default: 0.3)</li>
</ul>
</li>
<li>
<p><strong>Retry Budget</strong>:</p>
<ul>
<li>Time-based caps on total retry time</li>
<li>Prevents infinite retry loops</li>
</ul>
</li>
<li>
<p><strong>Failure Actions</strong>:</p>
<ul>
<li>Stop (default) - halt workflow</li>
<li>Continue - proceed despite failure</li>
<li>Fallback - execute alternative command</li>
</ul>
</li>
<li>
<p><strong>Circuit Breakers</strong>:</p>
<ul>
<li>Fail-fast when downstream is down</li>
<li>Automatic recovery testing (HalfOpen state)</li>
</ul>
</li>
</ol>
<h4 id="configuration-4"><a class="header" href="#configuration-4">Configuration</a></h4>
<pre><code class="language-yaml">commands:
  - shell: "curl https://api.example.com/data"
    retry_config:
      attempts: 5
      backoff: exponential
      initial_delay: "1s"
      max_delay: "30s"
      jitter: true
      jitter_factor: 0.3
      retry_on:
        - network
        - timeout
      retry_budget: "5m"
      on_failure: stop
</code></pre>
<p><strong>RetryConfig Fields</strong> (src/cook/retry_v2.rs:14-52):</p>
<ul>
<li><code>attempts: u32</code> - Maximum retry attempts</li>
<li><code>backoff: BackoffStrategy</code> - Delay calculation strategy</li>
<li><code>initial_delay: Duration</code> - Starting delay</li>
<li><code>max_delay: Duration</code> - Delay cap</li>
<li><code>jitter: bool</code> - Enable jitter</li>
<li><code>jitter_factor: f64</code> - Jitter randomization (0.0-1.0)</li>
<li><code>retry_on: Vec&lt;ErrorMatcher&gt;</code> - Selective retry matchers</li>
<li><code>retry_budget: Option&lt;Duration&gt;</code> - Total retry time limit</li>
<li><code>on_failure: FailureAction</code> - Final failure handling</li>
</ul>
<h4 id="when-to-use-command-level-retry"><a class="header" href="#when-to-use-command-level-retry">When to Use Command-Level Retry</a></h4>
<p>Use <code>retry_config</code> on individual commands for:</p>
<ul>
<li><strong>External API calls</strong> with transient failures</li>
<li><strong>Network operations</strong> that might timeout</li>
<li><strong>Database operations</strong> with lock conflicts</li>
<li><strong>Resource initialization</strong> that needs retry</li>
<li>Any <strong>single command</strong> that benefits from retry</li>
</ul>
<p><strong>Example</strong>:</p>
<pre><code class="language-yaml">commands:
  - shell: "make build"
    retry_config:
      attempts: 3
      retry_on:
        - network  # Only retry network errors during dependency fetch
</code></pre>
<h3 id="workflow-level-retry-error_policy"><a class="header" href="#workflow-level-retry-error_policy">Workflow-Level Retry (error_policy)</a></h3>
<p>The <strong>workflow-level retry system</strong> handles failures in MapReduce work items, integrating with the Dead Letter Queue (DLQ).</p>
<p><strong>Source</strong>: <code>src/cook/workflow/error_policy.rs</code></p>
<h4 id="key-features-1"><a class="header" href="#key-features-1">Key Features</a></h4>
<ol>
<li>
<p><strong>Work Item Failure Handling</strong>:</p>
<ul>
<li>DLQ (Dead Letter Queue) - Save failed items for retry</li>
<li>Retry - Immediate retry with backoff</li>
<li>Skip - Skip failed item, continue</li>
<li>Stop - Stop entire workflow</li>
<li>Custom - Custom failure handler</li>
</ul>
</li>
<li>
<p><strong>Failure Thresholds</strong>:</p>
<ul>
<li><code>max_failures</code> - Stop after N failures</li>
<li><code>failure_threshold</code> - Stop at failure rate (0.0-1.0)</li>
</ul>
</li>
<li>
<p><strong>Error Collection Strategies</strong>:</p>
<ul>
<li>Aggregate - Collect all errors before reporting</li>
<li>Immediate - Report errors as they occur</li>
<li>Batched - Report in batches of N errors</li>
</ul>
</li>
<li>
<p><strong>Circuit Breaker Integration</strong>:</p>
<ul>
<li>Workflow-level circuit breaker</li>
<li>Failure/success thresholds</li>
<li>Half-open request limits</li>
</ul>
</li>
<li>
<p><strong>DLQ Retry</strong>:</p>
<ul>
<li>Failed items stored in DLQ</li>
<li>Retry with <code>prodigy dlq retry &lt;job_id&gt;</code></li>
<li>Preserves correlation IDs</li>
</ul>
</li>
</ol>
<h4 id="configuration-5"><a class="header" href="#configuration-5">Configuration</a></h4>
<pre><code class="language-yaml">name: mapreduce-workflow
mode: mapreduce

error_policy:
  on_item_failure: dlq          # Send failures to DLQ
  continue_on_failure: true     # Keep processing other items
  max_failures: 10              # Stop after 10 failures
  failure_threshold: 0.25       # Or stop at 25% failure rate
  error_collection: aggregate   # Collect errors before reporting
  circuit_breaker:
    failure_threshold: 5        # Open after 5 failures
    success_threshold: 3        # Close after 3 successes
    timeout: "30s"              # Recovery timeout
    half_open_requests: 3       # Test requests in half-open
  retry_config:                 # Workflow-level retry (simpler)
    max_attempts: 3
    backoff: exponential

map:
  input: "items.json"
  json_path: "$.items[*]"
  agent_template:
    - shell: "process ${item.id}"
</code></pre>
<p><strong>WorkflowErrorPolicy Fields</strong> (src/cook/workflow/error_policy.rs:131-179):</p>
<ul>
<li><code>on_item_failure: ItemFailureAction</code> - What to do when item fails</li>
<li><code>continue_on_failure: bool</code> - Continue processing other items</li>
<li><code>max_failures: Option&lt;usize&gt;</code> - Maximum failures before stopping</li>
<li><code>failure_threshold: Option&lt;f64&gt;</code> - Failure rate threshold (0.0-1.0)</li>
<li><code>error_collection: ErrorCollectionStrategy</code> - Error reporting strategy</li>
<li><code>circuit_breaker: Option&lt;CircuitBreakerConfig&gt;</code> - Circuit breaker config</li>
<li><code>retry_config: Option&lt;RetryConfig&gt;</code> - Simplified retry config</li>
</ul>
<p><strong>Workflow-Level RetryConfig</strong> (src/cook/workflow/error_policy.rs:90-129):</p>
<ul>
<li><code>max_attempts: u32</code> - Maximum retry attempts</li>
<li><code>backoff: BackoffStrategy</code> - Simpler backoff variants</li>
</ul>
<h4 id="when-to-use-workflow-level-retry"><a class="header" href="#when-to-use-workflow-level-retry">When to Use Workflow-Level Retry</a></h4>
<p>Use <code>error_policy</code> in MapReduce workflows for:</p>
<ul>
<li><strong>Work item failure handling</strong> - DLQ, thresholds, collection</li>
<li><strong>MapReduce workflows</strong> - Parallel work item processing</li>
<li><strong>Failure rate monitoring</strong> - Stop at threshold percentage</li>
<li><strong>Batch error handling</strong> - Collect and report errors in batches</li>
</ul>
<p><strong>Example</strong>:</p>
<pre><code class="language-yaml">error_policy:
  on_item_failure: dlq        # Failed items go to DLQ
  continue_on_failure: true   # Process other items
  max_failures: 5             # But stop if more than 5 fail
</code></pre>
<h3 id="using-both-systems-together"><a class="header" href="#using-both-systems-together">Using Both Systems Together</a></h3>
<p>You can combine both retry systems for comprehensive error handling:</p>
<pre><code class="language-yaml">name: robust-mapreduce
mode: mapreduce

# Workflow-level error policy
error_policy:
  on_item_failure: dlq        # Failed items to DLQ
  continue_on_failure: true   # Keep processing
  max_failures: 10            # Stop after 10 failures
  circuit_breaker:
    failure_threshold: 5
    timeout: "30s"

map:
  input: "items.json"
  json_path: "$.items[*]"
  max_parallel: 10

  agent_template:
    # Command-level retry for transient failures
    - shell: "process ${item.id}"
      retry_config:
        attempts: 3              # Try 3 times per agent
        backoff: exponential
        initial_delay: "1s"
        max_delay: "30s"
        jitter: true             # Prevent thundering herd
        retry_on:
          - network
          - timeout
        on_failure: stop         # Let workflow error_policy handle final failure
</code></pre>
<p><strong>How they work together</strong>:</p>
<ol>
<li><strong>Agent attempts to process work item</strong></li>
<li><strong>Command fails</strong> → <code>retry_config</code> retries with exponential backoff (up to 3 attempts)</li>
<li><strong>All command retries fail</strong> → <code>on_failure: stop</code> ends agent execution</li>
<li><strong>Agent reports failure to workflow</strong> → <code>error_policy</code> sends item to DLQ</li>
<li><strong>Workflow continues processing other items</strong> → <code>continue_on_failure: true</code></li>
<li><strong>After map phase</strong> → Retry DLQ items with <code>prodigy dlq retry &lt;job_id&gt;</code></li>
</ol>
<p><strong>Benefits</strong>:</p>
<ul>
<li><strong>Fast recovery</strong> from transient errors (command-level retry)</li>
<li><strong>Isolation</strong> of persistent failures (DLQ)</li>
<li><strong>Continued processing</strong> of other items (workflow-level policy)</li>
<li><strong>Manual review</strong> of failed items before retry</li>
</ul>
<h3 id="key-differences"><a class="header" href="#key-differences">Key Differences</a></h3>
<h4 id="scope-of-retry"><a class="header" href="#scope-of-retry">Scope of Retry</a></h4>
<p><strong>Command-Level</strong>: Single command execution</p>
<pre><code class="language-yaml">commands:
  - shell: "api-call.sh"
    retry_config:
      attempts: 5
</code></pre>
<p>→ Retries the <code>api-call.sh</code> command 5 times</p>
<p><strong>Workflow-Level</strong>: Entire work item (may contain multiple commands)</p>
<pre><code class="language-yaml">map:
  agent_template:
    - shell: "step1.sh ${item.id}"
    - shell: "step2.sh ${item.id}"
    - shell: "step3.sh ${item.id}"
</code></pre>
<p>→ If agent fails, entire work item (all 3 steps) sent to DLQ for retry</p>
<h4 id="retry-granularity"><a class="header" href="#retry-granularity">Retry Granularity</a></h4>
<p><strong>Command-Level</strong>: Immediate retry after command failure</p>
<ul>
<li>Retry happens in same agent execution</li>
<li>Backoff delays between retries</li>
<li>Same environment/context</li>
</ul>
<p><strong>Workflow-Level</strong>: Work item retry after agent failure</p>
<ul>
<li>Retry happens in new agent execution (via DLQ)</li>
<li>Fresh environment/context</li>
<li>Manual triggering with <code>prodigy dlq retry</code></li>
</ul>
<h4 id="feature-comparison"><a class="header" href="#feature-comparison">Feature Comparison</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Feature</th><th>Command-Level</th><th>Workflow-Level</th></tr></thead><tbody>
<tr><td>Backoff strategies</td><td>5 strategies</td><td>4 strategies</td></tr>
<tr><td>Jitter support</td><td>✅ Yes</td><td>❌ No</td></tr>
<tr><td>Error matchers</td><td>✅ Yes</td><td>❌ No</td></tr>
<tr><td>Retry budget</td><td>✅ Yes</td><td>❌ No</td></tr>
<tr><td>Failure actions</td><td>✅ Yes (Stop/Continue/Fallback)</td><td>❌ No (uses error_policy)</td></tr>
<tr><td>Circuit breakers</td><td>✅ Yes (per RetryExecutor)</td><td>✅ Yes (per workflow)</td></tr>
<tr><td>DLQ integration</td><td>❌ No</td><td>✅ Yes</td></tr>
<tr><td>Failure thresholds</td><td>❌ No</td><td>✅ Yes</td></tr>
<tr><td>Error collection</td><td>❌ No</td><td>✅ Yes (Aggregate/Immediate/Batched)</td></tr>
</tbody></table>
</div>
<h3 id="decision-tree"><a class="header" href="#decision-tree">Decision Tree</a></h3>
<p><strong>Use command-level retry</strong> when:</p>
<ul>
<li>You need fine-grained control over which errors to retry</li>
<li>You want jitter to prevent thundering herd</li>
<li>You need retry budget to cap total time</li>
<li>You want fallback commands on failure</li>
<li>You’re configuring a single command, not a work item</li>
</ul>
<p><strong>Use workflow-level retry</strong> when:</p>
<ul>
<li>You’re running a MapReduce workflow</li>
<li>You want DLQ integration for failed work items</li>
<li>You need failure rate thresholds (stop at 25% failure)</li>
<li>You want batch error collection</li>
<li>You want to retry entire work items later</li>
</ul>
<p><strong>Use both</strong> when:</p>
<ul>
<li>You want fast recovery from transient errors (command-level)</li>
<li>AND you want to isolate persistent failures (DLQ)</li>
<li>AND you want to continue processing other items</li>
</ul>
<h3 id="examples-2"><a class="header" href="#examples-2">Examples</a></h3>
<h4 id="example-1-command-level-only-standard-workflow"><a class="header" href="#example-1-command-level-only-standard-workflow">Example 1: Command-Level Only (Standard Workflow)</a></h4>
<pre><code class="language-yaml">name: standard-workflow
mode: standard

commands:
  - shell: "fetch-data.sh"
    retry_config:
      attempts: 5
      backoff: exponential
      retry_on:
        - network
        - timeout

  - shell: "process-data.sh"
    retry_config:
      attempts: 3
      on_failure: continue  # Non-critical, can fail

  - shell: "upload-results.sh"
    retry_config:
      attempts: 5
      backoff: exponential
      retry_on:
        - network
        - server_error
      on_failure:
        fallback:
          command: "save-to-local.sh"
</code></pre>
<h4 id="example-2-workflow-level-only-simple-mapreduce"><a class="header" href="#example-2-workflow-level-only-simple-mapreduce">Example 2: Workflow-Level Only (Simple MapReduce)</a></h4>
<pre><code class="language-yaml">name: simple-mapreduce
mode: mapreduce

error_policy:
  on_item_failure: dlq
  continue_on_failure: true
  max_failures: 10

map:
  input: "items.json"
  json_path: "$.items[*]"
  agent_template:
    - shell: "process ${item.id}"
    # No retry_config - relies on DLQ for failed items
</code></pre>
<h4 id="example-3-both-systems-robust-mapreduce"><a class="header" href="#example-3-both-systems-robust-mapreduce">Example 3: Both Systems (Robust MapReduce)</a></h4>
<pre><code class="language-yaml">name: robust-mapreduce
mode: mapreduce

error_policy:
  on_item_failure: dlq
  continue_on_failure: true
  max_failures: 10
  circuit_breaker:
    failure_threshold: 5
    timeout: "30s"

map:
  input: "items.json"
  json_path: "$.items[*]"
  max_parallel: 10

  agent_template:
    - shell: "process ${item.id}"
      retry_config:
        attempts: 3           # Try 3 times quickly
        backoff: exponential
        jitter: true          # Prevent simultaneous retries
        retry_on:
          - network
          - timeout
        on_failure: stop      # Let DLQ handle final failure

reduce:
  - shell: "aggregate ${map.results}"
</code></pre>
<p><strong>Retry flow</strong>:</p>
<ol>
<li>Command fails with network error</li>
<li>Command retries (attempt 2, 3) with exponential backoff</li>
<li>All command retries fail → Agent fails</li>
<li>Work item sent to DLQ</li>
<li>Other work items continue processing</li>
<li>After workflow completes → <code>prodigy dlq retry &lt;job_id&gt;</code> to retry failed items</li>
</ol>
<h3 id="see-also-31"><a class="header" href="#see-also-31">See Also</a></h3>
<ul>
<li><a href="retry-configuration/./basic-retry-configuration.html">Basic Retry Configuration</a> - Command-level retry config</li>
<li><a href="retry-configuration/./backoff-strategies.html">Backoff Strategies</a> - Backoff strategy details</li>
<li><a href="retry-configuration/./best-practices.html">Best Practices</a> - When to use each system</li>
<li><a href="retry-configuration/./complete-examples.html">Complete Examples</a> - Full workflow examples</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="retry-metrics-and-observability"><a class="header" href="#retry-metrics-and-observability">Retry Metrics and Observability</a></h2>
<p>Prodigy provides comprehensive observability into retry behavior through <strong>logging and event tracking</strong>, rather than programmatic metrics access. This enables debugging retry issues and monitoring workflow resilience without requiring code changes.</p>
<h2 id="how-retry-observability-works"><a class="header" href="#how-retry-observability-works">How Retry Observability Works</a></h2>
<p>Retry operations are observable through three primary mechanisms:</p>
<ol>
<li><strong>Console Logs</strong> - Visible during workflow execution</li>
<li><strong>Tracing Infrastructure</strong> - Controlled by verbosity flags</li>
<li><strong>Event Tracking</strong> - Recorded for MapReduce workflows</li>
</ol>
<h3 id="internal-metrics-structure"><a class="header" href="#internal-metrics-structure">Internal Metrics Structure</a></h3>
<p>While retry metrics are tracked internally via the <code>RetryMetrics</code> structure (src/cook/retry_v2.rs:399-422), this data is <strong>not exposed</strong> as a queryable API for workflow users. Instead, metrics are surfaced through logging at key decision points.</p>
<p>The internal structure tracks:</p>
<ul>
<li><code>total_attempts</code> - Total number of attempts made</li>
<li><code>successful_attempts</code> - Number of successful operations</li>
<li><code>failed_attempts</code> - Number of failed operations</li>
<li><code>retries</code> - History of retry attempts with delays</li>
</ul>
<h2 id="observing-retry-behavior-in-workflows"><a class="header" href="#observing-retry-behavior-in-workflows">Observing Retry Behavior in Workflows</a></h2>
<h3 id="console-output-during-retries"><a class="header" href="#console-output-during-retries">Console Output During Retries</a></h3>
<p>When a command fails and triggers a retry, you’ll see log messages showing:</p>
<pre><code>Retrying &lt;operation&gt; (attempt 2/5) after 200ms
Retrying &lt;operation&gt; (attempt 3/5) after 400ms
</code></pre>
<p><strong>Source</strong>: These messages come from the retry executor’s logging (src/cook/retry_v2.rs:247-250).</p>
<h3 id="verbosity-flags-for-detailed-logs"><a class="header" href="#verbosity-flags-for-detailed-logs">Verbosity Flags for Detailed Logs</a></h3>
<p>Use Prodigy’s verbosity flags to see more retry details:</p>
<pre><code class="language-bash"># Default - shows retry attempts
prodigy run workflow.yml

# Verbose - shows retry decision-making
prodigy run workflow.yml -v

# Very verbose - shows all retry internals
prodigy run workflow.yml -vv
</code></pre>
<p>With <code>-v</code> or higher verbosity, you’ll see:</p>
<ul>
<li>Retry attempt counts and delays</li>
<li>Circuit breaker state transitions</li>
<li>Retry budget consumption</li>
<li>Error pattern matching results</li>
</ul>
<h3 id="retry-budget-exhaustion"><a class="header" href="#retry-budget-exhaustion">Retry Budget Exhaustion</a></h3>
<p>When retry budget is exhausted, a warning is logged:</p>
<pre><code>Retry budget exhausted for &lt;operation&gt;
</code></pre>
<p><strong>Source</strong>: src/cook/retry_v2.rs:238-243</p>
<p>This indicates the total retry delay exceeded the configured <code>retry_budget</code>, preventing further attempts even if <code>max_attempts</code> hasn’t been reached.</p>
<h2 id="circuit-breaker-observability"><a class="header" href="#circuit-breaker-observability">Circuit Breaker Observability</a></h2>
<p>Circuit breaker state changes are logged automatically:</p>
<h3 id="circuit-breaker-opens"><a class="header" href="#circuit-breaker-opens">Circuit Breaker Opens</a></h3>
<pre><code>Circuit breaker opened after 5 consecutive failures
</code></pre>
<p><strong>Source</strong>: src/cook/retry_v2.rs:391</p>
<p>When the failure threshold is exceeded, the circuit breaker opens and blocks further retry attempts until the recovery timeout expires.</p>
<h3 id="circuit-breaker-transitions"><a class="header" href="#circuit-breaker-transitions">Circuit Breaker Transitions</a></h3>
<pre><code>Circuit breaker transitioning to half-open
Circuit breaker closed after successful operation
</code></pre>
<p><strong>Source</strong>: src/cook/retry_v2.rs:359, 377</p>
<p>These debug-level messages show circuit breaker recovery:</p>
<ul>
<li><strong>Half-open</strong>: Testing if failures have cleared (one attempt allowed)</li>
<li><strong>Closed</strong>: Normal operation resumed after successful attempt</li>
</ul>
<h2 id="workflow-level-observability"><a class="header" href="#workflow-level-observability">Workflow-Level Observability</a></h2>
<h3 id="on-failure-handler-execution"><a class="header" href="#on-failure-handler-execution">On-Failure Handler Execution</a></h3>
<p>When retries are configured via <code>on_failure</code> handlers, you’ll see:</p>
<pre><code class="language-yaml">- shell: "flaky-command"
  on_failure:
    claude: "/fix-issue"
    max_attempts: 5
</code></pre>
<p><strong>Workflow output</strong> shows:</p>
<ol>
<li>Initial command failure</li>
<li>Retry attempt messages with delays</li>
<li>Handler command execution</li>
<li>Final success or failure after exhausting attempts</li>
</ol>
<h3 id="example-log-progression"><a class="header" href="#example-log-progression">Example Log Progression</a></h3>
<pre><code>❌ Command failed: exit code 1
Retrying (attempt 2/5) after 100ms
❌ Command failed: exit code 1
Retrying (attempt 3/5) after 200ms
✅ Command succeeded
</code></pre>
<h2 id="mapreduce-event-tracking"><a class="header" href="#mapreduce-event-tracking">MapReduce Event Tracking</a></h2>
<p>MapReduce workflows provide additional observability through the event tracking system. Retry attempts are recorded as events:</p>
<pre><code class="language-bash"># View events for a MapReduce job
prodigy events show &lt;job_id&gt;
</code></pre>
<p>Events include:</p>
<ul>
<li>Retry attempt counts per work item</li>
<li>Individual agent retry behavior</li>
<li>Correlation between retries and DLQ items</li>
</ul>
<p>See <a href="retry-configuration/../mapreduce/event-tracking.html">Event Tracking</a> for comprehensive event details.</p>
<h2 id="troubleshooting-with-retry-logs"><a class="header" href="#troubleshooting-with-retry-logs">Troubleshooting with Retry Logs</a></h2>
<h3 id="identifying-retry-configuration-issues"><a class="header" href="#identifying-retry-configuration-issues">Identifying Retry Configuration Issues</a></h3>
<p><strong>Problem</strong>: Commands retry unnecessarily on non-transient errors</p>
<p><strong>Check logs for</strong>:</p>
<pre><code>Retrying (attempt 2/5) after 100ms
Retrying (attempt 3/5) after 200ms
...
</code></pre>
<p><strong>Solution</strong>: Configure <code>retry_on</code> error matchers to only retry specific errors:</p>
<pre><code class="language-yaml">retry_config:
  max_attempts: 5
  retry_on:
    - pattern: "connection refused"
    - pattern: "timeout"
</code></pre>
<p>See <a href="retry-configuration/conditional-retry-with-error-matchers.html">Conditional Retry with Error Matchers</a>.</p>
<h3 id="debugging-circuit-breaker-behavior"><a class="header" href="#debugging-circuit-breaker-behavior">Debugging Circuit Breaker Behavior</a></h3>
<p><strong>Problem</strong>: Circuit breaker opening too aggressively</p>
<p><strong>Check logs for</strong>:</p>
<pre><code>Circuit breaker opened after 3 consecutive failures
</code></pre>
<p><strong>Solution</strong>: Adjust <code>failure_threshold</code> and <code>recovery_timeout</code>:</p>
<pre><code class="language-yaml">circuit_breaker:
  failure_threshold: 5      # Increase tolerance
  recovery_timeout_ms: 30000  # Longer recovery window
</code></pre>
<h3 id="analyzing-retry-budget-exhaustion"><a class="header" href="#analyzing-retry-budget-exhaustion">Analyzing Retry Budget Exhaustion</a></h3>
<p><strong>Problem</strong>: Retries stop before <code>max_attempts</code> reached</p>
<p><strong>Check logs for</strong>:</p>
<pre><code>Retry budget exhausted for &lt;operation&gt;
</code></pre>
<p><strong>Solution</strong>: Increase <code>retry_budget</code> or reduce <code>max_delay_ms</code>:</p>
<pre><code class="language-yaml">retry_config:
  retry_budget_ms: 120000  # Allow up to 2 minutes of retries
  max_delay_ms: 10000       # Cap individual delays at 10s
</code></pre>
<p>See <a href="retry-configuration/retry-budget.html">Retry Budget</a>.</p>
<h2 id="integration-with-error-handling"><a class="header" href="#integration-with-error-handling">Integration with Error Handling</a></h2>
<p>Retry observability integrates with Prodigy’s error handling pipeline:</p>
<ol>
<li><strong>Retry</strong> - First line of defense for transient failures</li>
<li><strong>On-Failure Handlers</strong> - Second line when retries exhausted</li>
<li><strong>Dead Letter Queue</strong> - Final capture for MapReduce failures</li>
</ol>
<p><strong>Example workflow progression</strong>:</p>
<pre><code>Command fails → Retry with backoff → Still failing → Execute on_failure handler →
Handler succeeds → Continue OR Handler fails → DLQ (MapReduce only)
</code></pre>
<p>Logs show the complete progression:</p>
<pre><code>❌ shell: flaky-script failed
Retrying (attempt 2/3) after 100ms
❌ shell: flaky-script failed
Retrying (attempt 3/3) after 200ms
❌ shell: flaky-script failed
Executing on_failure handler: claude /debug-failure
✅ on_failure handler succeeded
</code></pre>
<h3 id="mapreduce-dlq-relationship"><a class="header" href="#mapreduce-dlq-relationship">MapReduce DLQ Relationship</a></h3>
<p>For MapReduce workflows, retry exhaustion leads to DLQ:</p>
<pre><code>Retrying work item 'item-123' (attempt 3/3)
❌ Failed after 3 attempts
→ Item moved to Dead Letter Queue
</code></pre>
<p>View DLQ items with retry history:</p>
<pre><code class="language-bash">prodigy dlq show &lt;job_id&gt;
</code></pre>
<p>Each DLQ item preserves:</p>
<ul>
<li>Number of retry attempts made</li>
<li>Final error message</li>
<li>Retry delays used</li>
<li>Circuit breaker state at failure</li>
</ul>
<p>See <a href="retry-configuration/../mapreduce/dead-letter-queue-dlq.html">Dead Letter Queue (DLQ)</a>.</p>
<h2 id="monitoring-retry-patterns"><a class="header" href="#monitoring-retry-patterns">Monitoring Retry Patterns</a></h2>
<h3 id="identifying-flaky-operations"><a class="header" href="#identifying-flaky-operations">Identifying Flaky Operations</a></h3>
<p>Watch for patterns in logs indicating flaky behavior:</p>
<pre><code class="language-bash"># Count retry attempts across workflow execution
prodigy run workflow.yml -v 2&gt;&amp;1 | grep "Retrying" | wc -l

# Find operations that retry most frequently
prodigy run workflow.yml -v 2&gt;&amp;1 | grep "Retrying" | \
  grep -oE 'Retrying [^ ]+' | sort | uniq -c | sort -rn
</code></pre>
<h3 id="measuring-retry-effectiveness"><a class="header" href="#measuring-retry-effectiveness">Measuring Retry Effectiveness</a></h3>
<p>Track retry success vs. failure rates:</p>
<pre><code class="language-bash"># Find successful retries
grep "Retrying" workflow.log | grep -A 5 "succeeded"

# Find exhausted retries
grep "Retry budget exhausted" workflow.log
grep "Failed after .* attempts" workflow.log
</code></pre>
<h2 id="best-practices-for-observability"><a class="header" href="#best-practices-for-observability">Best Practices for Observability</a></h2>
<ol>
<li><strong>Use appropriate verbosity</strong> - <code>-v</code> for debugging, default for production</li>
<li><strong>Configure structured logging</strong> - Enable JSON logs for machine parsing</li>
<li><strong>Set meaningful operation contexts</strong> - Use descriptive command names</li>
<li><strong>Monitor circuit breaker openings</strong> - Alert on frequent openings</li>
<li><strong>Review DLQ regularly</strong> - Analyze patterns in failed items</li>
<li><strong>Correlate retries with external events</strong> - Match retry spikes to infrastructure issues</li>
</ol>
<h2 id="related-topics-7"><a class="header" href="#related-topics-7">Related Topics</a></h2>
<ul>
<li><a href="retry-configuration/basic-retry-configuration.html">Basic Retry Configuration</a> - Configure retry behavior</li>
<li><a href="retry-configuration/backoff-strategies.html">Backoff Strategies</a> - Choose appropriate backoff patterns</li>
<li><a href="retry-configuration/../mapreduce/event-tracking.html">Event Tracking</a> - MapReduce observability</li>
<li><a href="retry-configuration/troubleshooting.html">Troubleshooting</a> - Debug common retry issues</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="best-practices-22"><a class="header" href="#best-practices-22">Best Practices</a></h2>
<p>This guide covers best practices for configuring retry behavior in Prodigy workflows, based on the implementation patterns in <code>retry_v2</code>.</p>
<p><strong>Source</strong>: Best practices derived from retry_v2.rs implementation and test patterns (src/cook/retry_v2.rs:463-748)</p>
<h3 id="choosing-the-right-backoff-strategy"><a class="header" href="#choosing-the-right-backoff-strategy">Choosing the Right Backoff Strategy</a></h3>
<p>Different backoff strategies suit different failure modes:</p>
<h4 id="exponential-backoff-default---best-for-most-cases"><a class="header" href="#exponential-backoff-default---best-for-most-cases">Exponential Backoff (Default - Best for Most Cases)</a></h4>
<p><strong>When to use</strong>:</p>
<ul>
<li>Network requests to external APIs</li>
<li>Transient failures that self-heal over time</li>
<li>Rate-limited services</li>
<li>Database connection retries</li>
</ul>
<p><strong>Configuration</strong>:</p>
<pre><code class="language-yaml">retry_config:
  attempts: 5
  backoff: exponential  # or: { base: 2.0 }
  initial_delay: "1s"
  max_delay: "30s"
  jitter: true
</code></pre>
<p><strong>Why</strong>: Exponential backoff quickly backs off from rapid retries, reducing load on failing systems and allowing recovery time.</p>
<p><strong>Source</strong>: <code>src/cook/retry_v2.rs:82-85</code> (Exponential variant), tests at lines 626-648</p>
<h4 id="linear-backoff-2"><a class="header" href="#linear-backoff-2">Linear Backoff</a></h4>
<p><strong>When to use</strong>:</p>
<ul>
<li>Predictable delays needed</li>
<li>Testing and debugging (easier to reason about delays)</li>
<li>Systems with known recovery time patterns</li>
</ul>
<p><strong>Configuration</strong>:</p>
<pre><code class="language-yaml">retry_config:
  attempts: 5
  backoff:
    linear:
      increment: "5s"
  initial_delay: "1s"
</code></pre>
<p><strong>Delays</strong>: 1s, 6s, 11s, 16s, 21s (initial + n * increment)</p>
<p><strong>Source</strong>: <code>src/cook/retry_v2.rs:77-80</code> (Linear variant), tests at lines 604-625</p>
<h4 id="fibonacci-backoff-2"><a class="header" href="#fibonacci-backoff-2">Fibonacci Backoff</a></h4>
<p><strong>When to use</strong>:</p>
<ul>
<li>Gradual backoff with slower growth than exponential</li>
<li>Good balance between responsiveness and system protection</li>
<li>Distributed systems needing gentler backoff</li>
</ul>
<p><strong>Configuration</strong>:</p>
<pre><code class="language-yaml">retry_config:
  attempts: 6
  backoff: fibonacci
  initial_delay: "1s"
  max_delay: "60s"
</code></pre>
<p><strong>Delays</strong>: 1s, 2s, 3s, 5s, 8s, 13s</p>
<p><strong>Source</strong>: <code>src/cook/retry_v2.rs:87</code> (Fibonacci variant), Fibonacci calculation at lines 424-440</p>
<h4 id="fixed-delay"><a class="header" href="#fixed-delay">Fixed Delay</a></h4>
<p><strong>When to use</strong>:</p>
<ul>
<li>Polling operations</li>
<li>Systems with consistent retry requirements</li>
<li>Simplicity preferred over optimization</li>
</ul>
<p><strong>Configuration</strong>:</p>
<pre><code class="language-yaml">retry_config:
  attempts: 10
  backoff: fixed
  initial_delay: "5s"
</code></pre>
<p><strong>Source</strong>: <code>src/cook/retry_v2.rs:75</code> (Fixed variant), tests at lines 583-603</p>
<h3 id="setting-appropriate-max_delay"><a class="header" href="#setting-appropriate-max_delay">Setting Appropriate max_delay</a></h3>
<p>Always set <code>max_delay</code> to prevent unbounded delays:</p>
<p>✅ <strong>Good</strong>:</p>
<pre><code class="language-yaml">retry_config:
  backoff: exponential
  max_delay: "30s"  # Caps exponential growth
</code></pre>
<p>❌ <strong>Bad</strong>:</p>
<pre><code class="language-yaml">retry_config:
  backoff: exponential
  # No max_delay - could delay minutes or hours!
</code></pre>
<p><strong>Default</strong>: 30 seconds (src/cook/retry_v2.rs:64)</p>
<p><strong>Recommendation</strong>:</p>
<ul>
<li>Interactive workflows: 10-30 seconds</li>
<li>Background jobs: 60-300 seconds</li>
<li>Critical paths: 5-15 seconds</li>
</ul>
<h3 id="using-jitter-in-distributed-systems"><a class="header" href="#using-jitter-in-distributed-systems">Using Jitter in Distributed Systems</a></h3>
<p><strong>Always enable jitter for distributed systems</strong> to prevent thundering herd:</p>
<pre><code class="language-yaml">retry_config:
  attempts: 5
  backoff: exponential
  initial_delay: "1s"
  max_delay: "30s"
  jitter: true           # Enable jitter
  jitter_factor: 0.3     # 30% randomization
</code></pre>
<p><strong>Source</strong>: <code>src/cook/retry_v2.rs:308-317</code> (apply_jitter method), tests at lines 650-673</p>
<p><strong>Why</strong>: Without jitter, multiple parallel agents/processes retry at the same time, overwhelming recovering systems.</p>
<p><strong>Jitter Formula</strong> (src/cook/retry_v2.rs:311-315):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let jitter_range = delay_ms as f64 * jitter_factor;
let random_offset = thread_rng().gen_range(-jitter_range..=jitter_range);
adjusted_delay = delay + Duration::from_millis(random_offset as u64);
<span class="boring">}</span></code></pre></pre>
<p><strong>When to use jitter</strong>:</p>
<ul>
<li>MapReduce workflows with parallel agents</li>
<li>Multiple services hitting the same API</li>
<li>Distributed systems with shared resources</li>
<li>Any parallel retry scenario</li>
</ul>
<p><strong>When to skip jitter</strong>:</p>
<ul>
<li>Single-instance workflows</li>
<li>Deterministic testing</li>
<li>Debugging retry behavior</li>
</ul>
<h3 id="retry-budget-for-preventing-infinite-loops"><a class="header" href="#retry-budget-for-preventing-infinite-loops">Retry Budget for Preventing Infinite Loops</a></h3>
<p>Use <code>retry_budget</code> to cap total retry time:</p>
<pre><code class="language-yaml">retry_config:
  attempts: 100          # High attempt count
  backoff: exponential
  retry_budget: "5m"     # But limit total time to 5 minutes
</code></pre>
<p><strong>Source</strong>: <code>src/cook/retry_v2.rs:46-47</code> (retry_budget field)</p>
<p><strong>Why</strong>: Prevents workflows from retrying indefinitely when <code>attempts</code> is set high.</p>
<p><strong>Use Cases</strong>:</p>
<ul>
<li>Long-running operations where total time matters</li>
<li>SLA-constrained workflows</li>
<li>Preventing resource exhaustion</li>
<li>Fail-fast on persistent failures</li>
</ul>
<p><strong>Implementation Note</strong>: Retry budget is checked before each retry attempt. If budget is exhausted, retries stop immediately (verified in test at src/cook/retry_v2.rs:675-708).</p>
<h3 id="selective-retry-with-error-matchers"><a class="header" href="#selective-retry-with-error-matchers">Selective Retry with Error Matchers</a></h3>
<p><strong>Don’t retry everything</strong> - use <code>retry_on</code> for transient errors only:</p>
<p>✅ <strong>Good</strong> (selective retry):</p>
<pre><code class="language-yaml">retry_config:
  attempts: 5
  retry_on:
    - network
    - timeout
    - server_error  # 5xx errors
</code></pre>
<p>❌ <strong>Bad</strong> (retry all errors):</p>
<pre><code class="language-yaml">retry_config:
  attempts: 5
  # Empty retry_on retries everything, including 404, 401, etc.
</code></pre>
<p><strong>Why</strong>: Some errors are <strong>permanent</strong> and retrying wastes time:</p>
<ul>
<li>404 Not Found - resource doesn’t exist</li>
<li>401 Unauthorized - credentials are invalid</li>
<li>400 Bad Request - request is malformed</li>
</ul>
<p><strong>Transient errors worth retrying</strong>:</p>
<ul>
<li>Network connectivity issues</li>
<li>Timeouts</li>
<li>5xx server errors</li>
<li>Rate limits (with appropriate backoff)</li>
<li>Database locks (temporary)</li>
</ul>
<p><strong>Source</strong>: <code>src/cook/retry_v2.rs:100-151</code> (ErrorMatcher enum), tests at lines 463-582</p>
<h3 id="combining-retry-with-circuit-breakers"><a class="header" href="#combining-retry-with-circuit-breakers">Combining Retry with Circuit Breakers</a></h3>
<p>For high-reliability systems, combine retry with circuit breakers:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Circuit breaker configuration (applied via RetryExecutor)
let executor = RetryExecutor::new(retry_config)
    .with_circuit_breaker(
        5,                          // failure_threshold
        Duration::from_secs(30)     // recovery_timeout
    );
<span class="boring">}</span></code></pre></pre>
<p><strong>Source</strong>: <code>src/cook/retry_v2.rs:184-188</code> (with_circuit_breaker method), <code>src/cook/retry_v2.rs:325-397</code> (CircuitBreaker implementation)</p>
<p><strong>Circuit Breaker States</strong>:</p>
<ol>
<li><strong>Closed</strong>: Normal operation, requests flow through</li>
<li><strong>Open</strong>: Circuit tripped after threshold failures, requests fail immediately</li>
<li><strong>HalfOpen</strong>: Testing recovery, limited requests allowed</li>
</ol>
<p><strong>Why</strong>: Circuit breakers prevent cascading failures by failing fast when downstream systems are down, rather than retrying indefinitely.</p>
<p><strong>Best Practice</strong>: Use circuit breakers for:</p>
<ul>
<li>External API calls</li>
<li>Database connections</li>
<li>Microservice communication</li>
<li>Any dependency that might fail completely</li>
</ul>
<h3 id="failure-action-strategies"><a class="header" href="#failure-action-strategies">Failure Action Strategies</a></h3>
<p>Choose <code>on_failure</code> based on operation criticality:</p>
<h4 id="critical-operations-use-stop"><a class="header" href="#critical-operations-use-stop">Critical Operations (Use: Stop)</a></h4>
<pre><code class="language-yaml">retry_config:
  attempts: 3
  on_failure: stop
</code></pre>
<p><strong>Examples</strong>:</p>
<ul>
<li>Database migrations</li>
<li>Deployment prerequisites</li>
<li>Data integrity checks</li>
<li>Security validations</li>
</ul>
<h4 id="optional-operations-use-continue"><a class="header" href="#optional-operations-use-continue">Optional Operations (Use: Continue)</a></h4>
<pre><code class="language-yaml">retry_config:
  attempts: 2
  on_failure: continue
</code></pre>
<p><strong>Examples</strong>:</p>
<ul>
<li>Cache warmup</li>
<li>Metrics collection</li>
<li>Notifications</li>
<li>Non-critical cleanup</li>
</ul>
<h4 id="fallback-operations-use-fallback"><a class="header" href="#fallback-operations-use-fallback">Fallback Operations (Use: Fallback)</a></h4>
<pre><code class="language-yaml">retry_config:
  attempts: 3
  on_failure:
    fallback:
      command: "cat cached-data.json"
</code></pre>
<p><strong>Examples</strong>:</p>
<ul>
<li>API with cache fallback</li>
<li>Primary/secondary data sources</li>
<li>Graceful degradation scenarios</li>
</ul>
<p><strong>Source</strong>: <code>src/cook/retry_v2.rs:153-165</code> (FailureAction enum)</p>
<h3 id="retry-configuration-anti-patterns"><a class="header" href="#retry-configuration-anti-patterns">Retry Configuration Anti-Patterns</a></h3>
<p>❌ <strong>Don’t</strong>: Set very high attempts without retry_budget</p>
<pre><code class="language-yaml">retry_config:
  attempts: 1000  # Could retry for hours!
</code></pre>
<p>✅ <strong>Do</strong>: Combine high attempts with retry_budget</p>
<pre><code class="language-yaml">retry_config:
  attempts: 100
  retry_budget: "5m"  # Caps total time
</code></pre>
<hr />
<p>❌ <strong>Don’t</strong>: Use exponential backoff without max_delay</p>
<pre><code class="language-yaml">retry_config:
  backoff: exponential
  # Delay could grow to minutes!
</code></pre>
<p>✅ <strong>Do</strong>: Always set max_delay</p>
<pre><code class="language-yaml">retry_config:
  backoff: exponential
  max_delay: "30s"
</code></pre>
<hr />
<p>❌ <strong>Don’t</strong>: Retry non-idempotent operations</p>
<pre><code class="language-yaml">retry_config:
  attempts: 5
  # If command creates resources, retries might duplicate!
</code></pre>
<p>✅ <strong>Do</strong>: Make operations idempotent or skip retry</p>
<pre><code class="language-yaml"># Use idempotency tokens, check-before-create, etc.
</code></pre>
<hr />
<p>❌ <strong>Don’t</strong>: Skip jitter in parallel workflows</p>
<pre><code class="language-yaml"># MapReduce with 10 parallel agents
retry_config:
  jitter: false  # All agents retry simultaneously!
</code></pre>
<p>✅ <strong>Do</strong>: Enable jitter for parallel execution</p>
<pre><code class="language-yaml">retry_config:
  jitter: true
  jitter_factor: 0.3
</code></pre>
<h3 id="testing-retry-configuration"><a class="header" href="#testing-retry-configuration">Testing Retry Configuration</a></h3>
<p>Use tests to validate retry behavior (patterns from src/cook/retry_v2.rs:463-748):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_retry_with_exponential_backoff() {
    let config = RetryConfig {
        attempts: 3,
        backoff: BackoffStrategy::Exponential { base: 2.0 },
        initial_delay: Duration::from_millis(100),
        ..Default::default()
    };

    let executor = RetryExecutor::new(config);

    // Test that retries happen with correct delays
    // Verify exponential growth: 100ms, 200ms, 400ms
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Test Coverage</strong>:</p>
<ul>
<li>Verify retry attempts match configuration</li>
<li>Check backoff delays are correct</li>
<li>Ensure error matchers work as expected</li>
<li>Validate circuit breaker state transitions</li>
<li>Test retry budget enforcement</li>
</ul>
<h3 id="production-monitoring"><a class="header" href="#production-monitoring">Production Monitoring</a></h3>
<p>Monitor retry metrics for operational insight:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Access retry metrics
let metrics = executor.metrics();
println!("Total attempts: {}", metrics.total_attempts);
println!("Successful: {}", metrics.successful_attempts);
println!("Failed: {}", metrics.failed_attempts);
<span class="boring">}</span></code></pre></pre>
<p><strong>Source</strong>: <code>src/cook/retry_v2.rs:320-322</code> (metrics() method), <code>src/cook/retry_v2.rs:399-422</code> (RetryMetrics struct)</p>
<p><strong>Key Metrics</strong>:</p>
<ul>
<li>Total attempts vs successful attempts (success rate)</li>
<li>Retry counts per operation (identify problematic operations)</li>
<li>Delay distributions (verify backoff is working)</li>
<li>Circuit breaker state changes (detect system issues)</li>
</ul>
<h3 id="summary-1"><a class="header" href="#summary-1">Summary</a></h3>
<ol>
<li><strong>Use exponential backoff</strong> for most retry scenarios (default)</li>
<li><strong>Always set max_delay</strong> to prevent unbounded delays</li>
<li><strong>Enable jitter</strong> for distributed/parallel systems</li>
<li><strong>Use retry_budget</strong> to cap total retry time</li>
<li><strong>Be selective</strong> with <code>retry_on</code> - don’t retry permanent errors</li>
<li><strong>Combine with circuit breakers</strong> for high-reliability systems</li>
<li><strong>Choose appropriate failure actions</strong> based on operation criticality</li>
<li><strong>Test retry behavior</strong> to ensure it works as expected</li>
<li><strong>Monitor retry metrics</strong> in production</li>
</ol>
<h3 id="see-also-32"><a class="header" href="#see-also-32">See Also</a></h3>
<ul>
<li><a href="retry-configuration/./backoff-strategies.html">Backoff Strategies</a> - Detailed backoff documentation</li>
<li><a href="retry-configuration/./jitter-for-distributed-systems.html">Jitter for Distributed Systems</a> - Preventing thundering herd</li>
<li><a href="retry-configuration/./retry-budget.html">Retry Budget</a> - Time-based retry limits</li>
<li><a href="retry-configuration/./conditional-retry-with-error-matchers.html">Conditional Retry with Error Matchers</a> - Selective retry</li>
<li><a href="retry-configuration/./failure-actions.html">Failure Actions</a> - Handling final failures</li>
<li><a href="retry-configuration/./complete-examples.html">Complete Examples</a> - Real-world retry configurations</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="troubleshooting-15"><a class="header" href="#troubleshooting-15">Troubleshooting</a></h2>
<p>Common retry configuration issues and their solutions.</p>
<h3 id="issue-retries-not-triggering"><a class="header" href="#issue-retries-not-triggering">Issue: Retries Not Triggering</a></h3>
<p><strong>Symptoms</strong>:</p>
<ul>
<li>Command fails immediately without retry</li>
<li>Only see one attempt in logs</li>
<li>Expected retries don’t happen</li>
</ul>
<p><strong>Possible Causes and Solutions</strong>:</p>
<h4 id="1-retry_on-matchers-too-specific"><a class="header" href="#1-retry_on-matchers-too-specific">1. retry_on Matchers Too Specific</a></h4>
<pre><code class="language-yaml"># Problem: Error doesn't match any configured matcher
retry_config:
  attempts: 5
  retry_on:
    - network
    # Error is "connection timeout" but matcher only catches "network"
</code></pre>
<p><strong>Solution</strong>: Use broader matchers or add custom patterns</p>
<pre><code class="language-yaml">retry_config:
  attempts: 5
  retry_on:
    - network
    - timeout      # Add timeout matcher
    - pattern: "connection.*timeout"  # Or custom pattern
</code></pre>
<p><strong>Debug</strong>: Check actual error message in logs and verify it matches one of your configured matchers (remember: matching is case-insensitive).</p>
<p><strong>Source</strong>: ErrorMatcher matching logic in src/cook/retry_v2.rs:128-149</p>
<h4 id="2-empty-retry_on-with-unexpected-error-filtering"><a class="header" href="#2-empty-retry_on-with-unexpected-error-filtering">2. Empty retry_on with Unexpected Error Filtering</a></h4>
<pre><code class="language-yaml"># Misconception: Empty retry_on means "don't retry"
# Reality: Empty retry_on means "retry ALL errors"
retry_config:
  attempts: 3
  # retry_on is empty - retries everything!
</code></pre>
<p><strong>Solution</strong>: If you see retries happening when you don’t expect them, check if <code>retry_on</code> is empty.</p>
<p><strong>Source</strong>: Default behavior in src/cook/retry_v2.rs:42-43</p>
<h4 id="3-retry_config-missing-entirely"><a class="header" href="#3-retry_config-missing-entirely">3. retry_config Missing Entirely</a></h4>
<pre><code class="language-yaml">commands:
  - shell: "curl https://api.example.com"
    # No retry_config - no retry happens
</code></pre>
<p><strong>Solution</strong>: Add <code>retry_config</code> block to enable retry.</p>
<hr />
<h3 id="issue-retries-happening-but-taking-too-long"><a class="header" href="#issue-retries-happening-but-taking-too-long">Issue: Retries Happening But Taking Too Long</a></h3>
<p><strong>Symptoms</strong>:</p>
<ul>
<li>Workflow hangs during retries</li>
<li>Total retry time exceeds expectations</li>
<li>Delays grow too large</li>
</ul>
<p><strong>Possible Causes and Solutions</strong>:</p>
<h4 id="1-exponential-backoff-without-max_delay"><a class="header" href="#1-exponential-backoff-without-max_delay">1. Exponential Backoff Without max_delay</a></h4>
<pre><code class="language-yaml"># Problem: Exponential growth uncapped
retry_config:
  attempts: 10
  backoff: exponential
  initial_delay: "1s"
  # No max_delay - delay can grow to minutes!
</code></pre>
<p><strong>Solution</strong>: Always set <code>max_delay</code></p>
<pre><code class="language-yaml">retry_config:
  attempts: 10
  backoff: exponential
  initial_delay: "1s"
  max_delay: "30s"  # Cap delays at 30 seconds
</code></pre>
<p><strong>Default</strong>: max_delay defaults to 30 seconds if not specified (src/cook/retry_v2.rs:64)</p>
<h4 id="2-too-many-attempts-without-retry_budget"><a class="header" href="#2-too-many-attempts-without-retry_budget">2. Too Many Attempts Without retry_budget</a></h4>
<pre><code class="language-yaml"># Problem: High attempts can retry for hours
retry_config:
  attempts: 100
  backoff: exponential
</code></pre>
<p><strong>Solution</strong>: Use <code>retry_budget</code> to cap total time</p>
<pre><code class="language-yaml">retry_config:
  attempts: 100
  backoff: exponential
  retry_budget: "10m"  # Never exceed 10 minutes total
</code></pre>
<p><strong>Source</strong>: retry_budget enforcement in tests at src/cook/retry_v2.rs:675-708</p>
<h4 id="3-initial-delay-too-high"><a class="header" href="#3-initial-delay-too-high">3. Initial Delay Too High</a></h4>
<pre><code class="language-yaml"># Problem: First retry waits too long
retry_config:
  attempts: 3
  initial_delay: "60s"  # 1-minute delay before first retry!
</code></pre>
<p><strong>Solution</strong>: Use shorter initial delay, let backoff grow</p>
<pre><code class="language-yaml">retry_config:
  attempts: 3
  initial_delay: "1s"   # Start small
  backoff: exponential  # Let it grow
  max_delay: "30s"
</code></pre>
<hr />
<h3 id="issue-circuit-breaker-always-open"><a class="header" href="#issue-circuit-breaker-always-open">Issue: Circuit Breaker Always Open</a></h3>
<p><strong>Symptoms</strong>:</p>
<ul>
<li>Circuit breaker trips and stays open</li>
<li>Requests fail immediately after threshold</li>
<li>Recovery doesn’t happen</li>
</ul>
<p><strong>Possible Causes and Solutions</strong>:</p>
<h4 id="1-failure-threshold-too-low"><a class="header" href="#1-failure-threshold-too-low">1. Failure Threshold Too Low</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Problem: Circuit opens too easily
let executor = RetryExecutor::new(config)
    .with_circuit_breaker(
        1,                          // Opens after single failure!
        Duration::from_secs(30)
    );
<span class="boring">}</span></code></pre></pre>
<p><strong>Solution</strong>: Use appropriate threshold for failure rate</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let executor = RetryExecutor::new(config)
    .with_circuit_breaker(
        5,                          // Open after 5 consecutive failures
        Duration::from_secs(30)
    );
<span class="boring">}</span></code></pre></pre>
<p><strong>Source</strong>: CircuitBreaker implementation in src/cook/retry_v2.rs:325-397</p>
<h4 id="2-recovery-timeout-too-long"><a class="header" href="#2-recovery-timeout-too-long">2. Recovery Timeout Too Long</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Problem: Circuit stays open too long
let executor = RetryExecutor::new(config)
    .with_circuit_breaker(
        5,
        Duration::from_secs(600)    // 10-minute recovery time!
    );
<span class="boring">}</span></code></pre></pre>
<p><strong>Solution</strong>: Use shorter recovery timeout for faster testing</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let executor = RetryExecutor::new(config)
    .with_circuit_breaker(
        5,
        Duration::from_secs(30)     // 30-second recovery attempts
    );
<span class="boring">}</span></code></pre></pre>
<h4 id="3-no-success-to-close-circuit"><a class="header" href="#3-no-success-to-close-circuit">3. No Success to Close Circuit</a></h4>
<p><strong>Problem</strong>: Circuit opens but downstream never recovers, so circuit never closes</p>
<p><strong>Solution</strong>:</p>
<ul>
<li>Check if downstream service is actually recovering</li>
<li>Verify circuit enters HalfOpen state and test requests succeed</li>
<li>Monitor circuit state transitions with logging</li>
</ul>
<hr />
<h3 id="issue-thundering-herd-multiple-parallel-agents-retrying-simultaneously"><a class="header" href="#issue-thundering-herd-multiple-parallel-agents-retrying-simultaneously">Issue: Thundering Herd (Multiple Parallel Agents Retrying Simultaneously)</a></h3>
<p><strong>Symptoms</strong>:</p>
<ul>
<li>Service overwhelmed during recovery</li>
<li>All parallel agents retry at same time</li>
<li>Cascading failures</li>
</ul>
<p><strong>Problem</strong>:</p>
<pre><code class="language-yaml"># MapReduce with 10 parallel agents, no jitter
map:
  max_parallel: 10
  agent_template:
    - shell: "api-call.sh"
      retry_config:
        attempts: 5
        backoff: exponential
        jitter: false  # All agents retry at same time!
</code></pre>
<p><strong>Solution</strong>: Enable jitter</p>
<pre><code class="language-yaml">map:
  max_parallel: 10
  agent_template:
    - shell: "api-call.sh"
      retry_config:
        attempts: 5
        backoff: exponential
        jitter: true          # Randomize retry timing
        jitter_factor: 0.3    # 30% randomization
</code></pre>
<p><strong>Why</strong>: Without jitter, all 10 agents calculate the same exponential delay and retry simultaneously.</p>
<p><strong>Source</strong>: Jitter application in src/cook/retry_v2.rs:308-317</p>
<hr />
<h3 id="issue-retrying-non-transient-errors"><a class="header" href="#issue-retrying-non-transient-errors">Issue: Retrying Non-Transient Errors</a></h3>
<p><strong>Symptoms</strong>:</p>
<ul>
<li>Retrying 404 Not Found errors</li>
<li>Retrying authentication failures (401)</li>
<li>Wasting time on permanent failures</li>
</ul>
<p><strong>Problem</strong>:</p>
<pre><code class="language-yaml"># Empty retry_on retries everything
retry_config:
  attempts: 5
  # Retries 404, 401, 400, etc. - permanent errors!
</code></pre>
<p><strong>Solution</strong>: Use selective retry with error matchers</p>
<pre><code class="language-yaml">retry_config:
  attempts: 5
  retry_on:
    - network        # Transient
    - timeout        # Transient
    - server_error   # Transient (5xx)
    # Don't retry 404, 401, 400, etc.
</code></pre>
<p><strong>Best Practice</strong>: Only retry errors that might succeed on next attempt.</p>
<hr />
<h3 id="issue-retry-budget-not-working-as-expected"><a class="header" href="#issue-retry-budget-not-working-as-expected">Issue: Retry Budget Not Working as Expected</a></h3>
<p><strong>Symptoms</strong>:</p>
<ul>
<li>Retries exceed configured budget</li>
<li>Budget seems ignored</li>
</ul>
<p><strong>Possible Cause</strong>: Misunderstanding retry_budget behavior</p>
<p><strong>How retry_budget Works</strong>:</p>
<ul>
<li>Budget is checked <strong>before each retry</strong></li>
<li>If budget would be exceeded, retry stops</li>
<li>Budget includes backoff delay time</li>
<li>Budget does NOT include time for command execution itself</li>
</ul>
<p><strong>Example</strong>:</p>
<pre><code class="language-yaml">retry_config:
  attempts: 10
  backoff: exponential
  initial_delay: "1s"
  max_delay: "60s"
  retry_budget: "2m"  # 2-minute budget
</code></pre>
<p><strong>Behavior</strong>:</p>
<ul>
<li>If accumulated delays + next delay &gt; 2 minutes → Stop</li>
<li>Command execution time is NOT counted in budget</li>
<li>If command takes 1 minute to execute each time, total time could be: 2m (budget) + (attempts * 1m execution) = ~12 minutes</li>
</ul>
<p><strong>Source</strong>: retry_budget field in src/cook/retry_v2.rs:46-47, tests at lines 675-708</p>
<hr />
<h3 id="issue-fallback-command-also-failing"><a class="header" href="#issue-fallback-command-also-failing">Issue: Fallback Command Also Failing</a></h3>
<p><strong>Symptoms</strong>:</p>
<ul>
<li>Primary command fails after retries</li>
<li>Fallback command executes but also fails</li>
<li>Workflow stops</li>
</ul>
<p><strong>Problem</strong>: Fallback command isn’t reliable</p>
<pre><code class="language-yaml">retry_config:
  attempts: 3
  on_failure:
    fallback:
      command: "curl https://backup-api.com/data"  # This can also fail!
</code></pre>
<p><strong>Solution</strong>: Make fallback truly reliable</p>
<pre><code class="language-yaml">retry_config:
  attempts: 3
  on_failure:
    fallback:
      command: "cat /cache/data.json"  # Local cache, very reliable
</code></pre>
<p><strong>Best Practice</strong>: Fallback commands should be:</p>
<ul>
<li>Local operations (file reads, not network calls)</li>
<li>Idempotent</li>
<li>Very unlikely to fail</li>
<li>Fast</li>
</ul>
<hr />
<h3 id="issue-retry-metrics-not-matching-expectations"><a class="header" href="#issue-retry-metrics-not-matching-expectations">Issue: Retry Metrics Not Matching Expectations</a></h3>
<p><strong>Symptoms</strong>:</p>
<ul>
<li>Metrics show different attempt count than configured</li>
<li>Unexpected success/failure counts</li>
</ul>
<p><strong>Debugging</strong>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Access retry metrics for debugging
let metrics = executor.metrics();
println!("Total attempts: {}", metrics.total_attempts);
println!("Successful: {}", metrics.successful_attempts);
println!("Failed: {}", metrics.failed_attempts);
println!("Retries: {:?}", metrics.retries);  // Vec&lt;(attempt, delay)&gt;
<span class="boring">}</span></code></pre></pre>
<p><strong>Source</strong>: RetryMetrics struct in src/cook/retry_v2.rs:399-422</p>
<p><strong>Check</strong>:</p>
<ul>
<li><code>total_attempts</code> = successful + failed</li>
<li><code>retries</code> vector shows actual delays used</li>
<li>Compare with configured backoff strategy</li>
</ul>
<hr />
<h3 id="debugging-workflow"><a class="header" href="#debugging-workflow">Debugging Workflow</a></h3>
<p>When retry behavior is unexpected:</p>
<ol>
<li>
<p><strong>Check retry_on matchers</strong>:</p>
<ul>
<li>Verify error message matches configured matchers</li>
<li>Remember matching is case-insensitive</li>
<li>Empty <code>retry_on</code> = retry all errors</li>
</ul>
</li>
<li>
<p><strong>Check backoff configuration</strong>:</p>
<ul>
<li>Verify <code>max_delay</code> is set</li>
<li>Check <code>initial_delay</code> isn’t too high</li>
<li>Ensure <code>backoff</code> strategy matches intent</li>
</ul>
</li>
<li>
<p><strong>Check retry_budget</strong>:</p>
<ul>
<li>Remember budget is delay time, not total time</li>
<li>Budget checked before each retry</li>
<li>Command execution time NOT included</li>
</ul>
</li>
<li>
<p><strong>Enable jitter for parallel workflows</strong>:</p>
<ul>
<li>Always use jitter in MapReduce</li>
<li>Set <code>jitter_factor</code> between 0.1 and 0.5</li>
</ul>
</li>
<li>
<p><strong>Use selective retry</strong>:</p>
<ul>
<li>Don’t retry permanent errors (404, 401, 400)</li>
<li>Use <code>retry_on</code> to specify transient errors only</li>
</ul>
</li>
<li>
<p><strong>Monitor metrics</strong>:</p>
<ul>
<li>Access <code>RetryMetrics</code> for actual attempt counts</li>
<li>Verify delays match expectations</li>
<li>Check circuit breaker state transitions</li>
</ul>
</li>
</ol>
<hr />
<h3 id="getting-help-3"><a class="header" href="#getting-help-3">Getting Help</a></h3>
<p>If retry behavior is still unclear:</p>
<ol>
<li><strong>Check retry_v2.rs implementation</strong>: src/cook/retry_v2.rs</li>
<li><strong>Review tests</strong>: src/cook/retry_v2.rs:463-748 (comprehensive test coverage)</li>
<li><strong>Enable verbose logging</strong>: Add logging around retry logic to see what’s happening</li>
<li><strong>Test with simple cases</strong>: Start with fixed backoff and 2-3 attempts to isolate issue</li>
</ol>
<h3 id="see-also-33"><a class="header" href="#see-also-33">See Also</a></h3>
<ul>
<li><a href="retry-configuration/./basic-retry-configuration.html">Basic Retry Configuration</a> - Configuration fundamentals</li>
<li><a href="retry-configuration/./backoff-strategies.html">Backoff Strategies</a> - Understanding backoff behavior</li>
<li><a href="retry-configuration/./conditional-retry-with-error-matchers.html">Conditional Retry with Error Matchers</a> - Error matching details</li>
<li><a href="retry-configuration/./best-practices.html">Best Practices</a> - Avoiding common pitfalls</li>
<li><a href="retry-configuration/./complete-examples.html">Complete Examples</a> - Working configurations</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="related-topics-8"><a class="header" href="#related-topics-8">Related Topics</a></h2>
<p>This section provides links to related documentation that complements retry configuration. Understanding these topics will help you build more resilient workflows.</p>
<h3 id="within-this-chapter"><a class="header" href="#within-this-chapter">Within This Chapter</a></h3>
<p>The following subsections provide detailed information about specific aspects of retry configuration:</p>
<ul>
<li><a href="retry-configuration/./basic-retry-configuration.html">Basic Retry Configuration</a> - Start here to understand fundamental retry configuration syntax and options</li>
<li><a href="retry-configuration/./backoff-strategies.html">Backoff Strategies</a> - Control the timing between retry attempts using exponential, linear, Fibonacci, or constant delays</li>
<li><a href="retry-configuration/./backoff-strategy-comparison.html">Backoff Strategy Comparison</a> - Compare different backoff strategies with examples and use cases</li>
<li><a href="retry-configuration/./failure-actions.html">Failure Actions</a> - Define custom actions to execute when commands fail or exhaust retries</li>
<li><a href="retry-configuration/./conditional-retry-with-error-matchers.html">Conditional Retry with Error Matchers</a> - Use error patterns to selectively retry only specific failures</li>
<li><a href="retry-configuration/./jitter-for-distributed-systems.html">Jitter for Distributed Systems</a> - Add randomization to retry delays to prevent thundering herd problems</li>
<li><a href="retry-configuration/./retry-budget.html">Retry Budget</a> - Limit total retry attempts across your workflow to prevent infinite retry loops</li>
<li><a href="retry-configuration/./retry-metrics-and-observability.html">Retry Metrics and Observability</a> - Monitor retry behavior through events and logging</li>
<li><a href="retry-configuration/./workflow-level-vs-command-level-retry.html">Workflow-Level vs Command-Level Retry</a> - Understand the differences between retry scopes and when to use each</li>
<li><a href="retry-configuration/./best-practices.html">Best Practices</a> - Recommended patterns and anti-patterns for retry configuration</li>
<li><a href="retry-configuration/./complete-examples.html">Complete Examples</a> - Real-world retry configuration examples demonstrating various strategies</li>
<li><a href="retry-configuration/./troubleshooting.html">Troubleshooting</a> - Debug common retry configuration issues</li>
<li><a href="retry-configuration/./implementation-references.html">Implementation References</a> - Links to source code implementing retry logic</li>
</ul>
<h3 id="related-chapters-3"><a class="header" href="#related-chapters-3">Related Chapters</a></h3>
<p>These chapters cover topics that interact with or complement retry configuration:</p>
<ul>
<li><a href="retry-configuration/../error-handling.html">Error Handling</a> - Overall error handling strategy and how Prodigy propagates errors through workflows. Retry configuration is one component of a comprehensive error handling approach.</li>
<li><a href="retry-configuration/../configuration/workflow-configuration.html">Workflow Configuration</a> - Workflow-level settings including global retry defaults that apply to all commands unless overridden at the command level.</li>
<li><a href="retry-configuration/../mapreduce/index.html">MapReduce</a> - Retry behavior in MapReduce workflows, where individual map agents can retry independently. MapReduce adds complexity to retry semantics due to parallel execution.</li>
<li><a href="retry-configuration/../mapreduce/dead-letter-queue-dlq.html">Dead Letter Queue (DLQ)</a> - Handling failed work items in MapReduce workflows. When map agents exhaust all retries, items move to the DLQ for manual inspection and retry.</li>
<li><a href="retry-configuration/../environment/index.html">Environment Variables</a> - Use environment variables in retry configuration to parameterize retry behavior across different deployment environments (dev, staging, production).</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="implementation-references-1"><a class="header" href="#implementation-references-1">Implementation References</a></h2>
<p>This section provides pointers to the source code implementing the retry system. These references are useful for developers who want to understand implementation details, extend the retry functionality, or troubleshoot issues.</p>
<h3 id="core-retry-system"><a class="header" href="#core-retry-system">Core Retry System</a></h3>
<h4 id="enhanced-retry-configuration-srccookretry_v2rs14-461"><a class="header" href="#enhanced-retry-configuration-srccookretry_v2rs14-461">Enhanced Retry Configuration (<code>src/cook/retry_v2.rs:14-461</code>)</a></h4>
<p>The primary implementation of Prodigy’s retry system with comprehensive features:</p>
<ul>
<li>
<p><strong>RetryConfig struct</strong> (lines 14-52): Main configuration type with fields for:</p>
<ul>
<li>Maximum retry attempts</li>
<li>Backoff strategies (Fixed, Linear, Exponential, Fibonacci, Custom)</li>
<li>Initial and maximum delays</li>
<li>Jitter support for preventing thundering herd</li>
<li>Error matching for conditional retries</li>
<li>Retry budget (maximum total time)</li>
<li>Failure actions</li>
</ul>
</li>
<li>
<p><strong>BackoffStrategy enum</strong> (lines 70-90): Implements multiple delay calculation strategies:</p>
<ul>
<li><code>Fixed</code>: Constant delay between retries</li>
<li><code>Linear</code>: Incrementing delay (initial + n * increment)</li>
<li><code>Exponential</code>: Doubling delay (initial * base^n)</li>
<li><code>Fibonacci</code>: Fibonacci sequence delays</li>
<li><code>Custom</code>: User-defined delay sequence</li>
</ul>
</li>
<li>
<p><strong>Retry execution logic</strong>: Core retry loop with backoff calculation, jitter application, and timeout enforcement</p>
</li>
</ul>
<p><strong>Example usage</strong> (from src/cook/retry_v2.rs):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let config = RetryConfig {
    attempts: 5,
    backoff: BackoffStrategy::Exponential { base: 2.0 },
    initial_delay: Duration::from_millis(100),
    max_delay: Duration::from_secs(60),
    jitter: true,
    jitter_factor: 0.1,
    retry_on: vec![ErrorMatcher::Network, ErrorMatcher::Timeout],
    retry_budget: Some(Duration::from_secs(300)),
    on_failure: FailureAction::Fail,
};
<span class="boring">}</span></code></pre></pre>
<h4 id="workflow-error-policy-srccookworkflowerror_policyrs91-129"><a class="header" href="#workflow-error-policy-srccookworkflowerror_policyrs91-129">Workflow Error Policy (<code>src/cook/workflow/error_policy.rs:91-129</code>)</a></h4>
<p>Simpler retry configuration used at the workflow level:</p>
<ul>
<li>
<p><strong>RetryConfig struct</strong> (lines 91-99): Workflow-scoped retry settings with:</p>
<ul>
<li><code>max_attempts</code>: Maximum number of retry attempts (default: 3)</li>
<li><code>backoff</code>: Backoff strategy for delay calculation</li>
</ul>
</li>
<li>
<p><strong>BackoffStrategy enum</strong> (lines 105-120): Workflow backoff strategies with duration parameters:</p>
<ul>
<li><code>Fixed { delay }</code>: Fixed delay between retries</li>
<li><code>Linear { initial, increment }</code>: Linear backoff with configurable increment</li>
<li><code>Exponential { initial, multiplier }</code>: Exponential backoff with configurable base</li>
<li><code>Fibonacci { initial }</code>: Fibonacci sequence starting from initial delay</li>
</ul>
</li>
<li>
<p><strong>WorkflowErrorPolicy struct</strong> (lines 131-140+): Workflow-level error handling with:</p>
<ul>
<li><code>on_item_failure</code>: Action to take when items fail</li>
<li><code>continue_on_failure</code>: Whether to continue processing after failures</li>
</ul>
</li>
</ul>
<p><strong>Example usage</strong> (from workflow YAML):</p>
<pre><code class="language-yaml">retry_config:
  max_attempts: 5
  backoff:
    exponential:
      initial: 1s
      multiplier: 2.0
</code></pre>
<h4 id="command-metadata-srcconfigcommandrs135"><a class="header" href="#command-metadata-srcconfigcommandrs135">Command Metadata (<code>src/config/command.rs:135</code>)</a></h4>
<p>Command-level retry override configuration:</p>
<ul>
<li><strong>CommandMetadata struct</strong> (lines 133-149): Per-command settings including:
<ul>
<li><code>retries: Option&lt;u32&gt;</code>: Override global retry attempts for specific commands</li>
<li><code>timeout: Option&lt;u64&gt;</code>: Command-specific timeout in seconds</li>
<li><code>continue_on_error: Option&lt;bool&gt;</code>: Whether workflow continues if command fails</li>
<li><code>env</code>: Environment variables for the command</li>
<li><code>commit_required</code>: Whether command must create git commits</li>
</ul>
</li>
</ul>
<p>This allows fine-grained control over retry behavior at the command level, overriding workflow-level settings when needed.</p>
<p><strong>Example usage</strong> (from workflow YAML):</p>
<pre><code class="language-yaml">commands:
  - shell: "flaky-network-operation.sh"
    retries: 10
    timeout: 300
    continue_on_error: false
</code></pre>
<h3 id="error-handling-components"><a class="header" href="#error-handling-components">Error Handling Components</a></h3>
<h4 id="error-matchers-srccookretry_v2rs100-151"><a class="header" href="#error-matchers-srccookretry_v2rs100-151">Error Matchers (<code>src/cook/retry_v2.rs:100-151</code>)</a></h4>
<p>Conditional retry based on error patterns:</p>
<ul>
<li>
<p><strong>ErrorMatcher enum</strong> (lines 101-114): Pre-defined error categories:</p>
<ul>
<li><code>Network</code>: Connection, refused, unreachable errors</li>
<li><code>Timeout</code>: Timeout and “timed out” errors</li>
<li><code>ServerError</code>: HTTP 5xx status codes (500, 502, 503, 504)</li>
<li><code>RateLimit</code>: HTTP 429 and “rate limit” errors</li>
<li><code>Pattern(String)</code>: Custom regex pattern matching</li>
</ul>
</li>
<li>
<p><strong>Matching logic</strong> (lines 116-151): Case-insensitive error message matching with regex support</p>
</li>
</ul>
<p>This enables intelligent retry behavior that only retries on transient errors (network issues, timeouts, rate limits) while failing fast on permanent errors (validation failures, authentication errors).</p>
<p><strong>Example usage</strong> (from src/cook/retry_v2.rs):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>retry_on: vec![
    ErrorMatcher::Network,
    ErrorMatcher::Timeout,
    ErrorMatcher::ServerError,
    ErrorMatcher::Pattern("temporary.*unavailable".to_string()),
]
<span class="boring">}</span></code></pre></pre>
<h3 id="resilience-features"><a class="header" href="#resilience-features">Resilience Features</a></h3>
<h4 id="circuit-breaker-srccookretry_v2rs325-397"><a class="header" href="#circuit-breaker-srccookretry_v2rs325-397">Circuit Breaker (<code>src/cook/retry_v2.rs:325-397</code>)</a></h4>
<p>Protection against cascading failures:</p>
<ul>
<li>
<p><strong>CircuitBreaker struct</strong> (lines 325-331): Stateful circuit breaker with:</p>
<ul>
<li><code>failure_threshold</code>: Number of consecutive failures before opening</li>
<li><code>recovery_timeout</code>: Time to wait before attempting recovery</li>
<li><code>state</code>: Current circuit state (Closed, Open, HalfOpen)</li>
<li><code>consecutive_failures</code>: Counter for failure tracking</li>
</ul>
</li>
<li>
<p><strong>State machine</strong> (lines 333-338): Three-state circuit breaker:</p>
<ul>
<li><code>Closed</code>: Normal operation, requests allowed</li>
<li><code>Open { until }</code>: Failing, requests blocked until timeout</li>
<li><code>HalfOpen</code>: Testing recovery, limited requests allowed</li>
</ul>
</li>
<li>
<p><strong>State transitions</strong>:</p>
<ul>
<li><code>is_open()</code> (lines 351-367): Check state and transition from Open to HalfOpen after timeout</li>
<li><code>record_success()</code> (lines 369-379): Reset failures, close circuit if in HalfOpen</li>
<li><code>record_failure()</code> (lines 381-397): Increment failures, open circuit if threshold exceeded</li>
</ul>
</li>
</ul>
<p><strong>Example usage</strong>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let breaker = CircuitBreaker::new(5, Duration::from_secs(30));

// Check before operation
if breaker.is_open().await {
    return Err(anyhow!("Circuit breaker open"));
}

// Record result
match operation().await {
    Ok(result) =&gt; {
        breaker.record_success().await;
        Ok(result)
    }
    Err(e) =&gt; {
        breaker.record_failure().await;
        Err(e)
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="observability"><a class="header" href="#observability">Observability</a></h3>
<h4 id="retry-metrics-srccookretry_v2rs399-422"><a class="header" href="#retry-metrics-srccookretry_v2rs399-422">Retry Metrics (<code>src/cook/retry_v2.rs:399-422</code>)</a></h4>
<p>Tracking retry behavior for monitoring:</p>
<ul>
<li><strong>RetryMetrics struct</strong> (lines 399-422): Statistics collection including:
<ul>
<li><code>total_attempts</code>: Total retry attempts made</li>
<li><code>successful_attempts</code>: Number of successful retries</li>
<li><code>failed_attempts</code>: Number of failed retries</li>
<li><code>total_delay</code>: Cumulative delay time across all retries</li>
<li>Additional timing and success rate metrics</li>
</ul>
</li>
</ul>
<p>These metrics enable monitoring of retry effectiveness, identifying problematic operations, and tuning retry configurations for optimal performance.</p>
<p><strong>Example usage</strong>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let metrics = RetryMetrics::default();
// Metrics are updated during retry execution
println!("Success rate: {}/{}",
    metrics.successful_attempts,
    metrics.total_attempts);
<span class="boring">}</span></code></pre></pre>
<h2 id="organization-by-component"><a class="header" href="#organization-by-component">Organization by Component</a></h2>
<p>The retry system is organized into logical components:</p>
<p><strong>Core Retry System</strong>:</p>
<ul>
<li>Main retry configuration and execution (src/cook/retry_v2.rs)</li>
<li>Workflow-level configuration (src/cook/workflow/error_policy.rs)</li>
<li>Command-level overrides (src/config/command.rs)</li>
</ul>
<p><strong>Error Handling</strong>:</p>
<ul>
<li>Conditional retry with error matchers (src/cook/retry_v2.rs:100-151)</li>
<li>Error pattern matching and classification</li>
</ul>
<p><strong>Resilience</strong>:</p>
<ul>
<li>Circuit breaker implementation (src/cook/retry_v2.rs:325-397)</li>
<li>State management and failure protection</li>
</ul>
<p><strong>Observability</strong>:</p>
<ul>
<li>Retry metrics and monitoring (src/cook/retry_v2.rs:399-422)</li>
<li>Performance tracking and debugging</li>
</ul>
<h2 id="next-steps-4"><a class="header" href="#next-steps-4">Next Steps</a></h2>
<ul>
<li>For practical examples, see the <a href="retry-configuration/complete-examples.html">Complete Examples</a> subsection</li>
<li>For configuration details, see the <a href="retry-configuration/basic-retry-configuration.html">Basic Retry Configuration</a> subsection</li>
<li>For backoff strategies, see the <a href="retry-configuration/backoff-strategies.html">Backoff Strategies</a> subsection</li>
<li>For best practices, see the <a href="retry-configuration/best-practices.html">Best Practices</a> subsection</li>
<li>For troubleshooting, see the <a href="retry-configuration/troubleshooting.html">Troubleshooting</a> subsection</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="error-handling-5"><a class="header" href="#error-handling-5">Error Handling</a></h1>
<p>Prodigy provides comprehensive error handling at both the workflow level (for MapReduce jobs) and the command level (for individual workflow steps). This chapter covers the practical features available for handling failures gracefully.</p>
<hr />
<h2 id="command-level-error-handling"><a class="header" href="#command-level-error-handling">Command-Level Error Handling</a></h2>
<p>Command-level error handling allows you to specify what happens when a single workflow step fails. Use the <code>on_failure</code> configuration to define recovery, cleanup, or fallback strategies.</p>
<h3 id="simple-forms"><a class="header" href="#simple-forms">Simple Forms</a></h3>
<p>For basic error handling, use the simplest form that meets your needs:</p>
<pre><code class="language-yaml"># Ignore errors - don't fail the workflow
- shell: "optional-cleanup.sh"
  on_failure: true

# Single recovery command (shell or claude)
- shell: "npm install"
  on_failure: "npm cache clean --force"

- shell: "cargo clippy"
  on_failure: "/fix-warnings"

# Multiple recovery commands
- shell: "build-project"
  on_failure:
    - "cleanup-artifacts"
    - "/diagnose-build-errors"
    - "retry-build"
</code></pre>
<h3 id="advanced-configuration-1"><a class="header" href="#advanced-configuration-1">Advanced Configuration</a></h3>
<p>For more control over error handling behavior:</p>
<pre><code class="language-yaml">- shell: "cargo clippy"
  on_failure:
    claude: "/fix-warnings ${shell.output}"
    fail_workflow: false     # Continue workflow even if handler fails
    max_attempts: 3          # Retry original command up to 3 times (setting &gt; 1 enables auto-retry)
</code></pre>
<p><strong>Available Fields:</strong></p>
<ul>
<li><code>shell</code> - Shell command to run on failure</li>
<li><code>claude</code> - Claude command to run on failure</li>
<li><code>fail_workflow</code> - Whether to fail the entire workflow (default: <code>false</code>)</li>
<li><code>max_attempts</code> - Maximum retry attempts for the original command (default: <code>1</code>)</li>
<li><code>max_retries</code> - Alternative name for <code>max_attempts</code> (both are supported for backward compatibility)</li>
</ul>
<p><strong>Notes:</strong></p>
<ul>
<li>When <code>max_attempts &gt; 1</code>, Prodigy automatically retries the original command after running the failure handler (the deprecated <code>retry_original</code> flag is no longer needed)</li>
<li>Retry behavior is now controlled by the <code>max_attempts</code>/<code>max_retries</code> value, not a separate flag</li>
<li>You can specify both <code>shell</code> and <code>claude</code> commands - they will execute in sequence</li>
<li>By default, having a handler means the workflow continues even if the step fails</li>
</ul>
<blockquote>
<p><strong>Migration from <code>retry_original</code>:</strong> Previously you used <code>retry_original: true</code> with <code>max_retries: 3</code>. Now just use <code>max_attempts: 3</code> (retry is implicit when &gt; 1). Both <code>max_attempts</code> and <code>max_retries</code> are supported as aliases for backward compatibility.</p>
</blockquote>
<h3 id="detailed-handler-configuration"><a class="header" href="#detailed-handler-configuration">Detailed Handler Configuration</a></h3>
<p>For complex error handling scenarios with multiple commands and fine-grained control:</p>
<pre><code class="language-yaml">- shell: "deploy-production"
  on_failure:
    strategy: recovery        # Options: recovery, fallback, cleanup, custom
    timeout: 300             # Handler timeout in seconds
    handler_failure_fatal: true  # Fail workflow if handler fails
    fail_workflow: false     # Don't fail workflow if step fails
    capture:                 # Capture handler output to variables
      error_log: "handler_output"
      rollback_status: "rollback_result"
    commands:
      - shell: "rollback-deployment"
        continue_on_error: true
      - claude: "/analyze-deployment-failure"
      - shell: "notify-team"
</code></pre>
<p><strong>Handler Configuration Fields:</strong></p>
<ul>
<li><code>strategy</code> - Handler strategy (recovery, fallback, cleanup, custom)</li>
<li><code>timeout</code> - Handler timeout in seconds</li>
<li><code>handler_failure_fatal</code> - Fail workflow if handler fails</li>
<li><code>fail_workflow</code> - Whether to fail the entire workflow</li>
<li><code>capture</code> - Map of variable names to capture from handler output (e.g., <code>error_log: "handler_output"</code>). Note: capture applies to the handler’s combined output, not individual commands.</li>
<li><code>commands</code> - List of handler commands to execute</li>
</ul>
<p><strong>Handler Strategies:</strong></p>
<ul>
<li><code>recovery</code> - Try to fix the problem and retry (default)</li>
<li><code>fallback</code> - Use an alternative approach</li>
<li><code>cleanup</code> - Clean up resources</li>
<li><code>custom</code> - Custom handler logic</li>
</ul>
<p><strong>Handler Command Fields:</strong></p>
<ul>
<li><code>shell</code> or <code>claude</code> - The command to execute</li>
<li><code>continue_on_error</code> - Continue to next handler command even if this fails</li>
</ul>
<blockquote>
<p><strong>Simpler Alternative:</strong> For basic cases, you can use the Advanced format shown earlier instead of the detailed handler configuration. The Advanced format allows <code>shell</code> and <code>claude</code> fields directly without wrapping in a <code>commands</code> array: <code>on_failure: { shell: "command", fail_workflow: false, max_attempts: 3 }</code>.</p>
</blockquote>
<h3 id="success-handling"><a class="header" href="#success-handling">Success Handling</a></h3>
<p>Execute commands when a step succeeds. The <code>on_success</code> field accepts a full WorkflowStep configuration with all available fields.</p>
<p><strong>Simple Form:</strong></p>
<pre><code class="language-yaml">- shell: "deploy-staging"
  on_success:
    shell: "notify-success"
    claude: "/update-deployment-docs"
</code></pre>
<p><strong>Advanced Form with Full WorkflowStep Configuration:</strong></p>
<pre><code class="language-yaml">- shell: "build-production"
  on_success:
    claude: "/update-build-metrics"
    timeout: 60              # Success handler timeout
    capture: "metrics"       # Capture output to variable
    working_dir: "dist"      # Run in specific directory
    when: "${build.target} == 'release'"  # Conditional execution
</code></pre>
<p><strong>Note:</strong> The <code>on_success</code> handler supports all WorkflowStep fields including <code>timeout</code>, <code>capture</code>, <code>working_dir</code>, <code>when</code>, and nested <code>on_failure</code> handlers.</p>
<p><strong>Common Use Cases:</strong></p>
<p>Success handlers are useful for post-processing actions that should only occur when a step completes successfully:</p>
<ul>
<li><strong>Notifications:</strong> Send success notifications to teams via Slack, email, or other channels</li>
<li><strong>Metrics Updates:</strong> Update deployment metrics, dashboard statistics, or monitoring systems</li>
<li><strong>Downstream Workflows:</strong> Trigger dependent workflows or pipelines</li>
<li><strong>Artifact Archiving:</strong> Archive build artifacts, logs, or generated files for later use</li>
<li><strong>External System Updates:</strong> Update issue trackers, deployment records, or configuration management systems</li>
</ul>
<p>The handler receives access to step outputs via the <code>capture</code> field, allowing you to process results or pass data to subsequent steps. For example, using <code>capture: "metrics"</code> creates a <code>${metrics}</code> variable containing the handler output, which can be used in later workflow steps for processing or decision-making.</p>
<h3 id="commit-requirements"><a class="header" href="#commit-requirements">Commit Requirements</a></h3>
<p>Specify whether a workflow step must create a git commit:</p>
<pre><code class="language-yaml">- claude: "/implement-feature"
  commit_required: true   # Fail if no commit is made
</code></pre>
<p>This is useful for ensuring that Claude commands that are expected to make code changes actually do so.</p>
<hr />
<h2 id="workflow-level-error-policy-mapreduce"><a class="header" href="#workflow-level-error-policy-mapreduce">Workflow-Level Error Policy (MapReduce)</a></h2>
<p>For MapReduce workflows, you can configure workflow-level error policies that control how the entire job responds to failures. This is separate from command-level error handling and only applies to MapReduce mode.</p>
<h3 id="basic-configuration"><a class="header" href="#basic-configuration">Basic Configuration</a></h3>
<pre><code class="language-yaml">name: process-items
mode: mapreduce

error_policy:
  # What to do when a work item fails
  on_item_failure: dlq      # Options: dlq, retry, skip, stop, custom:&lt;handler_name&gt;

  # Continue processing after failures
  continue_on_failure: true

  # Stop after this many failures
  max_failures: 10

  # Stop if failure rate exceeds threshold (0.0 to 1.0)
  failure_threshold: 0.2    # Stop if 20% of items fail

  # How to report errors
  error_collection: aggregate  # Options: aggregate, immediate, batched
</code></pre>
<p><strong>Item Failure Actions:</strong></p>
<ul>
<li><code>dlq</code> - Send failed items to Dead Letter Queue for later retry (default)</li>
<li><code>retry</code> - Retry the item immediately with backoff (if retry_config is set)</li>
<li><code>skip</code> - Skip the failed item and continue</li>
<li><code>stop</code> - Stop the entire workflow on first failure</li>
<li><code>custom:&lt;name&gt;</code> - Use a custom failure handler (not yet implemented)</li>
</ul>
<p><strong>Error Collection Strategies:</strong></p>
<ul>
<li><code>aggregate</code> - Collect all errors and report at the end (default)</li>
<li><code>immediate</code> - Report errors as they occur</li>
<li><code>batched</code> - Report errors in batches of N items (e.g., <code>batched: { size: 10 }</code>)</li>
</ul>
<p><strong>Source</strong>: ErrorCollectionStrategy enum in src/cook/workflow/error_policy.rs:33-44</p>
<h3 id="circuit-breaker"><a class="header" href="#circuit-breaker">Circuit Breaker</a></h3>
<p>Prevent cascading failures by opening a circuit after consecutive failures:</p>
<pre><code class="language-yaml">error_policy:
  circuit_breaker:
    failure_threshold: 5      # Open circuit after 5 consecutive failures
    success_threshold: 2      # Close circuit after 2 successes
    timeout: 30s             # Duration in humantime format (e.g., 30s, 1m, 500ms)
    half_open_requests: 3    # Test requests in half-open state
</code></pre>
<p><strong>Note:</strong> The <code>timeout</code> field uses humantime format supporting <code>1s</code>, <code>100ms</code>, <code>2m</code>, <code>30s</code> for duration parsing.</p>
<h4 id="circuit-breaker-states"><a class="header" href="#circuit-breaker-states">Circuit Breaker States</a></h4>
<p>The circuit breaker operates in three states to protect against cascading failures:</p>
<ol>
<li>
<p><strong>Closed (Normal Operation)</strong></p>
<ul>
<li>All requests are processed normally</li>
<li>Failures are tracked; consecutive failures increment the failure counter</li>
<li>Transitions to <strong>Open</strong> after <code>failure_threshold</code> consecutive failures</li>
</ul>
</li>
<li>
<p><strong>Open (Rejecting Requests)</strong></p>
<ul>
<li>All requests are immediately rejected without attempting execution</li>
<li>Prevents further load on a failing dependency</li>
<li>Transitions to <strong>HalfOpen</strong> after <code>timeout</code> duration expires</li>
</ul>
</li>
<li>
<p><strong>HalfOpen (Testing Recovery)</strong></p>
<ul>
<li>Allows a limited number of test requests (<code>half_open_requests</code>) to verify recovery</li>
<li>If test requests succeed (reaching <code>success_threshold</code>), transitions back to <strong>Closed</strong></li>
<li>If any test request fails, transitions back to <strong>Open</strong> and resets the timeout</li>
</ul>
</li>
</ol>
<p><strong>State Transition Flow:</strong></p>
<pre><code>Closed → Open (after failure_threshold consecutive failures)
Open → HalfOpen (after timeout expires)
HalfOpen → Closed (after success_threshold successes)
HalfOpen → Open (if any half_open_request fails)
</code></pre>
<p><strong>Monitoring Circuit Breaker State:</strong></p>
<p>To check the circuit breaker state during MapReduce execution, monitor the event logs:</p>
<pre><code class="language-bash"># View circuit breaker events for a job
prodigy events ls --job-id &lt;job_id&gt; | grep -i circuit

# Follow circuit state changes in real-time
prodigy events follow --job-id &lt;job_id&gt;
</code></pre>
<p>Circuit breaker state transitions are logged as <code>CircuitOpen</code> and <code>CircuitClosed</code> events, allowing you to track when the circuit opens due to failures and when it recovers.</p>
<p><strong>Note</strong>: The <code>events</code> CLI commands are defined but currently have stub implementations. Event data is stored in <code>~/.prodigy/events/{repo_name}/{job_id}/</code> and can be inspected directly as JSONL files.</p>
<h3 id="retry-configuration-with-backoff"><a class="header" href="#retry-configuration-with-backoff">Retry Configuration with Backoff</a></h3>
<p>Configure automatic retry behavior for failed items:</p>
<pre><code class="language-yaml">error_policy:
  on_item_failure: retry
  retry_config:
    max_attempts: 3
    backoff:
      type: exponential
      initial: 1s            # Initial delay (duration format)
      multiplier: 2          # Double delay each retry
</code></pre>
<p><strong>Backoff Strategy Options:</strong></p>
<pre><code class="language-yaml"># Fixed delay between retries
# Always waits the same duration
backoff:
  type: fixed
  delay: 1s

# Linear increase in delay
# Calculates: delay = initial + (retry_count * increment)
# Example with initial=1s, increment=500ms:
#   Retry 1: 1s + (1 * 500ms) = 1.5s
#   Retry 2: 1s + (2 * 500ms) = 2s
#   Retry 3: 1s + (3 * 500ms) = 2.5s
backoff:
  type: linear
  initial: 1s
  increment: 500ms

# Exponential backoff (recommended)
# Calculates: delay = initial * (multiplier ^ retry_count)
# Example: 1s, 2s, 4s, 8s...
backoff:
  type: exponential
  initial: 1s
  multiplier: 2

# Fibonacci sequence delays
# Calculates: delay = initial * fibonacci(retry_count)
# Example: 1s, 1s, 2s, 3s, 5s...
backoff:
  type: fibonacci
  initial: 1s
</code></pre>
<p><strong>Important:</strong> All duration values use humantime format (e.g., <code>1s</code>, <code>100ms</code>, <code>2m</code>, <code>30s</code>), not milliseconds.</p>
<blockquote>
<p><strong>Note:</strong> All duration values use humantime format (e.g., <code>1s</code>, <code>100ms</code>, <code>2m</code>, <code>30s</code>) for consistency. This applies to both BackoffStrategy delays and CircuitBreakerConfig timeout.</p>
</blockquote>
<h3 id="error-metrics"><a class="header" href="#error-metrics">Error Metrics</a></h3>
<p>Prodigy automatically tracks error metrics for MapReduce jobs using the <code>ErrorMetrics</code> structure:</p>
<p><strong>Available Fields:</strong></p>
<ul>
<li><code>total_items</code> - Total number of work items processed</li>
<li><code>successful</code> - Number of items that completed successfully</li>
<li><code>failed</code> - Number of items that failed</li>
<li><code>skipped</code> - Number of items that were skipped</li>
<li><code>failure_rate</code> - Percentage of failures (0.0 to 1.0)</li>
<li><code>error_types</code> - Map of error types to their frequency counts</li>
<li><code>failure_patterns</code> - Detected recurring error patterns with suggested remediation</li>
</ul>
<p><strong>Accessing Metrics:</strong></p>
<p>Access metrics during execution or after completion to understand job health:</p>
<pre><code class="language-yaml"># In your reduce phase
reduce:
  - shell: "echo 'Processed ${map.successful}/${map.total} items'"
  - shell: "echo 'Failure rate: ${map.failure_rate}'"
</code></pre>
<p>You can also access metrics programmatically via the Prodigy API or through CLI commands like <code>prodigy events</code> to view detailed error statistics.</p>
<p><strong>Pattern Detection:</strong></p>
<p>Prodigy automatically detects recurring error patterns when an error type occurs 3 or more times. The following error types receive specific remediation suggestions in the <code>failure_patterns</code> field:</p>
<ul>
<li><strong>Timeout errors</strong> → “Consider increasing timeout_per_agent”</li>
<li><strong>Network errors</strong> → “Check network connectivity and retry settings”</li>
<li><strong>Permission errors</strong> → “Verify file permissions and access rights”</li>
</ul>
<p>All other error types receive a generic suggestion: “Review error logs for more details.”</p>
<p>These suggestions help diagnose and resolve systemic issues in MapReduce jobs.</p>
<p><strong>Note:</strong> Only the three error types listed above receive specific remediation suggestions. All other error types (such as compilation errors, runtime panics, or custom application errors) receive the generic “Review error logs” suggestion.</p>
<p><strong>Source</strong>: Pattern detection logic in src/cook/workflow/error_policy.rs:478-489</p>
<hr />
<h2 id="dead-letter-queue-dlq-1"><a class="header" href="#dead-letter-queue-dlq-1">Dead Letter Queue (DLQ)</a></h2>
<p>The Dead Letter Queue stores failed work items from MapReduce jobs for later retry or analysis. This is only available for MapReduce workflows, not regular workflows.</p>
<h3 id="sending-items-to-dlq"><a class="header" href="#sending-items-to-dlq">Sending Items to DLQ</a></h3>
<p>Configure your MapReduce workflow to use DLQ:</p>
<pre><code class="language-yaml">mode: mapreduce
error_policy:
  on_item_failure: dlq
</code></pre>
<p>Failed items are automatically sent to the DLQ with:</p>
<ul>
<li>Original work item data</li>
<li>Failure reason and error message</li>
<li>Timestamp of failure</li>
<li>Attempt history</li>
</ul>
<h3 id="retrying-failed-items"><a class="header" href="#retrying-failed-items">Retrying Failed Items</a></h3>
<p>Use the CLI to retry failed items:</p>
<pre><code class="language-bash"># Retry all failed items for a job
prodigy dlq retry &lt;job_id&gt;

# Retry with custom parallelism (default: 5)
prodigy dlq retry &lt;job_id&gt; --max-parallel 10

# Dry run to see what would be retried
prodigy dlq retry &lt;job_id&gt; --dry-run
</code></pre>
<p><strong>DLQ Retry Features:</strong></p>
<ul>
<li>Streams items to avoid memory issues with large queues</li>
<li>Respects original workflow’s max_parallel setting (unless overridden)</li>
<li>Preserves correlation IDs for tracking</li>
<li>Updates DLQ state (removes successful, keeps failed)</li>
<li>Supports interruption and resumption</li>
<li>Shared across worktrees for centralized failure tracking</li>
</ul>
<h3 id="dlq-storage"><a class="header" href="#dlq-storage">DLQ Storage</a></h3>
<p>DLQ data is stored in:</p>
<pre><code>~/.prodigy/dlq/{repo_name}/{job_id}/
</code></pre>
<p>This centralized storage allows multiple worktrees to share the same DLQ.</p>
<hr />
<h2 id="best-practices-23"><a class="header" href="#best-practices-23">Best Practices</a></h2>
<h3 id="choosing-the-right-error-handling-level"><a class="header" href="#choosing-the-right-error-handling-level">Choosing the Right Error Handling Level</a></h3>
<p>Understanding when to use command-level versus workflow-level error handling is crucial for building robust workflows.</p>
<div class="table-wrapper"><table><thead><tr><th><strong>Aspect</strong></th><th><strong>Command-Level (<code>on_failure</code>)</strong></th><th><strong>Workflow-Level (<code>error_policy</code>)</strong></th></tr></thead><tbody>
<tr><td><strong>Scope</strong></td><td>Single workflow step</td><td>Entire MapReduce job</td></tr>
<tr><td><strong>Availability</strong></td><td>All workflow modes</td><td>MapReduce mode only</td></tr>
<tr><td><strong>Use Case</strong></td><td>Step-specific recovery logic</td><td>Consistent handling across all items</td></tr>
<tr><td><strong>Retry Control</strong></td><td>Per-command retry with <code>max_attempts</code></td><td>Per-item retry with backoff strategies</td></tr>
<tr><td><strong>Failure Action</strong></td><td>Custom handler commands</td><td>DLQ, retry, skip, or stop</td></tr>
<tr><td><strong>Circuit Breaker</strong></td><td>Not available</td><td>Available with configurable thresholds</td></tr>
<tr><td><strong>Best For</strong></td><td>Targeted recovery, cleanup, notifications</td><td>Batch processing, rate limiting, cascading failure prevention</td></tr>
</tbody></table>
</div>
<h3 id="when-to-use-command-level-error-handling"><a class="header" href="#when-to-use-command-level-error-handling">When to Use Command-Level Error Handling</a></h3>
<ul>
<li><strong>Recovery:</strong> Use <code>on_failure</code> to fix issues and retry (e.g., clearing cache before reinstalling)</li>
<li><strong>Cleanup:</strong> Use <code>strategy: cleanup</code> to clean up resources after failures</li>
<li><strong>Fallback:</strong> Use <code>strategy: fallback</code> for alternative approaches</li>
<li><strong>Notifications:</strong> Use handler commands to notify teams of failures</li>
<li><strong>Step-Specific Logic:</strong> When different steps need different error handling strategies</li>
</ul>
<h3 id="when-to-use-workflow-level-error-policy"><a class="header" href="#when-to-use-workflow-level-error-policy">When to Use Workflow-Level Error Policy</a></h3>
<ul>
<li><strong>MapReduce jobs:</strong> Use error_policy for consistent failure handling across all work items</li>
<li><strong>Failure thresholds:</strong> Use max_failures or failure_threshold to prevent runaway jobs</li>
<li><strong>Circuit breakers:</strong> Use when external dependencies might fail cascading</li>
<li><strong>DLQ:</strong> Use for large batch jobs where you want to retry failures separately</li>
<li><strong>Rate Limiting:</strong> Use backoff strategies to avoid overwhelming external services</li>
<li><strong>Batch Processing:</strong> When processing hundreds or thousands of items with similar error patterns</li>
</ul>
<h3 id="error-information-available"><a class="header" href="#error-information-available">Error Information Available</a></h3>
<p>When a command fails, you can access error information in handler commands:</p>
<pre><code class="language-yaml">- shell: "risky-command"
  on_failure:
    claude: "/analyze-error ${shell.output}"
</code></pre>
<p>The <code>${shell.output}</code> variable contains the command’s stdout/stderr output.</p>
<h3 id="common-patterns-4"><a class="header" href="#common-patterns-4">Common Patterns</a></h3>
<p><strong>Cleanup and Retry:</strong></p>
<pre><code class="language-yaml">- shell: "npm install"
  on_failure:
    - "npm cache clean --force"
    - "rm -rf node_modules"
    - "npm install"
</code></pre>
<p><strong>Conditional Recovery:</strong></p>
<pre><code class="language-yaml">- shell: "cargo test"
  on_failure:
    claude: "/fix-failing-tests"
  max_attempts: 3
  fail_workflow: false
</code></pre>
<p><strong>Critical Step with Notification:</strong></p>
<pre><code class="language-yaml">- shell: "deploy-production"
  on_failure:
    commands:
      - shell: "rollback-deployment"
      - shell: "notify-team 'Deployment failed'"
    fail_workflow: true   # Still fail workflow after cleanup
</code></pre>
<p><strong>Combined Error Handling Strategies (MapReduce):</strong></p>
<p>For complex MapReduce workflows, combine multiple error handling features:</p>
<pre><code class="language-yaml"># Process API endpoints with comprehensive error handling
mode: mapreduce
error_policy:
  on_item_failure: retry          # Try immediate retry first
  continue_on_failure: true       # Don't stop entire job
  max_failures: 50                # Stop if too many failures
  failure_threshold: 0.15         # Stop if 15% failure rate

  # Retry with exponential backoff
  retry_config:
    max_attempts: 3
    backoff:
      type: exponential
      initial: 2s
      multiplier: 2

  # Protect against cascading failures
  circuit_breaker:
    failure_threshold: 10
    success_threshold: 3
    timeout: 60s
    half_open_requests: 5

  # Report errors in batches of 10
  error_collection:
    batched:
      size: 10

map:
  agent_template:
    - claude: "/process-endpoint ${item.path}"
      on_failure:
        # Item-level recovery before workflow-level retry
        claude: "/diagnose-api-error ${shell.output}"
        max_attempts: 2
</code></pre>
<p>This configuration provides multiple layers of protection:</p>
<ol>
<li>Item-level error handlers for immediate recovery attempts</li>
<li>Automatic retry with exponential backoff for transient failures</li>
<li>Circuit breaker to prevent overwhelming failing dependencies</li>
<li>Failure thresholds to stop runaway jobs early</li>
<li>Batched error reporting to reduce noise</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mapreduce-worktree-architecture"><a class="header" href="#mapreduce-worktree-architecture">MapReduce Worktree Architecture</a></h1>
<p>MapReduce workflows in Prodigy use an isolated git worktree architecture that ensures the main repository remains untouched during workflow execution. This chapter explains the worktree hierarchy, branch naming conventions, merge flows, and debugging strategies.</p>
<h2 id="overview-11"><a class="header" href="#overview-11">Overview</a></h2>
<p>When you run a MapReduce workflow, Prodigy creates a hierarchical worktree structure:</p>
<pre><code>Main Repository (untouched during execution)
    ↓
Parent Worktree (session-mapreduce-{id})
    ├── Setup Phase → Executes here
    ├── Reduce Phase → Executes here
    └── Map Phase → Each agent in child worktree
        ├── Child Worktree (mapreduce-agent-{id})
        ├── Child Worktree (mapreduce-agent-{id})
        └── Child Worktree (mapreduce-agent-{id})
</code></pre>
<p>This architecture provides complete isolation, allowing parallel agents to work independently while preserving a clean main repository.</p>
<h2 id="worktree-hierarchy"><a class="header" href="#worktree-hierarchy">Worktree Hierarchy</a></h2>
<h3 id="parent-worktree"><a class="header" href="#parent-worktree">Parent Worktree</a></h3>
<p>Created at the start of MapReduce workflow execution:</p>
<p><strong>Location</strong>: <code>~/.prodigy/worktrees/{project}/session-mapreduce-{timestamp}</code></p>
<p><strong>Purpose</strong>:</p>
<ul>
<li>Isolates all workflow execution from main repository</li>
<li>Hosts setup phase execution</li>
<li>Hosts reduce phase execution</li>
<li>Serves as merge target for agent results</li>
</ul>
<p><strong>Branch</strong>: Follows <code>prodigy-{session-id}</code> pattern. The session ID includes a timestamp (e.g., <code>session-mapreduce-20250112_143052</code>), so the full branch name becomes <code>prodigy-session-mapreduce-20250112_143052</code> (source: src/worktree/builder.rs:176-178)</p>
<p><strong>Worktree Allocation Strategies</strong>:</p>
<p>All worktrees in Prodigy have names, paths, and git branches - the distinction is in how they’re allocated:</p>
<ul>
<li>
<p><strong>Directly-Created Worktrees</strong>: MapReduce coordinators create session worktrees with explicit, predictable names (e.g., <code>session-mapreduce-20250112_143052</code>). These have deterministic paths and are easy to locate.</p>
</li>
<li>
<p><strong>Pool-Allocated Worktrees</strong>: When agents request worktrees via <code>WorktreeRequest::Anonymous</code> (source: src/cook/execution/mapreduce/resources/worktree.rs:42), they receive pre-allocated worktrees from a shared pool. These worktrees have pool-assigned names rather than request-specific names. The pool allocation strategy enables efficient resource reuse across multiple agents.</p>
</li>
</ul>
<p><strong>Important</strong>: Both allocation strategies produce worktrees with full identity (name, path, branch). The difference is in naming predictability and resource management approach.</p>
<h3 id="child-worktrees"><a class="header" href="#child-worktrees">Child Worktrees</a></h3>
<p>Created for each map agent:</p>
<p><strong>Location</strong>: <code>~/.prodigy/worktrees/{project}/mapreduce-agent-{agent_id}</code></p>
<p><strong>Purpose</strong>:</p>
<ul>
<li>Complete isolation per agent</li>
<li>Independent failure handling</li>
<li>Parallel execution safety</li>
</ul>
<p><strong>Branch</strong>: Follows <code>prodigy-{worktree-name}</code> pattern (branched from parent worktree)</p>
<p><strong>Resource Management</strong>: Agent worktrees can be acquired through two strategies:</p>
<ol>
<li>
<p><strong>Worktree Pool</strong> (preferred): Agents first attempt to acquire pre-allocated worktrees from a <code>WorktreePool</code>. This reduces creation overhead and enables efficient resource reuse.</p>
</li>
<li>
<p><strong>Direct Creation</strong> (fallback): If the pool is exhausted or unavailable, agents fall back to creating dedicated worktrees via <code>WorktreeManager</code>.</p>
</li>
</ol>
<p>The <code>acquire_session</code> method implements this pool-first strategy, ensuring optimal resource utilization while maintaining isolation guarantees.</p>
<p><strong>Note</strong>: The <code>agent_id</code> in the location path encodes the work item information. Agent worktrees are created dynamically as map agents execute.</p>
<h2 id="branch-naming-conventions"><a class="header" href="#branch-naming-conventions">Branch Naming Conventions</a></h2>
<p>Prodigy uses consistent branch naming to track worktree relationships:</p>
<h3 id="parent-worktree-branch"><a class="header" href="#parent-worktree-branch">Parent Worktree Branch</a></h3>
<p>Format: <code>prodigy-{session-id}</code></p>
<p>The branch name follows the universal worktree pattern where all worktrees use <code>prodigy-{name}</code>. For MapReduce workflows, the session ID itself includes the timestamp, so the full branch name looks like:</p>
<p>Example: <code>prodigy-session-mapreduce-20250112_143052</code></p>
<p>This is <code>prodigy-</code> + the session ID <code>session-mapreduce-20250112_143052</code></p>
<h3 id="agent-worktree-branch"><a class="header" href="#agent-worktree-branch">Agent Worktree Branch</a></h3>
<p>Format: <code>prodigy-{worktree-name}</code></p>
<p>All worktrees in Prodigy follow the universal <code>prodigy-{name}</code> branch naming pattern (source: src/worktree/builder.rs:178). The worktree name itself varies based on the allocation strategy:</p>
<p><strong>Pool-Allocated Worktrees</strong>: When agents acquire worktrees from the pre-allocated pool, the worktree name is generated by the pool and may not follow a predictable pattern. These are still tracked by the consistent <code>prodigy-{name}</code> branch format.</p>
<p><strong>Directly-Created Worktrees</strong>: When agents create dedicated worktrees (fallback when pool is exhausted), the name typically encodes job and agent information.</p>
<p>Example: <code>prodigy-mapreduce-agent-mapreduce-20251109_193734_agent_22</code></p>
<p>This is <code>prodigy-</code> + the worktree name <code>mapreduce-agent-mapreduce-20251109_193734_agent_22</code></p>
<p><strong>Directly-Created Worktree Name Components</strong>:</p>
<ul>
<li><code>mapreduce-agent-</code>: Indicates this is a MapReduce agent worktree</li>
<li><code>{job_id}</code>: The MapReduce job identifier (includes timestamp)</li>
<li><code>_agent_{n}</code>: Sequential agent number within the job</li>
</ul>
<p><strong>Note</strong>: The branch naming is always consistent (<code>prodigy-{name}</code>), but worktree naming varies based on allocation strategy.</p>
<h2 id="merge-flow"><a class="header" href="#merge-flow">Merge Flow</a></h2>
<p>MapReduce workflows involve multiple merge operations to aggregate results:</p>
<h3 id="1-agent-merge-child--parent"><a class="header" href="#1-agent-merge-child--parent">1. Agent Merge (Child → Parent)</a></h3>
<p>When an agent completes successfully:</p>
<pre><code>Child Worktree (agent branch)
    ↓ merge
Parent Worktree (session branch)
</code></pre>
<p><strong>Process</strong>:</p>
<ol>
<li>Agent completes all commands successfully</li>
<li>Agent commits changes to its branch</li>
<li>Merge coordinator adds agent to merge queue</li>
<li>Sequential merge into parent worktree branch</li>
<li>Child worktree cleanup</li>
</ol>
<h3 id="2-mapreduce-to-parent-merge"><a class="header" href="#2-mapreduce-to-parent-merge">2. MapReduce to Parent Merge</a></h3>
<p>After all map agents complete and reduce phase finishes:</p>
<pre><code>Parent Worktree (session branch)
    ↓ merge
Main Repository (original branch)
</code></pre>
<p><strong>Process</strong>:</p>
<ol>
<li>All agents merged into parent worktree</li>
<li>Reduce phase executes in parent worktree</li>
<li>User confirms merge to main repository</li>
<li>Sequential merge with conflict detection</li>
<li>Parent worktree cleanup</li>
</ol>
<h3 id="merge-strategies"><a class="header" href="#merge-strategies">Merge Strategies</a></h3>
<p><strong>Fast-Forward When Possible</strong>: If no divergence, use fast-forward merge</p>
<p><strong>Three-Way Merge</strong>: When branches have diverged, perform three-way merge</p>
<p><strong>Conflict Handling</strong>: Stop and report conflicts for manual resolution</p>
<h2 id="agent-merge-details"><a class="header" href="#agent-merge-details">Agent Merge Details</a></h2>
<h3 id="merge-queue"><a class="header" href="#merge-queue">Merge Queue</a></h3>
<p>Agents are added to a merge queue as they complete:</p>
<p><strong>Queue Architecture</strong>: Merge queue is managed in-memory by a background worker task using a tokio unbounded mpsc channel (<code>mpsc::unbounded_channel::&lt;MergeRequest&gt;()</code>). Merge requests are processed sequentially via this channel, eliminating MERGE_HEAD race conditions. Queue state is not persisted - merge operations are atomic (source: src/cook/execution/mapreduce/merge_queue.rs:70).</p>
<p><strong>Resume and Recovery</strong>: The merge queue state is reconstructed on resume from checkpoint data (source: src/cook/execution/mapreduce/merge_queue.rs:153). When a MapReduce workflow is interrupted and resumed, the queue is rebuilt based on:</p>
<ul>
<li>Completed agents: Already merged, skip re-merging</li>
<li>Failed agents: Tracked in DLQ, can be retried separately</li>
<li>In-progress agents: Moved back to pending status, will be re-executed</li>
<li>Pending agents: Continue processing from where left off</li>
</ul>
<p>Any in-progress merges at the time of interruption are retried from the agent worktree state. This ensures no agent results are lost during resume.</p>
<p><strong>Queue Processing</strong>: Queue processes <code>MergeRequest</code> objects containing:</p>
<ul>
<li><code>agent_id</code>: Unique agent identifier</li>
<li><code>branch_name</code>: Agent’s git branch to merge</li>
<li><code>item_id</code>: Work item identifier for correlation</li>
<li><code>env</code>: Execution environment context (variables, secrets)</li>
</ul>
<p>Merge requests are processed FIFO with automatic conflict detection.</p>
<h3 id="sequential-merge-processing"><a class="header" href="#sequential-merge-processing">Sequential Merge Processing</a></h3>
<p>Merges are processed sequentially to prevent conflicts:</p>
<ol>
<li>Lock merge queue</li>
<li>Take next agent from pending queue</li>
<li>Perform merge into parent worktree</li>
<li>Update queue (move to merged or failed)</li>
<li>Release lock</li>
</ol>
<h3 id="automatic-conflict-resolution"><a class="header" href="#automatic-conflict-resolution">Automatic Conflict Resolution</a></h3>
<p>If a standard git merge fails with conflicts, the merge queue automatically invokes Claude using the <code>/prodigy-merge-worktree</code> command to resolve conflicts intelligently:</p>
<p><strong>Conflict Resolution Flow</strong>:</p>
<ol>
<li>Standard git merge attempted</li>
<li>If conflicts detected, invoke Claude with <code>/prodigy-merge-worktree {branch_name}</code></li>
<li>Claude is executed with <code>PRODIGY_AUTOMATION=true</code> environment variable (source: src/cook/execution/mapreduce/merge_queue.rs:98-99)</li>
<li>Claude analyzes conflicts and attempts resolution</li>
<li>If Claude succeeds, merge completes automatically</li>
<li>If Claude fails, agent is marked as failed and added to DLQ</li>
</ol>
<p><strong>PRODIGY_AUTOMATION Environment Variable</strong>: When set to <code>true</code>, this signals to Claude Code that it’s operating in automated workflow mode and should use appropriate merge strategies without requiring user interaction. Claude will attempt to resolve conflicts autonomously using standard git merge strategies and code analysis.</p>
<p><strong>Benefits</strong>:</p>
<ul>
<li>Reduces manual merge conflict resolution overhead</li>
<li>Handles common conflict patterns automatically</li>
<li>Preserves full context for debugging via Claude logs</li>
<li>Falls back gracefully to DLQ for complex conflicts</li>
<li>Automated execution mode ensures non-interactive conflict resolution</li>
</ul>
<p>This automatic conflict resolution is especially useful when multiple agents modify overlapping code areas.</p>
<h2 id="parent-to-master-merge"><a class="header" href="#parent-to-master-merge">Parent to Master Merge</a></h2>
<h3 id="merge-confirmation"><a class="header" href="#merge-confirmation">Merge Confirmation</a></h3>
<p>After reduce phase completes, Prodigy prompts for merge confirmation:</p>
<pre><code>✓ MapReduce workflow completed successfully

Merge session-mapreduce-20250112_143052 to master? [y/N]
</code></pre>
<h3 id="custom-merge-workflows"><a class="header" href="#custom-merge-workflows">Custom Merge Workflows</a></h3>
<p>Configure custom merge validation:</p>
<pre><code class="language-yaml">merge:
  - shell: "git fetch origin"
  - shell: "cargo test"
  - shell: "cargo clippy"
  - claude: "/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}"
</code></pre>
<p><strong>Important</strong>: Always pass both <code>${merge.source_branch}</code> and <code>${merge.target_branch}</code> to the <code>/prodigy-merge-worktree</code> command (source: .claude/commands/prodigy-merge-worktree.md). This ensures the merge targets the branch you were on when you started the workflow, not a hardcoded main/master branch.</p>
<h3 id="merge-variables"><a class="header" href="#merge-variables">Merge Variables</a></h3>
<p>Available during merge workflows:</p>
<ul>
<li><code>${merge.worktree}</code> - Worktree name</li>
<li><code>${merge.source_branch}</code> - Session branch name</li>
<li><code>${merge.target_branch}</code> - Main repository branch (usually master/main)</li>
<li><code>${merge.session_id}</code> - Session ID for correlation</li>
</ul>
<h2 id="debugging-mapreduce-worktrees"><a class="header" href="#debugging-mapreduce-worktrees">Debugging MapReduce Worktrees</a></h2>
<h3 id="inspecting-worktree-state"><a class="header" href="#inspecting-worktree-state">Inspecting Worktree State</a></h3>
<pre><code class="language-bash"># List all worktrees
git worktree list

# View worktree details
cd ~/.prodigy/worktrees/{project}/session-mapreduce-*
git status
git log

# View agent worktree
cd ~/.prodigy/worktrees/{project}/agent-*
git log --oneline
</code></pre>
<h3 id="finding-agent-worktree-paths"><a class="header" href="#finding-agent-worktree-paths">Finding Agent Worktree Paths</a></h3>
<p>Agent worktrees may be <strong>directly-created</strong> (with predictable names) or <strong>pool-allocated</strong> (with pool-assigned names). To correlate agent IDs to worktree paths:</p>
<p><strong>Directly-Created Worktrees</strong> (deterministic paths):</p>
<pre><code class="language-bash"># Pattern: ~/.prodigy/worktrees/{project}/mapreduce-agent-{job_id}_agent_{n}
cd ~/.prodigy/worktrees/{project}/mapreduce-agent-*
</code></pre>
<p><strong>Pool-Allocated Worktrees</strong> (pool-assigned paths):</p>
<pre><code class="language-bash"># List all worktrees and correlate by branch name
git worktree list

# Look for branches matching agent pattern
git branch -a | grep prodigy-mapreduce-agent
</code></pre>
<p><strong>Note</strong>: Both allocation strategies produce fully-identified worktrees with names, paths, and branches. Pool allocation assigns names from the pool’s naming scheme, while direct creation uses request-specific naming patterns. Use <code>WorktreeInfo</code> tracking (described below) to correlate agent IDs to actual worktree locations.</p>
<p><strong>WorktreeInfo Tracking</strong>: Prodigy captures worktree metadata in <code>WorktreeInfo</code> structs containing:</p>
<ul>
<li><code>name</code>: Worktree identifier</li>
<li><code>path</code>: Full filesystem path</li>
<li><code>branch</code>: Git branch name</li>
</ul>
<p>This information is logged in MapReduce events and can be inspected via <code>prodigy events {job_id}</code> to correlate agent IDs to worktree paths.</p>
<h3 id="common-debugging-scenarios"><a class="header" href="#common-debugging-scenarios">Common Debugging Scenarios</a></h3>
<p><strong>Agent Failed to Merge:</strong></p>
<ol>
<li>Check DLQ for failure details: <code>prodigy dlq show {job_id}</code></li>
<li>Inspect failed agent worktree: <code>cd ~/.prodigy/worktrees/{project}/mapreduce-agent-*</code></li>
<li>Review agent changes: <code>git diff master</code></li>
<li>Check for conflicts: <code>git status</code></li>
<li>Review Claude merge logs if conflict resolution was attempted</li>
</ol>
<p><strong>Parent Worktree Not Merging:</strong></p>
<ol>
<li>Check parent worktree: <code>cd ~/.prodigy/worktrees/{project}/session-mapreduce-*</code></li>
<li>Verify all agents merged: <code>git log --oneline</code></li>
<li>Check for uncommitted changes: <code>git status</code></li>
<li>Review merge history: <code>git log --graph --oneline --all</code></li>
</ol>
<h3 id="merge-conflict-resolution"><a class="header" href="#merge-conflict-resolution">Merge Conflict Resolution</a></h3>
<p>If merge conflicts occur:</p>
<pre><code class="language-bash"># Navigate to parent worktree
cd ~/.prodigy/worktrees/{project}/session-mapreduce-*

# View conflicts
git status

# Resolve manually
vim &lt;conflicted-file&gt;

# Complete merge
git add &lt;conflicted-file&gt;
git commit
</code></pre>
<h2 id="verification-commands"><a class="header" href="#verification-commands">Verification Commands</a></h2>
<h3 id="verify-main-repository-is-clean"><a class="header" href="#verify-main-repository-is-clean">Verify Main Repository is Clean</a></h3>
<pre><code class="language-bash"># Main repository should have no changes from MapReduce execution
git status
# Expected: nothing to commit, working tree clean
</code></pre>
<h3 id="verify-worktree-isolation"><a class="header" href="#verify-worktree-isolation">Verify Worktree Isolation</a></h3>
<pre><code class="language-bash"># Check that parent worktree has changes
cd ~/.prodigy/worktrees/{project}/session-mapreduce-*
git status
git log --oneline

# Main repository should still be clean
cd /path/to/main/repo
git status
</code></pre>
<h3 id="verify-agent-merges"><a class="header" href="#verify-agent-merges">Verify Agent Merges</a></h3>
<pre><code class="language-bash"># Check for merge events
prodigy events {job_id}

# Verify merged agents in parent worktree
cd ~/.prodigy/worktrees/{project}/session-mapreduce-*
git log --oneline | grep "Merge"
</code></pre>
<h2 id="best-practices-24"><a class="header" href="#best-practices-24">Best Practices</a></h2>
<h3 id="worktree-management"><a class="header" href="#worktree-management">Worktree Management</a></h3>
<ul>
<li><strong>Cleanup</strong>: Remove old worktrees after successful merge: <code>prodigy worktree clean</code></li>
<li><strong>Monitoring</strong>: Check worktree disk usage periodically</li>
<li><strong>Inspection</strong>: Review worktrees before deleting to verify results</li>
</ul>
<h3 id="merge-workflows-1"><a class="header" href="#merge-workflows-1">Merge Workflows</a></h3>
<ul>
<li><strong>Test Before Merge</strong>: Run tests in merge workflow to catch issues</li>
<li><strong>Sync Upstream</strong>: Fetch and merge origin/main before merging to main</li>
<li><strong>Conflict Prevention</strong>: Keep MapReduce jobs focused to minimize conflicts</li>
</ul>
<h3 id="debugging"><a class="header" href="#debugging">Debugging</a></h3>
<ul>
<li><strong>Preserve Worktrees</strong>: Don’t delete worktrees until debugging is complete</li>
<li><strong>Event Logs</strong>: Review event logs for merge failures: <code>prodigy events {job_id}</code></li>
<li><strong>DLQ Review</strong>: Check failed items that might indicate merge issues</li>
</ul>
<h2 id="troubleshooting-16"><a class="header" href="#troubleshooting-16">Troubleshooting</a></h2>
<h3 id="worktree-creation-fails"><a class="header" href="#worktree-creation-fails">Worktree Creation Fails</a></h3>
<p><strong>Issue</strong>: Cannot create parent or child worktree
<strong>Solution</strong>: Check disk space, verify git repository is valid, ensure no existing worktree with same name</p>
<h3 id="agent-merge-fails"><a class="header" href="#agent-merge-fails">Agent Merge Fails</a></h3>
<p><strong>Issue</strong>: Agent results fail to merge into parent
<strong>Solution</strong>: Check merge queue, inspect agent worktree for conflicts, review agent changes</p>
<h3 id="parent-merge-conflicts"><a class="header" href="#parent-merge-conflicts">Parent Merge Conflicts</a></h3>
<p><strong>Issue</strong>: Merging parent worktree to main causes conflicts
<strong>Solution</strong>: Resolve conflicts manually, consider rebasing parent worktree on latest main</p>
<h3 id="orphaned-worktrees"><a class="header" href="#orphaned-worktrees">Orphaned Worktrees</a></h3>
<p><strong>Issue</strong>: Worktrees remain after workflow completion
<strong>Solution</strong>: Use <code>prodigy worktree clean</code> to remove old worktrees, or manually remove with <code>git worktree remove</code></p>
<h2 id="see-also-34"><a class="header" href="#see-also-34">See Also</a></h2>
<ul>
<li><a href="mapreduce/index.html">MapReduce Workflows</a> - MapReduce workflow basics</li>
<li><a href="error-handling.html">Error Handling</a> - Handling merge failures</li>
<li><a href="troubleshooting/index.html">Troubleshooting</a> - General troubleshooting guide</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="automated-documentation-with-mdbook"><a class="header" href="#automated-documentation-with-mdbook">Automated Documentation with mdBook</a></h1>
<p>This guide shows you how to set up automated, always-up-to-date documentation for any project using Prodigy’s book workflow system. This same system maintains the documentation you’re reading right now.</p>
<h2 id="overview-12"><a class="header" href="#overview-12">Overview</a></h2>
<p>The book workflow system:</p>
<ul>
<li><strong>Analyzes your codebase</strong> to build a feature inventory</li>
<li><strong>Detects documentation drift</strong> by comparing docs to implementation</li>
<li><strong>Updates documentation</strong> automatically using Claude</li>
<li><strong>Maintains consistency</strong> across all chapters</li>
<li><strong>Runs on any project</strong> - just configure and go</li>
</ul>
<p>The generalized commands work for any codebase: Rust, Python, JavaScript, etc.</p>
<h2 id="quick-start-3"><a class="header" href="#quick-start-3">Quick Start</a></h2>
<p>Ready to get started? Here’s the fastest path:</p>
<ol>
<li><strong>Install prerequisites</strong> (see below)</li>
<li><strong>Initialize your book</strong> structure:
<pre><code class="language-bash">mdbook init book
cd book
# Edit book.toml and src/SUMMARY.md as needed
</code></pre>
</li>
<li><strong>Create a documentation workflow</strong> (see <a href="automated-documentation/quick-start-30-minutes.html">Quick Start (30 Minutes)</a>)</li>
<li><strong>Run the workflow</strong>:
<pre><code class="language-bash">prodigy run workflows/book-docs.yml
</code></pre>
</li>
<li><strong>Review and merge</strong> the generated documentation</li>
</ol>
<p>For a detailed walkthrough, see <a href="automated-documentation/quick-start-30-minutes.html">Quick Start (30 Minutes)</a> or <a href="automated-documentation/quick-start.html">Quick Start</a>.</p>
<h2 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h2>
<p>Ensure you have the following tools installed and configured:</p>
<ol>
<li>
<p><strong>Prodigy</strong> (latest version recommended):</p>
<pre><code class="language-bash">cargo install prodigy
</code></pre>
<ul>
<li>Minimum: Any recent release</li>
<li>Tested with: Prodigy 1.0.0+</li>
<li>Source: <a href="https://github.com/iepathos/prodigy">github.com/iepathos/prodigy</a></li>
</ul>
</li>
<li>
<p><strong>mdBook</strong> (0.4.0 or later):</p>
<pre><code class="language-bash">cargo install mdbook
</code></pre>
<ul>
<li>Minimum: 0.4.0</li>
<li>Tested with: Latest stable release</li>
<li>Note: Older versions may lack features used in book.toml</li>
<li>Verify installation: <code>mdbook --version</code></li>
</ul>
</li>
<li>
<p><strong>Claude Code CLI</strong> with valid API credentials:</p>
<ul>
<li>Requires active Anthropic API key</li>
<li>Set up authentication before running workflows</li>
<li>Used for automated documentation analysis and generation</li>
<li>See <a href="https://docs.anthropic.com/claude/docs">Claude Code documentation</a></li>
</ul>
</li>
<li>
<p><strong>Git</strong> (2.25 or later) with initialized repository:</p>
<pre><code class="language-bash"># Verify git is installed (minimum 2.25)
git --version

# Initialize a repository if needed
git init
</code></pre>
<ul>
<li>Minimum: Git 2.25+ (for worktree support)</li>
<li>Recommended: Git 2.30+ for improved worktree handling</li>
<li>Required: Repository must be initialized (<code>git init</code>)</li>
</ul>
</li>
<li>
<p><strong>Rust toolchain</strong> (for Cargo-based installation):</p>
<ul>
<li>Edition 2021 or later</li>
<li>Required to build Prodigy and mdBook from source</li>
<li>Install via <a href="https://rustup.rs">rustup.rs</a></li>
</ul>
</li>
</ol>
<h2 id="how-it-works-1"><a class="header" href="#how-it-works-1">How It Works</a></h2>
<p>The documentation workflow uses a <strong>MapReduce pattern</strong> to process your codebase in parallel:</p>
<h3 id="workflow-phases"><a class="header" href="#workflow-phases">Workflow Phases</a></h3>
<ol>
<li>
<p><strong>Setup Phase</strong> (Feature Analysis):</p>
<ul>
<li>Analyzes your codebase to build a complete feature inventory</li>
<li>Detects documentation gaps by comparing existing docs to implementation</li>
<li>Creates missing chapter/subsection files with placeholders</li>
<li>Generates work items for the map phase</li>
<li>Source: workflows/book-docs-drift.yml:24-34</li>
</ul>
</li>
<li>
<p><strong>Map Phase</strong> (Parallel Processing):</p>
<ul>
<li>Processes each chapter/subsection in parallel using isolated git worktrees</li>
<li>For each documentation item:
<ul>
<li>Analyzes drift between documentation and implementation</li>
<li>Fixes identified issues with real code examples</li>
<li>Validates fixes meet quality standards</li>
</ul>
</li>
<li>Runs up to 3 items concurrently (configurable via MAX_PARALLEL)</li>
<li>Failed items go to Dead Letter Queue (DLQ) for retry</li>
<li>Source: workflows/book-docs-drift.yml:37-59</li>
</ul>
</li>
<li>
<p><strong>Reduce Phase</strong> (Validation):</p>
<ul>
<li>Rebuilds the entire book to ensure chapters compile together</li>
<li>Checks for broken links between chapters</li>
<li>Fixes any build errors discovered during compilation</li>
<li>Cleans up temporary analysis files</li>
<li>Source: workflows/book-docs-drift.yml:62-82</li>
</ul>
</li>
<li>
<p><strong>Merge Phase</strong> (Integration):</p>
<ul>
<li>Merges updated documentation back to your original branch</li>
<li>Preserves your working tree state</li>
<li>Uses Claude to handle any merge conflicts</li>
<li>Source: workflows/book-docs-drift.yml:93-100</li>
</ul>
</li>
</ol>
<h3 id="worktree-isolation"><a class="header" href="#worktree-isolation">Worktree Isolation</a></h3>
<p>All phases execute in an isolated git worktree:</p>
<ul>
<li>Your main repository remains untouched during execution</li>
<li>Each map agent runs in its own child worktree</li>
<li>Changes merge back only after successful completion</li>
<li>Failed workflows don’t pollute your working directory</li>
<li>Learn more: <a href="automated-documentation/understanding-the-workflow.html">Understanding the Workflow</a></li>
</ul>
<h3 id="quality-guarantees"><a class="header" href="#quality-guarantees">Quality Guarantees</a></h3>
<p>The workflow ensures documentation quality through:</p>
<ul>
<li><strong>Code-grounded examples</strong>: All examples extracted from actual implementation</li>
<li><strong>Validation checkpoints</strong>: Each fix validated before proceeding</li>
<li><strong>Build verification</strong>: Full book rebuild ensures no broken references</li>
<li><strong>Source attribution</strong>: Examples include file paths and line numbers</li>
<li><strong>Automatic retry</strong>: Failed items can be retried via <code>prodigy dlq retry</code></li>
</ul>
<p>For detailed information about each phase, see the subsections below.</p>
<h2 id="additional-topics-8"><a class="header" href="#additional-topics-8">Additional Topics</a></h2>
<p>See also:</p>
<ul>
<li><a href="automated-documentation/quick-start-30-minutes.html">Quick Start (30 Minutes)</a></li>
<li><a href="automated-documentation/installation.html">Installation</a></li>
<li><a href="automated-documentation/quick-start.html">Quick Start</a></li>
<li><a href="automated-documentation/understanding-the-workflow.html">Understanding the Workflow</a></li>
<li><a href="automated-documentation/automatic-gap-detection.html">Automatic Gap Detection</a></li>
<li><a href="automated-documentation/github-actions-integration.html">GitHub Actions Integration</a></li>
<li><a href="automated-documentation/customization-examples.html">Customization Examples</a></li>
<li><a href="automated-documentation/best-practices.html">Best Practices</a></li>
<li><a href="automated-documentation/troubleshooting.html">Troubleshooting</a></li>
<li><a href="automated-documentation/advanced-configuration.html">Advanced Configuration</a></li>
<li><a href="automated-documentation/real-world-example-prodigys-own-documentation.html">Real-World Example: Prodigy’s Own Documentation</a></li>
<li><a href="automated-documentation/documentation-versioning.html">Documentation Versioning</a></li>
<li><a href="automated-documentation/next-steps.html">Next Steps</a></li>
<li><a href="automated-documentation/benefits.html">Benefits</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="quick-start-30-minutes"><a class="header" href="#quick-start-30-minutes">Quick Start (30 Minutes)</a></h2>
<p>Get Prodigy’s automated documentation workflow running in your project in 30 minutes. This guide walks you through setting up and running your first documentation drift detection and fixing workflow.</p>
<h3 id="what-youll-accomplish"><a class="header" href="#what-youll-accomplish">What You’ll Accomplish</a></h3>
<p>By the end of this guide, you’ll have:</p>
<ul>
<li>Configured the automated documentation system for your project</li>
<li>Run the workflow to detect and fix documentation drift</li>
<li>Generated or updated an mdBook with accurate, code-grounded documentation</li>
<li>Understood the three phases: setup, map, and reduce</li>
</ul>
<h3 id="prerequisites-1"><a class="header" href="#prerequisites-1">Prerequisites</a></h3>
<p>Before starting, ensure you have:</p>
<ol>
<li>
<p><strong>Rust and Cargo</strong> - Install from <a href="https://rustup.rs/">rustup.rs</a></p>
<pre><code class="language-bash">curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
</code></pre>
</li>
<li>
<p><strong>Prodigy</strong> - Install via cargo:</p>
<pre><code class="language-bash">cargo install prodigy
</code></pre>
</li>
<li>
<p><strong>Claude Code CLI</strong> - Follow installation instructions at <a href="https://docs.anthropic.com/claude/docs/claude-code">Claude Code documentation</a></p>
</li>
<li>
<p><strong>mdBook</strong> - For building documentation:</p>
<pre><code class="language-bash">cargo install mdbook
</code></pre>
</li>
<li>
<p><strong>Git Repository</strong> - Your project should be in a git repository with existing or new documentation in a <code>book/</code> directory</p>
</li>
</ol>
<h3 id="step-1-initialize-your-book-structure"><a class="header" href="#step-1-initialize-your-book-structure">Step 1: Initialize Your Book Structure</a></h3>
<p>If you don’t already have an mdBook, create one:</p>
<pre><code class="language-bash"># Create book directory and initialize
mkdir book
cd book
mdbook init

# This creates:
# book/
#   ├── book.toml
#   └── src/
#       ├── SUMMARY.md
#       └── chapter_1.md
</code></pre>
<p><strong>Source</strong>: Standard mdBook initialization pattern</p>
<h3 id="step-2-create-book-configuration"><a class="header" href="#step-2-create-book-configuration">Step 2: Create Book Configuration</a></h3>
<p>Create <code>.prodigy/book-config.json</code> at your project root with your project details:</p>
<pre><code class="language-json">{
  "project_name": "YourProject",
  "project_type": "cli_tool",
  "book_dir": "book",
  "book_src": "book/src",
  "book_build_dir": "book/book",
  "analysis_targets": [
    {
      "area": "workflow_basics",
      "source_files": [
        "src/config/workflow.rs",
        "src/workflow/executor.rs"
      ],
      "feature_categories": [
        "structure",
        "execution_model",
        "commit_tracking"
      ]
    }
  ],
  "chapter_file": "workflows/data/your-project-chapters.json",
  "custom_analysis": {
    "include_examples": true,
    "include_best_practices": true,
    "include_troubleshooting": true
  }
}
</code></pre>
<p><strong>Source</strong>: Extracted from <code>.prodigy/book-config.json</code> structure (lines 1-219)</p>
<p><strong>Key Fields Explained</strong>:</p>
<ul>
<li><code>project_name</code> - Display name for your project</li>
<li><code>book_dir</code> - Root directory for your mdBook</li>
<li><code>analysis_targets</code> - Areas of your codebase to analyze for documentation
<ul>
<li><code>area</code> - Logical grouping name</li>
<li><code>source_files</code> - Files to analyze for this area</li>
<li><code>feature_categories</code> - Types of features to extract</li>
</ul>
</li>
<li><code>chapter_file</code> - Path to chapter definitions JSON</li>
</ul>
<h3 id="step-3-define-chapter-structure"><a class="header" href="#step-3-define-chapter-structure">Step 3: Define Chapter Structure</a></h3>
<p>Create <code>workflows/data/your-project-chapters.json</code> to define your documentation structure:</p>
<pre><code class="language-json">{
  "chapters": [
    {
      "id": "getting-started",
      "title": "Getting Started",
      "file": "getting-started.md",
      "topics": ["Installation", "Quick Start"],
      "feature_mapping": ["installation", "basic_usage"],
      "validation": "Check getting started guide matches current setup"
    },
    {
      "id": "advanced",
      "title": "Advanced Features",
      "file": "advanced/index.md",
      "topics": ["Configuration", "API Reference"],
      "feature_mapping": ["configuration", "api"],
      "validation": "Verify advanced features are documented"
    }
  ]
}
</code></pre>
<p><strong>Source</strong>: Based on <code>workflows/data/prodigy-chapters.json</code> structure pattern</p>
<h3 id="step-4-create-the-workflow-file"><a class="header" href="#step-4-create-the-workflow-file">Step 4: Create the Workflow File</a></h3>
<p>Create <code>workflows/book-docs-drift.yml</code>:</p>
<pre><code class="language-yaml">name: your-project-book-docs-drift-detection
mode: mapreduce

env:
  PROJECT_NAME: "YourProject"
  PROJECT_CONFIG: ".prodigy/book-config.json"
  FEATURES_PATH: ".prodigy/book-analysis/features.json"
  BOOK_DIR: "book"
  ANALYSIS_DIR: ".prodigy/book-analysis"
  CHAPTERS_FILE: "workflows/data/your-project-chapters.json"
  MAX_PARALLEL: "3"

setup:
  - shell: "mkdir -p $ANALYSIS_DIR"
  - claude: "/prodigy-analyze-features-for-book --project $PROJECT_NAME --config $PROJECT_CONFIG"
  - claude: "/prodigy-detect-documentation-gaps --project $PROJECT_NAME --config $PROJECT_CONFIG --features $FEATURES_PATH --chapters $CHAPTERS_FILE --book-dir $BOOK_DIR"

map:
  input: "${ANALYSIS_DIR}/flattened-items.json"
  json_path: "$[*]"

  agent_template:
    - claude: "/prodigy-analyze-subsection-drift --project $PROJECT_NAME --json '${item}' --features $FEATURES_PATH"
      commit_required: true

    - claude: "/prodigy-fix-subsection-drift --project $PROJECT_NAME --json '${item}'"
      commit_required: true
      validate:
        claude: "/prodigy-validate-doc-fix --project $PROJECT_NAME --json '${item}' --output .prodigy/validation-result.json"
        result_file: ".prodigy/validation-result.json"
        threshold: 100
        on_incomplete:
          claude: "/prodigy-complete-doc-fix --project $PROJECT_NAME --json '${item}' --gaps ${validation.gaps}"
          max_attempts: 3
          fail_workflow: false
          commit_required: true

  max_parallel: ${MAX_PARALLEL}

reduce:
  - shell: "cd book &amp;&amp; mdbook build"
    on_failure:
      claude: "/prodigy-fix-book-build-errors --project $PROJECT_NAME"
      commit_required: true

  - shell: "rm -rf ${ANALYSIS_DIR}"
  - shell: "git add -A &amp;&amp; git commit -m 'chore: remove temporary book analysis files' || true"

error_policy:
  on_item_failure: dlq
  continue_on_failure: true
  max_failures: 2
  error_collection: aggregate

merge:
  commands:
    - shell: "git fetch origin"
    - claude: "/prodigy-merge-master --project ${PROJECT_NAME}"
    - claude: "/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}"
</code></pre>
<p><strong>Source</strong>: Adapted from <code>workflows/book-docs-drift.yml</code> (lines 1-101)</p>
<h3 id="step-5-run-the-workflow"><a class="header" href="#step-5-run-the-workflow">Step 5: Run the Workflow</a></h3>
<p>Execute the documentation workflow:</p>
<pre><code class="language-bash"># Run with verbose output to see progress
prodigy run workflows/book-docs-drift.yml -v
</code></pre>
<p><strong>What Happens During Execution</strong>:</p>
<p><strong>Setup Phase</strong> (runs in parent worktree):</p>
<pre><code>✓ Creating analysis directory
✓ Analyzing codebase features → .prodigy/book-analysis/features.json
✓ Detecting documentation gaps → .prodigy/book-analysis/flattened-items.json
</code></pre>
<p><strong>Map Phase</strong> (parallel agents, each in isolated worktree):</p>
<pre><code>Agent 1: Analyzing chapter 'Getting Started'
  ✓ Drift analysis complete → drift report
  ✓ Fixing outdated examples
  ✓ Validation: 100% complete
  ✓ Merged to parent worktree

Agent 2: Analyzing chapter 'Advanced Features'
  ✓ Drift analysis complete → drift report
  ✓ Adding missing configuration docs
  ✓ Validation: 100% complete
  ✓ Merged to parent worktree
</code></pre>
<p><strong>Reduce Phase</strong> (runs in parent worktree):</p>
<pre><code>✓ Building book with mdbook
✓ Cleaning up temporary files
✓ Ready to merge to master
</code></pre>
<p><strong>Source</strong>: Execution flow based on MapReduce workflow phases in <code>workflows/book-docs-drift.yml</code> and features.json analysis</p>
<h3 id="step-6-review-and-merge-changes"><a class="header" href="#step-6-review-and-merge-changes">Step 6: Review and Merge Changes</a></h3>
<p>After the workflow completes, you’ll see:</p>
<pre><code>Workflow completed successfully!

Merge session-abc123 to master? [Y/n]
</code></pre>
<p>Before merging, review the changes:</p>
<pre><code class="language-bash"># Check what was modified
cd ~/.prodigy/worktrees/your-project/session-abc123/
git log --oneline
git diff master

# Review specific chapter changes
git show HEAD:book/src/getting-started.md
</code></pre>
<p>Type <code>Y</code> to merge changes back to your original branch.</p>
<h3 id="step-7-verify-the-updated-documentation"><a class="header" href="#step-7-verify-the-updated-documentation">Step 7: Verify the Updated Documentation</a></h3>
<p>After merging, view your updated documentation:</p>
<pre><code class="language-bash"># Build and serve locally
cd book
mdbook serve --open

# Your browser opens to http://localhost:3000
# Navigate through chapters to see updated content
</code></pre>
<p><strong>Expected Results</strong>:</p>
<ul>
<li>✓ All code examples reference actual source files</li>
<li>✓ Configuration examples match your codebase types</li>
<li>✓ API documentation reflects current function signatures</li>
<li>✓ Outdated sections updated or removed</li>
<li>✓ Cross-references between chapters are valid</li>
</ul>
<h3 id="understanding-the-workflow-phases"><a class="header" href="#understanding-the-workflow-phases">Understanding the Workflow Phases</a></h3>
<p>The automated documentation workflow uses Prodigy’s MapReduce pattern with three phases:</p>
<h4 id="setup-phase"><a class="header" href="#setup-phase">Setup Phase</a></h4>
<p><strong>Purpose</strong>: Analyze your codebase and prepare work items</p>
<p><strong>Commands</strong>:</p>
<ol>
<li><code>mkdir -p $ANALYSIS_DIR</code> - Create temporary analysis directory</li>
<li><code>/prodigy-analyze-features-for-book</code> - Extract features from source files into features.json</li>
<li><code>/prodigy-detect-documentation-gaps</code> - Compare features to chapters, create flattened-items.json</li>
</ol>
<p><strong>Output</strong>: JSON file with list of chapters/subsections to process</p>
<p><strong>Source</strong>: Setup phase from <code>workflows/book-docs-drift.yml:24-34</code></p>
<h4 id="map-phase"><a class="header" href="#map-phase">Map Phase</a></h4>
<p><strong>Purpose</strong>: Process each chapter/subsection in parallel to detect and fix drift</p>
<p><strong>For each chapter</strong>:</p>
<ol>
<li><code>/prodigy-analyze-subsection-drift</code> - Compare chapter to codebase, identify outdated/missing content</li>
<li><code>/prodigy-fix-subsection-drift</code> - Update markdown file with accurate, grounded examples</li>
<li>Validation - Ensure documentation meets quality standards (100% threshold)</li>
<li>Gap filling - If validation fails, run completion attempts (max 3)</li>
</ol>
<p><strong>Parallelism</strong>: Configured via <code>max_parallel: 3</code> - three chapters processed simultaneously</p>
<p><strong>Isolation</strong>: Each chapter processed in its own git worktree, merged back to parent automatically</p>
<p><strong>Source</strong>: Map phase from <code>workflows/book-docs-drift.yml:37-59</code></p>
<h4 id="reduce-phase"><a class="header" href="#reduce-phase">Reduce Phase</a></h4>
<p><strong>Purpose</strong>: Validate the complete book and clean up</p>
<p><strong>Commands</strong>:</p>
<ol>
<li><code>mdbook build</code> - Compile the book to catch broken links or formatting errors</li>
<li><code>/prodigy-fix-book-build-errors</code> - Fix any build errors (only runs if build fails)</li>
<li>Cleanup temporary analysis files</li>
</ol>
<p><strong>Source</strong>: Reduce phase from <code>workflows/book-docs-drift.yml:62-69</code></p>
<h3 id="customization-tips"><a class="header" href="#customization-tips">Customization Tips</a></h3>
<p><strong>Adjust Parallelism</strong>:</p>
<pre><code class="language-yaml">env:
  MAX_PARALLEL: "5"  # Process 5 chapters at once (default: 3)
</code></pre>
<p><strong>Focus on Specific Areas</strong>:
Edit <code>.prodigy/book-config.json</code> to analyze only certain parts of your codebase:</p>
<pre><code class="language-json">{
  "analysis_targets": [
    {
      "area": "api",
      "source_files": ["src/api/**/*.rs"],
      "feature_categories": ["endpoints", "authentication"]
    }
  ]
}
</code></pre>
<p><strong>Change Validation Threshold</strong>:</p>
<pre><code class="language-yaml">validate:
  threshold: 95  # Allow 95% instead of 100%
</code></pre>
<h3 id="troubleshooting-17"><a class="header" href="#troubleshooting-17">Troubleshooting</a></h3>
<p><strong>Issue</strong>: “Claude command not found”</p>
<ul>
<li><strong>Solution</strong>: Ensure Claude Code CLI is installed and in your PATH</li>
<li>Verify: <code>claude --version</code></li>
</ul>
<p><strong>Issue</strong>: “features.json not generated”</p>
<ul>
<li><strong>Cause</strong>: Setup phase failed to analyze codebase</li>
<li><strong>Solution</strong>: Check that <code>source_files</code> in book-config.json exist and are valid</li>
<li>Debug: Run with <code>-vv</code> for detailed logs</li>
</ul>
<p><strong>Issue</strong>: “mdbook build fails with broken links”</p>
<ul>
<li><strong>Cause</strong>: Cross-references to non-existent chapters</li>
<li><strong>Solution</strong>: The workflow automatically fixes this in reduce phase</li>
<li>Manual fix: Check <code>book/src/SUMMARY.md</code> for invalid links</li>
</ul>
<p><strong>Issue</strong>: “Validation threshold not met”</p>
<ul>
<li><strong>Cause</strong>: Documentation doesn’t meet 100% quality standard</li>
<li><strong>Solution</strong>: The <code>on_incomplete</code> handler attempts to complete gaps (max 3 attempts)</li>
<li>If still incomplete: Review <code>.prodigy/validation-result.json</code> for details</li>
</ul>
<p><strong>Source</strong>: Common issues from features.json troubleshooting section (lines 802-885)</p>
<h3 id="next-steps-5"><a class="header" href="#next-steps-5">Next Steps</a></h3>
<p>Now that you have automated documentation working, explore:</p>
<ul>
<li><strong><a href="automated-documentation/./understanding-the-workflow.html">Understanding the Workflow</a></strong> - Deep dive into how the workflow operates</li>
<li><strong><a href="automated-documentation/./customization-examples.html">Customization Examples</a></strong> - Adapt the workflow for your project’s needs</li>
<li><strong><a href="automated-documentation/./github-actions-integration.html">GitHub Actions Integration</a></strong> - Automate documentation updates on every commit</li>
<li><strong><a href="automated-documentation/./best-practices.html">Best Practices</a></strong> - Guidelines for maintaining high-quality documentation</li>
<li><strong><a href="automated-documentation/./troubleshooting.html">Troubleshooting</a></strong> - Solutions to common problems</li>
</ul>
<h3 id="what-youve-learned"><a class="header" href="#what-youve-learned">What You’ve Learned</a></h3>
<p>✓ How to configure automated documentation for any project
✓ The structure of a MapReduce documentation workflow
✓ How setup/map/reduce phases process your codebase
✓ How to review and merge documentation updates
✓ Basic troubleshooting techniques</p>
<h3 id="time-investment-breakdown"><a class="header" href="#time-investment-breakdown">Time Investment Breakdown</a></h3>
<ul>
<li>Prerequisites setup: 5-10 minutes (one-time)</li>
<li>Configuration files: 10 minutes</li>
<li>First workflow run: 5-10 minutes (depending on project size)</li>
<li>Review and merge: 5 minutes</li>
</ul>
<p><strong>Total: 25-35 minutes</strong> for your first complete run</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="installation"><a class="header" href="#installation">Installation</a></h2>
<p>This guide covers installing Prodigy, the prerequisite tools, and optional components for automated documentation workflows.</p>
<h2 id="prerequisites-2"><a class="header" href="#prerequisites-2">Prerequisites</a></h2>
<p>Before installing Prodigy, ensure you have:</p>
<p><strong>Required:</strong></p>
<ul>
<li><strong>Claude Code CLI</strong> - Prodigy executes Claude commands via the Claude Code CLI
<ul>
<li>Install from: https://github.com/anthropics/claude-code</li>
<li>Verify: <code>claude --version</code></li>
</ul>
</li>
<li><strong>Rust 1.70+</strong> - Required for building Prodigy from source
<ul>
<li>Install from: https://rustup.rs/</li>
<li>Verify: <code>rustc --version</code></li>
<li>Source: Cargo.toml:4 (edition = “2021” requires Rust 1.56+, recommended 1.70+)</li>
</ul>
</li>
</ul>
<p><strong>Optional:</strong></p>
<ul>
<li><strong>Git 2.25+</strong> - Required for worktree management features</li>
<li><strong>jq</strong> - Useful for working with JSON outputs and DLQ inspection</li>
</ul>
<h2 id="using-cargo-recommended"><a class="header" href="#using-cargo-recommended">Using Cargo (Recommended)</a></h2>
<p>The simplest way to install Prodigy is via Cargo, Rust’s package manager:</p>
<pre><code class="language-bash">cargo install prodigy
</code></pre>
<p>This command:</p>
<ol>
<li>Downloads the latest version from crates.io</li>
<li>Compiles the binary with optimizations</li>
<li>Installs to <code>~/.cargo/bin/prodigy</code> (ensure this is in your PATH)</li>
</ol>
<p><strong>Verify installation:</strong></p>
<pre><code class="language-bash">prodigy --version
# Expected output: prodigy 0.2.7
</code></pre>
<p><strong>Source</strong>: README.md:51-55</p>
<h2 id="from-source"><a class="header" href="#from-source">From Source</a></h2>
<p>To install the latest development version or contribute to Prodigy, build from source:</p>
<pre><code class="language-bash"># Clone the repository
git clone https://github.com/iepathos/prodigy
cd prodigy

# Build and install
cargo build --release
cargo install --path .
</code></pre>
<p><strong>Build process:</strong></p>
<ol>
<li><code>cargo build --release</code> compiles with optimizations (takes 3-5 minutes)</li>
<li>Binary is created at <code>target/release/prodigy</code></li>
<li><code>cargo install --path .</code> copies binary to <code>~/.cargo/bin/</code></li>
</ol>
<p><strong>Verify installation:</strong></p>
<pre><code class="language-bash">prodigy --version
</code></pre>
<p><strong>Source</strong>: README.md:57-66</p>
<h2 id="optional-man-pages"><a class="header" href="#optional-man-pages">Optional: Man Pages</a></h2>
<p>Prodigy includes comprehensive man pages for CLI reference. Install them with:</p>
<pre><code class="language-bash">./scripts/install-man-pages.sh
</code></pre>
<p>This installs man pages to <code>/usr/local/share/man/man1/</code>. After installation:</p>
<pre><code class="language-bash"># View main Prodigy documentation
man prodigy

# View specific command documentation
man prodigy-run
man prodigy-resume
man prodigy-dlq
</code></pre>
<p><strong>Source</strong>: README.md:68-69, scripts/install-man-pages.sh</p>
<h2 id="verification"><a class="header" href="#verification">Verification</a></h2>
<p>After installation, verify Prodigy is working correctly:</p>
<p><strong>Check version:</strong></p>
<pre><code class="language-bash">prodigy --version
</code></pre>
<p><strong>View available commands:</strong></p>
<pre><code class="language-bash">prodigy --help
</code></pre>
<p><strong>Test basic functionality:</strong></p>
<pre><code class="language-bash"># Initialize Claude commands
prodigy init

# Should create .claude/commands/ directory
ls -la .claude/commands/
</code></pre>
<h2 id="troubleshooting-18"><a class="header" href="#troubleshooting-18">Troubleshooting</a></h2>
<h3 id="command-not-found-prodigy"><a class="header" href="#command-not-found-prodigy">Command not found: prodigy</a></h3>
<p><strong>Cause</strong>: <code>~/.cargo/bin</code> is not in your PATH</p>
<p><strong>Fix</strong>: Add to your shell profile (~/.bashrc, ~/.zshrc, etc.):</p>
<pre><code class="language-bash">export PATH="$HOME/.cargo/bin:$PATH"
</code></pre>
<p>Then reload: <code>source ~/.bashrc</code> (or restart terminal)</p>
<h3 id="cargo-command-not-found"><a class="header" href="#cargo-command-not-found">cargo: command not found</a></h3>
<p><strong>Cause</strong>: Rust toolchain not installed</p>
<p><strong>Fix</strong>: Install Rust via rustup:</p>
<pre><code class="language-bash">curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
</code></pre>
<h3 id="build-fails-with-linker-not-found"><a class="header" href="#build-fails-with-linker-not-found">Build fails with “linker not found”</a></h3>
<p><strong>Cause</strong>: Missing C compiler/linker (required by some Rust dependencies)</p>
<p><strong>Fix</strong>:</p>
<ul>
<li><strong>macOS</strong>: Install Xcode Command Line Tools: <code>xcode-select --install</code></li>
<li><strong>Linux</strong>: Install build essentials: <code>sudo apt-get install build-essential</code> (Debian/Ubuntu)</li>
<li><strong>Windows</strong>: Install Visual Studio Build Tools</li>
</ul>
<h3 id="permission-denied-when-installing-man-pages"><a class="header" href="#permission-denied-when-installing-man-pages">Permission denied when installing man pages</a></h3>
<p><strong>Cause</strong>: <code>/usr/local/share/man/man1/</code> requires elevated permissions</p>
<p><strong>Fix</strong>: Run with sudo:</p>
<pre><code class="language-bash">sudo ./scripts/install-man-pages.sh
</code></pre>
<h2 id="platform-specific-notes"><a class="header" href="#platform-specific-notes">Platform-Specific Notes</a></h2>
<p><strong>macOS:</strong></p>
<ul>
<li>Xcode Command Line Tools recommended for best compatibility</li>
<li>Man pages install to <code>/usr/local/share/man/man1/</code> by default</li>
<li>Homebrew users: Cargo is included with <code>brew install rust</code></li>
</ul>
<p><strong>Linux:</strong></p>
<ul>
<li>Build essentials package required for compilation</li>
<li>Man pages may require sudo for installation</li>
<li>Consider using system package manager if available</li>
</ul>
<p><strong>Windows:</strong></p>
<ul>
<li>Visual Studio Build Tools required for Rust compilation</li>
<li>Consider using WSL2 for better compatibility with git worktrees</li>
<li>Man pages not supported on Windows (use <code>prodigy --help</code> instead)</li>
</ul>
<h2 id="next-steps-6"><a class="header" href="#next-steps-6">Next Steps</a></h2>
<p>After installation, see:</p>
<ul>
<li><a href="automated-documentation/quick-start.html">Quick Start</a> - Create your first automated documentation workflow</li>
<li><a href="automated-documentation/understanding-the-workflow.html">Understanding the Workflow</a> - Learn how the automation works</li>
<li><a href="automated-documentation/index.html">index.md</a> - Overview of automated documentation features</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="quick-start-4"><a class="header" href="#quick-start-4">Quick Start</a></h2>
<p>This guide walks you through setting up your first automated documentation workflow. You’ll create a basic mdBook structure, configure Prodigy to analyze your codebase, and run the workflow to generate up-to-date documentation.</p>
<p><strong>Time Required</strong>: 15-20 minutes</p>
<p><strong>Prerequisites</strong>: Ensure you have completed the <a href="automated-documentation/installation.html">Installation</a> steps before proceeding.</p>
<hr />
<h2 id="step-1-initialize-your-book-structure-1"><a class="header" href="#step-1-initialize-your-book-structure-1">Step 1: Initialize Your Book Structure</a></h2>
<p>First, create the basic mdBook directory structure for your documentation:</p>
<pre><code class="language-bash"># Create and initialize the book
mdbook init book
cd book

# View the generated structure
ls -la
# Expected output:
# book.toml      - mdBook configuration
# src/           - Markdown source files
#   SUMMARY.md   - Book navigation/table of contents
#   chapter_1.md - Example chapter
</code></pre>
<p><strong>What this creates:</strong></p>
<ul>
<li><code>book.toml</code>: Configuration file for mdBook (title, authors, build settings)</li>
<li><code>src/SUMMARY.md</code>: Defines your book’s structure and navigation</li>
<li><code>src/chapter_1.md</code>: Example chapter (you can delete or modify this)</li>
</ul>
<p><strong>Source</strong>: book/book.toml:1-43, book/src/SUMMARY.md:1-122</p>
<hr />
<h2 id="step-2-configure-your-book"><a class="header" href="#step-2-configure-your-book">Step 2: Configure Your Book</a></h2>
<p>Edit <code>book/book.toml</code> to customize your book settings:</p>
<pre><code class="language-toml">[book]
title = "My Project Documentation"
authors = ["Your Name &lt;you@example.com&gt;"]
description = "Automated documentation for My Project"
src = "src"
language = "en"

[build]
build-dir = "book"
create-missing = false

[output.html]
default-theme = "rust"
preferred-dark-theme = "navy"
git-repository-url = "https://github.com/youruser/yourproject"
git-repository-icon = "fa-github"

[output.html.search]
enable = true
</code></pre>
<p><strong>Key settings:</strong></p>
<ul>
<li><code>title</code>: Your documentation title</li>
<li><code>git-repository-url</code>: Link to your source repository</li>
<li><code>create-missing = false</code>: Prevents mdBook from auto-creating missing chapters (Prodigy will manage this)</li>
</ul>
<p><strong>Source</strong>: book/book.toml:1-43</p>
<hr />
<h2 id="step-3-define-your-documentation-structure"><a class="header" href="#step-3-define-your-documentation-structure">Step 3: Define Your Documentation Structure</a></h2>
<p>Edit <code>book/src/SUMMARY.md</code> to define your book’s chapters:</p>
<pre><code class="language-markdown"># Summary

[Introduction](intro.md)

# User Guide

- [Getting Started](getting-started.md)
- [Configuration](configuration.md)
- [Commands](commands.md)

# Reference

- [API Reference](api-reference.md)
- [Troubleshooting](troubleshooting.md)
</code></pre>
<p><strong>Tips:</strong></p>
<ul>
<li>Start with 3-5 core chapters</li>
<li>Use simple, descriptive chapter names</li>
<li>Group related topics under section headers</li>
<li>You can add more chapters later</li>
</ul>
<p><strong>Source</strong>: book/src/SUMMARY.md:1-122</p>
<hr />
<h2 id="step-4-create-prodigy-configuration-files"><a class="header" href="#step-4-create-prodigy-configuration-files">Step 4: Create Prodigy Configuration Files</a></h2>
<p>Now create the configuration files that tell Prodigy how to analyze your codebase.</p>
<h3 id="4a-create-book-configuration"><a class="header" href="#4a-create-book-configuration">4a. Create Book Configuration</a></h3>
<p>Create <code>.prodigy/book-config.json</code>:</p>
<pre><code class="language-bash">mkdir -p .prodigy
cat &gt; .prodigy/book-config.json &lt;&lt; 'EOF'
{
  "project_name": "MyProject",
  "project_type": "cli_tool",
  "book_dir": "book",
  "book_src": "book/src",
  "book_build_dir": "book/book",
  "analysis_targets": [
    {
      "area": "getting_started",
      "source_files": [
        "README.md",
        "examples/"
      ],
      "feature_categories": [
        "installation",
        "basic_usage",
        "first_steps"
      ]
    },
    {
      "area": "configuration",
      "source_files": [
        "src/config/"
      ],
      "feature_categories": [
        "config_files",
        "settings",
        "environment"
      ]
    }
  ],
  "chapter_file": "workflows/data/book-chapters.json",
  "custom_analysis": {
    "include_examples": true,
    "include_best_practices": true,
    "include_troubleshooting": true
  }
}
EOF
</code></pre>
<p><strong>Configuration Explained:</strong></p>
<ul>
<li><code>analysis_targets</code>: Defines which source files to analyze and what features to extract</li>
<li><code>area</code>: Name for this analysis area (maps to documentation chapters)</li>
<li><code>source_files</code>: Paths to analyze (can be files or directories)</li>
<li><code>feature_categories</code>: What types of features to document</li>
</ul>
<p><strong>Source</strong>: .prodigy/book-config.json:1-220</p>
<h3 id="4b-create-chapter-definitions"><a class="header" href="#4b-create-chapter-definitions">4b. Create Chapter Definitions</a></h3>
<p>Create <code>workflows/data/book-chapters.json</code>:</p>
<pre><code class="language-bash">mkdir -p workflows/data
cat &gt; workflows/data/book-chapters.json &lt;&lt; 'EOF'
{
  "chapters": [
    {
      "id": "intro",
      "title": "Introduction",
      "type": "single-file",
      "file": "book/src/intro.md",
      "topics": ["Project Overview", "Goals"],
      "validation": "Check introduction explains project purpose and value"
    },
    {
      "id": "getting-started",
      "title": "Getting Started",
      "type": "single-file",
      "file": "book/src/getting-started.md",
      "topics": ["Installation", "First Steps", "Quick Example"],
      "validation": "Verify installation steps and first example work"
    },
    {
      "id": "configuration",
      "title": "Configuration",
      "type": "single-file",
      "file": "book/src/configuration.md",
      "topics": ["Config Files", "Settings", "Environment"],
      "validation": "Check all config options are documented"
    }
  ]
}
EOF
</code></pre>
<p><strong>Chapter Definition Explained:</strong></p>
<ul>
<li><code>id</code>: Unique identifier for the chapter</li>
<li><code>type</code>: “single-file” (one markdown file) or “multi-subsection” (chapter with subsections)</li>
<li><code>file</code>: Path to the markdown file</li>
<li><code>topics</code>: What this chapter should cover</li>
<li><code>validation</code>: Instructions for verifying documentation quality</li>
</ul>
<p><strong>Source</strong>: workflows/data/prodigy-chapters.json:1-1262</p>
<hr />
<h2 id="step-5-create-the-documentation-workflow"><a class="header" href="#step-5-create-the-documentation-workflow">Step 5: Create the Documentation Workflow</a></h2>
<p>Create <code>workflows/book-docs-drift.yml</code>:</p>
<pre><code class="language-bash">mkdir -p workflows
cat &gt; workflows/book-docs-drift.yml &lt;&lt; 'EOF'
name: book-docs-drift-detection
mode: mapreduce

# Environment variables
env:
  PROJECT_NAME: "MyProject"
  PROJECT_CONFIG: ".prodigy/book-config.json"
  FEATURES_PATH: ".prodigy/book-analysis/features.json"
  BOOK_DIR: "book"
  ANALYSIS_DIR: ".prodigy/book-analysis"
  CHAPTERS_FILE: "workflows/data/book-chapters.json"
  MAX_PARALLEL: "3"

# Setup phase: Analyze codebase
setup:
  - shell: "mkdir -p $ANALYSIS_DIR"
  - claude: "/prodigy-analyze-features-for-book --project $PROJECT_NAME --config $PROJECT_CONFIG"
  - claude: "/prodigy-detect-documentation-gaps --project $PROJECT_NAME --config $PROJECT_CONFIG --features $FEATURES_PATH --chapters $CHAPTERS_FILE --book-dir $BOOK_DIR"

# Map phase: Fix each chapter in parallel
map:
  input: "${ANALYSIS_DIR}/flattened-items.json"
  json_path: "$[*]"

  agent_template:
    - claude: "/prodigy-analyze-subsection-drift --project $PROJECT_NAME --json '${item}' --features $FEATURES_PATH"
      commit_required: true
    - claude: "/prodigy-fix-subsection-drift --project $PROJECT_NAME --json '${item}'"
      commit_required: true

  max_parallel: ${MAX_PARALLEL}

# Reduce phase: Build and validate
reduce:
  - shell: "cd book &amp;&amp; mdbook build"
    on_failure:
      claude: "/prodigy-fix-book-build-errors --project $PROJECT_NAME"
      commit_required: true
  - shell: "rm -rf ${ANALYSIS_DIR}"
  - shell: "git add -A &amp;&amp; git commit -m 'chore: clean up analysis files' || true"

# Error handling
error_policy:
  on_item_failure: dlq
  continue_on_failure: true

# Merge workflow
merge:
  commands:
    - shell: "git fetch origin"
    - claude: "/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}"
EOF
</code></pre>
<p><strong>Workflow Structure:</strong></p>
<ul>
<li><strong>Setup Phase</strong>: Analyzes your codebase and detects documentation gaps</li>
<li><strong>Map Phase</strong>: Processes each chapter in parallel to fix drift</li>
<li><strong>Reduce Phase</strong>: Builds the complete book and validates</li>
<li><strong>Merge Phase</strong>: Integrates changes back to your branch</li>
</ul>
<p><strong>Source</strong>: workflows/book-docs-drift.yml:1-101</p>
<hr />
<h2 id="step-6-initialize-prodigy-commands"><a class="header" href="#step-6-initialize-prodigy-commands">Step 6: Initialize Prodigy Commands</a></h2>
<p>Initialize the Claude commands that Prodigy uses for documentation generation:</p>
<pre><code class="language-bash">prodigy init
</code></pre>
<p>This creates the <code>.claude/commands/</code> directory with commands like:</p>
<ul>
<li><code>/prodigy-analyze-features-for-book</code> - Analyzes codebase for features</li>
<li><code>/prodigy-detect-documentation-gaps</code> - Finds missing documentation</li>
<li><code>/prodigy-fix-subsection-drift</code> - Fixes outdated documentation</li>
<li>And others needed by the workflow</li>
</ul>
<p><strong>Verify commands were created:</strong></p>
<pre><code class="language-bash">ls -la .claude/commands/ | grep "prodigy-.*book\|doc\|gap"
</code></pre>
<p><strong>Source</strong>: .claude/commands/prodigy-analyze-features-for-book.md:1-80, .claude/commands/prodigy-detect-documentation-gaps.md:1-80</p>
<hr />
<h2 id="step-7-run-your-first-documentation-workflow"><a class="header" href="#step-7-run-your-first-documentation-workflow">Step 7: Run Your First Documentation Workflow</a></h2>
<p>Now run the workflow to generate your documentation:</p>
<pre><code class="language-bash">prodigy run workflows/book-docs-drift.yml
</code></pre>
<p><strong>What happens:</strong></p>
<ol>
<li>
<p><strong>Setup Phase</strong> (~2-5 minutes):</p>
<ul>
<li>Analyzes your source code</li>
<li>Builds feature inventory</li>
<li>Detects documentation gaps</li>
<li>Creates stub files for missing chapters</li>
</ul>
</li>
<li>
<p><strong>Map Phase</strong> (~5-10 minutes):</p>
<ul>
<li>Processes each chapter in parallel</li>
<li>Fixes documentation drift</li>
<li>Adds code examples from your source</li>
<li>Validates quality</li>
</ul>
</li>
<li>
<p><strong>Reduce Phase</strong> (~1-2 minutes):</p>
<ul>
<li>Builds complete book with <code>mdbook build</code></li>
<li>Validates all links work</li>
<li>Cleans up temporary files</li>
</ul>
</li>
<li>
<p><strong>Merge Prompt</strong>:</p>
<ul>
<li>Asks if you want to merge changes to your branch</li>
<li>Type <code>y</code> to accept, <code>n</code> to review first</li>
</ul>
</li>
</ol>
<p><strong>Example output:</strong></p>
<pre><code>🔧 Setup Phase
✓ Created .prodigy/book-analysis/
✓ Analyzed codebase features
✓ Detected 3 documentation gaps

🗺️  Map Phase (3 parallel agents)
✓ Fixed getting-started.md
✓ Fixed configuration.md
✓ Fixed api-reference.md

🔻 Reduce Phase
✓ Built book successfully
✓ Cleaned up analysis files

📊 Summary:
   3/3 chapters updated
   12 commits created

Merge to main? [y/N]
</code></pre>
<hr />
<h2 id="step-8-review-and-build-your-documentation"><a class="header" href="#step-8-review-and-build-your-documentation">Step 8: Review and Build Your Documentation</a></h2>
<p>After the workflow completes, review the generated documentation:</p>
<pre><code class="language-bash"># View the built book locally
cd book
mdbook serve

# Open in browser: http://localhost:3000
</code></pre>
<p><strong>What to review:</strong></p>
<ul>
<li>Check that all chapters have content</li>
<li>Verify code examples are accurate</li>
<li>Ensure links between chapters work</li>
<li>Validate examples match your codebase</li>
</ul>
<p><strong>If you need to make changes:</strong></p>
<pre><code class="language-bash"># Edit any chapter
vim book/src/getting-started.md

# Rebuild the book
mdbook build

# Or use watch mode for live reload
mdbook serve
</code></pre>
<hr />
<h2 id="next-steps-7"><a class="header" href="#next-steps-7">Next Steps</a></h2>
<p>Congratulations! You’ve created your first automated documentation workflow. Here’s what to do next:</p>
<h3 id="keep-documentation-updated"><a class="header" href="#keep-documentation-updated">Keep Documentation Updated</a></h3>
<p>Run the workflow regularly to keep docs in sync with code:</p>
<pre><code class="language-bash"># After adding new features
prodigy run workflows/book-docs-drift.yml

# Schedule in CI/CD (see GitHub Actions Integration)
</code></pre>
<h3 id="expand-your-documentation"><a class="header" href="#expand-your-documentation">Expand Your Documentation</a></h3>
<p>Add more chapters to <code>workflows/data/book-chapters.json</code>:</p>
<ul>
<li>Add new <code>analysis_targets</code> in <code>.prodigy/book-config.json</code></li>
<li>Define new chapters in <code>book-chapters.json</code></li>
<li>Update <code>book/src/SUMMARY.md</code> with new chapters</li>
<li>Re-run the workflow</li>
</ul>
<h3 id="customize-the-workflow"><a class="header" href="#customize-the-workflow">Customize the Workflow</a></h3>
<ul>
<li>Adjust <code>MAX_PARALLEL</code> for faster/slower processing</li>
<li>Add validation steps in the reduce phase</li>
<li>Customize error handling with <code>error_policy</code></li>
<li>See <a href="automated-documentation/customization-examples.html">Customization Examples</a></li>
</ul>
<h3 id="integrate-with-cicd"><a class="header" href="#integrate-with-cicd">Integrate with CI/CD</a></h3>
<p>Automate documentation updates in your CI/CD pipeline:</p>
<ul>
<li>See <a href="automated-documentation/github-actions-integration.html">GitHub Actions Integration</a></li>
<li>Run on every PR or nightly</li>
<li>Deploy to GitHub Pages or docs hosting</li>
</ul>
<hr />
<h2 id="troubleshooting-19"><a class="header" href="#troubleshooting-19">Troubleshooting</a></h2>
<h3 id="workflow-fails-in-setup-phase"><a class="header" href="#workflow-fails-in-setup-phase">Workflow fails in setup phase</a></h3>
<p><strong>Symptoms</strong>: Error during feature analysis</p>
<p><strong>Fixes</strong>:</p>
<ul>
<li>Verify <code>analysis_targets</code> in <code>.prodigy/book-config.json</code> point to existing files</li>
<li>Check that source files exist and are readable</li>
<li>Ensure Claude Code CLI is authenticated</li>
</ul>
<h3 id="mdbook-build-fails"><a class="header" href="#mdbook-build-fails">mdBook build fails</a></h3>
<p><strong>Symptoms</strong>: Error in reduce phase when building book</p>
<p><strong>Fixes</strong>:</p>
<ul>
<li>Verify <code>book/book.toml</code> is valid TOML</li>
<li>Check <code>book/src/SUMMARY.md</code> references only existing files</li>
<li>Ensure all linked chapters exist</li>
<li>Run <code>cd book &amp;&amp; mdbook build</code> manually to see detailed error</li>
</ul>
<h3 id="no-chapters-were-updated"><a class="header" href="#no-chapters-were-updated">No chapters were updated</a></h3>
<p><strong>Symptoms</strong>: Workflow completes but no changes made</p>
<p><strong>Fixes</strong>:</p>
<ul>
<li>Check that chapters in <code>book-chapters.json</code> exist in your repo</li>
<li>Verify <code>analysis_targets</code> match your project structure</li>
<li>Ensure chapters actually need updates (no drift = no changes)</li>
</ul>
<h3 id="agent-failures-in-map-phase"><a class="header" href="#agent-failures-in-map-phase">Agent failures in map phase</a></h3>
<p><strong>Symptoms</strong>: Some chapters fail to update, sent to DLQ</p>
<p><strong>Fixes</strong>:</p>
<ul>
<li>Review DLQ: <code>prodigy dlq show &lt;job_id&gt;</code></li>
<li>Check Claude JSON logs for detailed errors</li>
<li>Retry failed items: <code>prodigy dlq retry &lt;job_id&gt;</code></li>
<li>See <a href="automated-documentation/troubleshooting.html">Troubleshooting</a> for common issues</li>
</ul>
<hr />
<h2 id="summary-2"><a class="header" href="#summary-2">Summary</a></h2>
<p>You’ve learned how to:</p>
<ul>
<li>✅ Initialize an mdBook structure</li>
<li>✅ Configure Prodigy to analyze your codebase</li>
<li>✅ Define documentation chapters and structure</li>
<li>✅ Create a MapReduce workflow for documentation</li>
<li>✅ Run the workflow to generate docs automatically</li>
<li>✅ Review and build your documentation</li>
</ul>
<p>Your documentation is now linked to your code and can be kept up-to-date automatically!</p>
<hr />
<h2 id="related-topics-9"><a class="header" href="#related-topics-9">Related Topics</a></h2>
<ul>
<li><a href="automated-documentation/installation.html">Installation</a> - Installing prerequisites and Prodigy</li>
<li><a href="automated-documentation/understanding-the-workflow.html">Understanding the Workflow</a> - Deep dive into how it works</li>
<li><a href="automated-documentation/customization-examples.html">Customization Examples</a> - Advanced configuration options</li>
<li><a href="automated-documentation/github-actions-integration.html">GitHub Actions Integration</a> - Automate in CI/CD</li>
<li><a href="automated-documentation/best-practices.html">Best Practices</a> - Tips for maintaining great documentation</li>
<li><a href="automated-documentation/troubleshooting.html">Troubleshooting</a> - Solutions to common issues</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="understanding-the-workflow"><a class="header" href="#understanding-the-workflow">Understanding the Workflow</a></h2>
<p>The automated documentation workflow uses a MapReduce architecture to process documentation in parallel, ensuring efficiency and isolation. This section explains how the workflow phases execute, how worktrees provide isolation, and how the system ensures quality.</p>
<h3 id="four-phase-execution-model"><a class="header" href="#four-phase-execution-model">Four-Phase Execution Model</a></h3>
<p>The documentation workflow consists of four sequential phases, each with a specific responsibility:</p>
<h4 id="1-setup-phase-feature-analysis"><a class="header" href="#1-setup-phase-feature-analysis">1. Setup Phase (Feature Analysis)</a></h4>
<p><strong>Purpose</strong>: Analyze the codebase and prepare work items for parallel processing.</p>
<p><strong>What it does</strong>:</p>
<ul>
<li>Scans source code to build a complete feature inventory</li>
<li>Compares existing documentation against implementation to detect gaps</li>
<li>Creates missing chapter and subsection placeholder files</li>
<li>Generates a JSON file containing work items for the map phase</li>
</ul>
<p><strong>Source</strong>: <code>workflows/book-docs-drift.yml:24-34</code></p>
<p><strong>Example output</strong>: <code>.prodigy/book-analysis/doc-items.json</code> containing:</p>
<pre><code class="language-json">[
  {
    "type": "subsection",
    "id": "checkpoint-and-resume",
    "parent_chapter_id": "mapreduce",
    "file": "book/src/mapreduce/checkpoint-and-resume.md",
    "feature_mapping": ["mapreduce.checkpoint", "mapreduce.resume"],
    "topics": ["Checkpoint behavior", "Resume strategies"]
  }
]
</code></pre>
<h4 id="2-map-phase-parallel-processing"><a class="header" href="#2-map-phase-parallel-processing">2. Map Phase (Parallel Processing)</a></h4>
<p><strong>Purpose</strong>: Process each documentation item concurrently in isolated environments.</p>
<p><strong>What it does</strong>:</p>
<ul>
<li>Distributes work items across multiple parallel agents (default: 3 concurrent)</li>
<li>Each agent runs in an isolated git worktree (child of the parent worktree)</li>
<li>For each documentation item:
<ul>
<li>Analyzes drift between documentation and implementation</li>
<li>Searches codebase for real examples and type definitions</li>
<li>Fixes identified issues with validated, code-grounded content</li>
<li>Commits changes to the agent’s worktree</li>
</ul>
</li>
<li>Failed items automatically added to Dead Letter Queue (DLQ) for retry</li>
<li>Successful agents merge their changes back to the parent worktree</li>
</ul>
<p><strong>Source</strong>: <code>workflows/book-docs-drift.yml:37-59</code></p>
<p><strong>Parallel execution control</strong> (from <code>src/cook/execution/mapreduce/coordination/executor.rs:617-824</code>):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Create semaphore to control concurrency
let semaphore = Arc::new(Semaphore::new(max_parallel));

// Spawn async task per work item
let agent_futures: Vec&lt;_&gt; = work_items
    .into_iter()
    .map(|(item)| {
        tokio::spawn(async move {
            let _permit = sem.acquire().await?;  // Wait for slot
            execute_agent_for_item(...).await
        })
    })
    .collect();
<span class="boring">}</span></code></pre></pre>
<h4 id="3-reduce-phase-validation"><a class="header" href="#3-reduce-phase-validation">3. Reduce Phase (Validation)</a></h4>
<p><strong>Purpose</strong>: Validate that all documentation changes work together correctly.</p>
<p><strong>What it does</strong>:</p>
<ul>
<li>Rebuilds the entire book using <code>mdbook build</code></li>
<li>Checks for broken links between chapters and subsections</li>
<li>Detects any compilation errors or missing references</li>
<li>Fixes build errors if found (using Claude)</li>
<li>Cleans up temporary analysis files</li>
</ul>
<p><strong>Source</strong>: <code>workflows/book-docs-drift.yml:62-82</code></p>
<p><strong>Variable context</strong> (from <code>src/cook/execution/mapreduce/aggregation/mod.rs:1637-1659</code>):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Reduce phase has access to map results
context.set("map.successful", summary.successful);
context.set("map.failed", summary.failed);
context.set("map.total", summary.total);
context.set("map.results", results_value);  // Full agent results
<span class="boring">}</span></code></pre></pre>
<h4 id="4-merge-phase-integration"><a class="header" href="#4-merge-phase-integration">4. Merge Phase (Integration)</a></h4>
<p><strong>Purpose</strong>: Integrate validated changes back to your original branch.</p>
<p><strong>What it does</strong>:</p>
<ul>
<li>Prompts user for confirmation to merge</li>
<li>Merges parent worktree changes to the branch you started from</li>
<li>Uses Claude to resolve any merge conflicts</li>
<li>Preserves your working tree state throughout</li>
</ul>
<p><strong>Source</strong>: <code>workflows/book-docs-drift.yml:93-100</code></p>
<blockquote>
<p><strong>Note</strong>: All workflow phases execute in isolated git worktrees using a parent/child architecture. A single parent worktree hosts setup, reduce, and merge phases, while each map agent runs in a child worktree branched from the parent. Agents automatically merge back to the parent upon completion. The parent is merged to the original branch only with user confirmation at the end. This isolation ensures the main repository remains untouched during execution (Spec 127).</p>
</blockquote>
<h3 id="worktree-architecture"><a class="header" href="#worktree-architecture">Worktree Architecture</a></h3>
<p>The workflow uses a sophisticated parent/child worktree hierarchy to achieve complete isolation:</p>
<pre><code>Your Original Branch (e.g., main, develop, feature/docs)
    ↓
Parent Worktree (session-abc123)
    ├─ Setup Phase runs here
    ├─ Setup generates work items
    │
    ├─ Each map agent gets child worktree
    │  ├─ Agent-1 Worktree → processes item-1 → commits
    │  ├─ Agent-2 Worktree → processes item-2 → commits
    │  └─ Agent-N Worktree → processes item-N → commits
    │
    ├─ Agent changes merge back to parent (serially via MergeQueue)
    ├─ Reduce Phase runs here with aggregated results
    └─ User confirms → merge parent to original branch
</code></pre>
<p><strong>Source</strong>: <code>src/worktree/manager.rs</code>, <code>src/cook/execution/mapreduce/resources/worktree.rs</code></p>
<h4 id="isolation-guarantees"><a class="header" href="#isolation-guarantees">Isolation Guarantees</a></h4>
<ol>
<li>
<p><strong>Setup Phase Isolation</strong>:</p>
<ul>
<li>Executes in parent worktree</li>
<li>All file modifications occur in worktree, not main repo</li>
<li>Git commits created in worktree context</li>
</ul>
</li>
<li>
<p><strong>Map Phase Isolation</strong>:</p>
<ul>
<li>Each agent runs in its own child worktree branched from parent</li>
<li>No cross-contamination between agents</li>
<li>Independent failure isolation (agent failures don’t affect siblings)</li>
<li>Automatic merge back to parent worktree after success</li>
</ul>
</li>
<li>
<p><strong>Reduce Phase Isolation</strong>:</p>
<ul>
<li>Executes in parent worktree with aggregated agent results</li>
<li>Continues isolation guarantee from setup</li>
</ul>
</li>
<li>
<p><strong>Final Merge</strong>:</p>
<ul>
<li>User confirmation required before merging to original branch</li>
<li>Main repository never modified until user approves</li>
<li>Custom merge workflows supported (Spec 117)</li>
</ul>
</li>
</ol>
<p><strong>Branch tracking</strong> (Spec 110): The parent worktree tracks whatever branch you were on when you started the workflow. This is stored as <code>original_branch</code> in the worktree state, ensuring the final merge targets the correct branch.</p>
<h3 id="execution-model"><a class="header" href="#execution-model">Execution Model</a></h3>
<p>The MapReduce coordinator orchestrates parallel execution with precise control over resources and failures.</p>
<p><strong>Source</strong>: <code>src/cook/execution/mapreduce/coordination/executor.rs:200-269</code></p>
<h4 id="work-item-distribution"><a class="header" href="#work-item-distribution">Work Item Distribution</a></h4>
<p>Work items are loaded from the setup phase output and distributed to agents:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub async fn execute_map_phase_internal(
    &amp;self,
    map_phase: MapPhase,
    work_items: Vec&lt;Value&gt;,
    env: &amp;ExecutionEnvironment,
) -&gt; MapReduceResult&lt;Vec&lt;AgentResult&gt;&gt; {
    let total_items = work_items.len();
    let max_parallel = map_phase.config.max_parallel.min(total_items);

    // Create semaphore to control concurrency
    let semaphore = Arc::new(Semaphore::new(max_parallel));

    // Process items in parallel...
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Key features</strong>:</p>
<ul>
<li>Semaphore-based concurrency control prevents resource exhaustion</li>
<li>Tokio async tasks enable efficient parallel execution</li>
<li>Automatic retry count tracking for failed items</li>
<li>DLQ integration for failure recovery</li>
</ul>
<h4 id="agent-lifecycle"><a class="header" href="#agent-lifecycle">Agent Lifecycle</a></h4>
<p>Each agent follows a strict lifecycle managed by the <code>AgentLifecycleManager</code>:</p>
<p><strong>Source</strong>: <code>src/cook/execution/mapreduce/agent/lifecycle.rs</code></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[async_trait]
pub trait AgentLifecycleManager: Send + Sync {
    async fn create_agent(&amp;self, config: AgentConfig, commands: Vec&lt;WorkflowStep&gt;)
        -&gt; LifecycleResult&lt;AgentHandle&gt;;

    async fn cleanup_agent(&amp;self, handle: AgentHandle)
        -&gt; LifecycleResult&lt;()&gt;;

    async fn merge_agent_to_parent(&amp;self, agent_branch: &amp;str, env: &amp;ExecutionEnvironment)
        -&gt; LifecycleResult&lt;()&gt;;
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Lifecycle stages</strong>:</p>
<ol>
<li><strong>Create</strong>: Agent worktree created from parent worktree</li>
<li><strong>Execute</strong>: Commands run in agent worktree with variable interpolation</li>
<li><strong>Merge</strong>: Successful agents merge to parent via <code>MergeQueue</code> (serial)</li>
<li><strong>Cleanup</strong>: Worktree removed (failures tracked in orphaned registry, Spec 136)</li>
</ol>
<h4 id="result-collection-and-aggregation"><a class="header" href="#result-collection-and-aggregation">Result Collection and Aggregation</a></h4>
<p>Agent results are collected and aggregated for the reduce phase:</p>
<p><strong>Source</strong>: <code>src/cook/execution/mapreduce/agent/results.rs:44-79</code></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct AgentResult {
    pub item_id: String,
    pub status: AgentStatus,
    pub output: Option&lt;String&gt;,
    pub commits: Vec&lt;String&gt;,
    pub files_modified: Vec&lt;String&gt;,
    pub duration: Duration,
    pub error: Option&lt;String&gt;,
    pub worktree_path: Option&lt;PathBuf&gt;,
    pub branch_name: Option&lt;String&gt;,
    pub json_log_location: Option&lt;String&gt;,
    pub cleanup_status: Option&lt;CleanupStatus&gt;,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Aggregation summary</strong> (<code>src/cook/execution/mapreduce/aggregation/mod.rs</code>):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct AggregationSummary {
    pub successful: usize,
    pub failed: usize,
    pub total: usize,
    pub avg_duration_secs: f64,
    pub total_duration_secs: f64,
}
<span class="boring">}</span></code></pre></pre>
<h4 id="error-handling-and-recovery"><a class="header" href="#error-handling-and-recovery">Error Handling and Recovery</a></h4>
<p><strong>Dead Letter Queue (DLQ)</strong>: Failed work items are automatically added to the DLQ for later retry.</p>
<p><strong>Orphaned Worktree Tracking</strong> (Spec 136): If agent cleanup fails, the worktree path is registered as orphaned. Use <code>prodigy worktree clean-orphaned &lt;job_id&gt;</code> to clean up after resolving issues.</p>
<p><strong>On-Failure Handlers</strong>: Workflows can define custom error recovery commands that execute when agents fail.</p>
<p><strong>Source</strong>: <code>src/cook/execution/mapreduce/coordination/executor.rs:1486-1591</code></p>
<h3 id="quality-guarantees-1"><a class="header" href="#quality-guarantees-1">Quality Guarantees</a></h3>
<p>The workflow ensures documentation quality through multiple validation mechanisms:</p>
<h4 id="code-grounded-examples"><a class="header" href="#code-grounded-examples">Code-Grounded Examples</a></h4>
<p><strong>All examples are extracted from actual implementation</strong>:</p>
<ul>
<li>Type definitions verified from source files</li>
<li>Field names match struct/class definitions exactly</li>
<li>Enum variants validated against source code</li>
<li>CLI syntax verified from argument parser definitions</li>
<li>Examples include source file references (e.g., <code>src/config/retry.rs:45</code>)</li>
</ul>
<p><strong>Source</strong>: Documented in parent chapter at <code>index.md:126-134</code></p>
<h4 id="validation-checkpoints"><a class="header" href="#validation-checkpoints">Validation Checkpoints</a></h4>
<p><strong>Each phase includes validation</strong>:</p>
<ul>
<li>Setup: Verifies feature inventory is complete</li>
<li>Map: Each agent validates fixes meet minimum content requirements</li>
<li>Reduce: Full book build ensures no broken references</li>
<li>Merge: User confirmation before final integration</li>
</ul>
<h4 id="build-verification"><a class="header" href="#build-verification">Build Verification</a></h4>
<p>The reduce phase rebuilds the entire book:</p>
<pre><code class="language-yaml">reduce:
  - shell: "mdbook build book"
  - claude: "/fix-build-errors"
</code></pre>
<p>This catches:</p>
<ul>
<li>Broken links between chapters</li>
<li>Missing cross-references</li>
<li>Invalid markdown syntax</li>
<li>Compilation errors</li>
</ul>
<h4 id="automatic-retry"><a class="header" href="#automatic-retry">Automatic Retry</a></h4>
<p>Failed items can be retried with preserved context:</p>
<pre><code class="language-bash"># Retry all failed items for a job
prodigy dlq retry &lt;job_id&gt;

# Retry with custom parallelism
prodigy dlq retry &lt;job_id&gt; --max-parallel 5
</code></pre>
<p><strong>Source</strong>: DLQ retry implementation in <code>src/cook/execution/mapreduce/dlq/</code></p>
<h3 id="variable-interpolation-5"><a class="header" href="#variable-interpolation-5">Variable Interpolation</a></h3>
<p>The workflow uses variable interpolation to pass data between phases:</p>
<h4 id="setup-phase-variables"><a class="header" href="#setup-phase-variables">Setup Phase Variables</a></h4>
<ul>
<li>Output captured to files (e.g., <code>doc-items.json</code>)</li>
<li>Variables available to map phase</li>
</ul>
<h4 id="map-phase-variables"><a class="header" href="#map-phase-variables">Map Phase Variables</a></h4>
<ul>
<li><code>${item.field}</code> - Access work item fields</li>
<li><code>${item.id}</code> - Work item identifier</li>
<li><code>${item.file}</code> - Documentation file path</li>
<li>Environment variables from workflow <code>env:</code> block</li>
</ul>
<p><strong>Source</strong>: <code>src/cook/execution/mapreduce/coordination/executor.rs:1132-1158</code></p>
<h4 id="reduce-phase-variables"><a class="header" href="#reduce-phase-variables">Reduce Phase Variables</a></h4>
<ul>
<li><code>${map.successful}</code> - Count of successful agents</li>
<li><code>${map.failed}</code> - Count of failed agents</li>
<li><code>${map.total}</code> - Total work items</li>
<li><code>${map.results}</code> - Full agent results as JSON</li>
</ul>
<p><strong>Source</strong>: <code>src/cook/execution/mapreduce/aggregation/mod.rs:1637-1659</code></p>
<h3 id="performance-characteristics"><a class="header" href="#performance-characteristics">Performance Characteristics</a></h3>
<p><strong>Parallel execution</strong>: The map phase processes multiple items concurrently:</p>
<ul>
<li>Default: 3 concurrent agents</li>
<li>Configurable via <code>max_parallel</code> in workflow YAML</li>
<li>Optimal parallelism calculated based on work item count</li>
</ul>
<p><strong>Resource management</strong>:</p>
<ul>
<li>Semaphore prevents resource exhaustion</li>
<li>Each agent isolated in separate worktree</li>
<li>Serial merge queue prevents conflicts</li>
</ul>
<p><strong>Scalability</strong>: Successfully processes large documentation sets (tested with 50+ chapters).</p>
<h2 id="see-also-35"><a class="header" href="#see-also-35">See Also</a></h2>
<ul>
<li><a href="automated-documentation/quick-start-30-minutes.html">Quick Start (30 Minutes)</a> - Get started with automated documentation</li>
<li><a href="automated-documentation/installation.html">Installation</a> - Set up the workflow</li>
<li><a href="automated-documentation/automatic-gap-detection.html">Automatic Gap Detection</a> - How the workflow finds missing documentation</li>
<li><a href="automated-documentation/advanced-configuration.html">Advanced Configuration</a> - Customize workflow behavior</li>
<li><a href="automated-documentation/troubleshooting.html">Troubleshooting</a> - Common issues and solutions</li>
<li><a href="automated-documentation/../../mapreduce/index.html">../../mapreduce/index.md</a> - General MapReduce workflows (not documentation-specific)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="automatic-gap-detection"><a class="header" href="#automatic-gap-detection">Automatic Gap Detection</a></h2>
<p>Automatic gap detection is a critical component of Prodigy’s documentation workflow that identifies undocumented features and automatically creates chapter/subsection definitions with stub markdown files. This ensures comprehensive documentation coverage and prevents features from being implemented without corresponding user guidance.</p>
<p><strong>Source</strong>: Implemented in <code>.claude/commands/prodigy-detect-documentation-gaps.md:1-1048</code> and tested in <code>tests/documentation_gap_detection_test.rs:1-678</code></p>
<h2 id="overview-13"><a class="header" href="#overview-13">Overview</a></h2>
<p>Gap detection runs in the <strong>setup phase</strong> of the book workflow (workflows/book-docs-drift.yml:31-34) and performs several key functions:</p>
<ol>
<li><strong>Analyzes</strong> features.json (from feature analysis) against existing chapters/subsections</li>
<li><strong>Classifies</strong> gaps by severity (high, medium, low)</li>
<li><strong>Validates</strong> content sufficiency before creating subsections (Step 0)</li>
<li><strong>Syncs</strong> chapters.json with actual file structure (Phase 7.5)</li>
<li><strong>Creates</strong> missing chapter definitions and stub markdown files</li>
<li><strong>Updates</strong> SUMMARY.md with proper hierarchy</li>
<li><strong>Generates</strong> flattened-items.json for the map phase (mandatory)</li>
</ol>
<p>The gap detection process ensures that:</p>
<ul>
<li>Features aren’t documented without sufficient codebase material (prevents stub subsections)</li>
<li>Multi-subsection chapter structures are accurately reflected in chapters.json</li>
<li>The map phase receives a complete, flat list of all chapters and subsections to process</li>
<li>Documentation organization matches implementation reality</li>
</ul>
<h2 id="command-usage"><a class="header" href="#command-usage">Command Usage</a></h2>
<p><strong>Command</strong>: <code>/prodigy-detect-documentation-gaps</code></p>
<p><strong>Parameters</strong> (.claude/commands/prodigy-detect-documentation-gaps.md:5-11):</p>
<pre><code class="language-bash">/prodigy-detect-documentation-gaps \
  --project "Prodigy" \
  --config ".prodigy/book-config.json" \
  --features ".prodigy/book-analysis/features.json" \
  --chapters "workflows/data/prodigy-chapters.json" \
  --book-dir "book"
</code></pre>
<p><strong>Workflow Integration</strong> (workflows/book-docs-drift.yml:31-34):</p>
<pre><code class="language-yaml">setup:
  # Step 1: Analyze features
  - claude: "/prodigy-analyze-features-for-book --project $PROJECT_NAME --config $PROJECT_CONFIG"

  # Step 2: Detect gaps and generate flattened-items.json
  - claude: "/prodigy-detect-documentation-gaps \
      --project $PROJECT_NAME \
      --config $PROJECT_CONFIG \
      --features $FEATURES_PATH \
      --chapters $CHAPTERS_FILE \
      --book-dir $BOOK_DIR"
</code></pre>
<h2 id="gap-severity-classification"><a class="header" href="#gap-severity-classification">Gap Severity Classification</a></h2>
<p>Gap detection classifies documentation gaps into three severity levels based on feature importance and documentation completeness (.claude/commands/prodigy-detect-documentation-gaps.md:66-112):</p>
<h3 id="high-severity-missing-chaptersubsection"><a class="header" href="#high-severity-missing-chaptersubsection">High Severity (Missing Chapter/Subsection)</a></h3>
<p><strong>Criteria</strong>:</p>
<ul>
<li>Feature area exists in features.json</li>
<li>NO corresponding chapter OR subsection found</li>
<li>Major user-facing capability with no guidance</li>
</ul>
<p><strong>Example</strong>:</p>
<pre><code class="language-json">{
  "severity": "high",
  "type": "missing_chapter",
  "feature_category": "agent_merge",
  "feature_description": "Custom merge workflows for map agents",
  "recommended_chapter_id": "agent-merge-workflows",
  "recommended_title": "Agent Merge Workflows"
}
</code></pre>
<p><strong>Action</strong>: Create new chapter definition with stub markdown file</p>
<h3 id="medium-severity-incomplete-chaptersubsection"><a class="header" href="#medium-severity-incomplete-chaptersubsection">Medium Severity (Incomplete Chapter/Subsection)</a></h3>
<p><strong>Criteria</strong>:</p>
<ul>
<li>Chapter or multi-subsection structure exists for feature area</li>
<li>But specific sub-capabilities are missing</li>
<li>Could be addressed by adding subsection or expanding content</li>
</ul>
<p><strong>Example</strong>:</p>
<ul>
<li>“mapreduce” chapter exists but missing “performance_tuning” subsection</li>
</ul>
<p><strong>Action</strong>: Create subsection definition and add to existing multi-subsection chapter</p>
<h3 id="low-severity-minor-gap"><a class="header" href="#low-severity-minor-gap">Low Severity (Minor Gap)</a></h3>
<p><strong>Criteria</strong>:</p>
<ul>
<li>Edge cases or advanced features not documented</li>
<li>Internal APIs exposed to users</li>
<li>Less common use cases</li>
</ul>
<p><strong>Action</strong>: Log as warning but may not create new content</p>
<h2 id="content-sufficiency-validation-step-0"><a class="header" href="#content-sufficiency-validation-step-0">Content Sufficiency Validation (Step 0)</a></h2>
<p><strong>CRITICAL SAFEGUARD</strong>: Before creating any subsection, gap detection validates that sufficient material exists in the codebase to support meaningful documentation.</p>
<p><strong>Source</strong>: <code>.claude/commands/prodigy-detect-documentation-gaps.md:166-335</code></p>
<h3 id="preservation-of-single-file-chapters"><a class="header" href="#preservation-of-single-file-chapters">Preservation of Single-File Chapters</a></h3>
<p>Gap detection <strong>ALWAYS preserves well-written single-file chapters</strong> (.claude/commands/prodigy-detect-documentation-gaps.md:174-209):</p>
<p><strong>Preservation Rules</strong>:</p>
<ul>
<li><strong>&lt; 1000 lines AND &lt; 10 H2 sections</strong>: PRESERVE as single-file</li>
<li><strong>≥ 1000 lines OR ≥ 10 H2 sections</strong>: Consider subsections for readability</li>
</ul>
<p><strong>Why</strong>: The original flat documentation structure works well for moderate-sized chapters. Subsections should only be created when they genuinely improve navigation.</p>
<h3 id="content-availability-validation"><a class="header" href="#content-availability-validation">Content Availability Validation</a></h3>
<p><strong>Step 0a: Discover Codebase Structure</strong> (.claude/commands/prodigy-detect-documentation-gaps.md:211-222)</p>
<p>Before counting content, the command discovers where code and examples are located using language-agnostic patterns:</p>
<pre><code class="language-bash"># Discover test locations
TEST_DIRS=$(find . -type d -name "*test*" -o -name "*spec*" | grep -v node_modules | grep -v .git | head -5)

# Discover example/workflow/config locations
EXAMPLE_DIRS=$(find . -type d -name "*example*" -o -name "*workflow*" -o -name "*sample*" -o -name "*config*" | grep -v node_modules | grep -v .git | head -5)

# Discover primary source locations (works for Rust, Python, JS, TS, Go, Java)
SOURCE_DIRS=$(find . -type f \( -name "*.rs" -o -name "*.py" -o -name "*.js" -o -name "*.ts" -o -name "*.go" -o -name "*.java" \) | sed 's|/[^/]*$||' | sort -u | grep -v node_modules | grep -v .git | head -10)
</code></pre>
<p><strong>Step 0b: Count Potential Content Sources</strong> (.claude/commands/prodigy-detect-documentation-gaps.md:224-255)</p>
<p>For each proposed subsection, the command counts language-agnostic content sources:</p>
<pre><code class="language-bash">FEATURE_CATEGORY="&lt;feature-category-name&gt;"

# Type definitions (struct, class, interface, enum, type)
TYPE_COUNT=$(rg "(struct|class|interface|type|enum).*${FEATURE_CATEGORY}" --hidden --iglob '!.git' --iglob '!node_modules' -c | awk '{s+=$1} END {print s}')

# Function/method definitions
FUNCTION_COUNT=$(rg "(fn|function|def|func|public|private).*${FEATURE_CATEGORY}" --hidden --iglob '!.git' --iglob '!node_modules' -c | awk '{s+=$1} END {print s}')

# Test mentions in discovered test directories
TEST_COUNT=0
for test_dir in $TEST_DIRS; do
  count=$(rg "${FEATURE_CATEGORY}" "$test_dir" --hidden -c 2&gt;/dev/null | awk '{s+=$1} END {print s}')
  TEST_COUNT=$((TEST_COUNT + count))
done

# Example/config file mentions in discovered example directories
EXAMPLE_COUNT=0
for example_dir in $EXAMPLE_DIRS; do
  count=$(rg "${FEATURE_CATEGORY}" "$example_dir" --hidden -c 2&gt;/dev/null | awk '{s+=$1} END {print s}')
  EXAMPLE_COUNT=$((EXAMPLE_COUNT + count))
done

# Calculate totals
TOTAL_MENTIONS=$((TYPE_COUNT + FUNCTION_COUNT + TEST_COUNT + EXAMPLE_COUNT))

# Estimate documentation lines (rule of thumb)
# Each type = ~30 lines docs, each function = ~10 lines, each example = ~40 lines, each test = ~15 lines
ESTIMATED_LINES=$((TYPE_COUNT * 30 + FUNCTION_COUNT * 10 + EXAMPLE_COUNT * 40 + TEST_COUNT * 15))
</code></pre>
<h3 id="content-sufficiency-thresholds"><a class="header" href="#content-sufficiency-thresholds">Content Sufficiency Thresholds</a></h3>
<p><strong>MUST HAVE</strong> (to create subsection) - (.claude/commands/prodigy-detect-documentation-gaps.md:259-265):</p>
<ul>
<li><code>TOTAL_MENTIONS &gt;= 5</code> - Feature mentioned in at least 5 places</li>
<li><code>ESTIMATED_LINES &gt;= 50</code> - Can generate at least 50 lines of documentation</li>
<li>At least ONE of:
<ul>
<li><code>TYPE_COUNT &gt;= 1</code> (has configuration type/struct/class)</li>
<li><code>EXAMPLE_COUNT &gt;= 1</code> (has real example/config file)</li>
</ul>
</li>
</ul>
<p><strong>SHOULD HAVE</strong> (for quality subsection) - (.claude/commands/prodigy-detect-documentation-gaps.md:266-269):</p>
<ul>
<li><code>TOTAL_MENTIONS &gt;= 10</code></li>
<li><code>ESTIMATED_LINES &gt;= 100</code></li>
<li><code>TYPE_COUNT &gt;= 1 AND EXAMPLE_COUNT &gt;= 1</code> (both type definition and example)</li>
</ul>
<h3 id="decision-tree-1"><a class="header" href="#decision-tree-1">Decision Tree</a></h3>
<p><strong>If TOTAL_MENTIONS &lt; 5 OR ESTIMATED_LINES &lt; 50</strong>:</p>
<ul>
<li>✗ <strong>DO NOT create subsection</strong></li>
<li><strong>Alternative</strong>: Add as section within parent chapter’s index.md</li>
<li><strong>Log</strong>: “⚠ Skipping subsection ‘${SUBSECTION_TITLE}’: only ${TOTAL_MENTIONS} mentions, ${ESTIMATED_LINES} estimated lines”</li>
<li><strong>Gap Report</strong>: Record as <code>"action": "skipped_subsection_creation", "reason": "insufficient_content"</code></li>
</ul>
<p><strong>If TOTAL_MENTIONS &gt;= 5 AND ESTIMATED_LINES &gt;= 50 BUT &lt; 100</strong>:</p>
<ul>
<li>~ Create subsection with “MINIMAL” flag</li>
<li>Add metadata: <code>{"content_warning": "minimal", "estimated_lines": ESTIMATED_LINES}</code></li>
<li>Signals to fix phase that limited content is expected</li>
</ul>
<p><strong>If TOTAL_MENTIONS &gt;= 10 AND ESTIMATED_LINES &gt;= 100</strong>:</p>
<ul>
<li>✓ <strong>Proceed with full subsection creation</strong></li>
</ul>
<h3 id="special-case-meta-subsections"><a class="header" href="#special-case-meta-subsections">Special Case: Meta-Subsections</a></h3>
<p>Meta-subsections like “Best Practices”, “Troubleshooting”, and “Examples” use different validation criteria (.claude/commands/prodigy-detect-documentation-gaps.md:306-334):</p>
<p><strong>Best Practices Subsection</strong>:</p>
<pre><code class="language-bash">BEST_PRACTICE_COUNT=$(rg "best.practice|pattern|guideline" --hidden --iglob '!.git' --iglob '!node_modules' -i -c | awk '{s+=$1} END {print s}')
# Requirement: BEST_PRACTICE_COUNT &gt;= 3 OR documented patterns in code
</code></pre>
<p><strong>Troubleshooting Subsection</strong>:</p>
<pre><code class="language-bash">ERROR_COUNT=$(rg "error|warn|fail" --hidden --iglob '!.git' --iglob '!node_modules' -c | awk '{s+=$1} END {print s}')
ISSUE_COUNT=$(rg "TODO|FIXME|XXX" --hidden --iglob '!.git' --iglob '!node_modules' -c | awk '{s+=$1} END {print s}')
# Requirement: ERROR_COUNT &gt;= 10 OR ISSUE_COUNT &gt;= 5
</code></pre>
<p><strong>Examples Subsection</strong>:</p>
<pre><code class="language-bash">EXAMPLE_FILE_COUNT=0
for example_dir in $EXAMPLE_DIRS; do
  count=$(find "$example_dir" -type f \( -name "*.yml" -o -name "*.yaml" -o -name "*.json" -o -name "*.toml" \) 2&gt;/dev/null | wc -l)
  EXAMPLE_FILE_COUNT=$((EXAMPLE_FILE_COUNT + count))
done
# Requirement: EXAMPLE_FILE_COUNT &gt;= 2 real config files
</code></pre>
<p><strong>If threshold not met</strong>: Add brief section to parent chapter’s index.md instead of creating separate subsection.</p>
<h2 id="structure-validation-phase-75"><a class="header" href="#structure-validation-phase-75">Structure Validation (Phase 7.5)</a></h2>
<p><strong>MANDATORY</strong>: Ensures chapters.json accurately reflects the actual file structure before generating flattened-items.json.</p>
<p><strong>Source</strong>: <code>.claude/commands/prodigy-detect-documentation-gaps.md:678-743</code></p>
<h3 id="validation-process"><a class="header" href="#validation-process">Validation Process</a></h3>
<p><strong>Step 1: Scan for Multi-Subsection Directories</strong></p>
<p>Find all directories under <code>book/src/</code> with an <code>index.md</code> file and count <code>.md</code> subsection files:</p>
<pre><code class="language-bash">for dir in $(find "${BOOK_DIR}/src/" -maxdepth 1 -type d); do
  if [ -f "${dir}/index.md" ]; then
    SUBSECTION_COUNT=$(find "${dir}" -maxdepth 1 -name "*.md" ! -name "index.md" | wc -l)
    if [ "$SUBSECTION_COUNT" -gt 0 ]; then
      # This is a multi-subsection chapter
      CHAPTER_ID=$(basename "$dir")
      echo "Found multi-subsection chapter: $CHAPTER_ID"
    fi
  fi
done
</code></pre>
<p><strong>Step 2: Compare Against chapters.json</strong></p>
<p>For each discovered multi-subsection chapter:</p>
<ol>
<li>Look up definition in chapters.json</li>
<li>Check if <code>type</code> field is “multi-subsection” or “single-file”</li>
<li><strong>If type is “single-file” or missing</strong>: MISMATCH - add to mismatches list</li>
<li><strong>If type is “multi-subsection”</strong>: Compare subsection counts
<ul>
<li>If counts don’t match: MISMATCH</li>
</ul>
</li>
</ol>
<p><strong>Step 3: Check for Orphaned Single-File Definitions</strong></p>
<p>For each chapter with <code>type: "single-file"</code>:</p>
<ol>
<li>Check if expected file (<code>book/src/chapter-id.md</code>) exists</li>
<li>Check if directory (<code>book/src/chapter-id/</code>) exists instead</li>
<li><strong>If file missing but directory exists</strong>: MISMATCH</li>
</ol>
<p><strong>Step 4: Auto-Migrate Mismatched Chapters</strong></p>
<p>For each mismatched chapter:</p>
<ol>
<li>Scan directory to discover all subsection files</li>
<li>For each <code>.md</code> file (excluding <code>index.md</code>):
<ul>
<li>Extract subsection ID from filename (remove <code>.md</code>)</li>
<li>Read file and extract title from first H1/H2 heading</li>
<li>Extract topics from section headings</li>
<li>Create subsection definition</li>
</ul>
</li>
<li>Update chapter in chapters.json:
<ul>
<li>Change <code>type</code> to “multi-subsection”</li>
<li>Change <code>file</code> to <code>index_file</code> (pointing to <code>index.md</code>)</li>
<li>Add <code>subsections</code> array with all discovered subsections</li>
<li>Preserve existing <code>topics</code> and <code>validation</code> fields</li>
</ul>
</li>
<li>Write updated chapters.json to disk</li>
<li>Record migration in gap report</li>
</ol>
<h3 id="example-migration"><a class="header" href="#example-migration">Example Migration</a></h3>
<p><strong>Before</strong> (chapters.json - incorrect):</p>
<pre><code class="language-json">{
  "id": "mapreduce",
  "title": "MapReduce Workflows",
  "file": "mapreduce.md",
  "type": "single-file",
  "topics": ["Map phase", "Reduce phase"]
}
</code></pre>
<p><strong>Actual File Structure</strong> (reality):</p>
<pre><code>book/src/mapreduce/
├── index.md
├── checkpoint-and-resume.md
├── performance-tuning.md
└── worktree-isolation.md
</code></pre>
<p><strong>After Migration</strong> (chapters.json - corrected):</p>
<pre><code class="language-json">{
  "id": "mapreduce",
  "title": "MapReduce Workflows",
  "index_file": "mapreduce/index.md",
  "type": "multi-subsection",
  "topics": ["Map phase", "Reduce phase"],
  "subsections": [
    {
      "id": "checkpoint-and-resume",
      "title": "Checkpoint and Resume",
      "file": "mapreduce/checkpoint-and-resume.md"
    },
    {
      "id": "performance-tuning",
      "title": "Performance Tuning",
      "file": "mapreduce/performance-tuning.md"
    },
    {
      "id": "worktree-isolation",
      "title": "Worktree Isolation",
      "file": "mapreduce/worktree-isolation.md"
    }
  ]
}
</code></pre>
<p><strong>Commit</strong>: Structure fixes are committed BEFORE generating flattened-items.json with message: “docs: sync chapters.json with actual file structure”</p>
<h2 id="flattened-items-generation-phase-8"><a class="header" href="#flattened-items-generation-phase-8">Flattened Items Generation (Phase 8)</a></h2>
<p><strong>CRITICAL</strong>: This file MUST be generated regardless of whether gaps are found. The map phase depends on it.</p>
<p><strong>Source</strong>: <code>.claude/commands/prodigy-detect-documentation-gaps.md:744-827</code></p>
<h3 id="purpose"><a class="header" href="#purpose">Purpose</a></h3>
<p>Creates a flat array of all chapters and subsections for parallel processing in the map phase. This enables each map agent to work on a single chapter or subsection independently.</p>
<h3 id="processing-logic"><a class="header" href="#processing-logic">Processing Logic</a></h3>
<pre><code>For each chapter in chapters.json:
  If type == "multi-subsection":
    For each subsection in chapter.subsections:
      Create item with parent metadata
      Add to flattened array

  If type == "single-file":
    Create item with type marker
    Add to flattened array
</code></pre>
<h3 id="output-structure"><a class="header" href="#output-structure">Output Structure</a></h3>
<p><strong>File</strong>: <code>.prodigy/book-analysis/flattened-items.json</code></p>
<p><strong>Example</strong>:</p>
<pre><code class="language-json">[
  {
    "id": "workflow-basics",
    "title": "Workflow Basics",
    "file": "book/src/workflow-basics.md",
    "topics": [
      "Setup phase",
      "Command types",
      "Variable interpolation"
    ],
    "validation": "Check that workflow syntax and variable documentation are complete",
    "type": "single-file"
  },
  {
    "id": "checkpoint-and-resume",
    "title": "Checkpoint and Resume",
    "file": "book/src/mapreduce/checkpoint-and-resume.md",
    "parent_chapter_id": "mapreduce",
    "parent_chapter_title": "MapReduce Workflows",
    "type": "subsection",
    "topics": [
      "Checkpoint creation",
      "Resume behavior",
      "State preservation"
    ],
    "validation": "Check that checkpoint mechanism and resume procedures are documented",
    "feature_mapping": [
      "mapreduce.checkpoint",
      "mapreduce.resume"
    ]
  },
  {
    "id": "performance-tuning",
    "title": "Performance Tuning",
    "file": "book/src/mapreduce/performance-tuning.md",
    "parent_chapter_id": "mapreduce",
    "parent_chapter_title": "MapReduce Workflows",
    "type": "subsection",
    "topics": [
      "Parallel execution",
      "Resource limits"
    ],
    "feature_mapping": [
      "mapreduce.performance",
      "mapreduce.resource_limits"
    ]
  }
]
</code></pre>
<h3 id="map-phase-integration"><a class="header" href="#map-phase-integration">Map Phase Integration</a></h3>
<p>The map phase consumes flattened-items.json (workflows/book-docs-drift.yml:36-48):</p>
<pre><code class="language-yaml">map:
  input: "${ANALYSIS_DIR}/flattened-items.json"
  json_path: "$[*]"  # Each item is a chapter or subsection

  agent_template:
    # Analyze drift for this specific chapter/subsection
    - claude: "/prodigy-analyze-subsection-drift --project $PROJECT_NAME --json '${item}' --features $FEATURES_PATH"

    # Fix drift for this specific chapter/subsection
    - claude: "/prodigy-fix-subsection-drift --project $PROJECT_NAME --json '${item}'"
</code></pre>
<p><strong>Why Required</strong>: Without flattened-items.json, the map phase cannot parallelize drift analysis and fixing across chapters/subsections.</p>
<h2 id="topic-normalization"><a class="header" href="#topic-normalization">Topic Normalization</a></h2>
<p>Gap detection uses normalization logic to accurately match feature categories against documented topics (.claude/commands/prodigy-detect-documentation-gaps.md:42-50):</p>
<h3 id="normalization-steps"><a class="header" href="#normalization-steps">Normalization Steps</a></h3>
<ol>
<li>Convert to lowercase</li>
<li>Remove punctuation and special characters</li>
<li>Trim whitespace</li>
<li>Extract key terms from compound names</li>
</ol>
<h3 id="examples-3"><a class="header" href="#examples-3">Examples</a></h3>
<pre><code>"MapReduce Workflows"     → ["mapreduce", "workflows"]
"agent_merge"             → "agent-merge"
"command-types"           → "command-types"
"Goal Seeking Operations" → ["goal", "seeking", "operations"]
</code></pre>
<h3 id="matching-logic"><a class="header" href="#matching-logic">Matching Logic</a></h3>
<p>For each feature area in features.json, the command checks if any of these match:</p>
<ol>
<li>Chapter ID contains normalized_category</li>
<li>normalized_category contains Chapter ID</li>
<li>Chapter title contains normalized_category</li>
<li>Chapter topics contain normalized_category</li>
<li>Section headings in markdown match normalized_category</li>
<li>Subsection feature_mapping arrays match</li>
</ol>
<p><strong>Test Case</strong> (tests/documentation_gap_detection_test.rs:236-274):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[test]
fn test_gap_detection_normalizes_topic_names() -&gt; Result&lt;()&gt; {
    // Features with underscores
    let features = vec![
        MockFeature {
            category: "command_types".to_string(),
            // ...
        },
    ];

    // Chapters with normalized names (hyphens)
    let chapters = vec![
        MockChapter {
            id: "command-types".to_string(),  // Hyphen vs underscore
            // ...
        },
    ];

    let gaps = detect_gaps(&amp;features, &amp;chapters);

    // Result: No gaps because normalization matches them
    assert_eq!(gaps.len(), 0, "Normalization should match underscore and hyphen variations");

    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h2 id="idempotence"><a class="header" href="#idempotence">Idempotence</a></h2>
<p>Gap detection can be run multiple times safely without creating duplicate chapters or subsections (.claude/commands/prodigy-detect-documentation-gaps.md:867-887).</p>
<h3 id="idempotence-guarantees"><a class="header" href="#idempotence-guarantees">Idempotence Guarantees</a></h3>
<ol>
<li><strong>Checks for existing chapters</strong> before creating</li>
<li><strong>Uses normalized comparison</strong> for matching</li>
<li><strong>Skips already-created chapters</strong></li>
<li><strong>Can run repeatedly</strong> without side effects</li>
</ol>
<h3 id="test-case"><a class="header" href="#test-case">Test Case</a></h3>
<p><strong>Source</strong>: tests/documentation_gap_detection_test.rs:236-274</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[test]
fn test_gap_detection_idempotence() -&gt; Result&lt;()&gt; {
    let features = vec![MockFeature {
        category: "new_feature".to_string(),
        description: "A new feature".to_string(),
        capabilities: vec!["capability1".to_string()],
    }];

    // First run with no chapters
    let gaps_first = detect_gaps(&amp;features, &amp;vec![]);
    assert_eq!(gaps_first.len(), 1, "First run detects 1 gap");

    // Simulate creating the chapter
    let updated_chapters = vec![MockChapter {
        id: "new-feature".to_string(),
        title: "New Feature".to_string(),
        file: "new-feature.md".to_string(),
        topics: vec!["New feature overview".to_string()],
    }];

    // Second run with the new chapter
    let gaps_second = detect_gaps(&amp;features, &amp;updated_chapters);
    assert_eq!(gaps_second.len(), 0, "Second run detects no gaps");

    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h2 id="gap-report-structure"><a class="header" href="#gap-report-structure">Gap Report Structure</a></h2>
<p><strong>Output</strong>: <code>.prodigy/book-analysis/gap-report.json</code></p>
<h3 id="example-report"><a class="header" href="#example-report">Example Report</a></h3>
<pre><code class="language-json">{
  "analysis_date": "2025-11-09T12:34:56Z",
  "features_analyzed": 12,
  "documented_topics": 10,
  "gaps_found": 2,
  "gaps": [
    {
      "severity": "high",
      "type": "missing_chapter",
      "feature_category": "agent_merge",
      "feature_description": "Custom merge workflows for map agents",
      "recommended_chapter_id": "agent-merge-workflows",
      "recommended_title": "Agent Merge Workflows",
      "recommended_location": "book/src/agent-merge-workflows.md",
      "is_subsection": false
    },
    {
      "severity": "high",
      "type": "missing_chapter",
      "feature_category": "circuit_breaker",
      "feature_description": "Circuit breaker for error handling",
      "recommended_chapter_id": "circuit-breaker",
      "recommended_title": "Circuit Breaker",
      "recommended_location": "book/src/circuit-breaker.md",
      "is_subsection": false
    }
  ],
  "actions_taken": [
    {
      "action": "created_chapter_definition",
      "chapter_id": "agent-merge-workflows",
      "file_path": "workflows/data/prodigy-chapters.json"
    },
    {
      "action": "created_stub_file",
      "file_path": "book/src/agent-merge-workflows.md",
      "type": "chapter"
    },
    {
      "action": "updated_summary",
      "file_path": "book/src/SUMMARY.md",
      "items_added": [
        {"type": "chapter", "id": "agent-merge-workflows"}
      ]
    }
  ],
  "structure_validation": {
    "mismatches_found": 1,
    "mismatched_chapters": ["mapreduce"],
    "migrations_performed": [
      {
        "chapter_id": "mapreduce",
        "action": "migrated_to_multi_subsection",
        "subsections_discovered": 3
      }
    ],
    "validation_timestamp": "2025-11-09T12:34:56Z"
  }
}
</code></pre>
<h2 id="execution-progress"><a class="header" href="#execution-progress">Execution Progress</a></h2>
<p>When gap detection runs, it displays progress through multiple phases:</p>
<pre><code>🔍 Analyzing documentation coverage...
   ✓ Loaded 12 feature areas from features.json
   ✓ Loaded 10 existing chapters
   ✓ Parsed SUMMARY.md structure

📊 Comparing features against documentation...
   ✓ Analyzed workflow_basics: documented ✓
   ✓ Analyzed mapreduce: documented ✓
   ⚠ Analyzed agent_merge: not documented (gap detected)
   ✓ Analyzed command_types: documented ✓
   ⚠ Analyzed circuit_breaker: not documented (gap detected)

🔍 Validating chapter structure (Phase 7.5)...
   ✓ Scanning for multi-subsection directories
   ✓ Comparing against chapters.json definitions
   ⚠ Found mismatch in mapreduce chapter (was single-file, now multi-subsection)
   ✓ Auto-migrated mapreduce chapter structure

📝 Creating missing chapters...
   ✓ Generated definition: agent-merge-workflows
   ✓ Created stub: book/src/agent-merge-workflows.md
   ✓ Generated definition: circuit-breaker
   ✓ Created stub: book/src/circuit-breaker.md
   ✓ Updated SUMMARY.md

💾 Generating flattened items for map phase...
   ✓ Processed 1 single-file chapter (workflow-basics)
   ✓ Processed 3 subsections from mapreduce chapter
   ✓ Processed 10 additional chapters/subsections
   ✓ Generated .prodigy/book-analysis/flattened-items.json

💾 Committing changes...
   ✓ Staged 6 files
   ✓ Committed: docs: auto-discover missing chapters for agent-merge-workflows, circuit-breaker
   ✓ Committed: docs: sync chapters.json with actual file structure
</code></pre>
<h3 id="final-summary"><a class="header" href="#final-summary">Final Summary</a></h3>
<pre><code>📊 Documentation Gap Analysis Complete
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Features Analyzed: 12
Documented Topics: 10
Gaps Found: 2

🔴 High Severity Gaps (Missing Chapters): 2
  • agent_merge - Custom merge workflows for map agents
  • circuit_breaker - Workflow error circuit breaking

✅ Actions Taken:
  ✓ Created 2 chapter definitions in workflows/data/prodigy-chapters.json
  ✓ Created 2 stub files in book/src/
  ✓ Updated book/src/SUMMARY.md
  ✓ Generated flattened-items.json with 14 items
  ✓ Auto-migrated 1 chapter structure
  ✓ Committed changes (2 commits)

📝 Next Steps:
  The map phase will now process 14 chapters/subsections to populate content.
  Review the generated stubs and customize as needed.
</code></pre>
<h2 id="error-handling-6"><a class="header" href="#error-handling-6">Error Handling</a></h2>
<p><strong>Source</strong>: <code>.claude/commands/prodigy-detect-documentation-gaps.md:889-919</code></p>
<h3 id="common-errors"><a class="header" href="#common-errors">Common Errors</a></h3>
<p><strong>Missing features.json</strong>:</p>
<ul>
<li><strong>Cause</strong>: Feature analysis step hasn’t run yet</li>
<li><strong>Solution</strong>: Ensure <code>/prodigy-analyze-features-for-book</code> runs before gap detection in setup phase</li>
<li><strong>Error Message</strong>: “Error: features.json not found at {path}. Run feature analysis first.”</li>
</ul>
<p><strong>Missing/Invalid chapters.json</strong>:</p>
<ul>
<li><strong>Cause</strong>: Chapter definitions file doesn’t exist or has invalid JSON</li>
<li><strong>Solution</strong>: Create valid chapters.json or fix JSON syntax errors</li>
<li><strong>Recovery</strong>: Gap detection can initialize empty chapters.json if needed</li>
</ul>
<p><strong>File Write Failures</strong>:</p>
<ul>
<li><strong>Cause</strong>: Permission issues or disk full</li>
<li><strong>Solution</strong>: Check directory permissions and disk space</li>
<li><strong>Rollback</strong>: Gap detection records partial state in gap report for manual cleanup</li>
</ul>
<p><strong>Invalid JSON Handling</strong>:</p>
<ul>
<li><strong>Cause</strong>: Malformed JSON in input files</li>
<li><strong>Solution</strong>: Validate JSON with <code>jq</code> before running workflow</li>
<li><strong>Error Recording</strong>: Details added to gap report for debugging</li>
</ul>
<h2 id="testing"><a class="header" href="#testing">Testing</a></h2>
<p>Gap detection has comprehensive test coverage in <code>tests/documentation_gap_detection_test.rs:1-678</code>:</p>
<h3 id="test-coverage-1"><a class="header" href="#test-coverage-1">Test Coverage</a></h3>
<p><strong>Core Functionality</strong>:</p>
<ul>
<li>Identifying missing chapters (tests/documentation_gap_detection_test.rs:1-50)</li>
<li>Idempotence behavior (tests/documentation_gap_detection_test.rs:236-274)</li>
<li>Topic normalization logic (tests/documentation_gap_detection_test.rs:275-320)</li>
<li>Chapter definition generation (tests/documentation_gap_detection_test.rs:321-370)</li>
</ul>
<p><strong>Edge Cases</strong>:</p>
<ul>
<li>False positive prevention via normalization</li>
<li>Handling chapters with multiple topics</li>
<li>Subsection discovery and validation</li>
<li>Structure migration for multi-subsection chapters</li>
</ul>
<p><strong>Quality Assurance</strong>:</p>
<ul>
<li>Stub file structure validation</li>
<li>SUMMARY.md update correctness</li>
<li>Gap report JSON schema validation</li>
</ul>
<h2 id="best-practices-25"><a class="header" href="#best-practices-25">Best Practices</a></h2>
<ol>
<li>
<p><strong>Always run feature analysis first</strong>: Gap detection depends on features.json from <code>/prodigy-analyze-features-for-book</code></p>
</li>
<li>
<p><strong>Review generated stubs</strong>: Customize stub content to match project style before map phase processes them</p>
</li>
<li>
<p><strong>Monitor content warnings</strong>: If subsections are created with “MINIMAL” flag, ensure they still provide value or merge into parent chapter</p>
</li>
<li>
<p><strong>Validate structure regularly</strong>: Run gap detection periodically to catch mismatches between chapters.json and actual files</p>
</li>
<li>
<p><strong>Commit structure fixes separately</strong>: Phase 7.5 commits structure validation fixes before creating new content, keeping history clean</p>
</li>
<li>
<p><strong>Trust idempotence</strong>: Safe to re-run gap detection without fear of duplicates</p>
</li>
</ol>
<h2 id="see-also-36"><a class="header" href="#see-also-36">See Also</a></h2>
<ul>
<li><a href="automated-documentation/./understanding-the-workflow.html">./understanding-the-workflow.md</a> - How gap detection fits in the overall workflow</li>
<li><a href="automated-documentation/../../mapreduce/index.html">../../mapreduce/index.md</a> - MapReduce phase that consumes flattened-items.json</li>
<li><a href="automated-documentation/./index.html">./index.md</a> - Automated documentation overview</li>
<li><a href="automated-documentation/./best-practices.html">./best-practices.md</a> - Documentation automation best practices</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="github-actions-integration"><a class="header" href="#github-actions-integration">GitHub Actions Integration</a></h2>
<p>Automate your mdBook documentation deployment to GitHub Pages using a standardized workflow that validates documentation on pull requests and deploys on merges to your main branch.</p>
<p><strong>Source</strong>: This guide is based on Prodigy’s production workflow (<code>.github/workflows/deploy-docs.yml</code>) and Spec 128 GitHub Workflow Documentation Standards.</p>
<h3 id="quick-start-5"><a class="header" href="#quick-start-5">Quick Start</a></h3>
<p>Deploy mdBook documentation to GitHub Pages in under 5 minutes:</p>
<p><strong>Step 1: Copy the workflow file</strong></p>
<pre><code class="language-bash">curl -o .github/workflows/deploy-docs.yml \
  https://raw.githubusercontent.com/iepathos/prodigy/main/.github/workflows/deploy-docs.yml
</code></pre>
<p>Or create <code>.github/workflows/deploy-docs.yml</code> manually:</p>
<pre><code class="language-yaml">name: Deploy Documentation

on:
  push:
    branches: [main, master]
    paths:
      - 'book/**'
      - '.github/workflows/deploy-docs.yml'
  pull_request:
    branches: [main, master]
    paths:
      - 'book/**'
      - '.github/workflows/deploy-docs.yml'

jobs:
  build-deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v5

      - name: Setup mdBook
        uses: peaceiris/actions-mdbook@v2
        with:
          mdbook-version: 'latest'

      - name: Build book
        run: mdbook build book

      - name: Deploy to GitHub Pages
        if: github.event_name == 'push' &amp;&amp; (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master')
        uses: peaceiris/actions-gh-pages@v4
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./book/book
</code></pre>
<p><strong>Source</strong>: <code>.github/workflows/deploy-docs.yml:1-38</code></p>
<p><strong>Step 2: Commit and push</strong></p>
<pre><code class="language-bash">git add .github/workflows/deploy-docs.yml
git commit -m "Add documentation deployment workflow"
git push
</code></pre>
<p><strong>Step 3: Enable GitHub Pages</strong></p>
<ol>
<li>Go to your repository’s <strong>Settings → Pages</strong></li>
<li>Under “Source”, select <strong>Deploy from a branch</strong></li>
<li>Choose branch: <strong>gh-pages</strong> and directory: <strong>/ (root)</strong></li>
<li>Click <strong>Save</strong></li>
</ol>
<p><strong>Done!</strong> Your documentation will deploy automatically on the next push to main/master.</p>
<p><strong>Source</strong>: Spec 128:136-142</p>
<h3 id="how-it-works-2"><a class="header" href="#how-it-works-2">How It Works</a></h3>
<p>The workflow executes in two contexts:</p>
<ol>
<li><strong>Pull Requests</strong>: Validates that documentation builds successfully (prevents merging broken docs)</li>
<li><strong>Push to main/master</strong>: Builds and deploys to GitHub Pages (publishes the documentation)</li>
</ol>
<p><strong>Key Components</strong>:</p>
<ul>
<li><strong>Triggers</strong>: Runs only when documentation files change (<code>book/**</code>) or the workflow itself is modified</li>
<li><strong>Path Filters</strong>: Prevents unnecessary workflow runs, saving CI/CD minutes</li>
<li><strong>Permissions</strong>: <code>contents: write</code> allows pushing to the <code>gh-pages</code> branch</li>
<li><strong>Conditional Deployment</strong>: The <code>if: github.event_name == 'push'</code> condition ensures PRs only validate, never deploy</li>
</ul>
<p><strong>Source</strong>: <code>.github/workflows/deploy-docs.yml:3-13</code>, <code>.github/workflows/deploy-docs.yml:18-19</code>, <code>.github/workflows/deploy-docs.yml:33</code></p>
<h3 id="workflow-file-structure-explained"><a class="header" href="#workflow-file-structure-explained">Workflow File Structure Explained</a></h3>
<pre><code class="language-yaml"># Triggers: When to run this workflow
on:
  push:
    branches: [main, master]    # Deploy on push to default branch
    paths:                      # Only when these files change
      - 'book/**'              # Documentation content
      - '.github/workflows/deploy-docs.yml'  # This workflow file
  pull_request:                 # Validate documentation in PRs
    branches: [main, master]
    paths:
      - 'book/**'
</code></pre>
<p><strong>Why path filters?</strong></p>
<ul>
<li>Prevents workflow from running on code changes unrelated to documentation</li>
<li>Saves CI/CD minutes (workflow only runs when docs actually change)</li>
<li>Faster feedback for non-documentation PRs</li>
</ul>
<p><strong>Source</strong>: <code>.github/workflows/deploy-docs.yml:6-13</code>, Spec 128:235-255</p>
<pre><code class="language-yaml"># Permissions needed for deployment
permissions:
  contents: write  # Required to push to gh-pages branch
</code></pre>
<p><strong>Why <code>contents: write</code>?</strong></p>
<p>The <code>peaceiris/actions-gh-pages</code> action deploys by pushing to the <code>gh-pages</code> branch. This requires write access to repository contents.</p>
<p><strong>Note</strong>: This differs from the newer <code>actions/deploy-pages</code> approach which uses <code>pages: write</code> and <code>id-token: write</code>. Prodigy standardizes on the <code>gh-pages</code> branch method for consistency and broader compatibility.</p>
<p><strong>Source</strong>: <code>.github/workflows/deploy-docs.yml:18-19</code>, Spec 128:150-157</p>
<pre><code class="language-yaml">steps:
  # Step 1: Get repository code
  - uses: actions/checkout@v5

  # Step 2: Install mdBook
  - name: Setup mdBook
    uses: peaceiris/actions-mdbook@v2
    with:
      mdbook-version: 'latest'

  # Step 3: Build documentation
  - name: Build book
    run: mdbook build book

  # Step 4: Deploy to GitHub Pages (only on push to main/master, not PRs)
  - name: Deploy to GitHub Pages
    if: github.event_name == 'push'
    uses: peaceiris/actions-gh-pages@v4
    with:
      github_token: ${{ secrets.GITHUB_TOKEN }}
      publish_dir: ./book/book
</code></pre>
<p><strong>Deployment Condition</strong>: <code>if: github.event_name == 'push'</code></p>
<p>This critical condition ensures:</p>
<ul>
<li><strong>Pull Requests</strong>: Build and validate documentation (catches errors before merge)</li>
<li><strong>Push to main/master</strong>: Build, validate, AND deploy to GitHub Pages</li>
</ul>
<p>Without this condition, every PR would attempt to deploy, which is unnecessary and can cause permission issues.</p>
<p><strong>Source</strong>: <code>.github/workflows/deploy-docs.yml:22-37</code>, Spec 128:225-233</p>
<h3 id="recommended-action-versions"><a class="header" href="#recommended-action-versions">Recommended Action Versions</a></h3>
<p>Use these specific action versions for stability and security:</p>
<ul>
<li><strong><code>actions/checkout@v5</code></strong> - Fetches repository code</li>
<li><strong><code>peaceiris/actions-mdbook@v2</code></strong> - Installs mdBook</li>
<li><strong><code>peaceiris/actions-gh-pages@v4</code></strong> - Deploys to gh-pages branch</li>
</ul>
<p><strong>Source</strong>: <code>.github/workflows/deploy-docs.yml:22,24,34</code>, Spec 128:669-674</p>
<h3 id="repository-settings"><a class="header" href="#repository-settings">Repository Settings</a></h3>
<p>After adding the workflow file, configure GitHub Pages in your repository settings:</p>
<ol>
<li>Navigate to <strong>Settings → Pages</strong></li>
<li>Under <strong>Source</strong>, select <strong>Deploy from a branch</strong></li>
<li>Choose <strong>Branch: gh-pages</strong> and <strong>Directory: / (root)</strong></li>
<li>Click <strong>Save</strong></li>
</ol>
<p>The workflow will create the <code>gh-pages</code> branch automatically on the first deployment. You don’t need to create it manually.</p>
<p><strong>Source</strong>: Spec 128:136-141</p>
<h3 id="integration-with-prodigy-workflows"><a class="header" href="#integration-with-prodigy-workflows">Integration with Prodigy Workflows</a></h3>
<p>This GitHub Actions workflow <strong>deploys</strong> the documentation that Prodigy’s book workflow <strong>generates and maintains</strong>.</p>
<p><strong>How they work together</strong>:</p>
<ol>
<li>
<p><strong>Prodigy MapReduce Workflow</strong> (<code>book-docs-drift.yml</code>):</p>
<ul>
<li>Analyzes code for features and changes</li>
<li>Detects documentation drift</li>
<li>Updates markdown files in <code>book/src/</code></li>
<li>Commits fixes to your repository</li>
</ul>
</li>
<li>
<p><strong>GitHub Actions Workflow</strong> (<code>deploy-docs.yml</code>):</p>
<ul>
<li>Detects changes to <code>book/**</code> files</li>
<li>Builds the mdBook</li>
<li>Deploys to GitHub Pages</li>
</ul>
</li>
</ol>
<p><strong>In Practice</strong>:</p>
<ul>
<li>Prodigy keeps your docs accurate and up-to-date</li>
<li>GitHub Actions makes your docs publicly accessible</li>
<li>Together, they create a fully automated documentation system</li>
</ul>
<p><strong>Source</strong>: <code>book/src/automated-documentation/index.md:82-135</code>, Spec 128:518-562</p>
<h3 id="common-mistakes-and-solutions"><a class="header" href="#common-mistakes-and-solutions">Common Mistakes and Solutions</a></h3>
<h4 id="mistake-1-wrong-filename"><a class="header" href="#mistake-1-wrong-filename">Mistake 1: Wrong Filename</a></h4>
<p>❌ <strong>Wrong</strong>:</p>
<pre><code>.github/workflows/docs.yml
.github/workflows/documentation.yml
.github/workflows/mdbook.yml
</code></pre>
<p>✅ <strong>Correct</strong>:</p>
<pre><code>.github/workflows/deploy-docs.yml
</code></pre>
<p><strong>Why it matters</strong>: Consistent naming across projects aids discovery and maintenance.</p>
<p><strong>Source</strong>: Spec 128:296-312</p>
<h4 id="mistake-2-wrong-deployment-action"><a class="header" href="#mistake-2-wrong-deployment-action">Mistake 2: Wrong Deployment Action</a></h4>
<p>❌ <strong>Wrong</strong>:</p>
<pre><code class="language-yaml">- uses: actions/upload-pages-artifact@v3
- uses: actions/deploy-pages@v4
</code></pre>
<p>✅ <strong>Correct</strong>:</p>
<pre><code class="language-yaml">- uses: peaceiris/actions-gh-pages@v4
  with:
    github_token: ${{ secrets.GITHUB_TOKEN }}
    publish_dir: ./book/book
</code></pre>
<p><strong>Why it matters</strong>: Different actions require different permissions and repository settings. Using <code>actions/deploy-pages</code> requires changing your GitHub Pages source to “GitHub Actions” and using different permissions (<code>pages: write</code>, <code>id-token: write</code>).</p>
<p><strong>Source</strong>: Spec 128:314-330</p>
<h4 id="mistake-3-missing-path-filters"><a class="header" href="#mistake-3-missing-path-filters">Mistake 3: Missing Path Filters</a></h4>
<p>❌ <strong>Wrong</strong>:</p>
<pre><code class="language-yaml">on:
  push:
    branches: [main]
</code></pre>
<p>✅ <strong>Correct</strong>:</p>
<pre><code class="language-yaml">on:
  push:
    branches: [main, master]
    paths:
      - 'book/**'
      - '.github/workflows/deploy-docs.yml'
</code></pre>
<p><strong>Impact</strong>: Workflow runs on every commit (even code-only changes), wasting CI resources and creating unnecessary deployments.</p>
<p><strong>Source</strong>: Spec 128:332-351</p>
<h4 id="mistake-4-wrong-permissions"><a class="header" href="#mistake-4-wrong-permissions">Mistake 4: Wrong Permissions</a></h4>
<p>❌ <strong>Wrong</strong>:</p>
<pre><code class="language-yaml">permissions:
  pages: write
  id-token: write
</code></pre>
<p>✅ <strong>Correct</strong>:</p>
<pre><code class="language-yaml">permissions:
  contents: write
</code></pre>
<p><strong>Why it matters</strong>: The <code>gh-pages</code> deployment method needs <code>contents: write</code> to push to the gh-pages branch. The permissions shown in “Wrong” are for the <code>actions/deploy-pages</code> approach.</p>
<p><strong>Source</strong>: Spec 128:353-368</p>
<h4 id="mistake-5-missing-pr-validation"><a class="header" href="#mistake-5-missing-pr-validation">Mistake 5: Missing PR Validation</a></h4>
<p>❌ <strong>Wrong</strong>:</p>
<pre><code class="language-yaml">on:
  push:
    branches: [main]
</code></pre>
<p>✅ <strong>Correct</strong>:</p>
<pre><code class="language-yaml">on:
  push:
    branches: [main, master]
    paths: ['book/**']
  pull_request:
    branches: [main, master]
    paths: ['book/**']
</code></pre>
<p><strong>Why it matters</strong>: Without PR validation, documentation build errors aren’t caught until after merge, potentially breaking your deployed documentation.</p>
<p><strong>Source</strong>: Spec 128:370-390</p>
<h4 id="mistake-6-deploying-on-pr"><a class="header" href="#mistake-6-deploying-on-pr">Mistake 6: Deploying on PR</a></h4>
<p>❌ <strong>Wrong</strong>:</p>
<pre><code class="language-yaml">- uses: peaceiris/actions-gh-pages@v4
  with:
    github_token: ${{ secrets.GITHUB_TOKEN }}
    publish_dir: ./book/book
</code></pre>
<p>✅ <strong>Correct</strong>:</p>
<pre><code class="language-yaml">- uses: peaceiris/actions-gh-pages@v4
  if: github.event_name == 'push'
  with:
    github_token: ${{ secrets.GITHUB_TOKEN }}
    publish_dir: ./book/book
</code></pre>
<p><strong>Why it matters</strong>: PRs should validate documentation builds but not deploy them. Deploying on PR can cause conflicts and unnecessary deployments.</p>
<p><strong>Source</strong>: Spec 128:392-410</p>
<h3 id="troubleshooting-20"><a class="header" href="#troubleshooting-20">Troubleshooting</a></h3>
<h4 id="issue-workflow-runs-on-every-commit"><a class="header" href="#issue-workflow-runs-on-every-commit">Issue: Workflow Runs on Every Commit</a></h4>
<p><strong>Symptom</strong>: Workflow executes even when documentation hasn’t changed</p>
<p><strong>Cause</strong>: Missing or incorrect path filters</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-yaml">on:
  push:
    paths:
      - 'book/**'
      - '.github/workflows/deploy-docs.yml'
</code></pre>
<p><strong>Source</strong>: Spec 128:415-428</p>
<h4 id="issue-permission-denied-when-deploying"><a class="header" href="#issue-permission-denied-when-deploying">Issue: Permission Denied When Deploying</a></h4>
<p><strong>Symptom</strong>: Error like “failed to push some refs” or “permission denied”</p>
<p><strong>Cause</strong>: Missing <code>contents: write</code> permission</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-yaml">permissions:
  contents: write
</code></pre>
<p>Also verify repository settings:</p>
<ul>
<li>Go to <strong>Settings → Actions → General</strong></li>
<li>Under <strong>Workflow permissions</strong>, select <strong>Read and write permissions</strong></li>
<li>Click <strong>Save</strong></li>
</ul>
<p><strong>Source</strong>: Spec 128:430-445</p>
<h4 id="issue-documentation-not-updating"><a class="header" href="#issue-documentation-not-updating">Issue: Documentation Not Updating</a></h4>
<p><strong>Symptom</strong>: Workflow succeeds but GitHub Pages shows old content</p>
<p><strong>Causes and Solutions</strong>:</p>
<ol>
<li>
<p><strong>Verify gh-pages branch updated</strong>:</p>
<pre><code class="language-bash">git fetch origin gh-pages
git log origin/gh-pages
</code></pre>
<p>Check if the latest commit matches your expectations.</p>
</li>
<li>
<p><strong>Check GitHub Pages settings</strong>:</p>
<ul>
<li>Go to <strong>Settings → Pages</strong></li>
<li>Verify <strong>Source: Deploy from branch</strong></li>
<li>Verify <strong>Branch: gh-pages</strong> and <strong>Directory: / (root)</strong></li>
</ul>
</li>
<li>
<p><strong>Force clear browser cache</strong>:</p>
<ul>
<li>Add query parameter to URL: <code>https://username.github.io/repo?v=2</code></li>
<li>Hard refresh: Ctrl+Shift+R (Windows/Linux) or Cmd+Shift+R (Mac)</li>
</ul>
</li>
<li>
<p><strong>Check publish_dir path</strong>:</p>
<pre><code class="language-yaml">publish_dir: ./book/book  # Correct - points to mdBook output
# NOT ./book              # Wrong - points to source directory
</code></pre>
</li>
</ol>
<p><strong>Source</strong>: Spec 128:447-471</p>
<h4 id="issue-404-error-on-github-pages"><a class="header" href="#issue-404-error-on-github-pages">Issue: 404 Error on GitHub Pages</a></h4>
<p><strong>Symptom</strong>: Page shows “404 There isn’t a GitHub Pages site here”</p>
<p><strong>Causes and Solutions</strong>:</p>
<ol>
<li>
<p><strong>GitHub Pages not enabled</strong>:</p>
<ul>
<li>Go to <strong>Settings → Pages</strong></li>
<li>Enable Pages if it shows as disabled</li>
</ul>
</li>
<li>
<p><strong>Wrong source branch</strong>:</p>
<ul>
<li>Change source to <strong>gh-pages</strong> branch</li>
</ul>
</li>
<li>
<p><strong>Wrong root directory</strong>:</p>
<ul>
<li>Ensure source is <strong>/ (root)</strong> not <strong>/docs</strong></li>
</ul>
</li>
<li>
<p><strong>Private repository without GitHub Pro</strong>:</p>
<ul>
<li>GitHub Pages on private repositories requires GitHub Pro, Team, or Enterprise</li>
<li>Make repository public or upgrade your GitHub plan</li>
</ul>
</li>
</ol>
<p><strong>Source</strong>: Spec 128:473-489</p>
<h4 id="issue-workflow-syntax-error"><a class="header" href="#issue-workflow-syntax-error">Issue: Workflow Syntax Error</a></h4>
<p><strong>Symptom</strong>: Workflow doesn’t appear in Actions tab</p>
<p><strong>Cause</strong>: Invalid YAML syntax</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># Validate YAML locally
yamllint .github/workflows/deploy-docs.yml

# Or use GitHub's workflow validator
# (GitHub shows syntax errors when you navigate to the workflow file)
</code></pre>
<p><strong>Source</strong>: Spec 128:491-503</p>
<h4 id="issue-deployment-job-skipped-on-push"><a class="header" href="#issue-deployment-job-skipped-on-push">Issue: Deployment Job Skipped on Push</a></h4>
<p><strong>Symptom</strong>: Build job runs but deploy step is skipped on push to main</p>
<p><strong>Cause</strong>: Missing or incorrect condition</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-yaml">- name: Deploy to GitHub Pages
  if: github.event_name == 'push'  # This line is critical
  uses: peaceiris/actions-gh-pages@v4
</code></pre>
<p>Verify the condition matches exactly. Common mistakes:</p>
<ul>
<li>Typo in <code>github.event_name</code></li>
<li>Using single <code>=</code> instead of <code>==</code></li>
<li>Wrong event name (e.g., <code>if: github.event == 'push'</code>)</li>
</ul>
<p><strong>Source</strong>: Spec 128:505-516</p>
<h3 id="see-also-37"><a class="header" href="#see-also-37">See Also</a></h3>
<ul>
<li><a href="automated-documentation/quick-start.html">Quick Start</a> - Get started with Prodigy’s automated documentation</li>
<li><a href="automated-documentation/understanding-the-workflow.html">Understanding the Workflow</a> - Deep dive into how the book workflow works</li>
<li><a href="automated-documentation/troubleshooting.html">Troubleshooting</a> - General troubleshooting for documentation workflows</li>
<li><a href="automated-documentation/best-practices.html">Best Practices</a> - Documentation best practices and conventions</li>
<li><a href="automated-documentation/real-world-example-prodigys-own-documentation.html">Real-World Example</a> - See Prodigy’s own documentation workflow in action</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="customization-examples"><a class="header" href="#customization-examples">Customization Examples</a></h2>
<p>This section demonstrates how to adapt the automated documentation workflow for different project types, languages, and requirements. All examples are based on real configurations from the Prodigy project.</p>
<h3 id="language-specific-adaptations"><a class="header" href="#language-specific-adaptations">Language-Specific Adaptations</a></h3>
<p>The automated documentation workflow can be customized for different programming languages by adjusting the <code>analysis_targets</code> in your <code>book-config.json</code>.</p>
<h4 id="rust-project-default-configuration"><a class="header" href="#rust-project-default-configuration">Rust Project (Default Configuration)</a></h4>
<p><strong>Source</strong>: <code>.prodigy/book-config.json:7-31</code></p>
<pre><code class="language-json">{
  "project_name": "Prodigy",
  "project_type": "cli_tool",
  "book_dir": "book",
  "analysis_targets": [
    {
      "area": "workflow_basics",
      "source_files": [
        "src/config/workflow.rs",
        "src/cook/workflow/executor.rs"
      ],
      "feature_categories": [
        "structure",
        "execution_model",
        "commit_tracking"
      ]
    },
    {
      "area": "mapreduce",
      "source_files": [
        "src/config/mapreduce.rs",
        "src/cook/execution/mapreduce/"
      ],
      "feature_categories": [
        "phases",
        "capabilities",
        "configuration"
      ]
    }
  ]
}
</code></pre>
<p><strong>Key customization points</strong>:</p>
<ul>
<li><code>source_files</code>: Paths to Rust source files (<code>.rs</code> extension)</li>
<li>Directory patterns: <code>src/cook/execution/mapreduce/</code> analyzes all files in that directory</li>
<li><code>feature_categories</code>: Organize by Rust-specific concepts (structs, enums, traits)</li>
</ul>
<h4 id="python-project-adaptation"><a class="header" href="#python-project-adaptation">Python Project Adaptation</a></h4>
<pre><code class="language-json">{
  "project_name": "MyPythonProject",
  "project_type": "library",
  "book_dir": "docs",
  "analysis_targets": [
    {
      "area": "core_api",
      "source_files": [
        "src/myproject/api.py",
        "src/myproject/client.py"
      ],
      "feature_categories": [
        "classes",
        "functions",
        "decorators"
      ]
    },
    {
      "area": "data_models",
      "source_files": [
        "src/myproject/models/"
      ],
      "feature_categories": [
        "dataclasses",
        "validation",
        "serialization"
      ]
    }
  ]
}
</code></pre>
<p><strong>Differences for Python</strong>:</p>
<ul>
<li>File extension: <code>.py</code> instead of <code>.rs</code></li>
<li>Feature categories: <code>classes</code>, <code>functions</code>, <code>decorators</code> (Python-specific)</li>
<li>Common patterns: Separate <code>models/</code> directory, <code>api.py</code> modules</li>
</ul>
<h4 id="javascripttypescript-project"><a class="header" href="#javascripttypescript-project">JavaScript/TypeScript Project</a></h4>
<pre><code class="language-json">{
  "project_name": "MyJSProject",
  "project_type": "web_framework",
  "book_dir": "book",
  "analysis_targets": [
    {
      "area": "components",
      "source_files": [
        "src/components/**/*.tsx",
        "src/components/**/*.ts"
      ],
      "feature_categories": [
        "react_components",
        "hooks",
        "context"
      ]
    },
    {
      "area": "api",
      "source_files": [
        "src/api/",
        "src/types/"
      ],
      "feature_categories": [
        "endpoints",
        "types",
        "interfaces"
      ]
    }
  ]
}
</code></pre>
<p><strong>JavaScript/TypeScript specifics</strong>:</p>
<ul>
<li>Glob patterns: <code>**/*.tsx</code>, <code>**/*.ts</code> for nested directories</li>
<li>Feature categories: <code>react_components</code>, <code>hooks</code> for React projects</li>
<li>Type definitions: Separate <code>types/</code> directory common in TypeScript</li>
</ul>
<h3 id="workflow-customization-patterns"><a class="header" href="#workflow-customization-patterns">Workflow Customization Patterns</a></h3>
<p>The workflow YAML can be customized for different use cases by adjusting parallelism, timeouts, and error handling.</p>
<h4 id="high-parallelism-workflow-large-codebases"><a class="header" href="#high-parallelism-workflow-large-codebases">High-Parallelism Workflow (Large Codebases)</a></h4>
<p><strong>Source</strong>: <code>workflows/book-docs-drift.yml:9-21</code></p>
<pre><code class="language-yaml">name: fast-documentation-update
mode: mapreduce

env:
  PROJECT_NAME: "LargeProject"
  PROJECT_CONFIG: ".prodigy/book-config.json"
  FEATURES_PATH: ".prodigy/book-analysis/features.json"
  BOOK_DIR: "book"
  ANALYSIS_DIR: ".prodigy/book-analysis"
  CHAPTERS_FILE: "workflows/data/chapters.json"
  MAX_PARALLEL: "10"  # High parallelism for large projects

setup:
  - shell: "mkdir -p $ANALYSIS_DIR"
  - claude: "/analyze-features --project $PROJECT_NAME --config $PROJECT_CONFIG"

map:
  input: "${ANALYSIS_DIR}/flattened-items.json"
  json_path: "$[*]"
  agent_template:
    - claude: "/analyze-drift --project $PROJECT_NAME --json '${item}' --features $FEATURES_PATH"
      commit_required: true
    - claude: "/fix-drift --project $PROJECT_NAME --json '${item}'"
      commit_required: true
  max_parallel: ${MAX_PARALLEL}  # Use environment variable
</code></pre>
<p><strong>Customization highlights</strong>:</p>
<ul>
<li><code>MAX_PARALLEL: "10"</code>: Process 10 chapters simultaneously (vs default 3)</li>
<li>Use for projects with 50+ documentation chapters</li>
<li>Requires adequate system resources (CPU, memory)</li>
</ul>
<p><strong>Source reference</strong>: Default parallelism setting from <code>workflows/book-docs-drift.yml:21</code></p>
<h4 id="conservative-workflow-strict-validation"><a class="header" href="#conservative-workflow-strict-validation">Conservative Workflow (Strict Validation)</a></h4>
<pre><code class="language-yaml">name: strict-documentation-workflow
mode: mapreduce

env:
  MAX_PARALLEL: "1"  # Sequential processing for careful review

map:
  input: "${ANALYSIS_DIR}/items.json"
  json_path: "$[*]"
  agent_template:
    - claude: "/analyze-drift --project $PROJECT_NAME --json '${item}'"
      commit_required: true
    - claude: "/fix-drift --project $PROJECT_NAME --json '${item}'"
      commit_required: true
      validate:
        claude: "/validate-fix --project $PROJECT_NAME --json '${item}' --output .prodigy/validation.json"
        result_file: ".prodigy/validation.json"
        threshold: 100  # 100% quality required
        on_incomplete:
          claude: "/complete-fix --project $PROJECT_NAME --json '${item}' --gaps ${validation.gaps}"
          max_attempts: 5  # More attempts for quality
          fail_workflow: true  # Fail if quality not met
  max_parallel: ${MAX_PARALLEL}

error_policy:
  on_item_failure: fail_immediately  # Stop on first error
  continue_on_failure: false
  max_failures: 0
</code></pre>
<p><strong>Use cases</strong>:</p>
<ul>
<li>Critical documentation (public APIs, compliance docs)</li>
<li>When every chapter must meet quality standards</li>
<li>Pre-release documentation review</li>
</ul>
<p><strong>Source reference</strong>: Validation configuration from <code>workflows/book-docs-drift.yml:49-56</code></p>
<h4 id="cicd-optimized-workflow"><a class="header" href="#cicd-optimized-workflow">CI/CD-Optimized Workflow</a></h4>
<pre><code class="language-yaml">name: ci-documentation-check
mode: mapreduce

env:
  MAX_PARALLEL: "5"
  CI_MODE: "true"

setup:
  - shell: "mkdir -p $ANALYSIS_DIR"
  - claude: "/analyze-features --project $PROJECT_NAME --config $PROJECT_CONFIG"
    timeout: 300  # 5-minute timeout for CI

map:
  input: "${ANALYSIS_DIR}/items.json"
  json_path: "$[*]"
  agent_template:
    - claude: "/analyze-drift --project $PROJECT_NAME --json '${item}'"
      timeout: 180  # 3-minute timeout per chapter
  max_parallel: ${MAX_PARALLEL}

reduce:
  - shell: "cd book &amp;&amp; mdbook build"
  - shell: "test -d book/book || exit 1"  # Verify build output

error_policy:
  on_item_failure: dlq
  continue_on_failure: true
  max_failures: 5  # Allow some failures in CI
  error_collection: aggregate
</code></pre>
<p><strong>CI/CD features</strong>:</p>
<ul>
<li>Timeouts on all commands to prevent CI hangs</li>
<li>Error aggregation instead of fail-fast</li>
<li>Verification steps in reduce phase</li>
<li>DLQ for failed items (can retry later)</li>
</ul>
<p><strong>Source reference</strong>: Error policy from <code>workflows/book-docs-drift.yml:86-90</code></p>
<h4 id="development-workflow-verbose-output"><a class="header" href="#development-workflow-verbose-output">Development Workflow (Verbose Output)</a></h4>
<p>Set environment variables for detailed logging:</p>
<pre><code class="language-bash"># Enable verbose Claude output
export PRODIGY_CLAUDE_CONSOLE_OUTPUT=true

# Run workflow with verbose flag
prodigy run workflows/book-docs-drift.yml -v
</code></pre>
<p><strong>Source reference</strong>: Verbosity control documented in <code>workflows/book-docs-drift.yml</code> comments and CLAUDE.md</p>
<h3 id="chapter-structure-customization"><a class="header" href="#chapter-structure-customization">Chapter Structure Customization</a></h3>
<p>The <code>chapters.json</code> file defines your documentation structure. You can choose between flat and hierarchical organization.</p>
<h4 id="simple-flat-structure-small-projects"><a class="header" href="#simple-flat-structure-small-projects">Simple Flat Structure (Small Projects)</a></h4>
<p><strong>Source</strong>: <code>workflows/data/prodigy-chapters.json:245-258</code></p>
<pre><code class="language-json">{
  "chapters": [
    {
      "id": "commands",
      "title": "Command Types",
      "type": "single-file",
      "file": "book/src/commands.md",
      "topics": [
        "Shell commands",
        "Claude commands",
        "Goal-seeking"
      ],
      "validation": "Check all command types documented"
    }
  ]
}
</code></pre>
<p><strong>When to use</strong>:</p>
<ul>
<li>Projects with &lt; 10 documentation chapters</li>
<li>Simple, linear documentation flow</li>
<li>Quick setup and maintenance</li>
</ul>
<h4 id="multi-level-hierarchical-structure-complex-projects"><a class="header" href="#multi-level-hierarchical-structure-complex-projects">Multi-Level Hierarchical Structure (Complex Projects)</a></h4>
<p><strong>Source</strong>: <code>workflows/data/prodigy-chapters.json:89-234</code></p>
<pre><code class="language-json">{
  "chapters": [
    {
      "id": "mapreduce",
      "title": "MapReduce Workflows",
      "type": "multi-subsection",
      "index_file": "book/src/mapreduce/index.md",
      "topics": [
        "MapReduce mode",
        "Setup phase",
        "Map phase",
        "Reduce phase"
      ],
      "subsections": [
        {
          "id": "checkpoint-and-resume",
          "title": "Checkpoint and Resume",
          "file": "book/src/mapreduce/checkpoint-and-resume.md",
          "topics": ["checkpoints", "resume", "recovery"],
          "feature_mapping": [
            "mapreduce.checkpoint",
            "mapreduce.resume"
          ]
        },
        {
          "id": "dead-letter-queue-dlq",
          "title": "Dead Letter Queue (DLQ)",
          "file": "book/src/mapreduce/dead-letter-queue-dlq.md",
          "topics": ["failed items", "retry", "DLQ"],
          "feature_mapping": [
            "mapreduce.dlq",
            "error_handling.dlq"
          ]
        }
      ]
    }
  ]
}
</code></pre>
<p><strong>Benefits</strong>:</p>
<ul>
<li>Scoped feature analysis per subsection</li>
<li>Parallel processing of subsections</li>
<li>Clear topic boundaries</li>
<li>Better organization for large chapters</li>
</ul>
<p><strong>Migration path</strong>: See <a href="automated-documentation/advanced-configuration.html">Advanced Configuration</a> for converting single-file chapters to multi-subsection format.</p>
<h3 id="project-type-examples"><a class="header" href="#project-type-examples">Project Type Examples</a></h3>
<p>Different project types require different documentation focus areas.</p>
<h4 id="cli-tool-configuration"><a class="header" href="#cli-tool-configuration">CLI Tool Configuration</a></h4>
<p><strong>Source</strong>: <code>.prodigy/book-config.json:2-3</code></p>
<pre><code class="language-json">{
  "project_type": "cli_tool",
  "analysis_targets": [
    {
      "area": "commands",
      "source_files": ["src/cli/", "src/commands/"],
      "feature_categories": [
        "subcommands",
        "arguments",
        "flags",
        "output_formatting"
      ]
    },
    {
      "area": "configuration",
      "source_files": ["src/config/"],
      "feature_categories": [
        "config_files",
        "precedence",
        "validation"
      ]
    }
  ]
}
</code></pre>
<p><strong>CLI-specific focus</strong>:</p>
<ul>
<li>Command-line interface documentation</li>
<li>Configuration file formats</li>
<li>Usage examples and flags</li>
<li>Exit codes and error messages</li>
</ul>
<h4 id="library-configuration"><a class="header" href="#library-configuration">Library Configuration</a></h4>
<pre><code class="language-json">{
  "project_type": "library",
  "analysis_targets": [
    {
      "area": "public_api",
      "source_files": ["src/lib.rs", "src/api/"],
      "feature_categories": [
        "public_functions",
        "types",
        "traits",
        "error_types"
      ]
    },
    {
      "area": "examples",
      "source_files": ["examples/"],
      "feature_categories": [
        "usage_patterns",
        "integration_examples"
      ]
    }
  ]
}
</code></pre>
<p><strong>Library-specific focus</strong>:</p>
<ul>
<li>Public API documentation</li>
<li>Usage examples</li>
<li>Integration patterns</li>
<li>Error handling for library users</li>
</ul>
<h3 id="environment-variable-customization"><a class="header" href="#environment-variable-customization">Environment Variable Customization</a></h3>
<p>Customize workflow behavior through environment variables without modifying YAML.</p>
<p><strong>Source</strong>: <code>workflows/book-docs-drift.yml:9-21</code></p>
<h4 id="development-environment"><a class="header" href="#development-environment">Development Environment</a></h4>
<pre><code class="language-bash"># Development: Verbose output, sequential processing
export PROJECT_NAME="MyProject"
export MAX_PARALLEL="1"
export PRODIGY_CLAUDE_CONSOLE_OUTPUT="true"

prodigy run workflows/book-docs-drift.yml -v
</code></pre>
<h4 id="cicd-environment"><a class="header" href="#cicd-environment">CI/CD Environment</a></h4>
<pre><code class="language-bash"># CI/CD: Moderate parallelism, no console output
export PROJECT_NAME="MyProject"
export MAX_PARALLEL="5"
export PRODIGY_CLAUDE_STREAMING="false"  # Disable for CI logs

prodigy run workflows/book-docs-drift.yml
</code></pre>
<h4 id="production-environment"><a class="header" href="#production-environment">Production Environment</a></h4>
<pre><code class="language-bash"># Production: High parallelism, optimized for speed
export PROJECT_NAME="MyProject"
export MAX_PARALLEL="10"
export ANALYSIS_DIR=".prodigy/book-analysis"

prodigy run workflows/book-docs-drift.yml
</code></pre>
<h3 id="custom-claude-commands"><a class="header" href="#custom-claude-commands">Custom Claude Commands</a></h3>
<p>You can create project-specific Claude commands for specialized analysis or formatting.</p>
<p><strong>Example location</strong>: <code>.claude/commands/</code> directory</p>
<h4 id="custom-analysis-command"><a class="header" href="#custom-analysis-command">Custom Analysis Command</a></h4>
<p>Create <code>.claude/commands/my-custom-analysis.md</code>:</p>
<pre><code class="language-markdown"># /my-custom-analysis

Analyze code for project-specific patterns.

## Variables
- `--file &lt;path&gt;` - File to analyze
- `--pattern &lt;name&gt;` - Pattern to check

## Execute
1. Read the file at ${file}
2. Check for pattern: ${pattern}
3. Generate report
4. Commit findings
</code></pre>
<p><strong>Usage in workflow</strong>:</p>
<pre><code class="language-yaml">map:
  agent_template:
    - claude: "/my-custom-analysis --file '${item.file}' --pattern 'async-await'"
</code></pre>
<p><strong>Source reference</strong>: Custom commands structure from <code>.prodigy/book-config.json:671</code></p>
<h3 id="quick-customization-checklist"><a class="header" href="#quick-customization-checklist">Quick Customization Checklist</a></h3>
<p><strong>For a new project, customize</strong>:</p>
<ol>
<li>
<p><strong>book-config.json</strong>:</p>
<ul>
<li><input disabled="" type="checkbox"/>
Set <code>project_name</code> and <code>project_type</code></li>
<li><input disabled="" type="checkbox"/>
Define <code>analysis_targets</code> for your language</li>
<li><input disabled="" type="checkbox"/>
Set <code>book_dir</code> path</li>
<li><input disabled="" type="checkbox"/>
Configure <code>feature_categories</code></li>
</ul>
</li>
<li>
<p><strong>chapters.json</strong>:</p>
<ul>
<li><input disabled="" type="checkbox"/>
Define chapter structure (flat or hierarchical)</li>
<li><input disabled="" type="checkbox"/>
Set validation rules per chapter</li>
<li><input disabled="" type="checkbox"/>
Add feature mappings for subsections</li>
</ul>
</li>
<li>
<p><strong>workflow YAML</strong>:</p>
<ul>
<li><input disabled="" type="checkbox"/>
Set <code>MAX_PARALLEL</code> for your system</li>
<li><input disabled="" type="checkbox"/>
Configure timeouts for your project size</li>
<li><input disabled="" type="checkbox"/>
Choose error handling strategy</li>
<li><input disabled="" type="checkbox"/>
Add project-specific validation</li>
</ul>
</li>
<li>
<p><strong>Environment variables</strong>:</p>
<ul>
<li><input disabled="" type="checkbox"/>
Set <code>PROJECT_NAME</code></li>
<li><input disabled="" type="checkbox"/>
Configure <code>ANALYSIS_DIR</code></li>
<li><input disabled="" type="checkbox"/>
Set parallelism level</li>
</ul>
</li>
</ol>
<h3 id="see-also-38"><a class="header" href="#see-also-38">See Also</a></h3>
<ul>
<li><a href="automated-documentation/advanced-configuration.html">Advanced Configuration</a> - Detailed configuration options</li>
<li><a href="automated-documentation/understanding-the-workflow.html">Understanding the Workflow</a> - How the workflow executes</li>
<li><a href="automated-documentation/quick-start-30-minutes.html">Quick Start (30 Minutes)</a> - Get started quickly</li>
<li><a href="automated-documentation/github-actions-integration.html">GitHub Actions Integration</a> - Automate with CI/CD</li>
<li><a href="automated-documentation/best-practices.html">Best Practices</a> - Recommended patterns</li>
</ul>
<h3 id="troubleshooting-customizations"><a class="header" href="#troubleshooting-customizations">Troubleshooting Customizations</a></h3>
<p><strong>Issue: Analysis not finding features</strong></p>
<ul>
<li>Verify <code>source_files</code> patterns match your project structure</li>
<li>Check file extensions are correct for your language</li>
<li>Use glob patterns (<code>**/*.ext</code>) for nested directories</li>
</ul>
<p><strong>Issue: Workflow too slow</strong></p>
<ul>
<li>Increase <code>MAX_PARALLEL</code> gradually (3 → 5 → 10)</li>
<li>Check system resources (CPU, memory)</li>
<li>Consider splitting large chapters into subsections</li>
</ul>
<p><strong>Issue: Quality validation failing</strong></p>
<ul>
<li>Lower <code>threshold</code> from 100 to 80 for initial setup</li>
<li>Increase <code>max_attempts</code> in <code>on_incomplete</code></li>
<li>Review validation error messages in <code>.prodigy/validation-result.json</code></li>
</ul>
<p><strong>Issue: Wrong language patterns detected</strong></p>
<ul>
<li>Ensure <code>feature_categories</code> match your language paradigm</li>
<li>Customize analysis prompts in custom Claude commands</li>
<li>Use language-specific terminology in topic definitions</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="best-practices-26"><a class="header" href="#best-practices-26">Best Practices</a></h2>
<p>This section covers best practices for creating and maintaining automated documentation workflows with Prodigy.</p>
<h3 id="workflow-design-best-practices"><a class="header" href="#workflow-design-best-practices">Workflow Design Best Practices</a></h3>
<h4 id="keep-workflows-simple-and-focused"><a class="header" href="#keep-workflows-simple-and-focused">Keep Workflows Simple and Focused</a></h4>
<p>Each workflow should have a single, clear purpose. Break complex workflows into smaller, composable pieces.</p>
<p><strong>Good Example:</strong></p>
<pre><code class="language-yaml"># Simple, focused workflow for drift detection
name: prodigy-book-docs-drift-detection
mode: mapreduce

setup:
  - claude: "/prodigy-analyze-features-for-book --project $PROJECT_NAME"
  - claude: "/prodigy-detect-documentation-gaps --project $PROJECT_NAME"
</code></pre>
<p><strong>Source</strong>: workflows/book-docs-drift.yml:1-34</p>
<p><strong>Anti-Pattern:</strong></p>
<pre><code class="language-yaml"># Overly complex workflow doing too many things
- claude: "/analyze-features"
- claude: "/detect-gaps"
- claude: "/fix-everything"
- claude: "/deploy-to-production"
- claude: "/send-notifications"
</code></pre>
<h4 id="use-environment-variables-for-parameterization"><a class="header" href="#use-environment-variables-for-parameterization">Use Environment Variables for Parameterization</a></h4>
<p>Define reusable values as environment variables instead of hardcoding them throughout your workflow.</p>
<pre><code class="language-yaml">env:
  PROJECT_NAME: "Prodigy"
  FEATURES_PATH: ".prodigy/book-analysis/features.json"
  BOOK_DIR: "book"
  MAX_PARALLEL: "3"

setup:
  - claude: "/prodigy-analyze-features --project $PROJECT_NAME --features $FEATURES_PATH"
</code></pre>
<p><strong>Source</strong>: workflows/book-docs-drift.yml:8-21</p>
<p><strong>Benefits:</strong></p>
<ul>
<li>Easy configuration changes without touching workflow logic</li>
<li>Consistent naming across all commands</li>
<li>Support for different environments via profiles</li>
</ul>
<p>See <a href="automated-documentation/../configuration/environment-variables.html">Environment Variables</a> for comprehensive documentation.</p>
<h4 id="include-validation-steps"><a class="header" href="#include-validation-steps">Include Validation Steps</a></h4>
<p>Always validate your outputs to ensure quality. Use <code>goal_seek</code> for iterative improvement.</p>
<pre><code class="language-yaml">- goal_seek:
    goal: "Achieve 90% test coverage"
    claude: "/prodigy-coverage --improve"
    validate: "cargo tarpaulin --print-summary 2&gt;/dev/null | grep 'Coverage' | sed 's/.*Coverage=\\([0-9]*\\).*/score: \\1/'"
    threshold: 90
    max_attempts: 5
</code></pre>
<p><strong>Source</strong>: workflows/goal-seeking-examples.yml:6-13</p>
<h4 id="capture-and-use-command-output"><a class="header" href="#capture-and-use-command-output">Capture and Use Command Output</a></h4>
<p>Use <code>capture_output: true</code> to make command results available to subsequent steps.</p>
<pre><code class="language-yaml">- shell: "prodigy validate-spec --spec specs/auth.md --json"
  capture_output: true

- claude: "/implement-missing-features ${shell.output}"
</code></pre>
<h4 id="add-commit-requirements"><a class="header" href="#add-commit-requirements">Add Commit Requirements</a></h4>
<p>Specify <code>commit_required: true</code> for steps that should produce changes. This ensures your workflow makes actual progress.</p>
<pre><code class="language-yaml">- claude: "/prodigy-fix-subsection-drift --project $PROJECT_NAME"
  commit_required: true
</code></pre>
<p><strong>Source</strong>: workflows/book-docs-drift.yml:48</p>
<h3 id="mapreduce-best-practices"><a class="header" href="#mapreduce-best-practices">MapReduce Best Practices</a></h3>
<h4 id="choose-appropriate-parallelism"><a class="header" href="#choose-appropriate-parallelism">Choose Appropriate Parallelism</a></h4>
<p>Set <code>max_parallel</code> based on:</p>
<ul>
<li>Resource constraints (CPU, memory)</li>
<li>Network/API rate limits</li>
<li>Complexity of each work item</li>
</ul>
<pre><code class="language-yaml">map:
  max_parallel: 4  # Run up to 4 agents in parallel
</code></pre>
<p><strong>Source</strong>: workflows/documentation-drift-mapreduce.yml:74</p>
<p><strong>Guidelines:</strong></p>
<ul>
<li><strong>Simple operations</strong> (file analysis, linting): 8-10 parallel agents</li>
<li><strong>Medium operations</strong> (code fixes, refactoring): 4-6 parallel agents</li>
<li><strong>Complex operations</strong> (feature implementation): 2-3 parallel agents</li>
<li><strong>Claude-heavy workflows</strong>: 3-5 parallel agents (respects rate limits)</li>
</ul>
<h4 id="use-setup-phase-for-work-item-generation"><a class="header" href="#use-setup-phase-for-work-item-generation">Use Setup Phase for Work Item Generation</a></h4>
<p>The setup phase is ideal for generating work items dynamically based on codebase analysis.</p>
<pre><code class="language-yaml">setup:
  - shell: "mkdir -p $ANALYSIS_DIR"
  - claude: "/prodigy-analyze-features-for-book --project $PROJECT_NAME"
  - claude: "/prodigy-detect-documentation-gaps --project $PROJECT_NAME"
</code></pre>
<p><strong>Source</strong>: workflows/book-docs-drift.yml:24-34</p>
<p>This ensures work items are always current and reflect the latest state of your codebase.</p>
<h4 id="filter-and-sort-work-items"><a class="header" href="#filter-and-sort-work-items">Filter and Sort Work Items</a></h4>
<p>Process high-priority items first and skip low-value work.</p>
<pre><code class="language-yaml">map:
  filter: "File.score &gt;= 10 OR Function.unified_score.final_score &gt;= 10"
  sort_by: "File.score DESC NULLS LAST, Function.unified_score.final_score DESC NULLS LAST"
</code></pre>
<p><strong>Source</strong>: workflows/debtmap-reduce.yml:79-80</p>
<p><strong>Common patterns:</strong></p>
<ul>
<li><code>filter: "severity == 'high' || severity == 'critical'"</code> - Process critical items only</li>
<li><code>sort_by: "priority DESC"</code> - High priority first</li>
<li><code>sort_by: "complexity ASC"</code> - Simple items first (builds momentum)</li>
</ul>
<h4 id="configure-error-handling-policies"><a class="header" href="#configure-error-handling-policies">Configure Error Handling Policies</a></h4>
<p>Use <code>on_item_failure: dlq</code> to continue processing when individual items fail.</p>
<pre><code class="language-yaml">error_policy:
  on_item_failure: dlq
  continue_on_failure: true
  max_failures: 2
  error_collection: aggregate
</code></pre>
<p><strong>Source</strong>: workflows/book-docs-drift.yml:86-90</p>
<p><strong>Error handling strategies:</strong></p>
<ul>
<li><code>on_item_failure: dlq</code> - Continue processing, collect failures in Dead Letter Queue</li>
<li><code>continue_on_failure: true</code> - Keep going when items fail</li>
<li><code>max_failures: 2</code> - Fail workflow if too many items fail (prevents cascading failures)</li>
<li><code>error_collection: aggregate</code> - Collect all errors for later analysis</li>
</ul>
<h4 id="monitor-events-and-dlq"><a class="header" href="#monitor-events-and-dlq">Monitor Events and DLQ</a></h4>
<p>Use Prodigy’s event system and DLQ commands to track progress and handle failures:</p>
<pre><code class="language-bash"># View events for a job
prodigy events &lt;job_id&gt;

# Check failed items in DLQ
prodigy dlq show &lt;job_id&gt;

# Retry failed items after fixing issues
prodigy dlq retry &lt;job_id&gt; --max-parallel 5
</code></pre>
<h4 id="use-reduce-phase-for-aggregation"><a class="header" href="#use-reduce-phase-for-aggregation">Use Reduce Phase for Aggregation</a></h4>
<p>The reduce phase runs after all map agents complete. Use it to:</p>
<ul>
<li>Validate combined results</li>
<li>Build summary reports</li>
<li>Perform final cleanup</li>
</ul>
<pre><code class="language-yaml">reduce:
  - shell: "cd book &amp;&amp; mdbook build"
    on_failure:
      claude: "/prodigy-fix-book-build-errors --project $PROJECT_NAME"
      commit_required: true

  - shell: "rm -rf ${ANALYSIS_DIR}"
</code></pre>
<p><strong>Source</strong>: workflows/book-docs-drift.yml:62-81</p>
<h3 id="testing-best-practices"><a class="header" href="#testing-best-practices">Testing Best Practices</a></h3>
<h4 id="include-test-validation-steps"><a class="header" href="#include-test-validation-steps">Include Test Validation Steps</a></h4>
<p>Always verify changes don’t break existing functionality:</p>
<pre><code class="language-yaml">- shell: "cargo test"
  on_failure:
    claude: "/debug-test-failures"
</code></pre>
<p><strong>Source</strong>: workflows/mapreduce-example.yml:18-20</p>
<h4 id="test-workflows-incrementally"><a class="header" href="#test-workflows-incrementally">Test Workflows Incrementally</a></h4>
<p>Start with a minimal test workflow before scaling up:</p>
<pre><code class="language-yaml"># Minimal MapReduce workflow to test merge-to-parent functionality
name: minimal-mapreduce-test
mode: mapreduce

setup:
  - shell: |
      echo '[
        {"id": 1, "name": "item-one"},
        {"id": 2, "name": "item-two"}
      ]' &gt; test-items.json

map:
  input: test-items.json
  json_path: $[*]
  max_parallel: 2

  agent_template:
    - shell: echo "Processed by agent: ${item.name}" &gt; output-${item.name}.txt
    - shell: git add output-${item.name}.txt
    - shell: git commit -m "Process ${item.name}"
</code></pre>
<p><strong>Source</strong>: workflows/tests/minimal-mapreduce.yml:1-50</p>
<p><strong>Testing strategy:</strong></p>
<ol>
<li>Start with 2-3 work items</li>
<li>Use <code>max_parallel: 2</code> for easier debugging</li>
<li>Verify results before processing full dataset</li>
<li>Scale up parallelism gradually</li>
</ol>
<h4 id="use-goal-seeking-for-automated-debugging"><a class="header" href="#use-goal-seeking-for-automated-debugging">Use Goal-Seeking for Automated Debugging</a></h4>
<p>Let Prodigy iteratively fix test failures:</p>
<pre><code class="language-yaml">- shell: "cargo test"
  on_failure:
    goal_seek:
      goal: "Fix all failing tests"
      claude: "/debug-test-failures"
      validate: |
        cargo test 2&gt;&amp;1 | grep -q "test result: ok" &amp;&amp; echo "score: 100" || {
          passed=$(cargo test 2&gt;&amp;1 | grep -oP '\d+(?= passed)' | head -1)
          failed=$(cargo test 2&gt;&amp;1 | grep -oP '\d+(?= failed)' | head -1)
          total=$((passed + failed))
          score=$((passed * 100 / total))
          echo "score: $score"
        }
      threshold: 100
      max_attempts: 3
</code></pre>
<p><strong>Source</strong>: workflows/goal-seeking-examples.yml:76-95</p>
<h4 id="verify-checkpointresume-functionality"><a class="header" href="#verify-checkpointresume-functionality">Verify Checkpoint/Resume Functionality</a></h4>
<p>Test that your workflow can resume from interruption:</p>
<pre><code class="language-bash"># Start workflow
prodigy run workflow.yml

# Interrupt it (Ctrl+C)

# Resume from checkpoint
prodigy resume &lt;session-id&gt;
</code></pre>
<p>All map phase progress should be preserved. See <a href="automated-documentation/../mapreduce/checkpoint-and-resume.html">Checkpoint and Resume</a> for details.</p>
<h3 id="error-handling-best-practices"><a class="header" href="#error-handling-best-practices">Error Handling Best Practices</a></h3>
<h4 id="use-on_failure-for-recovery"><a class="header" href="#use-on_failure-for-recovery">Use on_failure for Recovery</a></h4>
<p>Define recovery strategies for predictable failures:</p>
<pre><code class="language-yaml">- shell: "cargo build --release"
  on_failure:
    claude: "/fix-build-errors"
    commit_required: true
</code></pre>
<h4 id="provide-contextual-error-information"><a class="header" href="#provide-contextual-error-information">Provide Contextual Error Information</a></h4>
<p>Pass error output to Claude for better debugging:</p>
<pre><code class="language-yaml">- shell: "cargo test"
  on_failure:
    claude: "/debug-test ${shell.output}"
</code></pre>
<p><strong>Source</strong>: workflows/mapreduce-example.yml:18-20</p>
<h4 id="set-reasonable-max-attempts"><a class="header" href="#set-reasonable-max-attempts">Set Reasonable Max Attempts</a></h4>
<p>For iterative fixes with validation, limit attempts to prevent infinite loops:</p>
<pre><code class="language-yaml">validate:
  claude: "/prodigy-validate-doc-fix --project $PROJECT_NAME"
  threshold: 100
  on_incomplete:
    claude: "/prodigy-complete-doc-fix --project $PROJECT_NAME"
    max_attempts: 3
    fail_workflow: false
</code></pre>
<p><strong>Source</strong>: workflows/book-docs-drift.yml:49-56</p>
<p><strong>Guidelines:</strong></p>
<ul>
<li>Simple fixes: <code>max_attempts: 2-3</code></li>
<li>Complex fixes: <code>max_attempts: 5</code></li>
<li>Set <code>fail_workflow: false</code> if partial progress is acceptable</li>
</ul>
<h3 id="environment-management-best-practices"><a class="header" href="#environment-management-best-practices">Environment Management Best Practices</a></h3>
<h4 id="use-profiles-for-different-environments-1"><a class="header" href="#use-profiles-for-different-environments-1">Use Profiles for Different Environments</a></h4>
<p>Define environment-specific configurations:</p>
<pre><code class="language-yaml">env:
  API_URL:
    default: http://localhost:3000
    staging: https://staging.api.com
    prod: https://api.com

profiles:
  development:
    NODE_ENV: development
    DEBUG: "true"

  testing:
    NODE_ENV: test
    COVERAGE: "true"
</code></pre>
<p><strong>Source</strong>: workflows/environment-example.yml:4-39</p>
<p>Activate with: <code>prodigy run workflow.yml --profile prod</code></p>
<h4 id="mark-secrets-appropriately"><a class="header" href="#mark-secrets-appropriately">Mark Secrets Appropriately</a></h4>
<p>Use the <code>secrets</code> block to mask sensitive values in logs:</p>
<pre><code class="language-yaml">secrets:
  API_KEY: "${env:SECRET_API_KEY}"
</code></pre>
<p><strong>Source</strong>: workflows/environment-example.yml:21-23</p>
<p>Secrets are automatically masked in command output, error messages, and event logs.</p>
<h4 id="load-environment-files"><a class="header" href="#load-environment-files">Load Environment Files</a></h4>
<p>Use <code>env_files</code> to load variables from external files:</p>
<pre><code class="language-yaml">env_files:
  - .env.production
</code></pre>
<p><strong>Source</strong>: workflows/environment-example.yml:25-27</p>
<p>This keeps sensitive values out of version control.</p>
<h4 id="document-required-variables"><a class="header" href="#document-required-variables">Document Required Variables</a></h4>
<p>Add comments explaining what each environment variable does:</p>
<pre><code class="language-yaml">env:
  # Project configuration
  PROJECT_NAME: "Prodigy"
  PROJECT_CONFIG: ".prodigy/book-config.json"

  # Book-specific settings
  BOOK_DIR: "book"
  MAX_PARALLEL: "3"
</code></pre>
<p><strong>Source</strong>: workflows/book-docs-drift.yml:10-21</p>
<h3 id="documentation-specific-best-practices"><a class="header" href="#documentation-specific-best-practices">Documentation-Specific Best Practices</a></h3>
<h4 id="structure-feature-analysis-first"><a class="header" href="#structure-feature-analysis-first">Structure Feature Analysis First</a></h4>
<p>Always run feature analysis before gap detection:</p>
<pre><code class="language-yaml">setup:
  - claude: "/prodigy-analyze-features-for-book --project $PROJECT_NAME"
  - claude: "/prodigy-detect-documentation-gaps --project $PROJECT_NAME"
</code></pre>
<p>This ensures gap detection knows what features exist in your codebase.</p>
<h4 id="use-subsection-aware-commands"><a class="header" href="#use-subsection-aware-commands">Use Subsection-Aware Commands</a></h4>
<p>When working with multi-subsection chapters, use subsection-aware commands:</p>
<pre><code class="language-yaml">map:
  agent_template:
    - claude: "/prodigy-analyze-subsection-drift --project $PROJECT_NAME --json '${item}'"
    - claude: "/prodigy-fix-subsection-drift --project $PROJECT_NAME --json '${item}'"
</code></pre>
<p><strong>Source</strong>: workflows/book-docs-drift.yml:42-47</p>
<p>These commands understand subsection scope and preserve cross-references.</p>
<h4 id="configure-validation-thresholds"><a class="header" href="#configure-validation-thresholds">Configure Validation Thresholds</a></h4>
<p>Set appropriate quality thresholds for documentation:</p>
<pre><code class="language-yaml">validate:
  threshold: 100  # Documentation must meet 100% quality standards
  on_incomplete:
    max_attempts: 3
    fail_workflow: false
    commit_required: true
</code></pre>
<p><strong>Source</strong>: workflows/book-docs-drift.yml:52-57</p>
<p><strong>Recommended thresholds:</strong></p>
<ul>
<li><strong>100%</strong>: Complete, production-ready documentation</li>
<li><strong>95%</strong>: Minor improvements needed</li>
<li><strong>90%</strong>: Good enough for initial review</li>
</ul>
<h4 id="limit-parallel-processing"><a class="header" href="#limit-parallel-processing">Limit Parallel Processing</a></h4>
<p>Documentation workflows are Claude-intensive. Use moderate parallelism:</p>
<pre><code class="language-yaml">map:
  max_parallel: 3  # 3-4 agents optimal for documentation
</code></pre>
<p><strong>Source</strong>: workflows/book-docs-drift.yml:59</p>
<p>Too many parallel agents can hit API rate limits.</p>
<h4 id="handle-dlq-items-appropriately"><a class="header" href="#handle-dlq-items-appropriately">Handle DLQ Items Appropriately</a></h4>
<p>Some documentation items may fail due to missing features or unclear scope. Review DLQ items manually:</p>
<pre><code class="language-bash">prodigy dlq show &lt;job_id&gt;

# Fix issues (add features, clarify scope, etc.)

prodigy dlq retry &lt;job_id&gt;
</code></pre>
<h4 id="verify-mdbook-build"><a class="header" href="#verify-mdbook-build">Verify mdBook Build</a></h4>
<p>Always build the book in the reduce phase to catch broken links:</p>
<pre><code class="language-yaml">reduce:
  - shell: "cd book &amp;&amp; mdbook build"
    on_failure:
      claude: "/prodigy-fix-book-build-errors --project $PROJECT_NAME"
</code></pre>
<p><strong>Source</strong>: workflows/book-docs-drift.yml:63-67</p>
<h3 id="quick-wins-checklist"><a class="header" href="#quick-wins-checklist">Quick Wins Checklist</a></h3>
<p>Use this checklist when creating automated documentation workflows:</p>
<p><strong>Setup Phase:</strong></p>
<ul>
<li><input disabled="" type="checkbox"/>
Feature analysis command runs first</li>
<li><input disabled="" type="checkbox"/>
Gap detection generates work items dynamically</li>
<li><input disabled="" type="checkbox"/>
Environment variables defined for all paths and settings</li>
<li><input disabled="" type="checkbox"/>
Working directory is correct</li>
</ul>
<p><strong>Map Phase:</strong></p>
<ul>
<li><input disabled="" type="checkbox"/>
<code>max_parallel</code> set to 3-4 for Claude-heavy workflows</li>
<li><input disabled="" type="checkbox"/>
Agent template uses subsection-aware commands</li>
<li><input disabled="" type="checkbox"/>
<code>commit_required: true</code> for all fix commands</li>
<li><input disabled="" type="checkbox"/>
Validation configured with appropriate thresholds</li>
</ul>
<p><strong>Reduce Phase:</strong></p>
<ul>
<li><input disabled="" type="checkbox"/>
mdBook build step included</li>
<li><input disabled="" type="checkbox"/>
Build errors have recovery command</li>
<li><input disabled="" type="checkbox"/>
Temporary analysis files cleaned up</li>
</ul>
<p><strong>Error Handling:</strong></p>
<ul>
<li><input disabled="" type="checkbox"/>
<code>on_item_failure: dlq</code> configured</li>
<li><input disabled="" type="checkbox"/>
<code>continue_on_failure: true</code> set</li>
<li><input disabled="" type="checkbox"/>
Reasonable <code>max_failures</code> threshold</li>
</ul>
<p><strong>Testing:</strong></p>
<ul>
<li><input disabled="" type="checkbox"/>
Workflow tested with minimal dataset first</li>
<li><input disabled="" type="checkbox"/>
DLQ retry tested</li>
<li><input disabled="" type="checkbox"/>
Resume functionality verified</li>
</ul>
<h3 id="common-anti-patterns-to-avoid"><a class="header" href="#common-anti-patterns-to-avoid">Common Anti-Patterns to Avoid</a></h3>
<h4 id="over-parallelization"><a class="header" href="#over-parallelization">Over-Parallelization</a></h4>
<p><strong>Problem:</strong></p>
<pre><code class="language-yaml">map:
  max_parallel: 50  # Way too high!
</code></pre>
<p><strong>Impact:</strong> API rate limits, resource exhaustion, difficult debugging</p>
<p><strong>Solution:</strong> Use 3-5 for Claude-heavy workflows, 4-10 for simple operations</p>
<h4 id="missing-error-handlers"><a class="header" href="#missing-error-handlers">Missing Error Handlers</a></h4>
<p><strong>Problem:</strong></p>
<pre><code class="language-yaml">- shell: "cargo build"
# No on_failure handler - workflow fails on first error
</code></pre>
<p><strong>Impact:</strong> Workflow stops at first failure, no recovery</p>
<p><strong>Solution:</strong></p>
<pre><code class="language-yaml">- shell: "cargo build"
  on_failure:
    claude: "/fix-build-errors"
</code></pre>
<h4 id="incomplete-validation"><a class="header" href="#incomplete-validation">Incomplete Validation</a></h4>
<p><strong>Problem:</strong></p>
<pre><code class="language-yaml">- claude: "/fix-drift"
# No validation step - no way to know if it worked
</code></pre>
<p><strong>Impact:</strong> Can’t verify quality, may need manual review</p>
<p><strong>Solution:</strong></p>
<pre><code class="language-yaml">- claude: "/fix-drift"
  validate:
    claude: "/validate-fix"
    threshold: 95
</code></pre>
<h4 id="hardcoded-paths"><a class="header" href="#hardcoded-paths">Hardcoded Paths</a></h4>
<p><strong>Problem:</strong></p>
<pre><code class="language-yaml">- shell: "cd /Users/alice/projects/myapp &amp;&amp; make"
</code></pre>
<p><strong>Impact:</strong> Workflow breaks on other machines</p>
<p><strong>Solution:</strong></p>
<pre><code class="language-yaml">env:
  PROJECT_DIR: "."

commands:
  - shell: "make"
    working_dir: "${PROJECT_DIR}"
</code></pre>
<h4 id="ignoring-dlq-items"><a class="header" href="#ignoring-dlq-items">Ignoring DLQ Items</a></h4>
<p><strong>Problem:</strong> Running workflow, seeing failures, never checking DLQ</p>
<p><strong>Impact:</strong> Work items silently fail, issues accumulate</p>
<p><strong>Solution:</strong> Regularly check and retry DLQ:</p>
<pre><code class="language-bash">prodigy dlq show &lt;job_id&gt;
prodigy dlq retry &lt;job_id&gt;
</code></pre>
<h3 id="related-documentation-3"><a class="header" href="#related-documentation-3">Related Documentation</a></h3>
<ul>
<li><a href="automated-documentation/understanding-the-workflow.html">Understanding the Workflow</a> - How the automated documentation system works</li>
<li><a href="automated-documentation/troubleshooting.html">Troubleshooting</a> - Common issues and solutions</li>
<li><a href="automated-documentation/advanced-configuration.html">Advanced Configuration</a> - Fine-tuning workflow behavior</li>
<li><a href="automated-documentation/github-actions-integration.html">GitHub Actions Integration</a> - Running workflows in CI/CD</li>
<li><a href="automated-documentation/../configuration/environment-variables.html">Environment Variables</a> - Complete environment configuration guide</li>
<li><a href="automated-documentation/../advanced/goal-seeking-operations.html">Goal-Seeking Operations</a> - Iterative improvement workflows</li>
<li><a href="automated-documentation/../mapreduce/checkpoint-and-resume.html">Checkpoint and Resume</a> - Recovery from interruptions</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="troubleshooting-21"><a class="header" href="#troubleshooting-21">Troubleshooting</a></h2>
<p>This guide helps you diagnose and fix issues when running automated documentation workflows. The workflow executes in three phases (setup, map, reduce), and each phase has specific failure modes and debugging techniques.</p>
<p><strong>Source</strong>: Based on <code>workflows/book-docs-drift.yml</code> MapReduce workflow implementation and command definitions in <code>.claude/commands/prodigy-*-book*.md</code></p>
<h2 id="common-issues-by-phase"><a class="header" href="#common-issues-by-phase">Common Issues by Phase</a></h2>
<h3 id="setup-phase-issues"><a class="header" href="#setup-phase-issues">Setup Phase Issues</a></h3>
<p>The setup phase analyzes your codebase and detects documentation gaps. Common failures:</p>
<h4 id="issue-featuresjson-not-generated"><a class="header" href="#issue-featuresjson-not-generated">Issue: “features.json not generated”</a></h4>
<p><strong>Symptoms</strong>: Setup completes but <code>.prodigy/book-analysis/features.json</code> doesn’t exist</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Invalid <code>book-config.json</code> configuration</li>
<li>Missing or incorrect <code>analysis_targets</code> paths</li>
<li>Source files specified in config don’t exist</li>
</ul>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># 1. Verify book-config.json is valid JSON
cat .prodigy/book-config.json | jq .

# 2. Check that source files exist
cat .prodigy/book-config.json | jq -r '.analysis_targets[].source_files[]' | while read f; do
  [ -f "$f" ] || echo "Missing: $f"
done

# 3. Re-run feature analysis manually
prodigy run workflows/book-docs-drift.yml --stop-after setup
</code></pre>
<p><strong>Source</strong>: Configuration structure from <code>.prodigy/book-config.json:7-213</code></p>
<h4 id="issue-no-gaps-detected-when-gaps-should-exist"><a class="header" href="#issue-no-gaps-detected-when-gaps-should-exist">Issue: “No gaps detected when gaps should exist”</a></h4>
<p><strong>Symptoms</strong>: <code>/prodigy-detect-documentation-gaps</code> reports no gaps but documentation is clearly incomplete</p>
<p><strong>Causes</strong>:</p>
<ul>
<li><code>chapters_file</code> path is incorrect in workflow</li>
<li>Chapter definitions JSON is malformed</li>
<li>Features not properly mapped to chapters</li>
</ul>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># 1. Verify chapters file exists and is valid
cat workflows/data/prodigy-chapters.json | jq .

# 2. Check gap detection output
cat .prodigy/book-analysis/gaps.json | jq .

# 3. Validate flattened items were generated
cat .prodigy/book-analysis/flattened-items.json | jq 'length'
</code></pre>
<p><strong>Source</strong>: Gap detection logic from <code>.claude/commands/prodigy-detect-documentation-gaps.md</code></p>
<h4 id="issue-setup-phase-commits-nothing"><a class="header" href="#issue-setup-phase-commits-nothing">Issue: “Setup phase commits nothing”</a></h4>
<p><strong>Symptoms</strong>: Setup completes successfully but no commit is created</p>
<p><strong>Causes</strong>: This is often <strong>not an error</strong> - setup commands use <code>commit_required: false</code> by default and only commit when:</p>
<ul>
<li>Features have changed since last analysis</li>
<li>New documentation gaps are detected</li>
<li>New stub files are created</li>
</ul>
<p><strong>No action needed</strong> if this occurs - the workflow will continue to map phase with existing analysis files.</p>
<p><strong>Source</strong>: Setup phase design from <code>workflows/book-docs-drift.yml:24-34</code></p>
<h3 id="map-phase-issues"><a class="header" href="#map-phase-issues">Map Phase Issues</a></h3>
<p>The map phase processes each chapter/subsection in parallel. Each runs in its own agent worktree.</p>
<h4 id="issue-agent-failed-with-validation-error"><a class="header" href="#issue-agent-failed-with-validation-error">Issue: “Agent failed with validation error”</a></h4>
<p><strong>Symptoms</strong>: Agent completes but fails validation threshold (100% required)</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Documentation fix was incomplete</li>
<li>Examples don’t match codebase implementation</li>
<li>Required sections are missing</li>
</ul>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># 1. Check which agent failed
prodigy dlq show &lt;job_id&gt;

# 2. Find the drift report for the failed item
ls -la .prodigy/book-analysis/drift-*.json

# 3. Review what issues were identified
cat .prodigy/book-analysis/drift-&lt;chapter-id&gt;-&lt;subsection-id&gt;.json | jq '.issues'

# 4. Retry with DLQ
prodigy dlq retry &lt;job_id&gt;
</code></pre>
<p><strong>Source</strong>: Validation configuration from <code>workflows/book-docs-drift.yml:49-57</code></p>
<h4 id="issue-drift-analysis-creates-empty-report"><a class="header" href="#issue-drift-analysis-creates-empty-report">Issue: “Drift analysis creates empty report”</a></h4>
<p><strong>Symptoms</strong>: <code>/prodigy-analyze-subsection-drift</code> commits but drift report shows no issues when drift clearly exists</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Feature mappings are incorrect for the subsection</li>
<li>Source files referenced in feature analysis are empty</li>
<li>Chapter metadata doesn’t match actual file</li>
</ul>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># 1. Inspect the drift report
cat .prodigy/book-analysis/drift-&lt;chapter-id&gt;-&lt;subsection-id&gt;.json | jq .

# 2. Verify feature mappings for this item
cat .prodigy/book-analysis/flattened-items.json | jq '.[] | select(.id=="&lt;subsection-id&gt;")'

# 3. Check that features.json has content for this area
cat .prodigy/book-analysis/features.json | jq 'keys'
</code></pre>
<p><strong>Source</strong>: Drift analysis command from <code>.claude/commands/prodigy-analyze-subsection-drift.md</code></p>
<h4 id="issue-multiple-agents-fail-with-same-error"><a class="header" href="#issue-multiple-agents-fail-with-same-error">Issue: “Multiple agents fail with same error”</a></h4>
<p><strong>Symptoms</strong>: Several parallel agents all fail with identical error messages</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Shared resource conflict (rare with worktree isolation)</li>
<li>Configuration error affecting all agents</li>
<li>System resource exhaustion (disk space, memory)</li>
</ul>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># 1. Check system resources
df -h  # Disk space
free -h  # Memory (Linux) or vm_stat (macOS)

# 2. Review error collection
prodigy events &lt;job_id&gt; | jq 'select(.event_type=="AgentFailed")'

# 3. Reduce parallelism and retry
# Edit workflow: max_parallel: 1
prodigy run workflows/book-docs-drift.yml
</code></pre>
<p><strong>Source</strong>: Error handling policy from <code>workflows/book-docs-drift.yml:86-91</code></p>
<h3 id="reduce-phase-issues"><a class="header" href="#reduce-phase-issues">Reduce Phase Issues</a></h3>
<p>The reduce phase rebuilds the book and cleans up analysis files.</p>
<h4 id="issue-mdbook-build-fails-with-broken-links"><a class="header" href="#issue-mdbook-build-fails-with-broken-links">Issue: “mdbook build fails with broken links”</a></h4>
<p><strong>Symptoms</strong>: <code>mdbook build</code> exits with error listing broken internal links</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Cross-references to non-existent chapters</li>
<li>Relative paths calculated incorrectly</li>
<li>SUMMARY.md out of sync with actual files</li>
</ul>
<p><strong>Solution</strong>:
The workflow automatically handles this:</p>
<pre><code class="language-yaml">- shell: "cd book &amp;&amp; mdbook build"
  on_failure:
    claude: "/prodigy-fix-book-build-errors --project $PROJECT_NAME"
</code></pre>
<p>If manual fix needed:</p>
<pre><code class="language-bash"># 1. See the exact broken links
cd book &amp;&amp; mdbook build 2&gt;&amp;1 | grep "Broken link"

# 2. Fix broken links manually
# Edit the markdown files to use correct relative paths

# 3. Verify SUMMARY.md includes all files
cat book/src/SUMMARY.md
</code></pre>
<p><strong>Source</strong>: Reduce phase from <code>workflows/book-docs-drift.yml:62-68</code></p>
<h4 id="issue-analysis-files-not-cleaned-up"><a class="header" href="#issue-analysis-files-not-cleaned-up">Issue: “Analysis files not cleaned up”</a></h4>
<p><strong>Symptoms</strong>: <code>.prodigy/book-analysis/</code> directory still exists after workflow completion</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Reduce phase didn’t complete</li>
<li>Cleanup command failed silently (uses <code>|| true</code>)</li>
</ul>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># Manual cleanup is safe
rm -rf .prodigy/book-analysis

# Check if this was part of incomplete workflow
prodigy sessions list
</code></pre>
<p>This is cosmetic - analysis files are regenerated on each run.</p>
<p><strong>Source</strong>: Cleanup step from <code>workflows/book-docs-drift.yml:81-82</code></p>
<h2 id="debugging-techniques-1"><a class="header" href="#debugging-techniques-1">Debugging Techniques</a></h2>
<h3 id="inspecting-analysis-artifacts"><a class="header" href="#inspecting-analysis-artifacts">Inspecting Analysis Artifacts</a></h3>
<p>All intermediate analysis files are stored in <code>.prodigy/book-analysis/</code>:</p>
<pre><code class="language-bash"># Feature inventory from codebase analysis
cat .prodigy/book-analysis/features.json | jq .

# Documentation gaps detected
cat .prodigy/book-analysis/gaps.json | jq .

# Flattened items for map phase (chapters + subsections)
cat .prodigy/book-analysis/flattened-items.json | jq .

# Drift reports (one per chapter/subsection)
ls -la .prodigy/book-analysis/drift-*.json
cat .prodigy/book-analysis/drift-&lt;chapter-id&gt;-&lt;subsection-id&gt;.json | jq .
</code></pre>
<p><strong>Source</strong>: File locations from <code>workflows/book-docs-drift.yml:9-18</code> and setup phase commands</p>
<h3 id="reviewing-event-logs"><a class="header" href="#reviewing-event-logs">Reviewing Event Logs</a></h3>
<p>MapReduce workflows generate detailed event logs:</p>
<pre><code class="language-bash"># List all events for a job
prodigy events &lt;job_id&gt;

# Filter to agent failures
prodigy events &lt;job_id&gt; | jq 'select(.event_type=="AgentFailed")'

# See what items completed successfully
prodigy events &lt;job_id&gt; | jq 'select(.event_type=="AgentCompleted") | .agent_id'

# Find Claude JSON log locations for failed agents
prodigy events &lt;job_id&gt; | jq 'select(.event_type=="AgentCompleted") | .json_log_location'
</code></pre>
<p><strong>Source</strong>: Event tracking implementation from <code>src/cook/execution/events/event_types.rs</code> and CLI handler <code>src/cli/commands/events.rs</code></p>
<h3 id="checking-dead-letter-queue-dlq"><a class="header" href="#checking-dead-letter-queue-dlq">Checking Dead Letter Queue (DLQ)</a></h3>
<p>Failed work items are sent to the DLQ for review and retry:</p>
<pre><code class="language-bash"># Show all failed items for a job
prodigy dlq show &lt;job_id&gt;

# See failure details with JSON log locations
prodigy dlq show &lt;job_id&gt; | jq '.items[].failure_history'

# Get Claude log path for debugging
prodigy dlq show &lt;job_id&gt; | jq '.items[].failure_history[].json_log_location'

# Retry all failed items
prodigy dlq retry &lt;job_id&gt;

# Retry with custom parallelism
prodigy dlq retry &lt;job_id&gt; --max-parallel 10

# See DLQ statistics
prodigy dlq stats &lt;job_id&gt;
</code></pre>
<p><strong>Source</strong>: DLQ implementation from <code>src/cook/execution/dlq.rs</code> and CLI handler <code>src/cli/commands/dlq.rs</code></p>
<h3 id="examining-claude-command-logs"><a class="header" href="#examining-claude-command-logs">Examining Claude Command Logs</a></h3>
<p>Each Claude command execution creates a JSON log file with complete conversation history:</p>
<pre><code class="language-bash"># Find the log location from workflow output (with -v flag)
prodigy run workflows/book-docs-drift.yml -v

# Or from DLQ item failure details
LOG_PATH=$(prodigy dlq show &lt;job_id&gt; | jq -r '.items[0].failure_history[0].json_log_location')

# View the full conversation
cat "$LOG_PATH" | jq .

# Extract tool invocations
cat "$LOG_PATH" | jq '.messages[] | select(.role=="assistant") | .content[] | select(.type=="tool_use")'

# Check for errors
cat "$LOG_PATH" | jq '.messages[] | select(.type=="error")'
</code></pre>
<p><strong>Source</strong>: Claude log tracking from Spec 121 (JSON Log Location Tracking) and <code>src/cook/execution/mapreduce/agent_result.rs</code></p>
<h3 id="testing-individual-commands"><a class="header" href="#testing-individual-commands">Testing Individual Commands</a></h3>
<p>You can run workflow commands individually for debugging:</p>
<pre><code class="language-bash"># Run feature analysis only
claude /prodigy-analyze-features-for-book --project Prodigy --config .prodigy/book-config.json

# Run gap detection only (requires features.json first)
claude /prodigy-detect-documentation-gaps --project Prodigy --config .prodigy/book-config.json --features .prodigy/book-analysis/features.json --chapters workflows/data/prodigy-chapters.json --book-dir book

# Analyze specific chapter for drift (requires features.json)
claude /prodigy-analyze-subsection-drift --project Prodigy --json '{"type":"subsection","id":"troubleshooting","parent_chapter_id":"automated-documentation","file":"book/src/automated-documentation/troubleshooting.md"}' --features .prodigy/book-analysis/features.json

# Fix specific chapter drift (requires drift report)
claude /prodigy-fix-subsection-drift --project Prodigy --json '{"type":"subsection","id":"troubleshooting","parent_chapter_id":"automated-documentation","file":"book/src/automated-documentation/troubleshooting.md"}'
</code></pre>
<p><strong>Source</strong>: Command definitions from <code>.claude/commands/prodigy-*-book*.md</code></p>
<h2 id="resume-and-recovery"><a class="header" href="#resume-and-recovery">Resume and Recovery</a></h2>
<h3 id="resuming-interrupted-workflows"><a class="header" href="#resuming-interrupted-workflows">Resuming Interrupted Workflows</a></h3>
<p>MapReduce workflows support checkpoint-based resume. See the <a href="automated-documentation/../mapreduce/checkpoint-and-resume.html">Checkpoint and Resume</a> documentation for details.</p>
<pre><code class="language-bash"># Resume using session ID
prodigy resume session-mapreduce-1234567890

# Resume using job ID
prodigy resume-job mapreduce-1234567890

# Unified resume (auto-detects ID type)
prodigy resume mapreduce-1234567890
</code></pre>
<p><strong>Source</strong>: Resume functionality from Spec 134 (MapReduce Checkpoint and Resume)</p>
<h3 id="retrying-failed-items-from-dlq"><a class="header" href="#retrying-failed-items-from-dlq">Retrying Failed Items from DLQ</a></h3>
<p>After a workflow completes with failures:</p>
<pre><code class="language-bash"># 1. Review what failed
prodigy dlq show &lt;job_id&gt;

# 2. Retry all failed items
prodigy dlq retry &lt;job_id&gt;

# 3. Monitor progress
prodigy events &lt;job_id&gt;
</code></pre>
<p>The DLQ retry creates a new execution context but preserves correlation IDs for tracking.</p>
<p><strong>Source</strong>: DLQ retry implementation from <code>src/cook/execution/dlq_reprocessor.rs</code></p>
<h2 id="file-locations-reference"><a class="header" href="#file-locations-reference">File Locations Reference</a></h2>
<p>Key files and directories for troubleshooting:</p>
<div class="table-wrapper"><table><thead><tr><th>Location</th><th>Description</th><th>Phase</th></tr></thead><tbody>
<tr><td><code>.prodigy/book-config.json</code></td><td>Project configuration for documentation</td><td>Setup input</td></tr>
<tr><td><code>workflows/data/prodigy-chapters.json</code></td><td>Chapter structure definitions</td><td>Setup input</td></tr>
<tr><td><code>.prodigy/book-analysis/features.json</code></td><td>Extracted codebase features</td><td>Setup output</td></tr>
<tr><td><code>.prodigy/book-analysis/gaps.json</code></td><td>Detected documentation gaps</td><td>Setup output</td></tr>
<tr><td><code>.prodigy/book-analysis/flattened-items.json</code></td><td>Work items for map phase</td><td>Setup output</td></tr>
<tr><td><code>.prodigy/book-analysis/drift-*.json</code></td><td>Per-chapter drift reports</td><td>Map output</td></tr>
<tr><td><code>~/.prodigy/events/&lt;repo&gt;/&lt;job_id&gt;/</code></td><td>Event logs (global storage)</td><td>All phases</td></tr>
<tr><td><code>~/.prodigy/dlq/&lt;repo&gt;/&lt;job_id&gt;/</code></td><td>Dead letter queue items</td><td>Map phase</td></tr>
<tr><td><code>~/.prodigy/state/&lt;repo&gt;/mapreduce/jobs/&lt;job_id&gt;/</code></td><td>Checkpoint files</td><td>All phases</td></tr>
<tr><td><code>~/.local/state/claude/logs/session-*.json</code></td><td>Claude command logs</td><td>All phases</td></tr>
</tbody></table>
</div>
<p><strong>Source</strong>: Storage locations from global storage architecture (Spec 127) and workflow environment variables in <code>workflows/book-docs-drift.yml:9-18</code></p>
<h2 id="performance-tips"><a class="header" href="#performance-tips">Performance Tips</a></h2>
<h3 id="adjusting-parallelism"><a class="header" href="#adjusting-parallelism">Adjusting Parallelism</a></h3>
<p>The workflow uses <code>max_parallel: 3</code> by default. Adjust based on your system:</p>
<pre><code class="language-yaml">env:
  MAX_PARALLEL: "5"  # Process 5 chapters concurrently

map:
  max_parallel: ${MAX_PARALLEL}
</code></pre>
<p><strong>Trade-offs</strong>:</p>
<ul>
<li>Higher parallelism = faster completion, more system resources</li>
<li>Lower parallelism = slower completion, fewer failures from resource contention</li>
</ul>
<p><strong>Source</strong>: Parallelism configuration from <code>workflows/book-docs-drift.yml:21,59</code></p>
<h3 id="processing-subset-of-chapters"><a class="header" href="#processing-subset-of-chapters">Processing Subset of Chapters</a></h3>
<p>Use JSONPath filters to target specific documentation:</p>
<pre><code class="language-yaml">map:
  input: "${ANALYSIS_DIR}/flattened-items.json"
  json_path: "$[*]"
  filter: "item.parent_chapter_id == 'mapreduce'"  # Only MapReduce subsections
</code></pre>
<p>Or manually edit <code>flattened-items.json</code> to include only desired items.</p>
<p><strong>Source</strong>: Filter syntax from MapReduce workflow specification</p>
<h3 id="skipping-validation-for-drafts"><a class="header" href="#skipping-validation-for-drafts">Skipping Validation for Drafts</a></h3>
<p>For faster iteration during development, reduce validation threshold:</p>
<pre><code class="language-yaml">map:
  agent_template:
    - claude: "/prodigy-fix-subsection-drift --project $PROJECT_NAME --json '${item}'"
      commit_required: true
      validate:
        threshold: 70  # Accept 70% quality instead of 100%
</code></pre>
<p><strong>Warning</strong>: This may result in lower quality documentation.</p>
<p><strong>Source</strong>: Validation configuration from <code>workflows/book-docs-drift.yml:49-57</code></p>
<h2 id="configuration-issues"><a class="header" href="#configuration-issues">Configuration Issues</a></h2>
<h3 id="invalid-book-configjson"><a class="header" href="#invalid-book-configjson">Invalid book-config.json</a></h3>
<p><strong>Symptoms</strong>: Setup phase fails immediately or generates no features</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># Validate JSON syntax
cat .prodigy/book-config.json | jq empty

# Check required fields exist
cat .prodigy/book-config.json | jq '{project_name, analysis_targets, chapter_file}'
</code></pre>
<p><strong>Required fields</strong>:</p>
<ul>
<li><code>project_name</code> - Project display name</li>
<li><code>analysis_targets</code> - Array of areas with source files</li>
<li><code>chapter_file</code> - Path to chapter definitions</li>
</ul>
<p><strong>Source</strong>: Configuration structure from <code>.prodigy/book-config.json:1-220</code></p>
<h3 id="missing-source-files-in-analysis_targets"><a class="header" href="#missing-source-files-in-analysis_targets">Missing Source Files in analysis_targets</a></h3>
<p><strong>Symptoms</strong>: Features not extracted for certain areas</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># Check all referenced source files exist
cat .prodigy/book-config.json | jq -r '.analysis_targets[].source_files[]' | while read file; do
  if [ ! -e "$file" ]; then
    echo "Missing: $file"
  fi
done
</code></pre>
<p>Update paths in <code>book-config.json</code> to match actual source file locations.</p>
<p><strong>Source</strong>: Analysis targets from <code>.prodigy/book-config.json:7-213</code></p>
<h3 id="incorrect-chapter-definitions"><a class="header" href="#incorrect-chapter-definitions">Incorrect Chapter Definitions</a></h3>
<p><strong>Symptoms</strong>: Gaps detected for chapters that already exist, or no gaps when chapters are missing</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># Verify chapter definitions match actual book structure
diff &lt;(cat workflows/data/prodigy-chapters.json | jq -r '.chapters[].id' | sort) \
     &lt;(find book/src -name "index.md" -o -name "[!index]*.md" | sed 's|book/src/||; s|/index.md||; s|\.md||' | sort)
</code></pre>
<p>Update <code>workflows/data/prodigy-chapters.json</code> to match your book structure.</p>
<p><strong>Source</strong>: Chapter definitions referenced in <code>workflows/book-docs-drift.yml:18</code></p>
<h2 id="faq"><a class="header" href="#faq">FAQ</a></h2>
<p><strong>Q: Why does setup phase show “No changes” even though I modified source code?</strong></p>
<p>A: Feature analysis only commits when features <strong>change</strong>. Code changes don’t always mean feature changes (e.g., bug fixes, refactoring). This is expected behavior.</p>
<hr />
<p><strong>Q: Can I run the workflow on a subset of chapters?</strong></p>
<p>A: Yes. Either:</p>
<ol>
<li>Use <code>filter</code> in map phase to select specific items</li>
<li>Manually edit <code>.prodigy/book-analysis/flattened-items.json</code> after setup</li>
<li>Modify chapter definitions to exclude certain chapters</li>
</ol>
<hr />
<p><strong>Q: What happens if I interrupt the workflow?</strong></p>
<p>A: Use <code>prodigy resume &lt;job_id&gt;</code> to continue from the last checkpoint. See <a href="automated-documentation/../mapreduce/checkpoint-and-resume.html">Checkpoint and Resume</a> for details.</p>
<hr />
<p><strong>Q: How do I debug why a specific chapter failed validation?</strong></p>
<p>A:</p>
<pre><code class="language-bash"># 1. Find the validation result
cat .prodigy/validation-result.json | jq .

# 2. Check the drift report for this chapter
cat .prodigy/book-analysis/drift-&lt;chapter-id&gt;-&lt;subsection-id&gt;.json | jq .

# 3. Review Claude's attempt to fix it
prodigy dlq show &lt;job_id&gt; | jq '.items[] | select(.id=="&lt;subsection-id&gt;")'
</code></pre>
<hr />
<p><strong>Q: Can I customize what gets analyzed?</strong></p>
<p>A: Yes. Edit <code>.prodigy/book-config.json</code> to:</p>
<ul>
<li>Add/remove <code>analysis_targets</code> areas</li>
<li>Change which source files are analyzed per area</li>
<li>Adjust <code>feature_categories</code> to extract different information</li>
<li>Enable/disable examples, best practices, troubleshooting in <code>custom_analysis</code></li>
</ul>
<hr />
<p><strong>Q: The workflow is too slow. How can I speed it up?</strong></p>
<p>A:</p>
<ol>
<li>Increase <code>max_parallel</code> (default: 3)</li>
<li>Process fewer chapters using filters</li>
<li>Use <code>--stop-after setup</code> to only regenerate analysis files</li>
<li>Reduce validation threshold for draft iterations</li>
</ol>
<h2 id="see-also-39"><a class="header" href="#see-also-39">See Also</a></h2>
<ul>
<li><a href="automated-documentation/understanding-the-workflow.html">Understanding the Workflow</a> - Workflow phase details</li>
<li><a href="automated-documentation/../mapreduce/checkpoint-and-resume.html">Checkpoint and Resume</a> - Resume interrupted workflows</li>
<li><a href="automated-documentation/../mapreduce/dead-letter-queue-dlq.html">Dead Letter Queue</a> - Handling persistent failures</li>
<li><a href="automated-documentation/../mapreduce/event-tracking.html">Event Tracking</a> - Monitoring workflow execution</li>
<li><a href="automated-documentation/advanced-configuration.html">Advanced Configuration</a> - Customizing the workflow</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="advanced-configuration-2"><a class="header" href="#advanced-configuration-2">Advanced Configuration</a></h2>
<p>This subsection covers advanced configuration topics for optimizing and customizing your automated documentation workflows. These configurations enable fine-tuning of performance, security, and behavior for documentation generation at scale.</p>
<h3 id="configuration-files-and-locations"><a class="header" href="#configuration-files-and-locations">Configuration Files and Locations</a></h3>
<p>Prodigy supports configuration at multiple levels with a clear precedence chain:</p>
<p><strong>Configuration File Locations</strong> (Source: src/config/mod.rs:39-86):</p>
<ol>
<li>
<p><strong>Global Configuration</strong>: <code>~/.prodigy/config.toml</code></p>
<ul>
<li>Applies across all projects</li>
<li>Contains defaults for editor, log level, API keys, and global settings</li>
</ul>
</li>
<li>
<p><strong>Project Configuration</strong>: <code>.prodigy/config.toml</code></p>
<ul>
<li>Project-specific overrides</li>
<li>Contains project name, description, spec directory, and custom variables</li>
</ul>
</li>
<li>
<p><strong>Workflow Environment</strong>: <code>env:</code> block in workflow YAML files</p>
<ul>
<li>Workflow-specific configuration</li>
<li>Defines variables, secrets, and profiles for the workflow</li>
</ul>
</li>
</ol>
<p><strong>Configuration Precedence Chain</strong>:</p>
<pre><code>Step env &gt; Workflow profile &gt; Workflow env &gt; Project config &gt; Global config &gt; System env
</code></pre>
<p>Higher-priority configurations override lower-priority ones. For example, a step-level environment variable will override the same variable defined in the workflow env block.</p>
<h3 id="environment-variables-1"><a class="header" href="#environment-variables-1">Environment Variables</a></h3>
<p>Environment variables parameterize workflows and can be defined in the <code>env:</code> block at the workflow root (Source: src/cook/environment/config.rs:12-36).</p>
<p><strong>Environment Configuration Structure</strong> (Source: src/cook/environment/config.rs:12-36):</p>
<pre><code class="language-yaml">env:
  # Plain variables
  PROJECT_NAME: "Prodigy"
  VERSION: "1.0.0"
  BOOK_DIR: "book"

  # Secret variables (masked in logs)
  API_KEY:
    secret: true
    value: "sk-abc123"

  # Profile-specific variables
  DATABASE_URL:
    default: "postgres://localhost/dev"
    prod: "postgres://prod-server/db"
</code></pre>
<p><strong>Variable Interpolation Syntax</strong>:</p>
<ul>
<li><code>$VAR</code> - Simple variable reference (shell-style)</li>
<li><code>${VAR}</code> - Bracketed reference for clarity</li>
</ul>
<p><strong>Secret Masking</strong> (Source: src/cook/environment/mod.rs:45-61):</p>
<p>Variables marked with <code>secret: true</code> are automatically masked in command output logs, error messages, event logs, and checkpoint files. The masking utility replaces secret values with <code>***MASKED***</code>.</p>
<p><strong>Profile Support</strong>:</p>
<p>Activate different environment profiles using the <code>--profile</code> flag:</p>
<pre><code class="language-bash">prodigy run workflow.yml --profile prod
</code></pre>
<p><strong>Real-World Example</strong> (Source: workflows/book-docs-drift.yml:8-21):</p>
<pre><code class="language-yaml">env:
  # Project configuration
  PROJECT_NAME: "Prodigy"
  PROJECT_CONFIG: ".prodigy/book-config.json"
  FEATURES_PATH: ".prodigy/book-analysis/features.json"

  # Book-specific settings
  BOOK_DIR: "book"
  ANALYSIS_DIR: ".prodigy/book-analysis"
  CHAPTERS_FILE: "workflows/data/prodigy-chapters.json"

  # Workflow settings
  MAX_PARALLEL: "3"
</code></pre>
<p>These variables are referenced throughout the workflow using <code>$VARIABLE_NAME</code> or <code>${VARIABLE_NAME}</code> syntax.</p>
<h3 id="mapreduce-performance-tuning"><a class="header" href="#mapreduce-performance-tuning">MapReduce Performance Tuning</a></h3>
<p>For documentation workflows using MapReduce, several configuration options control parallelism and resource usage (Source: src/config/mapreduce.rs:238-241, 276-278).</p>
<p><strong>max_parallel Configuration</strong> (Source: src/config/mapreduce.rs:238-241):</p>
<p>Controls the number of concurrent documentation agents processing chapters/subsections in parallel:</p>
<pre><code class="language-yaml">map:
  input: "${ANALYSIS_DIR}/flattened-items.json"
  json_path: "$[*]"

  agent_template:
    - claude: "/prodigy-fix-subsection-drift --project $PROJECT_NAME --json '${item}'"

  max_parallel: ${MAX_PARALLEL}  # Default: 10
</code></pre>
<p><strong>Performance Trade-offs</strong>:</p>
<ul>
<li><strong>Higher parallelism</strong> (10+): Faster completion, higher resource usage (CPU, memory, disk I/O)</li>
<li><strong>Lower parallelism</strong> (3-5): More conservative resource usage, longer total execution time</li>
<li><strong>Balanced approach</strong> (5-7): Good for most documentation workflows</li>
</ul>
<p>The <code>book-docs-drift.yml</code> workflow uses <code>MAX_PARALLEL: 3</code> for balanced performance and resource management.</p>
<p><strong>Timeout Configuration</strong>:</p>
<p>While not explicitly shown in the MapReduce configuration, agent timeouts can be configured for long-running documentation tasks:</p>
<ul>
<li><code>agent_timeout_secs</code>: Maximum time allowed for each map agent</li>
<li><code>setup_timeout</code>: Maximum time for feature analysis phase</li>
<li><code>reduce_timeout</code>: Maximum time for book build phase</li>
</ul>
<h3 id="book-configuration"><a class="header" href="#book-configuration">Book Configuration</a></h3>
<p>The <code>.prodigy/book-config.json</code> file defines book-specific analysis and generation settings (Source: .prodigy/book-config.json:1-220).</p>
<p><strong>Book Configuration Structure</strong> (Source: .prodigy/book-config.json):</p>
<pre><code class="language-json">{
  "project_name": "Prodigy",
  "project_type": "cli_tool",
  "book_dir": "book",
  "book_src": "book/src",
  "book_build_dir": "book/book",
  "analysis_targets": [
    {
      "area": "configuration",
      "source_files": [
        "src/config/mod.rs",
        "src/config/settings.rs"
      ],
      "feature_categories": [
        "file_locations",
        "precedence",
        "claude_settings",
        "storage_settings"
      ]
    }
  ],
  "chapter_file": "workflows/data/prodigy-chapters.json",
  "custom_analysis": {
    "include_examples": true,
    "include_best_practices": true,
    "include_troubleshooting": true
  }
}
</code></pre>
<p><strong>Key Fields</strong>:</p>
<ul>
<li><code>analysis_targets</code>: Defines codebase areas to analyze for feature extraction</li>
<li><code>source_files</code>: Source code files to scan for each area</li>
<li><code>feature_categories</code>: Categories of features to document for each area</li>
<li><code>custom_analysis</code>: Options for including examples, best practices, and troubleshooting sections</li>
</ul>
<p><strong>Adapting for Different Project Types</strong>:</p>
<ul>
<li>Rust: Use <code>src/**/*.rs</code> patterns</li>
<li>Python: Use <code>src/**/*.py</code> or package structure</li>
<li>JavaScript: Use <code>src/**/*.js</code>, <code>src/**/*.ts</code></li>
</ul>
<h3 id="claude-specific-configuration"><a class="header" href="#claude-specific-configuration">Claude-Specific Configuration</a></h3>
<p>Control Claude’s behavior during documentation generation with environment variables and verbosity flags.</p>
<p><strong>Claude Streaming Configuration</strong>:</p>
<ul>
<li><code>PRODIGY_CLAUDE_STREAMING=false</code>: Disable JSON streaming output (useful in CI/CD)</li>
<li><code>PRODIGY_CLAUDE_CONSOLE_OUTPUT=true</code>: Force streaming output regardless of verbosity</li>
<li><code>-v</code> flag: Enable verbose mode to see Claude streaming output for debugging</li>
</ul>
<p><strong>Claude Log Locations</strong>:</p>
<p>Claude creates detailed JSON log files for each command execution at:</p>
<pre><code>~/.local/state/claude/logs/session-{session_id}.json
</code></pre>
<p><strong>Analyzing Claude Logs for Debugging</strong>:</p>
<pre><code class="language-bash"># View complete Claude interaction
cat ~/.local/state/claude/logs/session-abc123.json | jq '.messages'

# Check tool invocations
cat ~/.local/state/claude/logs/session-abc123.json | jq '.messages[].content[] | select(.type == "tool_use")'

# Analyze token usage
cat ~/.local/state/claude/logs/session-abc123.json | jq '.usage'
</code></pre>
<p>Use <code>-v</code> flag during workflow execution to see real-time streaming output from Claude for troubleshooting failed documentation agents.</p>
<h3 id="error-handling-configuration"><a class="header" href="#error-handling-configuration">Error Handling Configuration</a></h3>
<p>Configure how documentation workflows handle failures and errors (Source: workflows/book-docs-drift.yml:85-90).</p>
<p><strong>Error Policy Configuration</strong> (Source: workflows/book-docs-drift.yml:85-90):</p>
<pre><code class="language-yaml">error_policy:
  on_item_failure: dlq            # Send failed items to Dead Letter Queue
  continue_on_failure: true       # Continue processing other items
  max_failures: 2                 # Stop workflow after 2 failures
  error_collection: aggregate     # Aggregate errors for reporting
</code></pre>
<p><strong>Error Policy Options</strong>:</p>
<ul>
<li><code>on_item_failure</code>: <code>dlq</code> (Dead Letter Queue), <code>fail</code> (stop immediately), <code>skip</code> (continue)</li>
<li><code>continue_on_failure</code>: Whether to continue processing remaining items after a failure</li>
<li><code>max_failures</code>: Maximum number of failures before stopping the entire workflow</li>
<li><code>error_collection</code>: How to collect and report errors (<code>aggregate</code>, <code>individual</code>)</li>
</ul>
<p><strong>Dead Letter Queue (DLQ) Usage</strong>:</p>
<p>Failed documentation items are stored in <code>~/.prodigy/dlq/{repo_name}/{job_id}/</code> for review and retry:</p>
<pre><code class="language-bash"># View failed items
prodigy dlq show &lt;job_id&gt;

# Retry all failed items
prodigy dlq retry &lt;job_id&gt;

# Retry with custom parallelism
prodigy dlq retry &lt;job_id&gt; --max-parallel 5
</code></pre>
<p><strong>Retry Strategies</strong>:</p>
<p>While not shown in the example workflow, retry configuration can be added to commands:</p>
<ul>
<li>Backoff strategies: exponential, linear, fibonacci</li>
<li>Max retry attempts</li>
<li>Retry budget limits</li>
</ul>
<h3 id="storage-and-worktree-configuration"><a class="header" href="#storage-and-worktree-configuration">Storage and Worktree Configuration</a></h3>
<p>Prodigy uses global storage for centralized state management and git worktrees for isolation.</p>
<p><strong>Global Storage Locations</strong>:</p>
<ul>
<li>Events: <code>~/.prodigy/events/{repo_name}/{job_id}/</code></li>
<li>DLQ: <code>~/.prodigy/dlq/{repo_name}/{job_id}/</code></li>
<li>State: <code>~/.prodigy/state/{repo_name}/mapreduce/jobs/{job_id}/</code></li>
<li>Worktrees: <code>~/.prodigy/worktrees/{repo_name}/</code></li>
</ul>
<p><strong>Repository Grouping</strong>:</p>
<p>All storage is grouped by repository name, enabling:</p>
<ul>
<li>Cross-worktree event aggregation</li>
<li>Persistent state across worktree cleanup</li>
<li>Centralized monitoring of all jobs for a repository</li>
</ul>
<p><strong>Cleanup Policies</strong>:</p>
<ul>
<li><strong>Automatic cleanup on success</strong>: Worktrees are removed after successful agent completion</li>
<li><strong>Orphan registry on failure</strong>: Failed worktrees are registered in <code>~/.prodigy/orphaned_worktrees/{repo_name}/{job_id}.json</code></li>
</ul>
<p><strong>Cleaning Orphaned Worktrees</strong>:</p>
<pre><code class="language-bash"># List orphaned worktrees
prodigy worktree clean-orphaned &lt;job_id&gt;

# Clean with confirmation
prodigy worktree clean-orphaned &lt;job_id&gt; --force
</code></pre>
<h3 id="validation-configuration"><a class="header" href="#validation-configuration">Validation Configuration</a></h3>
<p>Configure quality gates and validation for documentation generation (Source: workflows/book-docs-drift.yml:49-57).</p>
<p><strong>Validation Configuration</strong> (Source: workflows/book-docs-drift.yml:49-57):</p>
<pre><code class="language-yaml">validate:
  claude: "/prodigy-validate-doc-fix --project $PROJECT_NAME --json '${item}' --output .prodigy/validation-result.json"
  result_file: ".prodigy/validation-result.json"
  threshold: 100  # Documentation must meet 100% quality standards
  on_incomplete:
    claude: "/prodigy-complete-doc-fix --project $PROJECT_NAME --json '${item}' --gaps ${validation.gaps}"
    max_attempts: 3
    fail_workflow: false  # Continue even if we can't reach 100%
    commit_required: true
</code></pre>
<p><strong>Validation Options</strong>:</p>
<ul>
<li><code>threshold</code>: Completion percentage required to pass (0-100)</li>
<li><code>result_file</code>: File where validation results are written</li>
<li><code>on_incomplete</code>: Handler to execute when validation threshold is not met</li>
<li><code>max_attempts</code>: Maximum attempts to complete validation</li>
<li><code>fail_workflow</code>: Whether to fail the entire workflow if validation never passes</li>
</ul>
<p><strong>Quality Gates</strong>:</p>
<p>The validation system ensures:</p>
<ul>
<li>All critical drift issues are addressed</li>
<li>Documentation meets minimum content requirements</li>
<li>Examples are grounded in actual codebase</li>
<li>Links are valid and point to existing files</li>
</ul>
<h3 id="configuration-checklist-for-optimizing-documentation-workflows"><a class="header" href="#configuration-checklist-for-optimizing-documentation-workflows">Configuration Checklist for Optimizing Documentation Workflows</a></h3>
<p><strong>Performance Optimization</strong>:</p>
<ul>
<li><input disabled="" type="checkbox"/>
Set <code>MAX_PARALLEL</code> based on available CPU cores (recommend: cores / 2)</li>
<li><input disabled="" type="checkbox"/>
Configure agent timeouts appropriate for documentation complexity</li>
<li><input disabled="" type="checkbox"/>
Use global storage for centralized state management</li>
</ul>
<p><strong>Security</strong>:</p>
<ul>
<li><input disabled="" type="checkbox"/>
Mark API keys and sensitive data as secrets (<code>secret: true</code>)</li>
<li><input disabled="" type="checkbox"/>
Use profiles to separate development and production credentials</li>
<li><input disabled="" type="checkbox"/>
Enable secret masking for logs and error output</li>
</ul>
<p><strong>Quality Control</strong>:</p>
<ul>
<li><input disabled="" type="checkbox"/>
Set validation threshold to 100% for production documentation</li>
<li><input disabled="" type="checkbox"/>
Configure <code>on_incomplete</code> handlers to automatically fix validation failures</li>
<li><input disabled="" type="checkbox"/>
Enable <code>error_policy.on_item_failure: dlq</code> for failed item recovery</li>
</ul>
<p><strong>Resource Management</strong>:</p>
<ul>
<li><input disabled="" type="checkbox"/>
Configure cleanup policies for worktrees</li>
<li><input disabled="" type="checkbox"/>
Set <code>max_failures</code> to prevent runaway workflows</li>
<li><input disabled="" type="checkbox"/>
Use <code>continue_on_failure: true</code> to maximize successful documentation coverage</li>
</ul>
<p><strong>Debugging</strong>:</p>
<ul>
<li><input disabled="" type="checkbox"/>
Enable Claude streaming in development (<code>-v</code> flag or <code>PRODIGY_CLAUDE_CONSOLE_OUTPUT=true</code>)</li>
<li><input disabled="" type="checkbox"/>
Configure verbose logging for troubleshooting</li>
<li><input disabled="" type="checkbox"/>
Preserve Claude JSON logs for post-mortem analysis</li>
</ul>
<h3 id="troubleshooting-common-configuration-issues"><a class="header" href="#troubleshooting-common-configuration-issues">Troubleshooting Common Configuration Issues</a></h3>
<p><strong>Issue: Documentation workflow is too slow</strong></p>
<ul>
<li>Solution: Increase <code>MAX_PARALLEL</code> value, but monitor resource usage</li>
<li>Check: CPU and memory utilization during workflow execution</li>
</ul>
<p><strong>Issue: Out of memory errors during MapReduce</strong></p>
<ul>
<li>Solution: Decrease <code>max_parallel</code> to reduce concurrent agent count</li>
<li>Check: Each agent may load large amounts of documentation into context</li>
</ul>
<p><strong>Issue: Secrets appearing in logs</strong></p>
<ul>
<li>Solution: Ensure secrets are marked with <code>secret: true</code> in environment config</li>
<li>Check: Review event logs and Claude logs for masked values</li>
</ul>
<p><strong>Issue: Validation always failing at 100% threshold</strong></p>
<ul>
<li>Solution: Review validation command output to identify quality gaps</li>
<li>Check: Use <code>on_incomplete</code> handler with <code>max_attempts</code> to iteratively improve</li>
</ul>
<p><strong>Issue: Orphaned worktrees consuming disk space</strong></p>
<ul>
<li>Solution: Run <code>prodigy worktree clean-orphaned &lt;job_id&gt;</code> regularly</li>
<li>Check: Monitor <code>~/.prodigy/worktrees/</code> directory size</li>
</ul>
<h3 id="see-also-40"><a class="header" href="#see-also-40">See Also</a></h3>
<ul>
<li><a href="automated-documentation/understanding-the-workflow.html">Understanding the Workflow</a> - Overview of documentation workflow phases</li>
<li><a href="automated-documentation/troubleshooting.html">Troubleshooting</a> - Common issues and solutions</li>
<li><a href="automated-documentation/quick-start.html">Quick Start</a> - Getting started with automated documentation</li>
<li><a href="automated-documentation/customization-examples.html">Customization Examples</a> - Real-world configuration patterns</li>
<li><a href="automated-documentation/../configuration/environment-variables.html">Environment Variables</a> - Detailed environment variable reference</li>
<li><a href="automated-documentation/../configuration/configuration-precedence-rules.html">Configuration Precedence Rules</a> - How configuration values are resolved</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="real-world-example-prodigys-own-documentation"><a class="header" href="#real-world-example-prodigys-own-documentation">Real-World Example: Prodigy’s Own Documentation</a></h2>
<p>This documentation you’re reading is maintained by the same workflow described in this chapter. This is a complete, production-ready workflow that demonstrates:</p>
<ul>
<li><strong>MapReduce parallelism</strong> for processing multiple chapters/subsections concurrently</li>
<li><strong>Validation with thresholds</strong> to ensure documentation meets quality standards</li>
<li><strong>Automatic gap-filling</strong> to complete incomplete documentation</li>
<li><strong>Multi-subsection chapter support</strong> for organizing complex topics</li>
<li><strong>Subsection-aware commands</strong> that handle both single-file chapters and individual subsections</li>
<li><strong>Error handling with DLQ</strong> for robust failure recovery</li>
</ul>
<p>You can examine the actual configuration files used to maintain this documentation:</p>
<h3 id="book-configuration-1"><a class="header" href="#book-configuration-1">Book Configuration</a></h3>
<p><strong>File</strong>: <code>.prodigy/book-config.json</code></p>
<pre><code class="language-json">{
  "project_name": "Prodigy",
  "project_type": "cli_tool",
  "book_dir": "book",
  "book_src": "book/src",
  "book_build_dir": "book/book",
  "analysis_targets": [
    {
      "area": "workflow_basics",
      "source_files": ["src/config/workflow.rs", "src/cook/workflow/executor.rs"],
      "feature_categories": ["structure", "execution_model", "commit_tracking"]
    },
    {
      "area": "mapreduce",
      "source_files": ["src/config/mapreduce.rs", "src/cook/execution/mapreduce/"],
      "feature_categories": ["phases", "capabilities", "configuration"]
    },
    {
      "area": "command_types",
      "source_files": ["src/config/command.rs"],
      "feature_categories": ["shell", "claude", "goal_seek", "foreach", "validation"]
    }
  ]
}
</code></pre>
<p><strong>Source</strong>: <code>.prodigy/book-config.json:1-46</code></p>
<h3 id="chapter-structure"><a class="header" href="#chapter-structure">Chapter Structure</a></h3>
<p><strong>File</strong>: <code>workflows/data/prodigy-chapters.json</code></p>
<p>This file defines both <strong>single-file chapters</strong> (one markdown file per chapter) and <strong>multi-subsection chapters</strong> (chapters split across multiple files):</p>
<pre><code class="language-json">{
  "chapters": [
    {
      "id": "workflow-basics",
      "title": "Workflow Basics",
      "type": "multi-subsection",
      "topics": ["Standard workflows", "Basic structure", "Command execution"],
      "validation": "Check basic workflow syntax and structure documentation",
      "index_file": "book/src/workflow-basics/index.md",
      "subsections": [
        {
          "id": "command-types",
          "title": "Command Types",
          "file": "book/src/workflow-basics/command-types.md",
          "topics": ["Command Types"],
          "validation": "Check command types documentation matches implementation"
        },
        {
          "id": "environment-configuration",
          "title": "Environment Configuration",
          "file": "book/src/workflow-basics/environment-configuration.md",
          "topics": ["Environment Configuration"],
          "validation": "Check environment configuration documentation matches implementation"
        }
      ]
    }
  ]
}
</code></pre>
<p><strong>Source</strong>: <code>workflows/data/prodigy-chapters.json:1-80</code></p>
<p>The setup phase command <code>/prodigy-detect-documentation-gaps</code> creates a <code>flattened-items.json</code> file containing both single-file chapters and individual subsections with parent metadata. This enables the map phase to process each subsection independently with full awareness of its parent chapter context.</p>
<h3 id="workflow-configuration-1"><a class="header" href="#workflow-configuration-1">Workflow Configuration</a></h3>
<p><strong>File</strong>: <code>workflows/book-docs-drift.yml</code></p>
<p>This MapReduce workflow orchestrates the entire documentation maintenance process:</p>
<p><strong>Source</strong>: <code>workflows/book-docs-drift.yml:1-101</code></p>
<p><strong>Key Features Demonstrated:</strong></p>
<p><strong>1. Setup Phase</strong> (lines 24-34):</p>
<ul>
<li>Analyzes codebase for feature coverage</li>
<li>Detects documentation gaps and creates missing chapters/subsections</li>
<li>Generates <code>flattened-items.json</code> for subsection-aware processing</li>
</ul>
<p><strong>2. Map Phase</strong> (lines 36-58):</p>
<ul>
<li>Processes each chapter/subsection in parallel using subsection-aware commands:
<ul>
<li><code>/prodigy-analyze-subsection-drift</code> - Analyzes drift for single-file chapters or individual subsections</li>
<li><code>/prodigy-fix-subsection-drift</code> - Fixes drift while preserving subsection scope and cross-references</li>
<li><code>/prodigy-validate-doc-fix</code> - Validates documentation meets quality standards</li>
<li><code>/prodigy-complete-doc-fix</code> - Fills gaps if validation score is below threshold</li>
</ul>
</li>
</ul>
<p><strong>3. Validation with Threshold</strong> (lines 49-57):</p>
<pre><code class="language-yaml">validate:
  claude: "/prodigy-validate-doc-fix --project $PROJECT_NAME --json '${item}' --output .prodigy/validation-result.json"
  result_file: ".prodigy/validation-result.json"
  threshold: 100  # Documentation must meet 100% quality standards
  on_incomplete:
    claude: "/prodigy-complete-doc-fix --project $PROJECT_NAME --json '${item}' --gaps ${validation.gaps}"
    max_attempts: 3
    fail_workflow: false  # Continue even if we can't reach 100%
    commit_required: true  # Require commit to verify improvements were made
</code></pre>
<p>The validation step ensures documentation quality by checking against a score threshold. If the score is below 100, the <code>on_incomplete</code> handler attempts to fill gaps with up to 3 attempts.</p>
<p><strong>4. Error Handling</strong> (lines 86-90):</p>
<pre><code class="language-yaml">error_policy:
  on_item_failure: dlq
  continue_on_failure: true
  max_failures: 2
  error_collection: aggregate
</code></pre>
<p>Failed items are sent to the Dead Letter Queue (DLQ) for later retry, allowing the workflow to continue processing other items.</p>
<p><strong>5. Custom Merge Workflow</strong> (lines 93-101):</p>
<pre><code class="language-yaml">merge:
  commands:
    - shell: "git fetch origin"
    - claude: "/prodigy-merge-master --project ${PROJECT_NAME}"
    - claude: "/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}"
</code></pre>
<p>Custom merge commands handle integration with the main branch and final merge back to the original branch.</p>
<h3 id="study-these-files"><a class="header" href="#study-these-files">Study These Files</a></h3>
<p>To understand the complete implementation:</p>
<ul>
<li><strong>Configuration</strong>: <code>.prodigy/book-config.json</code> - Book and analysis configuration</li>
<li><strong>Chapter Structure</strong>: <code>workflows/data/prodigy-chapters.json</code> - Chapter and subsection definitions</li>
<li><strong>Workflow</strong>: <code>workflows/book-docs-drift.yml</code> - Complete MapReduce workflow</li>
<li><strong>Commands</strong>: <code>.claude/commands/prodigy-analyze-subsection-drift.md</code> - Subsection-aware drift analysis</li>
<li><strong>Commands</strong>: <code>.claude/commands/prodigy-fix-subsection-drift.md</code> - Subsection-aware drift fixing</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="documentation-versioning"><a class="header" href="#documentation-versioning">Documentation Versioning</a></h2>
<blockquote>
<p><strong>Status: Planned Feature</strong></p>
<p>Documentation versioning is currently in the design phase (Specifications 154, 155, 156 in draft status). This page documents the planned implementation for projects that need to serve multiple documentation versions.</p>
</blockquote>
<p>For projects that need to serve multiple documentation versions (e.g., users on different software releases), Prodigy is designing a comprehensive versioned documentation system. This will allow users to select which version of the docs they want to view using a dropdown selector.</p>
<h3 id="overview-14"><a class="header" href="#overview-14">Overview</a></h3>
<p>The documentation versioning system consists of three integrated components:</p>
<ol>
<li><strong>Version Selector UI</strong> (Spec 154) - A dropdown component in the mdBook navigation that lets users switch between versions</li>
<li><strong>Versioned Deployment</strong> (Spec 155) - GitHub Actions workflow that deploys each version to its own subdirectory</li>
<li><strong>Version-Aware Workflows</strong> (Spec 156) - Enhanced book workflow that accepts a VERSION parameter for building version-specific documentation</li>
</ol>
<p><strong>Planned Architecture:</strong></p>
<pre><code>GitHub Repository
├── main branch (source code + docs)
├── Tags: v0.2.6, v0.2.5, v0.2.4, ...
└── gh-pages branch (deployed docs)
    ├── index.html → redirects to /latest/
    ├── versions.json
    ├── latest/ → copy of newest version
    ├── v0.2.6/ → built from v0.2.6 tag
    ├── v0.2.5/ → built from v0.2.5 tag
    └── v0.2.4/ → built from v0.2.4 tag
</code></pre>
<p><strong>Source</strong>: specs/155-versioned-documentation-deployment.md:80-102</p>
<h3 id="version-selector-ui-component"><a class="header" href="#version-selector-ui-component">Version Selector UI Component</a></h3>
<p><strong>Design Overview</strong> (Spec 154):</p>
<p>The version selector will be a lightweight JavaScript component that:</p>
<ul>
<li>Displays a dropdown in mdBook’s navigation bar</li>
<li>Fetches version metadata from <code>/versions.json</code> at the documentation root</li>
<li>Preserves the current page path when switching versions (e.g., switching from <code>/v0.2.6/mapreduce/index.html</code> to <code>/v0.2.5/mapreduce/index.html</code>)</li>
<li>Works across all mdBook themes with minimal configuration</li>
<li>Gracefully degrades if <code>versions.json</code> is missing</li>
</ul>
<p><strong>Integration Pattern:</strong></p>
<pre><code class="language-toml"># book/book.toml
[output.html]
additional-css = ["theme/version-selector.css"]
additional-js = ["theme/version-selector.js"]
</code></pre>
<p><strong>Source</strong>: specs/154-mdbook-version-selector-ui.md:110-116</p>
<p><strong>Key Features:</strong></p>
<ul>
<li><strong>Current Version Detection</strong>: Automatically detects which version the user is viewing based on URL path pattern</li>
<li><strong>Visual Indicators</strong>: Highlights current version and marks latest version with “(Latest)” label</li>
<li><strong>Fallback Handling</strong>: If the current page doesn’t exist in the target version, redirects to that version’s index</li>
<li><strong>Accessibility</strong>: Keyboard navigable (Tab, Enter, Arrow keys) and screen reader compatible</li>
<li><strong>Performance</strong>: &lt; 5KB combined JavaScript and CSS, no external dependencies</li>
</ul>
<p><strong>Source</strong>: specs/154-mdbook-version-selector-ui.md:36-53</p>
<h3 id="versionsjson-schema"><a class="header" href="#versionsjson-schema">versions.json Schema</a></h3>
<p>The version selector fetches version metadata from a central <code>versions.json</code> file:</p>
<pre><code class="language-json">{
  "latest": "v0.2.6",
  "versions": [
    {
      "version": "v0.2.6",
      "path": "/v0.2.6/",
      "label": "v0.2.6 (Latest)",
      "released": "2025-01-15"
    },
    {
      "version": "v0.2.5",
      "path": "/v0.2.5/",
      "label": "v0.2.5",
      "released": "2025-01-10"
    }
  ]
}
</code></pre>
<p><strong>Field Descriptions:</strong></p>
<ul>
<li><code>latest</code>: Version string of the newest release</li>
<li><code>version</code>: Semantic version tag (e.g., “v0.2.6”)</li>
<li><code>path</code>: URL path to that version’s documentation root</li>
<li><code>label</code>: Display text in dropdown (includes “(Latest)” for newest)</li>
<li><code>released</code>: ISO 8601 date of release (optional)</li>
</ul>
<p><strong>Source</strong>: specs/154-mdbook-version-selector-ui.md:118-137</p>
<h3 id="versioned-deployment-workflow"><a class="header" href="#versioned-deployment-workflow">Versioned Deployment Workflow</a></h3>
<p><strong>Design Overview</strong> (Spec 155):</p>
<p>The deployment system will automatically build and deploy documentation when version tags are pushed:</p>
<p><strong>Workflow Triggers:</strong></p>
<pre><code class="language-yaml">on:
  push:
    tags:
      - 'v*.*.*'  # Trigger on semver tags (e.g., v0.2.6)
  workflow_dispatch:
    inputs:
      version:
        description: 'Version to deploy (tag name, "all", or "latest")'
        required: true
        default: 'latest'
      rebuild_all:
        description: 'Rebuild all versions'
        type: boolean
        default: false
</code></pre>
<p><strong>Source</strong>: specs/155-versioned-documentation-deployment.md:108-124</p>
<p><strong>Deployment Process:</strong></p>
<ol>
<li>
<p><strong>Tag-Triggered Build</strong>: When you push a tag like <code>v0.2.6</code>, GitHub Actions automatically:</p>
<ul>
<li>Checks out that specific tag</li>
<li>Runs the book workflow for that version</li>
<li>Builds documentation with mdBook</li>
<li>Deploys to <code>gh-pages:/v0.2.6/</code> directory</li>
</ul>
</li>
<li>
<p><strong>versions.json Generation</strong>: After deploying a version, a script scans the <code>gh-pages</code> branch to generate <code>versions.json</code> with all deployed versions</p>
</li>
<li>
<p><strong>Latest Pointer Update</strong>: If the deployed version is the newest (highest semver), the <code>/latest/</code> directory is updated to point to it</p>
</li>
<li>
<p><strong>Root Redirect</strong>: An <code>index.html</code> at the root redirects visitors to <code>/latest/</code></p>
</li>
</ol>
<p><strong>Key Features:</strong></p>
<ul>
<li><strong>Preserves Existing Versions</strong>: Using <code>keep_files: true</code> ensures deploying v0.2.6 doesn’t delete v0.2.5</li>
<li><strong>Parallel Builds</strong>: Manual rebuild workflow can rebuild multiple versions concurrently</li>
<li><strong>Idempotent</strong>: Rebuilding the same version produces identical output</li>
<li><strong>Fail-Safe</strong>: Build failures don’t corrupt existing deployed versions</li>
</ul>
<p><strong>Source</strong>: specs/155-versioned-documentation-deployment.md:134-176</p>
<h3 id="version-aware-book-workflow"><a class="header" href="#version-aware-book-workflow">Version-Aware Book Workflow</a></h3>
<p><strong>Design Overview</strong> (Spec 156):</p>
<p>The current <code>book-docs-drift.yml</code> workflow will be enhanced to accept a <code>VERSION</code> parameter:</p>
<p><strong>Workflow Configuration:</strong></p>
<pre><code class="language-yaml">name: book-docs-drift-detection
mode: mapreduce

env:
  VERSION: "${VERSION:-latest}"  # Accept from caller or default to "latest"

  # Version-aware paths
  ANALYSIS_DIR: ".prodigy/book-analysis/${VERSION}"
  FEATURES_PATH: "${ANALYSIS_DIR}/features.json"
</code></pre>
<p><strong>Source</strong>: specs/156-version-aware-book-workflow.md:77-100</p>
<p><strong>Key Enhancements:</strong></p>
<ol>
<li>
<p><strong>VERSION Parameter</strong>: The workflow will accept a <code>VERSION</code> environment variable (e.g., “v0.2.6”, “latest”)</p>
</li>
<li>
<p><strong>Version-Scoped Analysis</strong>: Drift analysis results will be stored in version-specific directories:</p>
<ul>
<li><code>.prodigy/book-analysis/v0.2.6/</code></li>
<li><code>.prodigy/book-analysis/v0.2.5/</code></li>
<li><code>.prodigy/book-analysis/latest/</code></li>
</ul>
</li>
<li>
<p><strong>Version Validation</strong>: Before processing, the workflow will validate the VERSION format (semver <code>vX.Y.Z</code> or “latest”) and verify the tag exists</p>
</li>
<li>
<p><strong>Version in Documentation</strong>: Generated documentation will include version metadata in headers or footers</p>
</li>
</ol>
<p><strong>Claude Command Integration:</strong></p>
<pre><code class="language-yaml">setup:
  - claude: "/prodigy-analyze-features-for-book --project $PROJECT_NAME --version $VERSION"

map:
  agent_template:
    - claude: "/prodigy-analyze-book-chapter-drift --project $PROJECT_NAME --json '${item}' --version $VERSION"
</code></pre>
<p><strong>Source</strong>: specs/156-version-aware-book-workflow.md:143-153</p>
<p><strong>Backward Compatibility</strong>: If <code>VERSION</code> is not provided, the workflow defaults to “latest” and maintains current behavior.</p>
<h3 id="setup-instructions-planned"><a class="header" href="#setup-instructions-planned">Setup Instructions (Planned)</a></h3>
<p>When implemented, setting up versioned documentation will involve:</p>
<ol>
<li>
<p><strong>Add Version Selector Theme Files:</strong></p>
<pre><code class="language-bash"># Copy version selector components to your mdBook theme
cp version-selector.js book/theme/
cp version-selector.css book/theme/
</code></pre>
</li>
<li>
<p><strong>Update book.toml Configuration:</strong></p>
<pre><code class="language-toml">[output.html]
additional-css = ["theme/version-selector.css"]
additional-js = ["theme/version-selector.js"]
</code></pre>
</li>
<li>
<p><strong>Add Deployment Workflow:</strong></p>
<pre><code class="language-bash"># Copy the versioned deployment workflow
cp templates/workflows/deploy-docs-versioned.yml .github/workflows/
</code></pre>
</li>
<li>
<p><strong>Create Initial versions.json:</strong></p>
<pre><code class="language-json">{
  "latest": "v0.2.6",
  "versions": [
    {
      "version": "v0.2.6",
      "path": "/v0.2.6/",
      "label": "v0.2.6 (Latest)",
      "released": "2025-01-15"
    }
  ]
}
</code></pre>
</li>
<li>
<p><strong>Deploy and Test:</strong></p>
<ul>
<li>Push a version tag to trigger deployment</li>
<li>Verify version selector appears in navigation</li>
<li>Test switching between versions</li>
</ul>
</li>
</ol>
<p><strong>Source</strong>: specs/154-mdbook-version-selector-ui.md:163-175</p>
<h3 id="version-retention-strategy-planned"><a class="header" href="#version-retention-strategy-planned">Version Retention Strategy (Planned)</a></h3>
<p>The design includes a version retention policy to manage storage on GitHub Pages:</p>
<p><strong>Retention Rules:</strong></p>
<ul>
<li><strong>Keep all major versions</strong>: v1.0.0, v2.0.0, v3.0.0</li>
<li><strong>Keep last 3 minor versions per major</strong>: v2.3.0, v2.2.0, v2.1.0</li>
<li><strong>Keep last 5 patch versions per minor</strong>: v2.3.5, v2.3.4, v2.3.3, v2.3.2, v2.3.1</li>
</ul>
<p><strong>Cleanup Process:</strong>
A planned cleanup script will identify and remove old versions:</p>
<pre><code class="language-bash"># scripts/cleanup-old-versions.sh (planned)
# Removes versions not matching retention policy
# Regenerates versions.json after cleanup
</code></pre>
<p><strong>Considerations:</strong></p>
<ul>
<li>GitHub Pages has a 1GB soft limit</li>
<li>Each documentation version is typically 5-10MB</li>
<li>Retention policy allows ~50-100 versions before cleanup needed</li>
</ul>
<p><strong>Source</strong>: specs/155-versioned-documentation-deployment.md:392-402</p>
<h3 id="manual-deployment-planned"><a class="header" href="#manual-deployment-planned">Manual Deployment (Planned)</a></h3>
<p>The deployment workflow will support manual triggering for specific use cases:</p>
<p><strong>Rebuild Specific Version:</strong></p>
<pre><code class="language-bash"># Trigger workflow manually from GitHub UI
# Set version: v0.2.5
# Or use GitHub CLI:
gh workflow run deploy-docs-versioned.yml -f version=v0.2.5
</code></pre>
<p><strong>Rebuild All Versions:</strong></p>
<pre><code class="language-bash"># Useful after theme updates or global doc changes
gh workflow run deploy-docs-versioned.yml -f rebuild_all=true
</code></pre>
<p><strong>Use Cases for Manual Deployment:</strong></p>
<ul>
<li>Updating documentation theme across all versions</li>
<li>Fixing critical documentation errors in historical versions</li>
<li>Regenerating <code>versions.json</code> after manual gh-pages branch cleanup</li>
<li>Testing deployment workflow changes</li>
</ul>
<p><strong>Source</strong>: specs/155-versioned-documentation-deployment.md:436-444</p>
<h3 id="integration-with-automated-documentation"><a class="header" href="#integration-with-automated-documentation">Integration with Automated Documentation</a></h3>
<p>The versioning system will integrate with Prodigy’s existing automated documentation workflow:</p>
<p><strong>Workflow Integration:</strong></p>
<pre><code class="language-yaml"># .github/workflows/deploy-docs-versioned.yml
steps:
  - name: Run book workflow for version
    run: prodigy run workflows/book-docs-drift.yml
    env:
      VERSION: ${{ steps.version.outputs.version }}
</code></pre>
<p>When a version tag is pushed, the deployment workflow will:</p>
<ol>
<li>Check out the tagged code</li>
<li>Run the version-aware book workflow to analyze features at that version</li>
<li>Build documentation matching that version’s implementation</li>
<li>Deploy to the version-specific subdirectory</li>
</ol>
<p>This ensures documentation always matches the code at each version.</p>
<p><strong>Source</strong>: specs/155-versioned-documentation-deployment.md:155-167, specs/156-version-aware-book-workflow.md:143-153</p>
<h3 id="testing-versioned-documentation-locally"><a class="header" href="#testing-versioned-documentation-locally">Testing Versioned Documentation Locally</a></h3>
<p><strong>Planned Testing Workflow:</strong></p>
<ol>
<li>
<p><strong>Build Multiple Versions Locally:</strong></p>
<pre><code class="language-bash"># Checkout and build v0.2.6
git checkout v0.2.6
prodigy run workflows/book-docs-drift.yml
mdbook build book
mv book/book build/v0.2.6

# Checkout and build v0.2.5
git checkout v0.2.5
prodigy run workflows/book-docs-drift.yml
mdbook build book
mv book/book build/v0.2.5
</code></pre>
</li>
<li>
<p><strong>Create Test versions.json:</strong></p>
<pre><code class="language-bash">cat &gt; build/versions.json &lt;&lt;EOF
{
  "latest": "v0.2.6",
  "versions": [
    {"version": "v0.2.6", "path": "/v0.2.6/", "label": "v0.2.6 (Latest)"},
    {"version": "v0.2.5", "path": "/v0.2.5/", "label": "v0.2.5"}
  ]
}
EOF
</code></pre>
</li>
<li>
<p><strong>Serve Locally:</strong></p>
<pre><code class="language-bash">cd build
python -m http.server 8000
# Visit http://localhost:8000/v0.2.6/
</code></pre>
</li>
<li>
<p><strong>Test Version Selector:</strong></p>
<ul>
<li>Verify dropdown appears in navigation</li>
<li>Switch between versions</li>
<li>Confirm page paths are preserved</li>
<li>Test fallback when page doesn’t exist in older version</li>
</ul>
</li>
</ol>
<h3 id="browser-compatibility-planned"><a class="header" href="#browser-compatibility-planned">Browser Compatibility (Planned)</a></h3>
<p>The version selector will be designed to work across modern browsers:</p>
<p><strong>Supported Browsers:</strong></p>
<ul>
<li>Chrome, Firefox, Safari, Edge (latest versions)</li>
<li>Mobile browsers (iOS Safari, Chrome Mobile)</li>
</ul>
<p><strong>Technology Choices:</strong></p>
<ul>
<li>Uses <code>fetch()</code> API (ES6, widely supported)</li>
<li>Semantic HTML (<code>&lt;select&gt;</code> element)</li>
<li>CSS Grid/Flexbox for layout</li>
<li>No external dependencies (no jQuery)</li>
</ul>
<p><strong>Graceful Degradation:</strong></p>
<ul>
<li>If <code>fetch()</code> unavailable (very old browsers), selector won’t render but docs remain accessible</li>
<li>If <code>versions.json</code> missing, component silently skips rendering (no errors shown)</li>
</ul>
<p><strong>Source</strong>: specs/154-mdbook-version-selector-ui.md:233-247</p>
<h3 id="best-practices-planned"><a class="header" href="#best-practices-planned">Best Practices (Planned)</a></h3>
<p><strong>For Projects Adopting Versioned Documentation:</strong></p>
<ol>
<li>
<p><strong>Start with Current Version</strong>: Deploy your current version first as both <code>/vX.Y.Z/</code> and <code>/latest/</code> before adding historical versions</p>
</li>
<li>
<p><strong>Use Semantic Versioning</strong>: Tag releases with semver format (<code>vX.Y.Z</code>) for automatic version sorting</p>
</li>
<li>
<p><strong>Test Before Tagging</strong>: Run the book workflow locally before pushing version tags to catch documentation issues</p>
</li>
<li>
<p><strong>Document Breaking Changes</strong>: When releasing new major versions, clearly document breaking changes in the documentation</p>
</li>
<li>
<p><strong>Regular Cleanup</strong>: Review deployed versions quarterly and remove very old versions per retention policy</p>
</li>
<li>
<p><strong>Monitor Storage</strong>: Keep an eye on gh-pages branch size (GitHub Pages has 1GB soft limit)</p>
</li>
<li>
<p><strong>Version-Specific Examples</strong>: When updating documentation for new features, add version notes indicating when features were introduced</p>
</li>
</ol>
<p><strong>Source</strong>: specs/155-versioned-documentation-deployment.md:393-402</p>
<h3 id="troubleshooting-planned"><a class="header" href="#troubleshooting-planned">Troubleshooting (Planned)</a></h3>
<p><strong>Common Issues and Solutions:</strong></p>
<p><strong>Version Selector Not Appearing:</strong></p>
<ul>
<li>Verify <code>version-selector.js</code> and <code>.css</code> are in <code>book/theme/</code> directory</li>
<li>Check <code>book.toml</code> includes <code>additional-js</code> and <code>additional-css</code> entries</li>
<li>Ensure <code>versions.json</code> exists at documentation root</li>
<li>Check browser console for JavaScript errors</li>
</ul>
<p><strong>Broken Version Links:</strong></p>
<ul>
<li>Verify version paths in <code>versions.json</code> match actual deployed directories</li>
<li>Ensure all versions use consistent URL structure</li>
<li>Check that <code>/latest/</code> is updated after deploying new versions</li>
</ul>
<p><strong>Deployment Failures:</strong></p>
<ul>
<li>Verify git tag follows semver format (<code>vX.Y.Z</code>)</li>
<li>Check GitHub Actions logs for build errors</li>
<li>Ensure book workflow completes successfully for that version</li>
<li>Verify <code>peaceiris/actions-gh-pages</code> action permissions</li>
</ul>
<p><strong>Version Not Updating:</strong></p>
<ul>
<li>Check that <code>versions.json</code> generation script ran after deployment</li>
<li>Verify <code>/latest/</code> directory was updated for newest version</li>
<li>Clear browser cache to see updated version selector</li>
</ul>
<p><strong>Source</strong>: specs/155-versioned-documentation-deployment.md:410-419</p>
<h3 id="implementation-status-5"><a class="header" href="#implementation-status-5">Implementation Status</a></h3>
<p><strong>Current Status: Draft / Not Implemented</strong></p>
<p>All three specifications are currently in draft status:</p>
<ul>
<li><strong>Spec 154</strong> (mdBook Version Selector UI): Draft - UI component design complete, not yet implemented</li>
<li><strong>Spec 155</strong> (Versioned Documentation Deployment): Draft - Deployment workflow design complete, not yet implemented</li>
<li><strong>Spec 156</strong> (Version-Aware Book Workflow): Draft - Workflow enhancements designed, not yet implemented</li>
</ul>
<p><strong>Source</strong>: specs/154-mdbook-version-selector-ui.md:6, specs/155-versioned-documentation-deployment.md:6, specs/156-version-aware-book-workflow.md:6</p>
<p><strong>What Exists Today:</strong></p>
<ul>
<li>Single-version documentation deployment (<code>.github/workflows/deploy-docs.yml</code>)</li>
<li>Automated documentation workflow without version support (<code>workflows/book-docs-drift.yml</code>)</li>
<li>GitHub Pages deployment to root path only</li>
</ul>
<p><strong>Planned Implementation Path:</strong></p>
<ol>
<li><strong>Phase 1</strong>: Implement version selector UI component (Spec 154)</li>
<li><strong>Phase 2</strong>: Create versioned deployment workflow (Spec 155)</li>
<li><strong>Phase 3</strong>: Enhance book workflow with VERSION parameter (Spec 156)</li>
<li><strong>Phase 4</strong>: Deploy Prodigy’s own documentation with versioning</li>
<li><strong>Phase 5</strong>: Document lessons learned and create templates for other projects</li>
</ol>
<h3 id="future-enhancements"><a class="header" href="#future-enhancements">Future Enhancements</a></h3>
<p><strong>Beyond the current design, potential future additions include:</strong></p>
<ul>
<li>
<p><strong>Prodigy CLI Integration</strong>:</p>
<pre><code class="language-bash">prodigy book add-versioning  # Automatically set up versioning
prodigy book deploy --version v0.2.6  # Manual deployment command
</code></pre>
</li>
<li>
<p><strong>Automatic versions.json Generation</strong>: Parse git tags and generate <code>versions.json</code> automatically from repository metadata</p>
</li>
<li>
<p><strong>Version Comparison View</strong>: Side-by-side diff showing documentation changes between versions</p>
</li>
<li>
<p><strong>Search Across Versions</strong>: Enhanced search that can find content across all deployed versions</p>
</li>
<li>
<p><strong>Version-Specific Changelogs</strong>: Automatically extract and display changelog for each version in the docs</p>
</li>
</ul>
<p><strong>Source</strong>: specs/154-mdbook-version-selector-ui.md:269-273</p>
<h3 id="see-also-41"><a class="header" href="#see-also-41">See Also</a></h3>
<ul>
<li><a href="automated-documentation/github-actions-integration.html">GitHub Actions Integration</a> - Current (non-versioned) deployment workflow</li>
<li><a href="automated-documentation/understanding-the-workflow.html">Understanding the Workflow</a> - How the automated documentation system works</li>
<li><a href="automated-documentation/best-practices.html">Best Practices</a> - General documentation best practices</li>
<li><a href="automated-documentation/quick-start.html">Quick Start</a> - Getting started with automated documentation</li>
<li><a href="automated-documentation/advanced-configuration.html">Advanced Configuration</a> - Customizing the documentation workflow</li>
</ul>
<p><strong>Related Specifications:</strong></p>
<ul>
<li>Spec 154: mdBook Version Selector UI (specs/154-mdbook-version-selector-ui.md)</li>
<li>Spec 155: Versioned Documentation Deployment (specs/155-versioned-documentation-deployment.md)</li>
<li>Spec 156: Version-Aware Book Workflow (specs/156-version-aware-book-workflow.md)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="next-steps-8"><a class="header" href="#next-steps-8">Next Steps</a></h2>
<p>After setting up your automated documentation system, here’s how to build on your foundation and get the most value from continuous documentation maintenance.</p>
<h3 id="within-this-chapter-1"><a class="header" href="#within-this-chapter-1">Within This Chapter</a></h3>
<p>Complete your understanding of the automated documentation system:</p>
<ul>
<li>
<p><strong><a href="automated-documentation/quick-start.html">Quick Start</a></strong> - Get a minimal working documentation workflow running in minutes, from creating the book structure to running your first drift detection</p>
</li>
<li>
<p><strong><a href="automated-documentation/quick-start-30-minutes.html">Quick Start (30 Minutes)</a></strong> - Time-boxed tutorial that takes you from zero to a fully configured documentation workflow with validation</p>
</li>
<li>
<p><strong><a href="automated-documentation/understanding-the-workflow.html">Understanding the Workflow</a></strong> - Learn how the MapReduce workflow orchestrates setup/map/reduce phases, and how worktree isolation protects your main branch</p>
</li>
<li>
<p><strong><a href="automated-documentation/automatic-gap-detection.html">Automatic Gap Detection</a></strong> - Discover how the system identifies missing or incomplete documentation by analyzing your codebase features</p>
</li>
<li>
<p><strong><a href="automated-documentation/customization-examples.html">Customization Examples</a></strong> - Real-world examples of adapting the workflow for different project types (CLI tools, libraries, web applications)</p>
</li>
<li>
<p><strong><a href="automated-documentation/documentation-versioning.html">Documentation Versioning</a></strong> - Strategies for maintaining documentation across multiple versions and releases</p>
</li>
<li>
<p><strong><a href="automated-documentation/github-actions-integration.html">GitHub Actions Integration</a></strong> - Set up continuous documentation with automated drift detection on every push or on a schedule</p>
</li>
<li>
<p><strong><a href="automated-documentation/best-practices.html">Best Practices</a></strong> - Proven approaches for review workflows, handling merge conflicts, and maintaining documentation quality at scale</p>
</li>
<li>
<p><strong><a href="automated-documentation/troubleshooting.html">Troubleshooting</a></strong> - Solutions to common issues including mdBook build failures, Claude API problems, and workflow debugging techniques</p>
</li>
<li>
<p><strong><a href="automated-documentation/advanced-configuration.html">Advanced Configuration</a></strong> - Fine-tune feature extraction, MapReduce parallelism, custom drift rules, and multi-language project support</p>
</li>
</ul>
<h3 id="immediate-next-steps"><a class="header" href="#immediate-next-steps">Immediate Next Steps</a></h3>
<p><strong>1. Validate Your Setup</strong></p>
<p>After your first workflow run, verify everything is working:</p>
<pre><code class="language-bash"># Check that book builds successfully
cd book &amp;&amp; mdbook build

# Review generated drift reports
cat .prodigy/book-analysis/drift-*.json | jq '.severity'

# Verify feature extraction captured your project's capabilities
cat .prodigy/book-analysis/features.json | jq '.features | length'
</code></pre>
<p><strong>2. Review and Refine Documentation</strong></p>
<p>The automated workflow generates updates, but human review ensures quality:</p>
<ul>
<li>Read through updated chapters to verify accuracy</li>
<li>Check that code examples match your actual implementation (file paths in book/src/)</li>
<li>Validate that cross-references between chapters work correctly</li>
<li>Ensure terminology matches your project’s conventions</li>
</ul>
<p><strong>3. Set Up Continuous Updates</strong></p>
<p>Integrate documentation maintenance into your development workflow:</p>
<ul>
<li>Add GitHub Actions workflow for automated drift detection (see <a href="automated-documentation/github-actions-integration.html">GitHub Actions Integration</a>)</li>
<li>Configure workflow to run on push to main branch or on a schedule</li>
<li>Set up notifications for documentation drift detection results</li>
<li>Consider running validation before merging large feature branches</li>
</ul>
<p><strong>4. Customize for Your Project</strong></p>
<p>Adapt the system to your specific needs:</p>
<ul>
<li>Adjust chapter structure in your chapters JSON configuration file (workflows/data/[project]-chapters.json)</li>
<li>Modify feature extraction patterns for your programming language</li>
<li>Configure MapReduce parallelism based on your chapter count (workflows/book-docs-drift.yml:MAX_PARALLEL)</li>
<li>Add project-specific Claude commands for specialized documentation tasks</li>
</ul>
<p><strong>5. Expand Documentation Coverage</strong></p>
<p>As your project evolves, grow your documentation:</p>
<ul>
<li>Add new chapters for major feature areas</li>
<li>Create subsections for complex topics that need detailed explanation</li>
<li>Document edge cases and advanced usage patterns</li>
<li>Include troubleshooting guides based on user questions</li>
</ul>
<h3 id="advanced-topics-3"><a class="header" href="#advanced-topics-3">Advanced Topics</a></h3>
<p>Ready to explore the broader Prodigy ecosystem? These topics build on automated documentation concepts:</p>
<ul>
<li>
<p><strong><a href="automated-documentation/../mapreduce/index.html">MapReduce Workflows</a></strong> - Deep dive into the parallel processing architecture that powers book documentation workflows, including setup/map/reduce phases and error handling</p>
</li>
<li>
<p><strong><a href="automated-documentation/../environment/index.html">Environment Variables</a></strong> - Learn how to configure workflow behavior through environment variables, secrets management, and profile-based configuration</p>
</li>
<li>
<p><strong><a href="automated-documentation/../error-handling.html">Error Handling</a></strong> - Strategies for building resilient workflows that gracefully handle failures, including Dead Letter Queue (DLQ) usage and retry configuration</p>
</li>
<li>
<p><strong><a href="automated-documentation/../advanced/index.html">Advanced Features</a></strong> - Explore conditional execution, goal-seeking operations, and complex control flow for sophisticated automation workflows</p>
</li>
<li>
<p><strong><a href="automated-documentation/../troubleshooting/index.html">Troubleshooting</a></strong> - Comprehensive guide to debugging workflow issues, examining Claude execution logs, and resolving common problems</p>
</li>
</ul>
<h3 id="learning-path-recommendations"><a class="header" href="#learning-path-recommendations">Learning Path Recommendations</a></h3>
<p><strong>For Documentation Maintainers:</strong></p>
<ol>
<li>Start with <a href="automated-documentation/quick-start.html">Quick Start</a> to get the basics running</li>
<li>Read <a href="automated-documentation/understanding-the-workflow.html">Understanding the Workflow</a> to understand the system architecture</li>
<li>Set up <a href="automated-documentation/github-actions-integration.html">GitHub Actions Integration</a> for automation</li>
<li>Review <a href="automated-documentation/best-practices.html">Best Practices</a> for quality maintenance strategies</li>
</ol>
<p><strong>For Power Users:</strong></p>
<ol>
<li>Complete the <a href="automated-documentation/quick-start-30-minutes.html">Quick Start (30 Minutes)</a> tutorial</li>
<li>Explore <a href="automated-documentation/customization-examples.html">Customization Examples</a> for your project type</li>
<li>Configure <a href="automated-documentation/advanced-configuration.html">Advanced Configuration</a> options</li>
<li>Study <a href="automated-documentation/../mapreduce/index.html">MapReduce Workflows</a> to understand the orchestration engine</li>
</ol>
<p><strong>For Contributors:</strong></p>
<ol>
<li>Understand the <a href="automated-documentation/automatic-gap-detection.html">Automatic Gap Detection</a> algorithm</li>
<li>Review the workflow implementation in workflows/book-docs-drift.yml</li>
<li>Study Claude commands in .claude/commands/prodigy-<em>-book-</em>.md</li>
<li>Explore <a href="automated-documentation/../advanced/index.html">Advanced Features</a> for extending functionality</li>
</ol>
<h3 id="getting-help-4"><a class="header" href="#getting-help-4">Getting Help</a></h3>
<p>If you encounter issues or have questions:</p>
<ul>
<li>Check <a href="automated-documentation/troubleshooting.html">Troubleshooting</a> for common problems and solutions</li>
<li>Review Claude command execution logs in ~/.claude/projects/[worktree-path]/</li>
<li>Examine drift reports in .prodigy/book-analysis/drift-*.json for detailed analysis</li>
<li>Check the Dead Letter Queue (DLQ) for failed items: <code>prodigy dlq show [job-id]</code></li>
<li>Inspect MapReduce events for workflow execution details in ~/.prodigy/events/</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="benefits-1"><a class="header" href="#benefits-1">Benefits</a></h2>
<p>This approach provides:</p>
<ul>
<li>✅ <strong>Always up-to-date documentation</strong> - Runs automatically to detect drift</li>
<li>✅ <strong>Consistent quality</strong> - Same analysis across all chapters</li>
<li>✅ <strong>Reduced maintenance</strong> - Less manual documentation work</li>
<li>✅ <strong>Accurate examples</strong> - Extracted from actual code</li>
<li>✅ <strong>Version control</strong> - All changes tracked in git</li>
<li>✅ <strong>Easy to customize</strong> - Configuration-based, works for any project</li>
<li>✅ <strong>Multi-version support</strong> - Serve docs for multiple releases simultaneously</li>
</ul>
<p>The same commands that maintain Prodigy’s documentation can maintain yours.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="examples-4"><a class="header" href="#examples-4">Examples</a></h1>
<blockquote>
<p><strong>Last Verified</strong>: 2025-01-11 against codebase commit 753f90e0</p>
<p>All examples in this chapter have been validated against the current implementation. Field names, syntax, and configuration options are verified against source code definitions.</p>
</blockquote>
<p>This chapter demonstrates practical Prodigy workflows with real-world examples. Examples progress from simple to advanced, covering standard workflows, MapReduce parallel processing, error handling, and advanced features.</p>
<h2 id="quick-reference-2"><a class="header" href="#quick-reference-2">Quick Reference</a></h2>
<p>Find the right example for your use case:</p>
<div class="table-wrapper"><table><thead><tr><th>Use Case</th><th>Example</th><th>Key Features</th></tr></thead><tbody>
<tr><td>Simple build/test pipeline</td><td>Example 1</td><td>Basic commands, error handling</td></tr>
<tr><td>Iterative optimization</td><td>Example 2</td><td>Goal seeking, validation feedback</td></tr>
<tr><td>Loop over configurations</td><td>Example 3</td><td>Foreach iteration, parallel processing</td></tr>
<tr><td>Parallel code processing</td><td>Example 4, 8</td><td>MapReduce, distributed work</td></tr>
<tr><td>Conditional logic</td><td>Example 5</td><td>Capture output, when clauses</td></tr>
<tr><td>Multi-step validation</td><td>Example 6</td><td>Validation with gap filling</td></tr>
<tr><td>Environment configuration</td><td>Example 7</td><td>Env vars, secrets, profiles</td></tr>
<tr><td>Dead Letter Queue (DLQ)</td><td>Example 8</td><td>Error handling, retry failed items</td></tr>
<tr><td>Generate config files</td><td>Example 9</td><td>write_file with JSON/YAML/text</td></tr>
<tr><td>Advanced git tracking</td><td>Example 10</td><td>Git context variables, working_dir</td></tr>
<tr><td>External service resilience</td><td>Example 11</td><td>Circuit breakers, fail fast</td></tr>
<tr><td>Retry with backoff</td><td>Example 12</td><td>Exponential/linear/custom backoff</td></tr>
<tr><td>Reusable workflows</td><td>Example 13</td><td>Composition (preview feature)</td></tr>
<tr><td>Custom merge process</td><td>Example 14</td><td>Merge workflows, pre-merge validation</td></tr>
</tbody></table>
</div>
<h2 id="example-1-simple-build-and-test"><a class="header" href="#example-1-simple-build-and-test">Example 1: Simple Build and Test</a></h2>
<pre><code class="language-yaml">- shell: "cargo build"
- shell: "cargo test"
  on_failure:
    claude: "/fix-failing-tests"
- shell: "cargo clippy"
</code></pre>
<hr />
<h2 id="example-2-coverage-improvement-with-goal-seeking"><a class="header" href="#example-2-coverage-improvement-with-goal-seeking">Example 2: Coverage Improvement with Goal Seeking</a></h2>
<pre><code class="language-yaml">- goal_seek:
    goal: "Achieve 80% test coverage"
    claude: "/improve-coverage"  # Can also use 'shell' for shell commands
    validate: |
      coverage=$(cargo tarpaulin | grep 'Coverage' | sed 's/.*: \([0-9.]*\)%.*/\1/')
      echo "score: ${coverage%.*}"
    threshold: 80
    max_attempts: 5
    timeout_seconds: 1800  # Optional: 30 minute timeout for entire goal-seeking process
    fail_on_incomplete: false  # Optional: Continue workflow even if goal not reached
</code></pre>
<p><strong>Source</strong>: GoalSeekConfig from src/cook/goal_seek/mod.rs:14-41, execution engine from src/cook/goal_seek/engine.rs:23-116</p>
<p><strong>How Goal Seeking Works:</strong></p>
<p>Goal seeking provides iterative refinement with automatic convergence detection:</p>
<ol>
<li>Execute the improvement command (Claude or shell)</li>
<li>Run validation script to get a score (0-100)</li>
<li>Pass environment variables to next iteration:
<ul>
<li><code>PRODIGY_VALIDATION_SCORE</code> - Current score</li>
<li><code>PRODIGY_VALIDATION_OUTPUT</code> - Full validation output</li>
<li><code>PRODIGY_VALIDATION_GAPS</code> - Identified improvement areas</li>
</ul>
</li>
<li>Repeat until threshold reached, max attempts hit, or convergence detected</li>
<li>Auto-stops when no improvement in last 3 attempts (convergence)</li>
</ol>
<p><strong>Validation Script Format:</strong></p>
<pre><code class="language-bash"># Must output "score: N" where N is 0-100
echo "score: 75"
# Optional: output "gaps: description" for targeted improvements
echo "gaps: Missing tests for auth module"
</code></pre>
<p><strong>Goal-Seek vs Validate:</strong></p>
<ul>
<li><code>goal_seek</code>: Iterative optimization with feedback loop</li>
<li><code>validate</code>: One-time completion check (see Example 6)</li>
</ul>
<p><strong>Note:</strong> Changes are automatically committed during goal-seeking iterations. Use <code>commit_required: true</code> on the outer goal_seek step to control commit behavior.</p>
<hr />
<h2 id="example-3-foreach-iteration"><a class="header" href="#example-3-foreach-iteration">Example 3: Foreach Iteration</a></h2>
<pre><code class="language-yaml"># Test multiple configurations in sequence
- foreach:
    - rust-version: "1.70"
      profile: debug
    - rust-version: "1.71"
      profile: release
    - rust-version: "stable"
      profile: release
  do:
    - shell: "rustup install ${foreach.item.rust-version}"
    - shell: "cargo +${foreach.item.rust-version} build --profile ${foreach.item.profile}"
    - shell: "cargo +${foreach.item.rust-version} test"

# Parallel foreach with error handling
- foreach:
    - "web-service"
    - "api-gateway"
    - "worker-service"
  parallel: 3  # Options: false (sequential), true (default parallelism), or number (specific count)
  continue_on_error: true
  do:
    - shell: "cd services/${foreach.item} &amp;&amp; cargo build"
    - shell: "cd services/${foreach.item} &amp;&amp; cargo test"
      on_failure:
        claude: "/fix-service-tests ${foreach.item}"
</code></pre>
<hr />
<h2 id="example-4-parallel-code-review"><a class="header" href="#example-4-parallel-code-review">Example 4: Parallel Code Review</a></h2>
<pre><code class="language-yaml">name: parallel-code-review
mode: mapreduce

setup:
  - shell: "find src -name '*.rs' &gt; files.txt"
  - shell: "jq -R -s -c 'split(\"\n\") | map(select(length &gt; 0) | {path: .})' files.txt &gt; items.json"

map:
  input: items.json
  json_path: "$[*]"  # Process all items in root array
  agent_template:
    - claude: "/review-file ${item.path}"
      id: "review"
      capture_output: "review_result"  # Capture command output for use in later steps
      capture_format: "json"  # Parse output as JSON (see Example 5 for all format options)
    - shell: "cargo check ${item.path}"
  max_parallel: 5

reduce:
  - claude: "/summarize-reviews ${map.results}"
</code></pre>
<p><strong>Note:</strong> JSONPath <code>"$[*]"</code> matches all items in the root array. Since the setup phase creates an array of <code>{path: ...}</code> objects, each map agent receives an <code>item</code> object with <code>item.path</code> available for use in commands.</p>
<p><strong>Advanced JSONPath Patterns:</strong></p>
<ul>
<li><code>$.items[*]</code> - Extract items from nested object</li>
<li><code>$.items[*].files[*]</code> - Extract from nested arrays (flattens results)</li>
<li><code>$.items[?(@.priority &gt; 5)]</code> - Filter items by condition</li>
<li><code>$[?(@.severity == 'critical')]</code> - Filter array by field value</li>
</ul>
<hr />
<h2 id="example-5-conditional-deployment"><a class="header" href="#example-5-conditional-deployment">Example 5: Conditional Deployment</a></h2>
<pre><code class="language-yaml">- shell: "cargo test --quiet &amp;&amp; echo true || echo false"
  id: "test"
  capture_output: "test_result"  # Canonical field name (alias: 'capture')
  capture_format: "boolean"  # Supported formats explained below
  timeout: 300  # Timeout in seconds (5 minutes)

- shell: "cargo build --release"
  when: "${test_result} == true"

- shell: "docker build -t myapp ."
  when: "${test_result} == true"
  on_success:
    shell: "docker push myapp:latest"
</code></pre>
<p><strong>Note:</strong> <code>capture_format</code> options:</p>
<ul>
<li><code>string</code> - Raw text output (default)</li>
<li><code>json</code> - Parse output as JSON object</li>
<li><code>lines</code> - Split output into array of lines</li>
<li><code>number</code> - Parse output as numeric value</li>
<li><code>boolean</code> - Parse as true/false based on exit code or output text</li>
</ul>
<p><strong>Advanced capture options:</strong></p>
<pre><code class="language-yaml"># Capture specific streams (stdout, stderr, exit_code, success, duration)
- shell: "cargo build 2&gt;&amp;1"
  capture_output: "build_output"
  capture_streams: "stdout,stderr,exit_code"  # Capture multiple streams

# Access captured values
- shell: "echo 'Exit code was ${build_output.exit_code}'"
</code></pre>
<hr />
<h2 id="example-6-multi-step-validation"><a class="header" href="#example-6-multi-step-validation">Example 6: Multi-Step Validation</a></h2>
<pre><code class="language-yaml">- claude: "/implement-feature auth"
  commit_required: true
  validate:
    commands:
      - shell: "cargo test auth"
      - shell: "cargo clippy -- -D warnings"
      - claude: "/validate-implementation --output validation.json"
    result_file: "validation.json"
    threshold: 90
    on_incomplete:
      claude: "/complete-gaps ${validation.gaps}"
      commit_required: true
      max_attempts: 2
</code></pre>
<hr />
<h2 id="example-7-environment-aware-workflow"><a class="header" href="#example-7-environment-aware-workflow">Example 7: Environment-Aware Workflow</a></h2>
<pre><code class="language-yaml"># Global environment variables (including secrets with masking)
env:
  # Regular variables
  NODE_ENV: production
  API_URL: https://api.production.com

  # Secrets (automatically masked in logs)
  # Use secret: true and value fields for sensitive data
  API_KEY:
    secret: true
    value: "${SECRET_API_KEY}"

  # Secret with external provider
  DB_PASSWORD:
    secret: true
    value: "${DB_PASSWORD}"
    # provider: "vault"  # Optional: external secret store (not yet implemented)

# Environment profiles for different contexts
profiles:
  production:
    env:
      API_URL: https://api.production.com
      LOG_LEVEL: error
    description: "Production environment with error-level logging"

  staging:
    env:
      API_URL: https://api.staging.com
      LOG_LEVEL: warn
    description: "Staging environment with warning-level logging"

# Load additional variables from .env files
# Note: Paths are relative to workflow file location
env_files:
  - .env
  - .env.production

# Workflow steps
- shell: "cargo build --release"

# Use environment variables in commands
- shell: "echo 'Deploying to ${NODE_ENV} at ${API_URL}'"

# Override environment for specific step using env field
- shell: "./deploy.sh"
  env:
    LOG_LEVEL: debug
</code></pre>
<p><strong>Source</strong>: Environment configuration from src/cook/environment/config.rs:12-36, secret masking from src/cook/environment/config.rs:84-96</p>
<p><strong>Note:</strong> Profiles are activated using the <code>--profile &lt;name&gt;</code> CLI flag when running workflows. For example:</p>
<pre><code class="language-bash"># Use production profile
prodigy run workflow.yml --profile production

# Use staging profile
prodigy run workflow.yml --profile staging
</code></pre>
<p><strong>Secrets Masking</strong>: Variables with <code>secret: true</code> are automatically masked in:</p>
<ul>
<li>Command output logs</li>
<li>Error messages</li>
<li>Event logs</li>
<li>Checkpoint files</li>
</ul>
<p>Example masked output:</p>
<pre><code>$ echo 'API key is ***'
</code></pre>
<p><strong>Source</strong>: Example workflow from workflows/mapreduce-env-example.yml:7-40, profile structure from tests/environment_workflow_test.rs:68-88</p>
<hr />
<h2 id="example-8-complex-mapreduce-with-error-handling"><a class="header" href="#example-8-complex-mapreduce-with-error-handling">Example 8: Complex MapReduce with Error Handling</a></h2>
<pre><code class="language-yaml">name: tech-debt-elimination
mode: mapreduce

setup:
  - shell: "debtmap analyze . --output debt.json"

map:
  input: debt.json
  json_path: "$.items[*]"
  filter: "item.severity == 'critical'"
  sort_by: "item.priority DESC"
  max_items: 20
  max_parallel: 5
  distinct: "item.id"  # Deduplication: Prevents processing duplicate items based on this field

  # Timeout configuration (optional - default is 600 seconds / 10 minutes)
  timeout_config:
    agent_timeout_secs: 600  # Maximum time per agent execution
    cleanup_grace_period_secs: 30  # Time allowed for cleanup after timeout

  agent_template:
    - claude: "/fix-debt-item '${item.description}'"
      commit_required: true
    - shell: "cargo test"
      on_failure:
        claude: "/debug-and-fix"

reduce:
  - shell: "debtmap analyze . --output debt-after.json"
  - claude: "/compare-debt-reports --before debt.json --after debt-after.json"

error_policy:
  on_item_failure: dlq  # Default: dlq (failed items to Dead Letter Queue)
  continue_on_failure: true  # Default: true (continue despite failures)
  max_failures: 5  # Optional: stop after N failures
  failure_threshold: 0.3  # Optional: stop if &gt;30% fail
  error_collection: aggregate  # Default: aggregate (Options: aggregate, immediate, batched:{size})
</code></pre>
<p><strong>Note:</strong> The entire <code>error_policy</code> block is optional with sensible defaults. If not specified, failed items go to the Dead Letter Queue (<code>on_item_failure: dlq</code>), workflow continues despite failures (<code>continue_on_failure: true</code>), and errors are aggregated at the end (<code>error_collection: aggregate</code>). Use <code>max_failures</code> or <code>failure_threshold</code> to fail fast if too many items fail.</p>
<p><strong>Deduplication with <code>distinct</code></strong>: The <code>distinct</code> field enables idempotent processing by preventing duplicate work items. When specified, Prodigy extracts the value of the given field (e.g., <code>item.id</code>) from each work item and ensures only unique values are processed. This is useful when:</p>
<ul>
<li>Input data may contain duplicates</li>
<li>Resuming a workflow after adding new items</li>
<li>Preventing wasted work on identical items</li>
<li>Ensuring exactly-once processing semantics</li>
</ul>
<p>Example: With <code>distinct: "item.id"</code>, if your input contains <code>[{id: "1", ...}, {id: "2", ...}, {id: "1", ...}]</code>, only items with IDs “1” and “2” will be processed (the duplicate “1” is skipped).</p>
<p><strong>Resuming MapReduce Workflows:</strong>
MapReduce jobs can be resumed using either the session ID or job ID:</p>
<pre><code class="language-bash"># Resume using session ID
prodigy resume session-mapreduce-1234567890

# Resume using job ID
prodigy resume-job mapreduce-1234567890

# Unified resume command (auto-detects ID type)
prodigy resume mapreduce-1234567890
</code></pre>
<p><strong>Session-Job ID Mapping</strong>: The bidirectional mapping between session IDs and job IDs is stored in <code>~/.prodigy/state/{repo_name}/mappings/</code> and created automatically when the MapReduce workflow starts. This allows you to resume using either the session ID (e.g., <code>session-mapreduce-1234567890</code>) or the job ID (e.g., <code>mapreduce-1234567890</code>), and Prodigy will automatically find the correct checkpoint data.</p>
<p><strong>Source</strong>: Session-job mapping implementation from MapReduce checkpoint and resume (Spec 134)</p>
<p><strong>Debugging Failed Agents:</strong>
When agents fail, DLQ entries include a <code>json_log_location</code> field pointing to the Claude JSON log file for debugging:</p>
<pre><code class="language-bash"># View failed items and their log locations
prodigy dlq show &lt;job_id&gt; | jq '.items[].failure_history[].json_log_location'

# Inspect the Claude interaction for a failed agent
cat &lt;json_log_location&gt; | jq
</code></pre>
<p>This allows you to see exactly what tools Claude invoked and why the agent failed.</p>
<hr />
<h2 id="example-9-generating-configuration-files"><a class="header" href="#example-9-generating-configuration-files">Example 9: Generating Configuration Files</a></h2>
<pre><code class="language-yaml"># Generate a JSON configuration file
- write_file:
    path: "config/deployment.json"
    format: json  # Options: text, json, yaml
    create_dirs: true  # Create parent directories if they don't exist
    content:
      environment: production
      api_url: "${API_URL}"
      features:
        - auth
        - analytics
        - notifications
      timeout: 30

# Generate a YAML configuration file
- write_file:
    path: "config/services.yml"
    format: yaml
    content:
      services:
        web:
          image: "myapp:latest"
          ports:
            - "8080:8080"
        database:
          image: "postgres:15"
          environment:
            POSTGRES_DB: "${DB_NAME}"

# Generate a plain text report
- write_file:
    path: "reports/summary.txt"
    format: text
    mode: "0644"  # File permissions (optional)
    content: |
      Deployment Summary
      ==================
      Environment: ${NODE_ENV}
      API URL: ${API_URL}
      Timestamp: $(date)
</code></pre>
<hr />
<h2 id="example-10-advanced-features"><a class="header" href="#example-10-advanced-features">Example 10: Advanced Features</a></h2>
<pre><code class="language-yaml"># Nested error handling with retry configuration
- shell: "cargo build --release"
  on_failure:
    shell: "cargo clean"
    on_success:
      shell: "cargo build --release"
      max_attempts: 2
  on_success:
    shell: "cargo test --release"

# Complex conditional execution with max_attempts
- shell: "cargo test"
  id: "test"
  capture_output: "test_output"

- claude: "/fix-tests"
  when: "${test_output} contains 'FAILED'"
  max_attempts: 3

# Conditional deployment based on test results
- shell: "cargo build --release"
  when: "${test.exit_code} == 0"

# Multi-condition logic
- shell: "./deploy.sh"
  when: "${test_output} contains 'passed' and ${build_output} contains 'Finished'"
</code></pre>
<p><strong>Note:</strong> Advanced features currently supported:</p>
<ul>
<li><strong>Nested handlers</strong>: Chain <code>on_failure</code> and <code>on_success</code> handlers for complex error recovery</li>
<li><strong>Max attempts</strong>: Combine with conditional execution for automatic retry logic</li>
<li><strong>Conditional execution</strong>: Use <code>when</code> clauses with captured output or variables</li>
<li><strong>Complex conditionals</strong>: Combine multiple conditions with <code>and</code>/<code>or</code> operators</li>
<li><strong>Working directory</strong>: Per-command directory control using <code>working_dir</code> field</li>
<li><strong>Git context variables</strong>: Automatic tracking of file changes during workflow execution</li>
</ul>
<p><strong>Example of working_dir usage:</strong></p>
<p><strong>Source</strong>: Field definition from src/commands/handlers/shell.rs:40, examples from workflows/environment-example.yml:52-64</p>
<pre><code class="language-yaml"># Run command in specific directory
- name: "Build frontend"
  shell: "npm run build"
  working_dir: ./frontend      # Execute in frontend/ directory

# Combine with environment variables
- name: "Run backend tests"
  shell: "pytest"
  env:
    PYTHONPATH: "./src:./tests"
  working_dir: ./backend

# Use variable interpolation for dynamic paths
- name: "Deploy to environment"
  shell: "echo 'Deploying...'"
  working_dir: "${env.DEPLOY_DIR}"  # Path from environment variable
</code></pre>
<p><strong>Note:</strong> The <code>working_dir</code> field is fully implemented and production-ready:</p>
<ul>
<li>Accepts relative or absolute paths</li>
<li>Supports variable interpolation (e.g., <code>"${env.PROJECT_DIR}"</code>)</li>
<li>Falls back to current execution context if not specified</li>
<li>Paths are resolved to absolute paths automatically</li>
<li>Relative paths are resolved via the workflow execution context</li>
</ul>
<p><strong>Git Context Variables (Spec 122):</strong>
Prodigy automatically tracks file changes during workflow execution and exposes them as variables:</p>
<pre><code class="language-yaml"># Access files changed in current step
- shell: "echo 'Modified files: ${step.files_modified}'"
- shell: "echo 'Added files: ${step.files_added}'"
- shell: "echo 'Deleted files: ${step.files_deleted}'"

# Format as JSON array
- shell: "echo '${step.files_modified:json}'"

# Filter by glob pattern (only Rust files)
- shell: "echo 'Rust files changed: ${step.files_modified:*.rs}'"

# Access workflow-level aggregations
- shell: "echo 'Total commits: ${workflow.commit_count}'"
- shell: "echo 'All modified files: ${workflow.files_modified}'"

# Access uncommitted changes
- shell: "echo 'Staged files: ${git.staged_files}'"
- shell: "echo 'Unstaged files: ${git.unstaged_files}'"

# Pattern filtering for git context
- claude: "/review-changes ${git.modified_files|pattern:**/*.rs}"
- shell: "echo 'Changed Rust files: ${git.staged_files|pattern:**/*.rs}'"

# Basename-only output (no paths)
- shell: "echo 'File names: ${git.modified_files|basename}'"
</code></pre>
<p><strong>Available Git Context Variables:</strong></p>
<ul>
<li><code>${step.files_added}</code> - Files added in current step</li>
<li><code>${step.files_modified}</code> - Files modified in current step</li>
<li><code>${step.files_deleted}</code> - Files deleted in current step</li>
<li><code>${step.commits}</code> - Commit SHAs created in this step</li>
<li><code>${step.insertions}</code> - Lines inserted in this step</li>
<li><code>${step.deletions}</code> - Lines deleted in this step</li>
<li><code>${workflow.commit_count}</code> - Total commits in workflow</li>
<li><code>${workflow.files_modified}</code> - All files modified across workflow</li>
<li><code>${git.staged_files}</code> - Currently staged files (uncommitted)</li>
<li><code>${git.unstaged_files}</code> - Modified but unstaged files</li>
<li><code>${git.modified_files}</code> - All uncommitted modifications</li>
</ul>
<p><strong>Supported Formats and Filters:</strong></p>
<ul>
<li><code>:json</code> - Format as JSON array</li>
<li><code>:*.rs</code> - Filter by glob pattern (e.g., <code>*.rs</code>, <code>src/**/*.py</code>)</li>
<li><code>|pattern:**/*.rs</code> - Alternative syntax for glob pattern filtering (equivalent to <code>:pattern</code>)</li>
<li><code>|basename</code> - Extract just file names without paths</li>
</ul>
<p><strong>Note:</strong> Both <code>:pattern</code> and <code>|pattern:</code> syntaxes are valid and equivalent. Use whichever is more readable in your context:</p>
<ul>
<li><code>${git.modified_files:*.rs}</code> - Colon syntax (more concise)</li>
<li><code>${git.modified_files|pattern:**/*.rs}</code> - Pipe syntax (more explicit)</li>
</ul>
<p><strong>Source</strong>: Git context tracking from src/cook/workflow/git_context.rs:1-120, variable resolution from src/cook/workflow/git_context.rs:36-42</p>
<p><strong>Troubleshooting MapReduce Cleanup:</strong>
If agent worktree cleanup fails (due to disk full, permission errors, etc.), use the orphaned worktree cleanup command:</p>
<pre><code class="language-bash"># List and clean orphaned worktrees for a specific job
prodigy worktree clean-orphaned &lt;job_id&gt;

# Dry run to preview what would be cleaned
prodigy worktree clean-orphaned &lt;job_id&gt; --dry-run

# Force cleanup without confirmation
prodigy worktree clean-orphaned &lt;job_id&gt; --force
</code></pre>
<p>Note: Agent execution status is independent of cleanup status. If an agent completes successfully but cleanup fails, the agent is still marked as successful and results are preserved.</p>
<p><strong>Source</strong>: Orphaned worktree cleanup from MapReduce cleanup failure handling (Spec 136)</p>
<hr />
<h2 id="example-11-circuit-breaker-for-resilient-error-handling"><a class="header" href="#example-11-circuit-breaker-for-resilient-error-handling">Example 11: Circuit Breaker for Resilient Error Handling</a></h2>
<pre><code class="language-yaml">name: api-processing-with-circuit-breaker
mode: mapreduce

setup:
  - shell: "curl https://api.example.com/items &gt; items.json"

map:
  input: items.json
  json_path: "$[*]"
  max_parallel: 10

  agent_template:
    - shell: "curl -X POST https://api.example.com/process -d '${item}'"
      max_attempts: 3
    - claude: "/validate-processing ${item.id}"

  # Circuit breaker prevents cascading failures from external service issues
  error_policy:
    on_item_failure: dlq
    continue_on_failure: true
    circuit_breaker:
      failure_threshold: 5      # Open circuit after 5 consecutive failures
      success_threshold: 3      # Close circuit after 3 consecutive successes
      timeout: 30s             # Wait 30s before testing recovery (half-open state)
      half_open_requests: 3    # Allow 3 test requests in half-open state

reduce:
  - claude: "/summarize-results ${map.results}"
</code></pre>
<p><strong>Source</strong>: CircuitBreakerConfig from src/cook/workflow/error_policy.rs:46-88, state machine from error_policy.rs:251-343</p>
<p><strong>Circuit Breaker State Transitions:</strong></p>
<pre><code>Closed → Open (after failure_threshold consecutive failures)
Open → HalfOpen (after timeout expires)
HalfOpen → Closed (after success_threshold successes)
HalfOpen → Open (if any half_open_request fails)
</code></pre>
<p><strong>When to Use Circuit Breakers:</strong></p>
<ul>
<li>External API calls that may become unavailable</li>
<li>Database operations during maintenance windows</li>
<li>Network-dependent operations with potential outages</li>
<li>Any operation where cascading failures should be prevented</li>
</ul>
<p><strong>Circuit Breaker Benefits:</strong></p>
<ul>
<li><strong>Fail Fast</strong>: Stop wasting resources on doomed requests</li>
<li><strong>Self-Healing</strong>: Automatically test recovery after timeout</li>
<li><strong>Prevent Overload</strong>: Give external services time to recover</li>
<li><strong>Clear Signals</strong>: Circuit state indicates system health</li>
</ul>
<p><strong>Default Values</strong> (from error_policy.rs:63-76):</p>
<ul>
<li><code>failure_threshold: 5</code> - Balance between sensitivity and stability</li>
<li><code>success_threshold: 3</code> - Require consistent success before full recovery</li>
<li><code>timeout: 30s</code> - Typical recovery time for transient issues</li>
<li><code>half_open_requests: 3</code> - Minimal testing before full recovery</li>
</ul>
<p><strong>See Also</strong>: <a href="error-handling.html">Error Handling Guide</a> for comprehensive error handling patterns</p>
<hr />
<h2 id="example-12-retry-configuration-with-backoff-strategies"><a class="header" href="#example-12-retry-configuration-with-backoff-strategies">Example 12: Retry Configuration with Backoff Strategies</a></h2>
<pre><code class="language-yaml">name: resilient-deployment
mode: standard

# Global retry defaults for all commands
error_policy:
  retry_config:
    max_attempts: 3          # Maximum retry attempts
    backoff: exponential      # Backoff strategy (see variants below)

# Workflow steps
- shell: "curl https://api.example.com/deploy"
  # Inherits global retry_config

- shell: "docker push myapp:latest"
  # Override with custom retry config
  retry_config:
    max_attempts: 5
    backoff:
      exponential:
        initial: 2s          # Start with 2 second delay
        multiplier: 2.0      # Double delay each retry (2s, 4s, 8s, 16s, 32s)
</code></pre>
<p><strong>Source</strong>: BackoffStrategy enum from src/cook/retry_v2.rs:70-98, RetryConfig from retry_v2.rs:14-52</p>
<p><strong>Backoff Strategy Variants:</strong></p>
<p><strong>1. Exponential (Default)</strong> - Delay doubles each retry:</p>
<pre><code class="language-yaml">backoff:
  exponential:
    initial: 1s         # First retry after 1s
    multiplier: 2.0     # Second after 2s, third after 4s, fourth after 8s
</code></pre>
<p><strong>2. Linear</strong> - Delay increases by fixed increment:</p>
<pre><code class="language-yaml">backoff:
  linear:
    initial: 1s         # First retry after 1s
    increment: 2s       # Second after 3s, third after 5s, fourth after 7s
</code></pre>
<p><strong>3. Fibonacci</strong> - Delay follows Fibonacci sequence:</p>
<pre><code class="language-yaml">backoff:
  fibonacci:
    initial: 1s         # Delays: 1s, 1s, 2s, 3s, 5s, 8s, 13s...
</code></pre>
<p><strong>4. Fixed</strong> - Same delay for all retries:</p>
<pre><code class="language-yaml">backoff:
  fixed:
    delay: 5s           # All retries wait exactly 5s
</code></pre>
<p><strong>5. Custom</strong> - Specify exact delays:</p>
<pre><code class="language-yaml">backoff:
  custom:
    delays: [1s, 5s, 15s, 30s, 60s]
</code></pre>
<p><strong>Advanced Retry Configuration:</strong></p>
<pre><code class="language-yaml">- shell: "curl https://api.example.com/data"
  retry_config:
    max_attempts: 5
    backoff:
      exponential:
        initial: 1s
        multiplier: 2.0
    initial_delay: 1s           # Base delay before backoff
    max_delay: 60s              # Cap maximum delay
    jitter: true                # Add randomness to prevent thundering herd
    jitter_factor: 0.3          # ±30% random variance
    retry_budget: 300s          # Total retry time budget (5 minutes)
    retry_on:                   # Only retry specific error types
      - network                 # Network errors
      - timeout                 # Timeout errors
      - server_error            # HTTP 5xx errors
      - rate_limit              # Rate limiting errors
    on_failure: continue        # What to do after all retries fail
</code></pre>
<p><strong>Source</strong>: Test examples from src/cook/retry_v2.rs:582-659, backoff calculation from retry_v2.rs:283-305</p>
<p><strong>Backoff Strategy Comparison:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>Strategy</th><th>Use Case</th><th>Delay Pattern</th><th>Example</th></tr></thead><tbody>
<tr><td><strong>Exponential</strong></td><td>Most scenarios, fast backoff</td><td>2^n × initial</td><td>1s, 2s, 4s, 8s</td></tr>
<tr><td><strong>Linear</strong></td><td>Steady load reduction</td><td>initial + (n × increment)</td><td>1s, 3s, 5s, 7s</td></tr>
<tr><td><strong>Fibonacci</strong></td><td>Gradual backoff</td><td>Fibonacci(n) × initial</td><td>1s, 1s, 2s, 3s, 5s</td></tr>
<tr><td><strong>Fixed</strong></td><td>Rate limiting, polling</td><td>constant</td><td>5s, 5s, 5s, 5s</td></tr>
<tr><td><strong>Custom</strong></td><td>Complex SLA requirements</td><td>user-defined</td><td>1s, 5s, 15s, 60s</td></tr>
</tbody></table>
</div>
<p><strong>Jitter Benefits</strong> (from retry_v2.rs:644-659):</p>
<ul>
<li>Prevents thundering herd when multiple agents retry simultaneously</li>
<li>Adds ±30% random variance by default (configurable via <code>jitter_factor</code>)</li>
<li>Example: 10s delay becomes 7-13s with 0.3 jitter factor</li>
</ul>
<p><strong>See Also</strong>: <a href="mapreduce/backoff-strategies.html">Retry State Tracking</a> for persistence and recovery</p>
<hr />
<h2 id="example-13-workflow-composition-preview-feature"><a class="header" href="#example-13-workflow-composition-preview-feature">Example 13: Workflow Composition (Preview Feature)</a></h2>
<blockquote>
<p><strong>Note</strong>: Workflow composition features are partially implemented. Core composition logic exists but CLI integration is pending (Spec 131-133). This example shows the planned syntax.</p>
</blockquote>
<pre><code class="language-yaml"># Import reusable workflow fragments
imports:
  - "./workflows/common/test-suite.yml"
  - "./workflows/common/deploy.yml"

# Extend base workflow
extends: "./workflows/base-ci.yml"

name: extended-ci-workflow
mode: standard

# Template for reusable command sets
templates:
  rust_test:
    - shell: "cargo build"
    - shell: "cargo test"
    - shell: "cargo clippy"

  deploy_to_env:
    parameters:
      - env_name
      - target_url
    commands:
      - shell: "echo 'Deploying to ${env_name}'"
      - shell: "curl -X POST ${target_url}/deploy"

# Use templates in workflow
steps:
  - template: rust_test
  - template: deploy_to_env
    with:
      env_name: "production"
      target_url: "${API_URL}"
</code></pre>
<p><strong>Source</strong>: Composition architecture from features.json:workflow_composition, implementation status note from drift analysis</p>
<p><strong>Planned Composition Features:</strong></p>
<ul>
<li><strong>Imports</strong>: Reuse workflow fragments across projects</li>
<li><strong>Extends</strong>: Inherit from base workflows with overrides</li>
<li><strong>Templates</strong>: Parameterized command sets for DRY workflows</li>
<li><strong>Parameters</strong>: Type-safe template parameterization</li>
</ul>
<p><strong>Current Status:</strong></p>
<ul>
<li>Core composition logic: ✓ Implemented</li>
<li>Configuration parsing: ✓ Implemented</li>
<li>CLI integration: ⏳ Pending (Spec 131-133)</li>
<li>Template rendering: ⏳ Pending</li>
</ul>
<p><strong>Workaround Until CLI Integration:</strong>
Use YAML anchors and aliases for basic composition:</p>
<pre><code class="language-yaml"># Define reusable blocks with anchors
.rust_test: &amp;rust_test
  - shell: "cargo build"
  - shell: "cargo test"

.deploy: &amp;deploy
  - shell: "echo 'Deploying...'"

# Reference with aliases
workflow:
  - *rust_test
  - *deploy
</code></pre>
<hr />
<h2 id="example-14-custom-merge-workflows"><a class="header" href="#example-14-custom-merge-workflows">Example 14: Custom Merge Workflows</a></h2>
<p>MapReduce workflows execute in isolated git worktrees. When the workflow completes, you can define a custom merge workflow to control how changes are merged back to your original branch.</p>
<pre><code class="language-yaml">name: code-review-with-merge
mode: mapreduce

# Environment variables available to merge commands
env:
  PROJECT_NAME: "my-project"
  NOTIFICATION_URL: "https://api.slack.com/webhooks/..."

setup:
  - shell: "find src -name '*.rs' &gt; files.json"
  - shell: "jq -R -s -c 'split(\"\n\") | map(select(length &gt; 0) | {path: .})' files.json &gt; items.json"

map:
  input: "items.json"
  json_path: "$[*]"
  agent_template:
    - claude: "/review-code ${item.path}"
      commit_required: true
  max_parallel: 5

reduce:
  - claude: "/summarize-reviews ${map.results}"

# Custom merge workflow (executed when merging worktree back to original branch)
merge:
  commands:
    # Merge-specific variables are available:
    # ${merge.worktree} - Worktree name (e.g., "session-abc123")
    # ${merge.source_branch} - Source branch in worktree
    # ${merge.target_branch} - Target branch (where you started workflow)
    # ${merge.session_id} - Session ID for correlation

    # Pre-merge validation
    - shell: "echo 'Preparing to merge ${merge.worktree}'"
    - shell: "echo 'Source: ${merge.source_branch} → Target: ${merge.target_branch}'"

    # Run tests before merging
    - shell: "cargo test --all"
      on_failure:
        claude: "/fix-failing-tests before merge"
        commit_required: true
        max_attempts: 2

    # Run linting
    - shell: "cargo clippy -- -D warnings"
      on_failure:
        claude: "/fix-clippy-warnings"
        commit_required: true

    # Optional: Custom validation via Claude
    - claude: "/validate-merge-readiness ${merge.source_branch} ${merge.target_branch}"

    # Actually perform the merge using prodigy-merge-worktree
    # IMPORTANT: Always pass both source and target branches
    - claude: "/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}"

    # Post-merge notifications (using env vars)
    - shell: "echo 'Successfully merged ${PROJECT_NAME} changes from ${merge.worktree}'"
    # - shell: "curl -X POST ${NOTIFICATION_URL} -d 'Merge completed for ${PROJECT_NAME}'"

  # Optional: Timeout for entire merge phase (seconds)
  timeout: 600  # 10 minutes
</code></pre>
<p><strong>Source</strong>: Merge workflow configuration from src/config/mapreduce.rs:84-94, merge variables from worktree merge orchestrator, example from workflows/mapreduce-env-example.yml:83-94, test from tests/merge_workflow_integration.rs:64-121</p>
<p><strong>Merge Workflow Features:</strong></p>
<ol>
<li>
<p><strong>Merge-Specific Variables</strong> (automatically provided):</p>
<ul>
<li><code>${merge.worktree}</code> - Name of the worktree being merged</li>
<li><code>${merge.source_branch}</code> - Branch in worktree (usually <code>prodigy-mapreduce-...</code>)</li>
<li><code>${merge.target_branch}</code> - Your original branch (main, master, feature-xyz, etc.)</li>
<li><code>${merge.session_id}</code> - Session ID for tracking</li>
</ul>
</li>
<li>
<p><strong>Pre-Merge Validation</strong>:</p>
<ul>
<li>Run tests, linting, or custom checks before merging</li>
<li>Use Claude commands for intelligent validation</li>
<li>Use <code>on_failure</code> handlers to fix issues automatically</li>
</ul>
</li>
<li>
<p><strong>Environment Variables</strong>:</p>
<ul>
<li>Global <code>env</code> variables are available in merge commands</li>
<li>Useful for notifications, project-specific settings</li>
<li>Secrets are masked in merge command output</li>
</ul>
</li>
<li>
<p><strong>Timeout Control</strong>:</p>
<ul>
<li>Optional <code>timeout</code> field (in seconds) for the merge phase</li>
<li>Prevents merge workflows from hanging indefinitely</li>
</ul>
</li>
</ol>
<p><strong>Important Notes:</strong></p>
<ul>
<li>Always pass <strong>both</strong> <code>${merge.source_branch}</code> and <code>${merge.target_branch}</code> to <code>/prodigy-merge-worktree</code></li>
<li>This ensures the merge targets your original branch, not a hardcoded main/master</li>
<li>Without a custom merge workflow, you’ll be prompted interactively to merge</li>
</ul>
<p><strong>Simplified Format:</strong>
If you don’t need timeout configuration, you can use the simplified format:</p>
<pre><code class="language-yaml">merge:
  - shell: "cargo test"
  - claude: "/prodigy-merge-worktree ${merge.source_branch} ${merge.target_branch}"
</code></pre>
<p>This is equivalent to <code>merge.commands</code> but more concise.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="troubleshooting-22"><a class="header" href="#troubleshooting-22">Troubleshooting</a></h1>
<p>This chapter provides comprehensive guidance for diagnosing and resolving common issues with Prodigy workflows. Whether you’re experiencing MapReduce failures, checkpoint issues, or variable interpolation problems, you’ll find practical solutions here.</p>
<h2 id="common-issues"><a class="header" href="#common-issues">Common Issues</a></h2>
<h3 id="variables-not-interpolating-1"><a class="header" href="#variables-not-interpolating-1">Variables not interpolating</a></h3>
<p><strong>Symptoms:</strong> Literal <code>${var}</code> appears in output instead of value</p>
<p><strong>Causes:</strong></p>
<ul>
<li>Variable name typo or case mismatch</li>
<li>Variable not in scope</li>
<li>Incorrect syntax</li>
<li>Variable not captured</li>
</ul>
<p><strong>Solutions:</strong></p>
<ul>
<li>Check variable name spelling and case sensitivity</li>
<li>Verify variable is available in current scope (step vs workflow)</li>
<li>Ensure proper syntax: <code>${var}</code> not <code>$var</code> for complex expressions</li>
<li>Verify capture_output command succeeded</li>
<li>Check variable was set before use (e.g., in previous step)</li>
</ul>
<h3 id="mapreduce-items-not-found-1"><a class="header" href="#mapreduce-items-not-found-1">MapReduce items not found</a></h3>
<p><strong>Symptoms:</strong> No items to process, empty JSONPath result, or “items.json not found”</p>
<p><strong>Causes:</strong></p>
<ul>
<li>Input file doesn’t exist</li>
<li>Incorrect JSONPath</li>
<li>Setup phase failed</li>
<li>Wrong file format</li>
</ul>
<p><strong>Solutions:</strong></p>
<ul>
<li>Verify input file exists with correct path</li>
<li>Test JSONPath expression with jsonpath-cli or jq</li>
<li>Check json_path field syntax (default: <code>$[*]</code>)</li>
<li>Ensure setup phase generated the input file successfully</li>
<li>Validate JSON format with jq or json validator</li>
</ul>
<h3 id="timeout-errors"><a class="header" href="#timeout-errors">Timeout errors</a></h3>
<p><strong>Symptoms:</strong> Commands or phases timing out before completion</p>
<p><strong>Causes:</strong></p>
<ul>
<li>Operation too slow</li>
<li>Insufficient timeout</li>
<li>Hung processes</li>
<li>Deadlock</li>
</ul>
<p><strong>Solutions:</strong></p>
<ul>
<li>Increase timeout value for long operations</li>
<li>Optimize command execution for better performance</li>
<li>Split work into smaller chunks (use max_items, offset)</li>
<li>Check for hung processes with ps or top</li>
<li>Look for deadlocks in concurrent operations</li>
<li>Use agent_timeout_secs for MapReduce agents</li>
</ul>
<h3 id="checkpoint-resume-not-working"><a class="header" href="#checkpoint-resume-not-working">Checkpoint resume not working</a></h3>
<p><strong>Symptoms:</strong> Resume starts from beginning, fails to load state, or “checkpoint not found”</p>
<p><strong>Causes:</strong></p>
<ul>
<li>Checkpoint files missing</li>
<li>Wrong session/job ID</li>
<li>Workflow changed</li>
<li>Concurrent resume</li>
</ul>
<p><strong>Solutions:</strong></p>
<ul>
<li>Verify checkpoint files exist in <code>~/.prodigy/state/{repo}/mapreduce/jobs/{job_id}/</code></li>
<li>Check session/job ID is correct with <code>prodigy sessions list</code></li>
<li>Ensure workflow file hasn’t changed significantly</li>
<li>Check for concurrent resume lock in <code>~/.prodigy/resume_locks/</code></li>
<li>Review checkpoint file contents for corruption</li>
</ul>
<p>See <a href="troubleshooting/../mapreduce/checkpoint-and-resume.html">MapReduce Checkpoint and Resume</a> for detailed information.</p>
<h3 id="dlq-items-not-retrying-or-re-failing"><a class="header" href="#dlq-items-not-retrying-or-re-failing">DLQ items not retrying or re-failing</a></h3>
<p><strong>Symptoms:</strong> Retry command fails, items immediately fail again, or no progress</p>
<p><strong>Causes:</strong></p>
<ul>
<li>Systematic error not transient</li>
<li>DLQ file corrupted</li>
<li>Underlying issue not fixed</li>
</ul>
<p><strong>Solutions:</strong></p>
<ul>
<li>Check DLQ file format and contents with <code>prodigy dlq show &lt;job_id&gt;</code></li>
<li>Verify error was transient not systematic (e.g., rate limit vs bug)</li>
<li>Fix underlying issue before retry (e.g., API credentials, file permissions)</li>
<li>Increase max-parallel for retry if parallelism helps</li>
<li>Check json_log_location in DLQ for detailed error info</li>
</ul>
<p>See <a href="troubleshooting/../mapreduce/dead-letter-queue-dlq.html">Dead Letter Queue (DLQ)</a> for complete DLQ management details.</p>
<h3 id="worktree-cleanup-failures"><a class="header" href="#worktree-cleanup-failures">Worktree cleanup failures</a></h3>
<p><strong>Symptoms:</strong> Orphaned worktrees after failures, “permission denied” on cleanup</p>
<p><strong>Causes:</strong></p>
<ul>
<li>Locked files</li>
<li>Running processes</li>
<li>Permission issues</li>
<li>Disk full</li>
</ul>
<p><strong>Solutions:</strong></p>
<ul>
<li>Use <code>prodigy worktree clean-orphaned &lt;job_id&gt;</code> for automatic cleanup</li>
<li>Check for locked files with lsof or similar tools</li>
<li>Verify no running processes using worktree with ps</li>
<li>Check disk space with <code>df -h</code></li>
<li>Verify file permissions on worktree directory</li>
<li>Manual cleanup if necessary: <code>rm -rf ~/.prodigy/worktrees/&lt;path&gt;</code></li>
</ul>
<p>For more on cleanup failures, see “Cleanup Failure Handling (Spec 136)” in the CLAUDE.md file.</p>
<h3 id="environment-variables-not-resolved"><a class="header" href="#environment-variables-not-resolved">Environment variables not resolved</a></h3>
<p><strong>Symptoms:</strong> Literal <code>${VAR}</code> or <code>$VAR</code> appears in commands instead of value</p>
<p><strong>Causes:</strong></p>
<ul>
<li>Variable not defined</li>
<li>Wrong profile</li>
<li>Scope issue</li>
<li>Syntax error</li>
</ul>
<p><strong>Solutions:</strong></p>
<ul>
<li>Check variable defined in env, secrets, or profiles section</li>
<li>Verify correct profile activated with –profile flag</li>
<li>Use proper syntax: <code>${VAR}</code> for workflow vars, <code>$VAR</code> may work for shell</li>
<li>Check variable scope (global vs step-level)</li>
<li>Ensure env_files loaded correctly</li>
</ul>
<p>See <a href="troubleshooting/../environment/index.html">Environment Variables</a> for variable configuration details.</p>
<h3 id="git-context-variables-empty"><a class="header" href="#git-context-variables-empty">Git context variables empty</a></h3>
<p><strong>Symptoms:</strong> <code>${step.files_added}</code> returns empty string or undefined</p>
<p><strong>Causes:</strong></p>
<ul>
<li>No commits created</li>
<li>Git repo not initialized</li>
<li>Step not completed</li>
<li>Wrong format</li>
</ul>
<p><strong>Solutions:</strong></p>
<ul>
<li>Ensure commands created commits (use <code>commit_required: true</code>)</li>
<li>Check git repository is initialized in working directory</li>
<li>Verify step completed before accessing variables</li>
<li>Use appropriate format modifier (e.g., :json, :newline)</li>
<li>Check git status to verify changes exist</li>
</ul>
<p>See <a href="troubleshooting/../git-context-advanced.html">Advanced Git Context</a> for available git variables.</p>
<h3 id="foreach-iteration-failures"><a class="header" href="#foreach-iteration-failures">Foreach iteration failures</a></h3>
<p><strong>Symptoms:</strong> Foreach command fails partway through, items skipped, or parallel execution errors</p>
<p><strong>Causes:</strong></p>
<ul>
<li>Command failure with continue_on_error disabled</li>
<li>Parallel execution resource exhaustion</li>
<li>Variable interpolation errors in item context</li>
<li>Max items limit reached unexpectedly</li>
</ul>
<p><strong>Solutions:</strong></p>
<ul>
<li>Enable continue_on_error to process remaining items on failure</li>
<li>Reduce parallelism: <code>parallel: 5</code> instead of <code>parallel: true</code></li>
<li>Verify ${item}, ${index}, ${total} variable interpolation</li>
<li>Check max_items setting matches expectations</li>
<li>Review progress bar output for failure patterns</li>
<li>Use shell command for debugging: <code>shell: "echo Processing ${item}"</code></li>
</ul>
<p>Example foreach with error handling:</p>
<pre><code class="language-yaml">foreach:
  input:
    list: ["file1.py", "file2.py", "file3.py"]
  parallel: 5
  max_items: 10
  continue_on_error: true
  commands:
    - shell: "echo Processing ${item} (${index}/${total})"
    - claude: "/refactor ${item}"
</code></pre>
<p><strong>Source</strong>: src/cook/execution/foreach.rs:44-515</p>
<h3 id="claude-command-fails-with-command-not-found"><a class="header" href="#claude-command-fails-with-command-not-found">Claude command fails with “command not found”</a></h3>
<p><strong>Symptoms:</strong> Shell error about claude command not existing</p>
<p><strong>Causes:</strong></p>
<ul>
<li>Claude Code not installed</li>
<li>Not in PATH</li>
<li>Wrong executable name</li>
</ul>
<p><strong>Solutions:</strong></p>
<ul>
<li>Install Claude Code CLI if not present</li>
<li>Verify claude is in PATH with <code>which claude</code></li>
<li>Check command name matches Claude Code CLI (not “claude-code”)</li>
<li>Use full path if necessary: <code>/path/to/claude</code></li>
</ul>
<h2 id="debug-tips"><a class="header" href="#debug-tips">Debug Tips</a></h2>
<h3 id="use-dry-run-mode-to-preview-execution"><a class="header" href="#use-dry-run-mode-to-preview-execution">Use dry-run mode to preview execution</a></h3>
<pre><code class="language-bash">prodigy run workflow.yml --dry-run
</code></pre>
<p><strong>Shows:</strong> Preview of commands that would be executed without actually running them</p>
<p><strong>Use when:</strong> Verifying workflow steps before execution, testing variable interpolation, checking command syntax</p>
<p><strong>Source</strong>: src/cli/args.rs:64</p>
<h3 id="use-verbose-mode-for-execution-details"><a class="header" href="#use-verbose-mode-for-execution-details">Use verbose mode for execution details</a></h3>
<pre><code class="language-bash">prodigy run workflow.yml -v
</code></pre>
<p><strong>Shows:</strong> Claude streaming output, tool invocations, and execution timeline</p>
<p><strong>Use when:</strong> Understanding what Claude is doing, debugging tool calls</p>
<h3 id="check-claude-json-logs-for-full-interaction"><a class="header" href="#check-claude-json-logs-for-full-interaction">Check Claude JSON logs for full interaction</a></h3>
<pre><code class="language-bash">prodigy logs --latest --summary
</code></pre>
<p><strong>Shows:</strong> Full Claude interaction including messages, tools, token usage, errors</p>
<p><strong>Use when:</strong> Claude command failed, understanding why Claude made certain decisions</p>
<p>For more on Claude JSON logs, see the “Viewing Claude Execution Logs (Spec 126)” section in the project CLAUDE.md file.</p>
<h3 id="inspect-event-logs-for-execution-timeline"><a class="header" href="#inspect-event-logs-for-execution-timeline">Inspect event logs for execution timeline</a></h3>
<pre><code class="language-bash"># List events for a job
prodigy events ls --job-id &lt;job_id&gt;

# Follow events in real-time
prodigy events follow --job-id &lt;job_id&gt;

# Show event statistics
prodigy events stats
</code></pre>
<p><strong>Shows:</strong> Detailed execution timeline, agent starts/completions, durations, real-time event stream</p>
<p><strong>Use when:</strong> Understanding workflow execution flow, finding bottlenecks, monitoring active jobs</p>
<p><strong>Source</strong>: src/cli/commands/events.rs:22-98</p>
<h3 id="review-dlq-for-failed-item-details"><a class="header" href="#review-dlq-for-failed-item-details">Review DLQ for failed item details</a></h3>
<pre><code class="language-bash">prodigy dlq show &lt;job_id&gt;
</code></pre>
<p><strong>Shows:</strong> Failed items with full error details, retry history, json_log_location</p>
<p><strong>Use when:</strong> MapReduce items failing, understanding failure patterns</p>
<h3 id="check-checkpoint-state-for-resume-issues"><a class="header" href="#check-checkpoint-state-for-resume-issues">Check checkpoint state for resume issues</a></h3>
<p><strong>Location:</strong> <code>~/.prodigy/state/{repo}/mapreduce/jobs/{job_id}/</code></p>
<p><strong>Shows:</strong> Saved execution state, completed items, variables, phase progress</p>
<p><strong>Use when:</strong> Resume not working, understanding saved state</p>
<h3 id="examine-worktree-git-log-for-commits"><a class="header" href="#examine-worktree-git-log-for-commits">Examine worktree git log for commits</a></h3>
<pre><code class="language-bash">cd ~/.prodigy/worktrees/{repo}/{session}/ &amp;&amp; git log
</code></pre>
<p><strong>Shows:</strong> All commits created during workflow execution with full details</p>
<p><strong>Use when:</strong> Understanding what changed, verifying commits created</p>
<h3 id="tail-claude-json-log-in-real-time"><a class="header" href="#tail-claude-json-log-in-real-time">Tail Claude JSON log in real-time</a></h3>
<pre><code class="language-bash">prodigy logs --latest --tail
</code></pre>
<p><strong>Shows:</strong> Live streaming of Claude JSON log as it’s being written</p>
<p><strong>Use when:</strong> Watching long-running Claude command, debugging in real-time</p>
<h2 id="additional-topics-9"><a class="header" href="#additional-topics-9">Additional Topics</a></h2>
<p>For more specific troubleshooting guidance, see:</p>
<ul>
<li><a href="troubleshooting/faq.html">FAQ</a> - Frequently asked questions</li>
<li><a href="troubleshooting/common-error-messages.html">Common Error Messages</a> - Specific error messages explained</li>
<li><a href="troubleshooting/best-practices-for-debugging.html">Best Practices for Debugging</a> - Proven debugging strategies</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="faq-1"><a class="header" href="#faq-1">FAQ</a></h2>
<h3 id="how-do-i-debug-variable-interpolation-issues"><a class="header" href="#how-do-i-debug-variable-interpolation-issues">How do I debug variable interpolation issues?</a></h3>
<p>When <code>${var}</code> appears literally in output instead of being replaced:</p>
<ol>
<li><strong>Check spelling and case</strong>: Variable names are case-sensitive</li>
<li><strong>Verify scope</strong>: Ensure the variable is available (step vs workflow level)</li>
<li><strong>Use verbose mode</strong>: Run with <code>-v</code> to see variable interpolation in real-time</li>
<li><strong>Verify capture</strong>: If using <code>capture_output</code>, ensure the command succeeded</li>
<li><strong>Check syntax</strong>: Use <code>${var}</code> for workflow variables, not just <code>$var</code></li>
</ol>
<p>Example debugging:</p>
<pre><code class="language-bash"># Add echo to verify variable value
- shell: "echo Variable value: ${my_var}"
</code></pre>
<h3 id="what-should-i-do-when-checkpoint-resume-fails"><a class="header" href="#what-should-i-do-when-checkpoint-resume-fails">What should I do when checkpoint resume fails?</a></h3>
<p>If resume starts from the beginning or shows “checkpoint not found”:</p>
<ol>
<li><strong>Verify checkpoint exists</strong>: Check <code>~/.prodigy/state/{repo}/mapreduce/jobs/{job_id}/</code></li>
<li><strong>Confirm job ID</strong>: Use <code>prodigy sessions list</code> to find the correct ID</li>
<li><strong>Check for concurrent resume</strong>: Look for lock files in <code>~/.prodigy/resume_locks/</code></li>
<li><strong>Review checkpoint integrity</strong>: Read the checkpoint JSON to ensure it’s valid</li>
<li><strong>Ensure workflow unchanged</strong>: Significant workflow changes may prevent resume</li>
</ol>
<p>See <a href="troubleshooting/../mapreduce/checkpoint-and-resume.html">MapReduce Checkpoint and Resume</a> for complete details.</p>
<h3 id="how-do-i-retry-failed-dlq-items"><a class="header" href="#how-do-i-retry-failed-dlq-items">How do I retry failed DLQ items?</a></h3>
<p>To retry items that failed during MapReduce execution:</p>
<pre><code class="language-bash"># View failed items
prodigy dlq list --job-id &lt;job_id&gt;

# Retry all failed items
prodigy dlq retry &lt;job_id&gt;

# Retry with custom parallelism
prodigy dlq retry &lt;job_id&gt; --parallel 10

# Dry run to preview retry
prodigy dlq retry &lt;job_id&gt; --dry-run
</code></pre>
<p><strong>Important</strong>: Before retrying, ensure the underlying issue is fixed. If the error is systematic (not transient), items will fail again.</p>
<p>Check <code>json_log_location</code> in DLQ entries to debug the original failure.</p>
<h3 id="why-are-my-mapreduce-items-not-being-found"><a class="header" href="#why-are-my-mapreduce-items-not-being-found">Why are my MapReduce items not being found?</a></h3>
<p>If you see “No items to process” or “items.json not found”:</p>
<ol>
<li><strong>Verify input file exists</strong>: Check the path specified in <code>input:</code></li>
<li><strong>Confirm setup succeeded</strong>: Ensure setup phase created the input file</li>
<li><strong>Test JSONPath</strong>: Use <code>jq</code> to test your <code>json_path</code> expression</li>
<li><strong>Validate JSON format</strong>: Ensure the file is valid JSON with <code>jq .</code></li>
<li><strong>Check file location</strong>: Input file path is relative to workflow directory</li>
</ol>
<p>Example JSONPath test:</p>
<pre><code class="language-bash">cat items.json | jq '$.items[*]'
</code></pre>
<h3 id="how-do-i-view-claude-execution-logs"><a class="header" href="#how-do-i-view-claude-execution-logs">How do I view Claude execution logs?</a></h3>
<p>To see detailed logs of what Claude did during a command:</p>
<pre><code class="language-bash"># View most recent log
prodigy logs --latest

# View with summary
prodigy logs --latest --summary

# Tail log in real-time
prodigy logs --latest --tail

# Direct access to logs
cat ~/.local/state/claude/logs/session-*.json
</code></pre>
<p>Logs contain:</p>
<ul>
<li>Complete message history</li>
<li>All tool invocations with parameters</li>
<li>Token usage statistics</li>
<li>Error details and stack traces</li>
</ul>
<p>For detailed log analysis techniques, see “Viewing Claude Execution Logs (Spec 126)” in the project CLAUDE.md file.</p>
<h3 id="what-does-command-not-found-claude-mean"><a class="header" href="#what-does-command-not-found-claude-mean">What does “command not found: claude” mean?</a></h3>
<p>This error indicates Claude Code CLI is not installed or not in your PATH:</p>
<ol>
<li><strong>Verify installation</strong>: Check if Claude Code is installed</li>
<li><strong>Check PATH</strong>: Run <code>which claude</code> to see if it’s accessible</li>
<li><strong>Use full path</strong>: Specify <code>/path/to/claude</code> in workflow if needed</li>
<li><strong>Verify executable name</strong>: Should be <code>claude</code>, not <code>claude-code</code></li>
</ol>
<p>Installation varies by platform - refer to Claude Code documentation.</p>
<h3 id="how-do-i-clean-up-orphaned-worktrees"><a class="header" href="#how-do-i-clean-up-orphaned-worktrees">How do I clean up orphaned worktrees?</a></h3>
<p>When worktree cleanup fails during MapReduce execution:</p>
<pre><code class="language-bash"># Clean orphaned worktrees for a job
prodigy worktree clean-orphaned &lt;job_id&gt;

# Dry run to preview cleanup
prodigy worktree clean-orphaned &lt;job_id&gt; --dry-run

# Force cleanup without confirmation
prodigy worktree clean-orphaned &lt;job_id&gt; --force
</code></pre>
<p>Common causes of cleanup failures:</p>
<ul>
<li>Locked files (check with <code>lsof</code>)</li>
<li>Running processes (check with <code>ps</code>)</li>
<li>Permission issues (verify with <code>ls -ld</code>)</li>
<li>Insufficient disk space (check with <code>df -h</code>)</li>
</ul>
<p>For details on cleanup failures, see “Cleanup Failure Handling (Spec 136)” in the CLAUDE.md file.</p>
<h3 id="why-are-environment-variables-not-being-resolved"><a class="header" href="#why-are-environment-variables-not-being-resolved">Why are environment variables not being resolved?</a></h3>
<p>If <code>${VAR}</code> or <code>$VAR</code> appears literally in commands:</p>
<ol>
<li><strong>Check definition</strong>: Ensure variable is defined in <code>env:</code> section</li>
<li><strong>Verify profile</strong>: Use <code>--profile</code> flag if using profile-specific values</li>
<li><strong>Check scope</strong>: Confirm variable is global or in correct scope</li>
<li><strong>Use correct syntax</strong>: <code>${VAR}</code> for workflow vars, <code>$VAR</code> for shell vars</li>
<li><strong>Validate env_files</strong>: Ensure external env files are loaded correctly</li>
</ol>
<p>Example:</p>
<pre><code class="language-yaml">env:
  API_KEY: "my-key"
  DATABASE_URL:
    default: "localhost"
    prod: "prod-server"
</code></pre>
<p>See <a href="troubleshooting/../environment/index.html">Environment Variables</a> for configuration details.</p>
<h3 id="how-do-i-debug-timeout-errors"><a class="header" href="#how-do-i-debug-timeout-errors">How do I debug timeout errors?</a></h3>
<p>When commands or phases time out:</p>
<ol>
<li><strong>Increase timeout</strong>: Adjust timeout values for long operations</li>
<li><strong>Check for hung processes</strong>: Use <code>ps</code> or <code>top</code> to find stuck processes</li>
<li><strong>Optimize performance</strong>: Split work into smaller chunks</li>
<li><strong>Use agent_timeout_secs</strong>: Set per-agent timeout for MapReduce</li>
<li><strong>Look for deadlocks</strong>: Check for concurrent operations blocking each other</li>
</ol>
<p>MapReduce-specific timeout configuration:</p>
<pre><code class="language-yaml">map:
  agent_timeout_secs: 600  # 10 minutes per agent
  max_items: 10  # Process fewer items per run
</code></pre>
<h3 id="where-are-event-logs-stored"><a class="header" href="#where-are-event-logs-stored">Where are event logs stored?</a></h3>
<p>Event logs use a global storage architecture:</p>
<p><strong>Location</strong>: <code>~/.prodigy/events/{repo_name}/{job_id}/</code></p>
<p><strong>What’s stored</strong>:</p>
<ul>
<li>Agent lifecycle events (started, completed, failed)</li>
<li>Work item processing status</li>
<li>Checkpoint saves</li>
<li>Error details with correlation IDs</li>
</ul>
<p><strong>How to view</strong>:</p>
<pre><code class="language-bash"># List events for a job
prodigy events ls --job-id &lt;job_id&gt;

# Show event statistics
prodigy events stats

# View detailed event timeline
cat ~/.prodigy/events/{repo_name}/{job_id}/events-*.jsonl

# For real-time monitoring, tail the event file:
tail -f ~/.prodigy/events/{repo_name}/{job_id}/events-*.jsonl
</code></pre>
<p><strong>Source</strong>: src/cli/commands/events.rs:22-98</p>
<p>Events are shared across worktrees, enabling centralized monitoring of parallel jobs.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="common-error-messages-1"><a class="header" href="#common-error-messages-1">Common Error Messages</a></h2>
<p>This section documents specific error messages you may encounter while using Prodigy, along with their meanings and solutions.</p>
<h3 id="quick-reference-3"><a class="header" href="#quick-reference-3">Quick Reference</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Error Message</th><th>Category</th><th>Common Cause</th></tr></thead><tbody>
<tr><td><a href="troubleshooting/common-error-messages.html#checkpoint-not-found">checkpoint not found</a></td><td>Resume</td><td>Missing checkpoint files</td></tr>
<tr><td><a href="troubleshooting/common-error-messages.html#itemsjson-not-found">items.json not found</a></td><td>MapReduce</td><td>Setup phase failed</td></tr>
<tr><td><a href="troubleshooting/common-error-messages.html#command-not-found-claude">command not found: claude</a></td><td>Environment</td><td>Claude Code not in PATH</td></tr>
<tr><td><a href="troubleshooting/common-error-messages.html#permission-denied">permission denied</a></td><td>File System</td><td>Insufficient permissions</td></tr>
<tr><td><a href="troubleshooting/common-error-messages.html#timeout-exceeded">timeout exceeded</a></td><td>Execution</td><td>Slow operation</td></tr>
<tr><td><a href="troubleshooting/common-error-messages.html#resume-already-in-progress">Resume already in progress</a></td><td>Concurrency</td><td>Lock conflict</td></tr>
<tr><td><a href="troubleshooting/common-error-messages.html#jsonpath-returned-no-results">JSONPath returned no results</a></td><td>MapReduce</td><td>Invalid JSONPath</td></tr>
<tr><td><a href="troubleshooting/common-error-messages.html#no-commits-found">No commits found</a></td><td>Git</td><td>No file changes</td></tr>
<tr><td><a href="troubleshooting/common-error-messages.html#variable-not-found-var">Variable not found</a></td><td>Variables</td><td>Undefined variable</td></tr>
<tr><td><a href="troubleshooting/common-error-messages.html#invalid-profile-name">Invalid profile</a></td><td>Configuration</td><td>Profile not defined</td></tr>
<tr><td><a href="troubleshooting/common-error-messages.html#disk-quota-exceeded">Disk quota exceeded</a></td><td>File System</td><td>Out of space</td></tr>
<tr><td><a href="troubleshooting/common-error-messages.html#job-already-completed">Job already completed</a></td><td>Resume</td><td>Job finished</td></tr>
<tr><td><a href="troubleshooting/common-error-messages.html#template-not-found">Template not found</a></td><td>Composition</td><td>Template missing</td></tr>
<tr><td><a href="troubleshooting/common-error-messages.html#required-parameter-not-provided">Required parameter not provided</a></td><td>Composition</td><td>Missing parameter</td></tr>
<tr><td><a href="troubleshooting/common-error-messages.html#circular-dependency-detected">Circular dependency detected</a></td><td>Composition</td><td>Circular imports</td></tr>
<tr><td><a href="troubleshooting/common-error-messages.html#concurrent-modification-detected">Concurrent modification detected</a></td><td>Concurrency</td><td>Race condition</td></tr>
</tbody></table>
</div>
<h3 id="checkpoint-not-found"><a class="header" href="#checkpoint-not-found">“checkpoint not found”</a></h3>
<p><strong>Full message</strong>: <code>Error: Checkpoint not found for session/job {id}</code></p>
<p><strong>What it means</strong>: Prodigy cannot locate checkpoint files needed to resume execution.</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Session or job ID is incorrect</li>
<li>Checkpoint files were deleted or moved</li>
<li>Wrong repository context</li>
<li>Checkpoint never created (workflow didn’t reach checkpoint phase)</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li>Verify the correct ID: <code>prodigy sessions list</code> or <code>prodigy resume-job list</code></li>
<li>Check checkpoint directory: <code>~/.prodigy/state/{repo_name}/mapreduce/jobs/{job_id}/</code></li>
<li>Ensure you’re in the correct git repository</li>
<li>Start fresh if checkpoint is unrecoverable</li>
</ol>
<h3 id="itemsjson-not-found"><a class="header" href="#itemsjson-not-found">“items.json not found”</a></h3>
<p><strong>Full message</strong>: <code>Error: Input file not found: items.json</code></p>
<p><strong>What it means</strong>: The MapReduce input file specified in the workflow doesn’t exist.</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Setup phase failed to create the file</li>
<li>Wrong file path in workflow configuration</li>
<li>File created in wrong directory</li>
<li>File path is relative but CWD is incorrect</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li>Check setup phase output for errors</li>
<li>Verify <code>input:</code> path in workflow YAML</li>
<li>Ensure file path is correct (relative to workflow directory)</li>
<li>Run setup phase manually to debug file creation</li>
</ol>
<h3 id="command-not-found-claude"><a class="header" href="#command-not-found-claude">“command not found: claude”</a></h3>
<p><strong>Full message</strong>: <code>bash: claude: command not found</code></p>
<p><strong>What it means</strong>: The Claude Code CLI executable is not found in the system PATH.</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Claude Code not installed</li>
<li>Installation directory not in PATH</li>
<li>Wrong executable name in workflow</li>
<li>Shell environment not configured</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li>Install Claude Code if not present</li>
<li>Verify installation: <code>which claude</code></li>
<li>Add Claude Code to PATH if needed</li>
<li>Use full path in workflow: <code>/usr/local/bin/claude</code></li>
</ol>
<h3 id="permission-denied"><a class="header" href="#permission-denied">“permission denied”</a></h3>
<p><strong>Full message</strong>: <code>Error: Permission denied: {path}</code> or <code>rm: cannot remove '{path}': Permission denied</code></p>
<p><strong>What it means</strong>: Insufficient permissions to access or modify a file/directory.</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>File/directory owned by different user</li>
<li>Read-only filesystem</li>
<li>Locked files or directories</li>
<li>Insufficient user permissions</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li>Check file ownership: <code>ls -l {path}</code></li>
<li>Verify permissions: <code>ls -ld {directory}</code></li>
<li>Check for locked files: <code>lsof {path}</code></li>
<li>Run with appropriate permissions or fix ownership</li>
<li>For worktree cleanup: Use <code>prodigy worktree clean-orphaned</code></li>
</ol>
<h3 id="timeout-exceeded"><a class="header" href="#timeout-exceeded">“timeout exceeded”</a></h3>
<p><strong>Full message</strong>: <code>Error: Operation timed out after {n} seconds</code></p>
<p><strong>What it means</strong>: A command or phase took longer than the configured timeout.</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Operation genuinely slow</li>
<li>Hung process or deadlock</li>
<li>Insufficient timeout value</li>
<li>Resource exhaustion (CPU, memory, disk I/O)</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li>Increase timeout in workflow configuration</li>
<li>Check for hung processes: <code>ps aux | grep prodigy</code></li>
<li>Optimize command performance</li>
<li>Split work into smaller chunks (use <code>max_items</code>)</li>
<li>Check system resources: <code>top</code>, <code>df -h</code></li>
</ol>
<h3 id="resume-already-in-progress"><a class="header" href="#resume-already-in-progress">“Resume already in progress”</a></h3>
<p><strong>Full message</strong>: <code>Error: Resume already in progress for job {job_id}. Lock held by: PID {pid} on {hostname}</code></p>
<p><strong>What it means</strong>: Another process is currently resuming this job.</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Concurrent resume attempt</li>
<li>Stale lock from crashed process</li>
<li>Multiple terminals running resume</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li>Wait for other process to complete</li>
<li>Check if process is running: <code>ps aux | grep {pid}</code></li>
<li>Remove stale lock if process is dead: <code>rm ~/.prodigy/resume_locks/{job_id}.lock</code></li>
<li>Retry - stale locks are auto-detected and cleaned</li>
</ol>
<p>For details on concurrent resume protection, see “Concurrent Resume Protection (Spec 140)” in the CLAUDE.md file.</p>
<h3 id="jsonpath-returned-no-results"><a class="header" href="#jsonpath-returned-no-results">“JSONPath returned no results”</a></h3>
<p><strong>Full message</strong>: <code>Error: JSONPath expression '{path}' returned no results</code></p>
<p><strong>What it means</strong>: The JSONPath query didn’t match any items in the input file.</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Incorrect JSONPath syntax</li>
<li>Wrong data structure in input file</li>
<li>Empty input file</li>
<li>Case-sensitive key mismatch</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li>Test JSONPath with jq: <code>cat items.json | jq '{your_path}'</code></li>
<li>Verify input file structure: <code>cat items.json | jq .</code></li>
<li>Check for typos in key names</li>
<li>Ensure array brackets are correct: <code>$[*]</code> vs <code>$.items[*]</code></li>
<li>Validate JSON format: <code>jq . items.json</code></li>
</ol>
<h3 id="no-commits-found"><a class="header" href="#no-commits-found">“No commits found”</a></h3>
<p><strong>Full message</strong>: <code>Error: No commits found in worktree</code> or <code>${step.files_added} returned empty</code></p>
<p><strong>What it means</strong>: Git context variables are empty because no commits were created.</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Commands didn’t modify any files</li>
<li>Changes not committed</li>
<li>Wrong git repository context</li>
<li>Worktree not initialized properly</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li>Verify commands created changes: <code>git status</code></li>
<li>Use <code>commit_required: true</code> to enforce commits</li>
<li>Check git log: <code>git log -1</code></li>
<li>Ensure working in correct repository</li>
<li>Check if files were actually modified</li>
</ol>
<h3 id="variable-not-found-var"><a class="header" href="#variable-not-found-var">“Variable not found: {var}”</a></h3>
<p><strong>Full message</strong>: <code>Error: Variable not found: {var}</code> or literal <code>${var}</code> in output</p>
<p><strong>What it means</strong>: A workflow variable reference couldn’t be resolved.</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Variable name typo or case mismatch</li>
<li>Variable not defined in workflow</li>
<li>Variable out of scope</li>
<li>Syntax error in interpolation</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li>Check variable spelling and case</li>
<li>Verify variable defined in <code>env:</code> or previous step</li>
<li>Use <code>capture_output</code> to capture command results</li>
<li>Check scope (step vs workflow level)</li>
<li>Verify syntax: <code>${var}</code> not <code>$var</code></li>
</ol>
<h3 id="invalid-profile-name"><a class="header" href="#invalid-profile-name">“Invalid profile: {name}”</a></h3>
<p><strong>Full message</strong>: <code>Error: Invalid profile: {name}</code></p>
<p><strong>What it means</strong>: The specified profile doesn’t exist in the workflow configuration.</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Profile name typo</li>
<li>Profile not defined in workflow</li>
<li>Wrong flag syntax</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li>Check profile name spelling</li>
<li>Verify profile exists in workflow <code>env:</code> section</li>
<li>Use correct flag: <code>--profile prod</code></li>
<li>List available profiles in workflow YAML</li>
</ol>
<h3 id="disk-quota-exceeded"><a class="header" href="#disk-quota-exceeded">“Disk quota exceeded”</a></h3>
<p><strong>Full message</strong>: <code>Error: No space left on device</code> or <code>write: disk quota exceeded</code></p>
<p><strong>What it means</strong>: Insufficient disk space to complete operation.</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Disk full</li>
<li>Quota limit reached</li>
<li>Large log files accumulating</li>
<li>Orphaned worktrees consuming space</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li>Check disk space: <code>df -h</code></li>
<li>Clean orphaned worktrees: <code>prodigy worktree clean-orphaned</code></li>
<li>Remove old logs: <code>rm ~/.prodigy/events/old-job-*/</code></li>
<li>Clean Claude logs: <code>rm ~/.local/state/claude/logs/old-*.json</code></li>
<li>Increase disk space or quota</li>
</ol>
<h3 id="job-already-completed"><a class="header" href="#job-already-completed">“Job already completed”</a></h3>
<p><strong>Full message</strong>: <code>Error: Cannot resume job {job_id}: already completed</code></p>
<p><strong>What it means</strong>: Attempting to resume a job that finished successfully.</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Job actually completed</li>
<li>Wrong job ID</li>
<li>Attempting re-run instead of resume</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li>Verify job status: <code>prodigy sessions list</code></li>
<li>Check for correct job ID</li>
<li>Start new run instead of resume: <code>prodigy run workflow.yml</code></li>
<li>Review job results if completion was successful</li>
</ol>
<h3 id="template-not-found"><a class="header" href="#template-not-found">“Template not found”</a></h3>
<p><strong>Full message</strong>: <code>Error: Template '{name}' not found in registry or file system</code></p>
<p><strong>What it means</strong>: A workflow template referenced in composition cannot be located.</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Template name typo</li>
<li>Template not registered</li>
<li>Template file doesn’t exist</li>
<li>Wrong template path</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li>Check template spelling and case</li>
<li>List available templates: <code>prodigy template list</code></li>
<li>Register template if needed: <code>prodigy template register &lt;path&gt;</code></li>
<li>Verify template file exists at specified path</li>
<li>Check template registry: <code>~/.prodigy/templates/</code></li>
</ol>
<p><strong>Source</strong>: src/cook/workflow/composer_integration.rs:20, src/cook/workflow/composition/registry.rs:119</p>
<h3 id="required-parameter-not-provided"><a class="header" href="#required-parameter-not-provided">“Required parameter not provided”</a></h3>
<p><strong>Full message</strong>: <code>Error: Required parameter '{name}' not provided</code></p>
<p><strong>What it means</strong>: A template requires a parameter that wasn’t provided during workflow composition.</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Missing –param flag</li>
<li>Parameter not in param file</li>
<li>Template definition requires parameter</li>
<li>Parameter name mismatch</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li>Provide parameter: <code>prodigy run workflow.yml --param NAME=value</code></li>
<li>Use parameter file: <code>--param-file params.json</code></li>
<li>Check template parameter definitions</li>
<li>Verify parameter names match exactly (case-sensitive)</li>
</ol>
<p>Example parameter file:</p>
<pre><code class="language-json">{
  "project_name": "my-project",
  "version": "1.0.0"
}
</code></pre>
<p><strong>Source</strong>: src/cook/workflow/composer_integration.rs:23</p>
<h3 id="circular-dependency-detected"><a class="header" href="#circular-dependency-detected">“Circular dependency detected”</a></h3>
<p><strong>Full message</strong>: <code>Error: Circular dependency detected: {description}</code> or <code>Error: Circular dependency detected in workflow composition</code></p>
<p><strong>What it means</strong>: Workflow composition has a circular dependency where templates reference each other in a loop.</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Template A extends/imports Template B, which extends/imports Template A</li>
<li>Chain of dependencies forms a cycle</li>
<li>Sub-workflow references create circular imports</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li>Review template dependencies and break the cycle</li>
<li>Restructure templates to have clear dependency hierarchy</li>
<li>Extract common functionality into a shared template</li>
<li>Check extends and imports chains</li>
</ol>
<p>Example circular dependency:</p>
<pre><code class="language-yaml"># template-a.yml
extends: template-b

# template-b.yml
extends: template-a  # Circular!
</code></pre>
<p><strong>Source</strong>: src/cook/workflow/composition/composer.rs:265, 727, src/error/codes.rs:78</p>
<h3 id="concurrent-modification-detected"><a class="header" href="#concurrent-modification-detected">“Concurrent modification detected”</a></h3>
<p><strong>Full message</strong>: <code>Error: Concurrent modification detected in checkpoint file</code></p>
<p><strong>What it means</strong>: Multiple processes tried to modify the same checkpoint simultaneously.</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Parallel resume attempts</li>
<li>File system race condition</li>
<li>Stale file handle</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li>Ensure only one resume process runs at a time</li>
<li>Check for concurrent resume lock</li>
<li>Wait and retry</li>
<li>Use resume lock mechanism (automatic in newer versions)</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h2 id="best-practices-for-debugging"><a class="header" href="#best-practices-for-debugging">Best Practices for Debugging</a></h2>
<p>This guide provides practical techniques for diagnosing and resolving issues in Prodigy workflows.</p>
<h3 id="quick-debugging-checklist"><a class="header" href="#quick-debugging-checklist">Quick Debugging Checklist</a></h3>
<ol>
<li><strong>Start simple</strong>: Test commands individually before adding to workflow</li>
<li><strong>Use verbosity flags</strong>: <code>-v</code> for Claude interactions, <code>-vv</code> for debug logs, <code>-vvv</code> for trace</li>
<li><strong>Check recent logs</strong>: <code>prodigy logs --latest</code> for the last Claude execution</li>
<li><strong>Enable environment variables</strong> for detailed output (see below)</li>
<li><strong>Validate input data</strong> before running MapReduce workflows</li>
<li><strong>Test incrementally</strong>: Add commands one at a time and verify after each</li>
<li><strong>Version control</strong>: Commit working workflows before making changes</li>
</ol>
<h3 id="debugging-environment-variables-1"><a class="header" href="#debugging-environment-variables-1">Debugging Environment Variables</a></h3>
<p>Control debugging output and logging behavior with environment variables (Source: <code>src/config/mod.rs:116</code>, <code>src/cook/execution/claude.rs:429-435</code>):</p>
<h4 id="prodigy_log_level-1"><a class="header" href="#prodigy_log_level-1"><code>PRODIGY_LOG_LEVEL</code></a></h4>
<p><strong>Purpose</strong>: Control log verbosity
<strong>Values</strong>: <code>trace</code>, <code>debug</code>, <code>info</code>, <code>warn</code>, <code>error</code>
<strong>Default</strong>: <code>info</code></p>
<pre><code class="language-bash">export PRODIGY_LOG_LEVEL=debug
prodigy run workflow.yml
</code></pre>
<p>Use <code>debug</code> or <code>trace</code> to see internal state transitions, command execution details, and variable interpolation.</p>
<h4 id="prodigy_claude_console_output-1"><a class="header" href="#prodigy_claude_console_output-1"><code>PRODIGY_CLAUDE_CONSOLE_OUTPUT</code></a></h4>
<p><strong>Purpose</strong>: Force streaming output regardless of verbosity
<strong>Values</strong>: <code>true</code>, <code>false</code>
<strong>Default</strong>: Not set (respects verbosity level)</p>
<pre><code class="language-bash">export PRODIGY_CLAUDE_CONSOLE_OUTPUT=true
prodigy run workflow.yml  # Shows Claude output even without -v flag
</code></pre>
<p>Useful for debugging specific runs without changing command flags.</p>
<h4 id="prodigy_claude_streaming-1"><a class="header" href="#prodigy_claude_streaming-1"><code>PRODIGY_CLAUDE_STREAMING</code></a></h4>
<p><strong>Purpose</strong>: Control Claude JSON streaming mode
<strong>Values</strong>: <code>true</code>, <code>false</code>
<strong>Default</strong>: <code>true</code></p>
<pre><code class="language-bash">export PRODIGY_CLAUDE_STREAMING=false  # Disable JSON streaming
prodigy run workflow.yml
</code></pre>
<p>Streaming is enabled by default for auditability. Only disable in CI/CD environments with storage constraints.</p>
<p><strong>See also</strong>: <a href="troubleshooting/../configuration/environment-variables.html">Environment Variables</a> for complete reference.</p>
<h3 id="variable-interpolation-debugging"><a class="header" href="#variable-interpolation-debugging">Variable Interpolation Debugging</a></h3>
<p><strong>Techniques for diagnosing variable issues</strong>:</p>
<ol>
<li>
<p><strong>Use echo statements</strong> to verify variable values:</p>
<pre><code class="language-yaml">commands:
  - shell: "echo 'Debug: item=${item.name}, path=${item.path}'"
  - claude: "/process ${item.name}"
</code></pre>
</li>
<li>
<p><strong>Capture command outputs</strong> with <code>capture_output</code> for later use:</p>
<pre><code class="language-yaml">commands:
  - shell: "git rev-parse HEAD"
    capture_output: current_commit
  - shell: "echo 'Current commit: ${current_commit}'"
</code></pre>
</li>
<li>
<p><strong>Enable verbose mode</strong> (<code>-v</code>) to see variable interpolation in real-time</p>
</li>
<li>
<p><strong>Check variable scope</strong>: Distinguish step-level vs workflow-level variables</p>
</li>
</ol>
<p><strong>See also</strong>: <a href="troubleshooting/index.html#variable-interpolation-issues">Common Issues</a> for variable troubleshooting patterns.</p>
<h3 id="log-and-state-inspection"><a class="header" href="#log-and-state-inspection">Log and State Inspection</a></h3>
<p><strong>Event Logs</strong> (MapReduce execution tracking):</p>
<pre><code class="language-bash"># View event log for a specific job
ls ~/.prodigy/events/{repo_name}/{job_id}/

# Follow events in real-time
tail -f ~/.prodigy/events/{repo_name}/{job_id}/events-*.jsonl
</code></pre>
<p><strong>Unified Sessions</strong> (workflow state):</p>
<pre><code class="language-bash"># Check current session state
cat ~/.prodigy/sessions/{session-id}.json | jq .

# View session status
jq '.status, .metadata' ~/.prodigy/sessions/{session-id}.json
</code></pre>
<p><strong>Checkpoints</strong> (MapReduce resume state):</p>
<pre><code class="language-bash"># List available checkpoints
ls ~/.prodigy/state/{repo_name}/mapreduce/jobs/{job_id}/
</code></pre>
<p><strong>See also</strong>: <a href="troubleshooting/../mapreduce/event-tracking.html">Event Tracking</a> for event log structure.</p>
<h3 id="advanced-claude-log-analysis"><a class="header" href="#advanced-claude-log-analysis">Advanced Claude Log Analysis</a></h3>
<p>Prodigy creates detailed JSON logs for every Claude command execution. Use <code>jq</code> to analyze these logs:</p>
<p><strong>View most recent log</strong>:</p>
<pre><code class="language-bash">prodigy logs --latest
</code></pre>
<p><strong>Watch live execution</strong>:</p>
<pre><code class="language-bash">prodigy logs --latest --tail
</code></pre>
<p><strong>Extract specific information</strong> (Source: CLAUDE.md “Viewing Claude Execution Logs”):</p>
<pre><code class="language-bash"># Extract error messages
cat ~/.local/state/claude/logs/session-abc123.json | jq 'select(.type == "error")'

# View all tool invocations
cat ~/.local/state/claude/logs/session-abc123.json | jq '.messages[].content[] | select(.type == "tool_use")'

# Analyze token usage
cat ~/.local/state/claude/logs/session-abc123.json | jq '.usage'

# Filter by specific tool
cat ~/.local/state/claude/logs/session-abc123.json | jq '.messages[].content[] | select(.type == "tool_use" and .name == "Bash")'

# View assistant responses
cat ~/.local/state/claude/logs/session-abc123.json | jq '.messages[] | select(.role == "assistant")'
</code></pre>
<p><strong>JSONL format</strong> (newer logs use line-delimited JSON):</p>
<pre><code class="language-bash"># Count messages by type
cat ~/.claude/projects/.../6ded63ac.jsonl | jq -r '.type' | sort | uniq -c

# Extract tool uses
cat ~/.claude/projects/.../6ded63ac.jsonl | jq -c 'select(.type == "assistant") | .content[]? | select(.type == "tool_use")'

# View token usage
cat ~/.claude/projects/.../6ded63ac.jsonl | jq -c 'select(.usage)'
</code></pre>
<h3 id="mapreduce-debugging"><a class="header" href="#mapreduce-debugging">MapReduce Debugging</a></h3>
<p><strong>Dead Letter Queue (DLQ)</strong> for failed work items:</p>
<pre><code class="language-bash"># View failed items with error details
prodigy dlq show &lt;job_id&gt;

# Check Claude execution logs for failed items
prodigy dlq show &lt;job_id&gt; | jq '.items[].failure_history[].json_log_location'

# Retry failed items (with dry run first)
prodigy dlq retry &lt;job_id&gt; --dry-run
prodigy dlq retry &lt;job_id&gt;
</code></pre>
<p><strong>See also</strong>: <a href="troubleshooting/../mapreduce/dead-letter-queue-dlq.html">Dead Letter Queue</a> for DLQ management.</p>
<h3 id="checkpoint-inspection"><a class="header" href="#checkpoint-inspection">Checkpoint Inspection</a></h3>
<p>When resume fails, inspect checkpoint state to identify issues:</p>
<p><strong>Checkpoint file types</strong> (Source: CLAUDE.md “MapReduce Checkpoint and Resume”):</p>
<ul>
<li><code>setup-checkpoint.json</code>: Setup phase results and artifacts</li>
<li><code>map-checkpoint-{timestamp}.json</code>: Map phase progress and work item state</li>
<li><code>reduce-checkpoint-v1-{timestamp}.json</code>: Reduce phase step results and variables</li>
<li><code>job-state.json</code>: Overall job state and metadata</li>
</ul>
<p><strong>What to check</strong>:</p>
<pre><code class="language-bash"># Inspect map checkpoint
cat ~/.prodigy/state/{repo}/mapreduce/jobs/{job_id}/map-checkpoint-*.json | jq '
  {
    total: .work_items | length,
    completed: [.work_items[] | select(.state == "Completed")] | length,
    pending: [.work_items[] | select(.state == "Pending")] | length,
    in_progress: [.work_items[] | select(.state == "InProgress")] | length
  }
'

# Check for failed items with error details
cat ~/.prodigy/state/{repo}/mapreduce/jobs/{job_id}/map-checkpoint-*.json | \
  jq '.work_items[] | select(.state == "Failed") | {item_id, error}'

# Verify reduce checkpoint variables
cat ~/.prodigy/state/{repo}/mapreduce/jobs/{job_id}/reduce-checkpoint-*.json | jq '.variables'
</code></pre>
<p><strong>Signs of corruption or inconsistencies</strong>:</p>
<ul>
<li>Work items with <code>InProgress</code> state but no active agent</li>
<li>Mismatch between <code>completed_count</code> and actual completed items</li>
<li>Missing or null <code>variables</code> in reduce checkpoint</li>
<li>Duplicate <code>item_id</code> values in work items array</li>
</ul>
<p><strong>Recovery strategies</strong>:</p>
<ul>
<li><strong>Minor corruption</strong>: Manually edit checkpoint to fix state (move <code>InProgress</code> items back to <code>Pending</code>)</li>
<li><strong>Major corruption</strong>: Delete checkpoint and restart phase from beginning</li>
<li><strong>Variable issues</strong>: Restore missing variables from previous checkpoint or setup output</li>
</ul>
<p><strong>See also</strong>: <a href="troubleshooting/../mapreduce/checkpoint-and-resume.html">Checkpoint and Resume</a> for checkpoint structure.</p>
<h3 id="workflow-specific-debugging"><a class="header" href="#workflow-specific-debugging">Workflow-Specific Debugging</a></h3>
<p>Different workflow types have different debugging considerations:</p>
<p><strong>Standard Workflows</strong>:</p>
<ul>
<li>Verify variable interpolation with echo commands</li>
<li>Test steps individually before chaining</li>
<li>Check captured outputs are in correct scope</li>
</ul>
<p><strong>MapReduce Workflows</strong>:</p>
<ul>
<li>Verify agent isolation (check individual agent worktrees)</li>
<li>Debug parallel execution issues with event logs</li>
<li>Investigate merge conflicts in parent worktree after agent completion</li>
<li>Check work item discovery with <code>--dry-run</code> on map phase</li>
</ul>
<p><strong>Goal Seek Workflows</strong>:</p>
<ul>
<li>Validate score calculation logic</li>
<li>Check convergence criteria (threshold, max iterations)</li>
<li>Debug score trends with iteration history</li>
<li>Verify goal direction (maximize vs minimize)</li>
</ul>
<p><strong>Foreach Workflows</strong>:</p>
<ul>
<li>Test with small item lists first</li>
<li>Check parallel execution limits (<code>max_parallel</code>)</li>
<li>Verify item processing order if order matters</li>
<li>Debug failed items with individual command execution</li>
</ul>
<h3 id="performance-debugging"><a class="header" href="#performance-debugging">Performance Debugging</a></h3>
<p>Identify and resolve performance bottlenecks:</p>
<p><strong>Timing Analysis</strong>:</p>
<pre><code class="language-bash"># Check session timings
cat ~/.prodigy/sessions/{session-id}.json | jq '.timings'

# View step durations
cat ~/.prodigy/sessions/{session-id}.json | jq '.timings | to_entries | sort_by(.value.secs) | reverse'
</code></pre>
<p><strong>MapReduce Agent Duration</strong>:</p>
<pre><code class="language-bash"># Track agent execution times from event logs
cat ~/.prodigy/events/{repo}/{job_id}/events-*.jsonl | \
  jq 'select(.event_type == "AgentCompleted") | {agent_id, duration_secs: .duration.secs}'
</code></pre>
<p><strong>Resource Monitoring</strong>:</p>
<ul>
<li>Check disk space: <code>df -h ~/.prodigy/</code></li>
<li>Monitor parallel execution: <code>ps aux | grep prodigy</code></li>
<li>Track memory usage: <code>top</code> or <code>htop</code> during execution</li>
</ul>
<p><strong>Timeout Issues</strong> (Source: <code>src/config/mapreduce.rs</code>, see <a href="troubleshooting/../advanced/timeout-configuration.html">Timeout Configuration</a>):</p>
<ul>
<li>Adjust <code>timeout_config</code> for map/reduce phases</li>
<li>Set <code>agent_timeout_secs</code> for long-running agents</li>
<li>Use <code>PRODIGY_TIMEOUT</code> environment variable for global timeout override</li>
</ul>
<h3 id="worktree-debugging"><a class="header" href="#worktree-debugging">Worktree Debugging</a></h3>
<p><strong>Examine worktree execution history</strong>:</p>
<pre><code class="language-bash"># Navigate to worktree
cd ~/.prodigy/worktrees/{repo}/{session}/

# View all commits (execution trace)
git log --oneline

# Check current branch state
git status

# View specific commit details
git show &lt;commit-hash&gt;

# Compare with parent branch
git diff origin/master
</code></pre>
<p><strong>Verify worktree isolation</strong>:</p>
<pre><code class="language-bash"># Check main repo is clean
git status

# Verify worktree has changes
cd ~/.prodigy/worktrees/{repo}/{session}/
git status
</code></pre>
<p><strong>See also</strong>: <a href="troubleshooting/../mapreduce-worktree-architecture.html">MapReduce Worktree Architecture</a> for worktree isolation guarantees.</p>
<h3 id="interactive-debugging-techniques"><a class="header" href="#interactive-debugging-techniques">Interactive Debugging Techniques</a></h3>
<p><strong>Dry-Run Mode</strong>:</p>
<pre><code class="language-bash"># Preview DLQ retry actions without executing
prodigy dlq retry &lt;job_id&gt; --dry-run

# Test workflow validation without execution
prodigy validate workflow.yml
</code></pre>
<p><strong>Manual Checkpoint Inspection and Modification</strong>:</p>
<ol>
<li>Pause workflow execution (Ctrl+C)</li>
<li>Inspect checkpoint files (see Checkpoint Inspection above)</li>
<li>Edit checkpoint JSON to fix state if needed</li>
<li>Resume with <code>prodigy resume &lt;session-id&gt;</code></li>
</ol>
<p><strong>Git History Tracing</strong>:</p>
<pre><code class="language-bash"># View execution sequence from commits
cd ~/.prodigy/worktrees/{repo}/{session}/
git log --oneline --decorate --graph

# Find when specific file was modified
git log --follow -- path/to/file

# See what changed in specific step
git show &lt;commit-hash&gt;
</code></pre>
<h3 id="error-message-analysis"><a class="header" href="#error-message-analysis">Error Message Analysis</a></h3>
<p><strong>Read error messages carefully</strong> - MapReduce error types indicate specific failure modes:</p>
<ul>
<li><code>WorkItemProcessingError</code>: Individual agent execution failure (check DLQ)</li>
<li><code>CheckpointLoadError</code>: Checkpoint corruption or missing file (inspect checkpoint directory)</li>
<li><code>TimeoutError</code>: Execution exceeded configured timeout (adjust timeout settings)</li>
<li><code>ValidationError</code>: Workflow configuration issue (check YAML syntax and validation output)</li>
<li><code>WorktreeError</code>: Git worktree operation failure (check disk space, permissions)</li>
</ul>
<p><strong>Common error patterns</strong> and fixes in <a href="troubleshooting/common-error-messages.html">Common Error Messages</a>.</p>
<h3 id="getting-help-5"><a class="header" href="#getting-help-5">Getting Help</a></h3>
<p>When seeking support, include:</p>
<ol>
<li><strong>Full error messages</strong> (not just excerpts)</li>
<li><strong>Workflow configuration</strong> (YAML file)</li>
<li><strong>Verbosity output</strong> (<code>-vv</code> or <code>-vvv</code>)</li>
<li><strong>Recent Claude log</strong>: <code>prodigy logs --latest</code></li>
<li><strong>Session state</strong>: <code>cat ~/.prodigy/sessions/{session-id}.json</code></li>
<li><strong>Environment details</strong>: OS, Prodigy version, Claude Code version</li>
<li><strong>Reproduction steps</strong>: Minimal example that demonstrates the issue</li>
</ol>
<p><strong>Where to get help</strong>:</p>
<ul>
<li>GitHub Issues: https://github.com/prodigy-ai/prodigy</li>
<li>Documentation: https://prodigy.dev/docs</li>
<li>Community Discord: (link in README)</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
