{
  "metadata": {
    "project_name": "Prodigy",
    "analysis_date": "2025-10-11",
    "analyzed_version": "0.2.0+",
    "feature_areas": 12
  },
  "workflow_basics": {
    "structure": {
      "simple_array": {
        "description": "Direct command array format for simple workflows",
        "example": "commands: [\"claude: /command1\", \"shell: command2\"]"
      },
      "full_config": {
        "description": "Full configuration with env, secrets, and profiles",
        "fields": ["commands", "env", "secrets", "env_files", "profiles", "merge"]
      }
    },
    "execution_model": {
      "description": "Sequential command execution with git commit tracking",
      "features": ["Sequential execution", "Git commit per command", "Variable capture", "Conditional execution"]
    },
    "commit_tracking": {
      "description": "Automatic git commit tracking for audit trail",
      "commit_required_flag": "Control per command with commit_required: true/false"
    }
  },
  "mapreduce": {
    "phases": {
      "setup": {
        "description": "Optional preparation phase before map execution",
        "fields": ["commands", "timeout", "capture_outputs"],
        "use_cases": ["Data generation", "Environment setup", "Dependency installation"]
      },
      "map": {
        "description": "Parallel processing of work items",
        "required_fields": ["input", "agent_template"],
        "optional_fields": ["json_path", "max_parallel", "filter", "sort_by", "max_items", "offset", "distinct", "agent_timeout_secs"],
        "use_cases": ["Parallel code review", "Batch processing", "Independent work items"]
      },
      "reduce": {
        "description": "Aggregation phase after map completes",
        "fields": ["commands", "timeout_secs"],
        "use_cases": ["Result aggregation", "Report generation", "Summary creation"]
      }
    },
    "capabilities": {
      "parallel_execution": true,
      "work_distribution": "Automatic across agents with max_parallel control",
      "result_aggregation": "Via reduce phase with ${map.results} variable",
      "checkpoint_resume": "Automatic checkpoint saves for interruption recovery",
      "dlq_support": "Dead Letter Queue for failed items with retry capability",
      "cross_worktree_events": "Shared event logs across parallel worktrees"
    },
    "configuration": {
      "input_sources": {
        "json_file": "Static JSON file with work items",
        "command_output": "Shell command that produces JSON",
        "inline_data": "Embedded JSON in workflow"
      },
      "item_processing": {
        "jsonpath_extraction": "$.items[*] syntax for selecting items",
        "filtering": "JMESPath-like expressions to filter items",
        "sorting": "field ASC/DESC syntax for processing order",
        "pagination": "offset and max_items for controlled batching",
        "deduplication": "distinct field for unique items"
      },
      "agent_template": {
        "simplified_syntax": "Direct array of commands under agent_template",
        "legacy_syntax": "Nested commands: key (deprecated)",
        "variable_access": "${item.field} for work item fields"
      }
    }
  },
  "command_types": {
    "shell": {
      "description": "Execute shell commands",
      "syntax": "shell: \"command args\"",
      "common_fields": ["shell", "timeout", "capture_output", "on_failure", "on_success", "when"],
      "use_cases": ["Build steps", "Test execution", "File operations", "Data processing"],
      "examples": [
        "shell: \"cargo test\"",
        "shell: \"debtmap analyze . --output report.json\""
      ]
    },
    "claude": {
      "description": "Execute Claude AI commands via Claude Code CLI",
      "syntax": "claude: \"/command-name args\"",
      "common_fields": ["claude", "commit_required", "validate", "on_failure", "capture_output"],
      "use_cases": ["Code generation", "Analysis", "Refactoring", "Documentation"],
      "examples": [
        "claude: \"/prodigy-implement-spec 01\"",
        "claude: \"/prodigy-fix-issue '${item.description}'\""
      ]
    },
    "goal_seek": {
      "description": "Iterative refinement to reach quality threshold",
      "syntax": "goal_seek: {goal, validate, threshold, max_attempts}",
      "fields": {
        "goal": "Human-readable goal description",
        "claude": "Claude command for refinement (optional)",
        "shell": "Shell command for refinement (optional)",
        "validate": "Command to validate attempt (returns score 0-100)",
        "threshold": "Success threshold (0-100)",
        "max_attempts": "Maximum attempts before giving up",
        "timeout_seconds": "Optional timeout for entire operation",
        "fail_on_incomplete": "Whether to fail workflow on incomplete"
      },
      "use_cases": ["Coverage improvement", "Performance optimization", "Quality gates"],
      "example": {
        "goal": "Achieve 80% test coverage",
        "claude": "/improve-tests",
        "validate": "coverage-checker --output json",
        "threshold": 80,
        "max_attempts": 5
      }
    },
    "foreach": {
      "description": "Iterate over lists with optional parallelism",
      "syntax": "foreach: {input, do, parallel, continue_on_error}",
      "fields": {
        "input": "Command or list of items (ForeachInput)",
        "do": "Commands to execute per item",
        "parallel": "Boolean or number for parallel execution",
        "continue_on_error": "Continue on item failure",
        "max_items": "Optional limit on items to process"
      },
      "use_cases": ["File processing", "Batch operations", "Multi-step iteration"],
      "example": {
        "foreach": "ls *.rs",
        "do": ["claude: /format ${item}", "shell: rustfmt ${item}"],
        "parallel": 3
      }
    },
    "write_file": {
      "description": "Write content to files with variable interpolation",
      "syntax": "write_file: {path, content, format, mode, create_dirs}",
      "fields": {
        "path": "File path (supports variable interpolation)",
        "content": "Content to write (supports variable interpolation)",
        "format": "text (default), json, or yaml",
        "mode": "File permissions in octal (default: 0644)",
        "create_dirs": "Create parent directories if needed"
      },
      "use_cases": ["Config generation", "Report writing", "Data export"],
      "example": {
        "path": "${OUTPUT_DIR}/report.json",
        "content": "${results}",
        "format": "json",
        "create_dirs": true
      }
    }
  },
  "variables": {
    "standard": {
      "description": "Built-in variables available across all workflows",
      "variables": {
        "last.output": "Output from last command",
        "last.exit_code": "Exit code from last command",
        "shell.output": "Output from last shell command",
        "claude.output": "Output from last Claude command"
      }
    },
    "mapreduce": {
      "description": "Variables available in MapReduce workflows",
      "item_variables": {
        "item": "Current work item (object)",
        "item.field": "Access nested fields with dot notation",
        "item_index": "Zero-based index of current item",
        "item_total": "Total number of items"
      },
      "map_phase_variables": {
        "map.results": "Aggregated results from all agents",
        "map.total": "Total items processed",
        "map.successful": "Successfully processed items",
        "map.failed": "Failed items",
        "worker.id": "Parallel worker ID"
      }
    },
    "validation": {
      "description": "Variables from validation results",
      "variables": {
        "validation.completion": "Completion percentage (0-100)",
        "validation.status": "Status (complete/incomplete/failed)",
        "validation.gaps": "JSON of missing requirements",
        "validation.missing": "List of missing items",
        "validation.attempt_number": "Current attempt number in on_incomplete"
      }
    },
    "git_context": {
      "description": "Git context variables for commit information",
      "variables": {
        "git.branch": "Current branch name",
        "git.commit": "Current commit hash",
        "git.changed_files": "List of changed files",
        "git.status": "Git status output"
      },
      "pattern_filtering": {
        "description": "Filter files by pattern",
        "syntax": "git.changed_files(*.rs)",
        "example": "git.changed_files(src/**/*.rs)"
      }
    },
    "merge_context": {
      "description": "Variables available in merge workflows",
      "variables": {
        "merge.worktree": "Name of worktree being merged",
        "merge.source_branch": "Source branch (worktree branch)",
        "merge.target_branch": "Target branch (main/master)",
        "merge.session_id": "Session ID for correlation"
      }
    },
    "capture_formats": {
      "description": "Output capture format options",
      "formats": {
        "string": "Raw string output (default)",
        "json": "Parse as JSON",
        "lines": "Split into array of lines",
        "number": "Parse as numeric value",
        "boolean": "Parse as boolean"
      },
      "capture_config": {
        "capture_output": "true or variable name",
        "capture_format": "Format type (string, json, etc)",
        "capture_streams": "stdout, stderr, or both",
        "output_file": "Redirect to file"
      }
    }
  },
  "environment": {
    "global_env": {
      "description": "Static and dynamic environment variables",
      "static_values": {
        "syntax": "VAR_NAME: \"value\"",
        "supports_interpolation": true
      },
      "dynamic_values": {
        "syntax": "VAR_NAME: {command: \"cmd\", cache: true}",
        "use_case": "Compute values at runtime"
      },
      "conditional_values": {
        "syntax": "VAR_NAME: {condition: \"expr\", when_true: \"val1\", when_false: \"val2\"}",
        "use_case": "Environment-dependent configuration"
      }
    },
    "secrets": {
      "description": "Secret values masked in logs",
      "simple_format": {
        "syntax": "API_KEY: \"${env:API_KEY}\"",
        "use_case": "Reference environment variable"
      },
      "provider_format": {
        "syntax": "API_KEY: {provider: vault, key: \"api/key\", version: \"v1\"}",
        "providers": ["env", "file", "vault", "aws", "custom"]
      },
      "masking": "Automatically masked in logs, errors, and events"
    },
    "profiles": {
      "description": "Environment-specific configurations",
      "syntax": {
        "definition": "profiles: {dev: {...}, staging: {...}, prod: {...}}",
        "activation": "prodigy run workflow.yml --profile prod"
      },
      "use_cases": ["Development/staging/production", "Feature flags", "Environment switching"]
    },
    "step_env": {
      "description": "Command-level environment overrides",
      "fields": ["env", "working_dir", "clear_env", "temporary"],
      "use_case": "Per-command environment customization"
    }
  },
  "advanced_features": {
    "conditional_execution": {
      "description": "Execute commands based on expressions",
      "syntax": "when: \"${condition} == true\"",
      "operators": ["==", "!=", ">", "<", ">=", "<=", "contains", "matches"],
      "examples": [
        "when: \"${env} == 'production'\"",
        "when: \"${last.exit_code} == 0\"",
        "when: \"${validation.completion} >= 80\""
      ]
    },
    "output_capture": {
      "description": "Capture command output into variables",
      "formats": ["string", "json", "lines", "number", "boolean"],
      "streams": ["stdout", "stderr", "both"],
      "metadata": ["exit_code", "success", "duration"],
      "example": {
        "capture_output": "build_result",
        "capture_format": "json",
        "capture_streams": "stdout"
      }
    },
    "nested_conditionals": {
      "description": "Chained conditional handlers",
      "handlers": ["on_success", "on_failure", "on_exit_code"],
      "on_success": {
        "description": "Execute command on success",
        "supports_nesting": true
      },
      "on_failure": {
        "description": "Execute command on failure",
        "fields": ["claude", "shell", "commands", "max_attempts", "fail_workflow"]
      },
      "on_exit_code": {
        "description": "Map exit codes to different actions",
        "example": {
          "0": "on_success commands",
          "1": "retry commands",
          "2": "skip and continue"
        }
      }
    },
    "timeout_control": {
      "description": "Granular timeout configuration",
      "levels": {
        "command_timeout": "Per-command timeout in seconds",
        "setup_timeout": "Setup phase timeout",
        "agent_timeout": "MapReduce agent timeout",
        "workflow_timeout": "Entire workflow timeout (planned)"
      }
    },
    "working_directory": {
      "description": "Per-command working directory",
      "syntax": "working_dir: \"/path/to/dir\"",
      "supports_interpolation": true,
      "use_case": "Execute commands in different directories"
    }
  },
  "error_handling": {
    "workflow_level": {
      "on_item_failure": {
        "description": "Action when MapReduce item fails",
        "options": ["dlq", "retry", "skip", "stop", "custom"],
        "dlq": "Send to Dead Letter Queue for later retry",
        "retry": "Retry with exponential backoff",
        "skip": "Skip item and continue",
        "stop": "Stop entire workflow"
      },
      "error_collection": {
        "aggregate": "Collect all errors before reporting",
        "immediate": "Report errors as they occur",
        "batched": "Report in batches of N errors"
      },
      "circuit_breaker": {
        "description": "Prevent cascading failures",
        "fields": ["failure_threshold", "success_threshold", "timeout", "half_open_requests"],
        "states": ["closed", "open", "half-open"]
      },
      "max_failures": {
        "description": "Stop after N failures",
        "type": "number"
      },
      "failure_threshold": {
        "description": "Stop when failure rate exceeds percentage",
        "type": "float (0.0 to 1.0)"
      }
    },
    "command_level": {
      "on_failure": {
        "description": "Commands to run on failure",
        "fields": ["claude", "shell", "commands", "max_attempts", "fail_workflow", "commit_required"],
        "supports_nested_on_failure": true
      },
      "on_success": {
        "description": "Commands to run on success",
        "syntax": "on_success: {claude: \"/next-command\"}",
        "use_case": "Conditional next steps"
      },
      "on_exit_code": {
        "description": "Map specific exit codes to actions",
        "syntax": "on_exit_code: {0: success_handler, 1: retry_handler}"
      },
      "retry_config": {
        "description": "Automatic retry with backoff",
        "fields": ["max_attempts", "backoff"],
        "backoff_strategies": ["fixed", "linear", "exponential", "fibonacci"]
      }
    }
  },
  "retry_configuration": {
    "retry_defaults": {
      "description": "Default retry behavior per command type",
      "max_attempts": 3,
      "backoff_strategy": "exponential"
    },
    "backoff_strategies": {
      "fixed": {
        "description": "Fixed delay between retries",
        "fields": ["delay"]
      },
      "linear": {
        "description": "Linear increase in delay",
        "fields": ["initial", "increment"]
      },
      "exponential": {
        "description": "Exponential backoff (recommended)",
        "fields": ["initial", "multiplier"],
        "default": {
          "initial": "1s",
          "multiplier": 2.0
        }
      },
      "fibonacci": {
        "description": "Fibonacci sequence delays",
        "fields": ["initial"]
      }
    },
    "retry_budget": {
      "description": "Limit total retry attempts across workflow",
      "planned": true
    },
    "conditional_retry": {
      "description": "Retry based on error type or exit code",
      "example": {
        "retry_on_exit_code": [1, 2, 124],
        "skip_on_exit_code": [0, 3]
      }
    },
    "jitter": {
      "description": "Add randomness to backoff timing",
      "planned": true,
      "use_case": "Prevent thundering herd"
    }
  },
  "workflow_composition": {
    "imports": {
      "description": "Import workflow definitions from other files",
      "planned": true,
      "use_case": "Reusable workflow components"
    },
    "templates": {
      "description": "Define reusable command templates",
      "planned": true,
      "use_case": "Common command patterns"
    },
    "extends": {
      "description": "Extend base workflow configurations",
      "planned": true,
      "use_case": "Workflow inheritance"
    },
    "parameters": {
      "description": "Parameterized workflows",
      "planned": true,
      "use_case": "Reusable workflows with input parameters"
    }
  },
  "configuration": {
    "file_locations": {
      "workflow_files": "*.yml in project root or workflows/ directory",
      "config_file": ".prodigy/config.toml (optional)",
      "state_directory": ".prodigy/ (local) or ~/.prodigy/ (global)",
      "claude_commands": ".claude/commands/*.md"
    },
    "precedence": {
      "description": "Configuration override order",
      "order": ["CLI flags", "Environment variables", "Workflow config", "Global config", "Defaults"]
    },
    "claude_settings": {
      "streaming_output": "Control via -v flag or PRODIGY_CLAUDE_CONSOLE_OUTPUT env",
      "json_log_location": "Captured in AgentResult and DLQ for debugging"
    },
    "worktree_settings": {
      "location": "~/.prodigy/worktrees/{repo_name}/",
      "branch_tracking": "Tracks original branch for merge target",
      "cleanup": "Manual via 'prodigy worktree clean'"
    },
    "storage_settings": {
      "global_storage": "~/.prodigy/ (default)",
      "local_storage": ".prodigy/ (opt-in)",
      "event_logs": "~/.prodigy/events/{repo_name}/{job_id}/",
      "dlq": "~/.prodigy/dlq/{repo_name}/{job_id}/",
      "checkpoints": "~/.prodigy/state/{repo_name}/mapreduce/jobs/{job_id}/"
    },
    "retry_defaults": {
      "max_attempts": 3,
      "backoff_strategy": "exponential",
      "initial_delay": "1s"
    }
  },
  "git_context_advanced": {
    "pattern_filtering": {
      "description": "Filter git context by file patterns",
      "syntax": "git.changed_files(pattern)",
      "examples": [
        "git.changed_files(*.rs)",
        "git.changed_files(src/**/*.ts)",
        "git.changed_files(!test/**)"
      ]
    },
    "format_modifiers": {
      "description": "Transform git context output",
      "modifiers": {
        "basename": "Get file basename only",
        "dirname": "Get directory name only",
        "relative": "Relative to project root",
        "absolute": "Absolute path"
      }
    },
    "file_patterns": {
      "glob_patterns": "Standard glob syntax (* ** ? [])",
      "exclusion_patterns": "Prefix with ! to exclude",
      "multiple_patterns": "Separate with comma"
    },
    "combined_filters": {
      "description": "Combine multiple filters",
      "example": "git.changed_files(src/**/*.rs, !src/**/test*.rs)"
    }
  },
  "automated_documentation": {
    "workflow_setup": {
      "description": "MapReduce workflow for documentation drift detection",
      "phases": ["Setup: Feature analysis", "Map: Per-chapter drift detection", "Reduce: Build validation"],
      "example_workflow": "workflows/book-docs-drift.yml"
    },
    "book_config_structure": {
      "description": "Configuration file for book documentation",
      "location": ".prodigy/book-config.json or .debtmap/book-config.json",
      "required_fields": ["project_name", "book_dir", "analysis_targets", "chapter_file"],
      "optional_fields": ["custom_analysis"]
    },
    "chapter_definitions": {
      "description": "JSON file defining book chapters",
      "location": "workflows/data/{project}-chapters.json",
      "schema": {
        "chapters": [
          {
            "id": "chapter_id",
            "title": "Chapter Title",
            "file": "book/src/file.md",
            "topics": ["topic1", "topic2"]
          }
        ]
      }
    },
    "claude_commands": {
      "feature_analysis": "/prodigy-analyze-features-for-book",
      "gap_detection": "/prodigy-detect-documentation-gaps",
      "drift_analysis": "/prodigy-analyze-book-chapter-drift",
      "drift_fixing": "/prodigy-fix-chapter-drift",
      "build_errors": "/prodigy-fix-book-build-errors"
    },
    "mapreduce_phases": {
      "setup": [
        "Create analysis directory",
        "Analyze codebase features",
        "Detect documentation gaps"
      ],
      "map": [
        "Analyze each chapter for drift",
        "Fix drift in chapter"
      ],
      "reduce": [
        "Rebuild book",
        "Fix build errors if any"
      ]
    },
    "github_actions": {
      "description": "CI/CD integration for automatic drift detection",
      "trigger": "Schedule or on push to main",
      "workflow": "Run book-docs-drift.yml and create PR"
    },
    "customization": {
      "book_config": "Configure project-specific analysis targets",
      "chapter_definitions": "Define chapter structure and topics",
      "claude_commands": "Customize analysis and fixing logic",
      "error_handling": "Configure DLQ and retry policies"
    }
  },
  "best_practices": {
    "workflow_design": [
      "Keep workflows simple and focused on single responsibility",
      "Use validation for quality gates and completeness checks",
      "Handle errors gracefully with on_failure handlers",
      "Capture important outputs for downstream commands",
      "Use environment variables for parameterization",
      "Document complex workflows with comments"
    ],
    "mapreduce": [
      "Set appropriate parallelism based on resource constraints",
      "Use DLQ for failed items to enable retry",
      "Monitor with events and checkpoints",
      "Design idempotent work items",
      "Use filter and sort_by for controlled processing",
      "Test with small max_items before full run"
    ],
    "testing": [
      "Include test steps in workflows",
      "Use on_failure for debugging test failures",
      "Validate before deploying or merging",
      "Use goal_seek for coverage improvement",
      "Capture test output for analysis"
    ],
    "error_handling": [
      "Always set on_failure for critical commands",
      "Use circuit breakers for external dependencies",
      "Configure max_failures to prevent runaway workflows",
      "Use DLQ for recoverable failures",
      "Log errors with appropriate detail level"
    ],
    "environment_management": [
      "Use profiles for different environments",
      "Mark secrets appropriately for masking",
      "Validate environment variables at workflow start",
      "Use dynamic env for runtime-computed values",
      "Document required environment variables"
    ]
  },
  "common_patterns": [
    {
      "name": "Build and Test",
      "description": "Standard CI workflow with build, test, and lint",
      "steps": ["Build", "Test with on_failure for debugging", "Lint and format"],
      "example": "workflows/implement-with-tests.yml"
    },
    {
      "name": "Parallel Processing",
      "description": "MapReduce for independent work items",
      "steps": ["Setup phase for data prep", "Map phase for parallel processing", "Reduce phase for aggregation"],
      "example": "workflows/book-docs-drift.yml"
    },
    {
      "name": "Goal Seeking",
      "description": "Iterative improvement to quality threshold",
      "steps": ["Set goal and threshold", "Iterate with refinement command", "Validate after each attempt"],
      "example": "workflows/goal-seeking-examples.yml"
    },
    {
      "name": "Validation with Retry",
      "description": "Validate implementation completeness with gap filling",
      "steps": ["Execute command", "Validate result", "Auto-retry with on_incomplete"],
      "example": "workflows/debtmap.yml"
    },
    {
      "name": "Environment-Specific Deployment",
      "description": "Deploy with environment profiles",
      "steps": ["Select profile", "Deploy with profile-specific config", "Validate deployment"],
      "example": "workflows/environment-example.yml"
    }
  ],
  "troubleshooting": {
    "common_issues": [
      {
        "issue": "Variables not interpolating",
        "causes": ["Incorrect ${} syntax", "Variable not available in context", "Typo in variable name"],
        "solution": "Check variable syntax, ensure variable is set before use, verify variable name spelling"
      },
      {
        "issue": "MapReduce items not found",
        "causes": ["Incorrect JSONPath expression", "Input file not found", "JSON structure mismatch"],
        "solution": "Verify JSONPath with test tool, check input file exists, inspect JSON structure"
      },
      {
        "issue": "Timeout errors",
        "causes": ["Command takes too long", "Insufficient timeout setting", "Blocked process"],
        "solution": "Increase timeout value, optimize command, check for hanging processes"
      },
      {
        "issue": "DLQ items not retrying",
        "causes": ["DLQ not configured", "Items marked as not reprocess_eligible", "Retry command missing"],
        "solution": "Configure on_item_failure: dlq, check item eligibility, use 'prodigy dlq retry'"
      },
      {
        "issue": "Environment variables not set",
        "causes": ["Variable not defined in workflow", "Profile not activated", "Typo in variable name"],
        "solution": "Define in env: block, use --profile flag, check variable name"
      },
      {
        "issue": "Validation always fails",
        "causes": ["Incorrect result_file path", "Invalid JSON output", "Threshold too high"],
        "solution": "Verify result_file exists, validate JSON format, adjust threshold"
      },
      {
        "issue": "Merge conflicts after MapReduce",
        "causes": ["Multiple agents modified same files", "Reduce phase missing conflict resolution"],
        "solution": "Use merge workflow with conflict resolution, add reduce step for merging changes"
      },
      {
        "issue": "Claude command not found",
        "causes": ["Command not in .claude/commands/", "Typo in command name", "Command file syntax error"],
        "solution": "Check command exists, verify command name, validate markdown syntax"
      }
    ]
  }
}
