{
  "workflow_basics": {
    "structure": {
      "simple_array": "Direct array of command steps (compact syntax)",
      "full_config": "Object with commands, env, secrets, profiles, and merge",
      "command_formats": [
        "Simple string: 'command-name'",
        "WorkflowStep: { claude: '...', shell: '...', etc. }",
        "Structured Command: { name, args, options, metadata }"
      ]
    },
    "execution_model": {
      "sequential": "Commands execute in order",
      "git_integration": "Each command can create commits",
      "audit_trail": "Full history via git log",
      "worktree_isolation": "Sessions run in separate git worktrees"
    },
    "commit_tracking": {
      "commit_required": "Flag to indicate if command should commit",
      "default_behavior": "commit_required defaults to false",
      "validation": "Can verify commits were created"
    }
  },
  "mapreduce": {
    "phases": {
      "setup": "Optional preparation phase with output capture",
      "map": "Parallel processing of work items",
      "reduce": "Result aggregation and finalization"
    },
    "capabilities": {
      "parallel_execution": "Multi-agent concurrent processing",
      "work_distribution": "Automatic item assignment to agents",
      "result_aggregation": "Collected in reduce phase via ${map.results}",
      "checkpoint_resume": "Save progress, resume from last checkpoint",
      "dlq_support": "Failed items sent to Dead Letter Queue",
      "filtering": "JSONPath expressions and filter conditions",
      "sorting": "Sort items before processing",
      "deduplication": "Remove duplicates via distinct field"
    },
    "configuration": {
      "setup": {
        "fields": ["commands", "timeout", "capture_outputs"],
        "syntax": "Direct array or full config object",
        "capture_outputs": "Named variables from setup commands"
      },
      "map": {
        "required": ["input", "agent_template"],
        "optional": [
          "json_path",
          "max_parallel",
          "filter",
          "sort_by",
          "max_items",
          "offset",
          "distinct",
          "agent_timeout_secs",
          "timeout_config"
        ],
        "agent_template_syntax": "Direct array (preferred) or nested with 'commands' key"
      },
      "reduce": {
        "fields": ["commands", "timeout_secs"],
        "syntax": "Direct array (preferred) or nested with 'commands' key",
        "available_variables": ["${map.results}", "${map.total}", "${map.successful}", "${map.failed}"]
      }
    },
    "input_sources": {
      "json_file": "Static JSON file with work items",
      "jsonpath": "Extract items via JSONPath expression",
      "setup_output": "Generated during setup phase"
    }
  },
  "command_types": {
    "shell": {
      "description": "Execute shell commands with full control",
      "syntax": "shell: 'command string'",
      "fields": [
        "shell",
        "timeout",
        "capture_output",
        "capture_format",
        "capture_streams",
        "output_file",
        "on_failure",
        "on_success",
        "when"
      ],
      "use_cases": [
        "Build and compile",
        "Run tests",
        "File operations",
        "Data processing",
        "External tool integration"
      ]
    },
    "claude": {
      "description": "Execute Claude AI commands via CLI",
      "syntax": "claude: '/command-name args'",
      "fields": [
        "claude",
        "commit_required",
        "validate",
        "timeout",
        "on_failure",
        "on_success",
        "when"
      ],
      "use_cases": [
        "Code generation and modification",
        "Code analysis and review",
        "Refactoring",
        "Bug fixing",
        "Documentation generation"
      ]
    },
    "goal_seek": {
      "description": "Iterative refinement to reach quality threshold",
      "syntax": "goal_seek: { goal, claude/shell, validate, threshold, max_attempts }",
      "fields": [
        "goal",
        "claude",
        "shell",
        "validate",
        "threshold",
        "max_attempts",
        "timeout_seconds",
        "fail_on_incomplete"
      ],
      "use_cases": [
        "Test coverage improvement",
        "Performance optimization",
        "Code quality enhancement",
        "Spec implementation validation",
        "Iterative bug fixing"
      ],
      "validation": {
        "format": "Validator outputs 'score: N' where N is 0-100",
        "threshold": "Goal achieved when score >= threshold",
        "convergence": "Stops if no improvement for multiple attempts"
      }
    },
    "foreach": {
      "description": "Iterate over lists with optional parallelism",
      "syntax": "foreach: { input, do, parallel, continue_on_error }",
      "fields": [
        "input (foreach)",
        "do",
        "parallel",
        "continue_on_error",
        "max_items"
      ],
      "input_types": {
        "command": "Shell command output becomes items",
        "list": "Static array of items"
      },
      "parallel_config": {
        "boolean": "true = default parallel, false = sequential",
        "number": "Specific parallel count"
      },
      "use_cases": [
        "File batch processing",
        "Multi-environment deployments",
        "Parallel test execution",
        "Data transformation pipelines"
      ]
    },
    "validation": {
      "description": "Check implementation completeness against requirements",
      "syntax": "validate: { spec_file, coverage_threshold }",
      "purpose": "Ensure all spec requirements are implemented",
      "outputs": {
        "validation.completion": "Percentage complete",
        "validation.gaps": "Missing requirements",
        "validation.status": "complete/incomplete/failed"
      }
    }
  },
  "variables": {
    "standard": {
      "last.output": "Output from last command (any type)",
      "last.exit_code": "Exit code from last command",
      "shell.output": "Output from last shell command",
      "claude.output": "Output from last Claude command"
    },
    "standard_item": {
      "item": "Current work item being processed",
      "item.value": "Item value (for simple types)",
      "item.path": "File path (for file inputs)",
      "item.name": "Display name",
      "item_index": "Zero-based index of current item",
      "item_total": "Total number of items",
      "item.*": "Access any field from JSON work items"
    },
    "workflow_context": {
      "workflow.name": "Workflow name",
      "workflow.id": "Unique workflow ID",
      "workflow.iteration": "Current iteration number"
    },
    "step_context": {
      "step.name": "Current step name",
      "step.index": "Current step index"
    },
    "mapreduce": {
      "map.total": "Total items in map phase",
      "map.successful": "Successfully processed items",
      "map.failed": "Failed items count",
      "map.results": "Aggregated results from all agents",
      "map.key": "Key for map output (when specified)",
      "worker.id": "Parallel worker/agent ID"
    },
    "merge": {
      "merge.worktree": "Name of worktree being merged",
      "merge.source_branch": "Source branch (worktree branch)",
      "merge.target_branch": "Target branch (usually main)",
      "merge.session_id": "Session ID for correlation"
    },
    "validation": {
      "validation.completion": "Completion percentage (0-100)",
      "validation.gaps": "Array of missing requirements",
      "validation.status": "Status: complete/incomplete/failed"
    },
    "git_context": {
      "git.branch": "Current git branch",
      "commit.hash": "Latest commit hash"
    },
    "interpolation_syntax": {
      "$VAR": "Simple variable reference (shell-style)",
      "${VAR}": "Bracketed reference (recommended)",
      "${obj.field}": "Nested field access",
      "${var|default:value}": "Default value fallback",
      "${var.*.field}": "Wildcard access for arrays"
    }
  },
  "environment": {
    "global_env": {
      "description": "Static and dynamic variables for all commands",
      "formats": {
        "static": "KEY: value",
        "dynamic": "KEY: { command: '...', cache: true }",
        "conditional": "KEY: { condition: '${expr}', when_true: '...', when_false: '...' }"
      }
    },
    "secrets": {
      "description": "Sensitive values masked in logs",
      "formats": {
        "env_ref": "API_KEY: '${env:SECRET_API_KEY}'",
        "direct": "PASSWORD: 'secret-value'",
        "provider": "Future: vault, aws-secrets-manager, etc."
      },
      "masking": "Automatically hidden in logs, events, checkpoints"
    },
    "env_files": {
      "description": "Load variables from .env files",
      "format": ".env file paths (array)",
      "precedence": "Workflow env > env_files > system env"
    },
    "profiles": {
      "description": "Environment-specific configurations",
      "usage": "prodigy run workflow.yml --profile production",
      "examples": ["development", "testing", "staging", "production"]
    },
    "step_env": {
      "description": "Command-level environment overrides",
      "scope": "Single command execution",
      "temporary": "Can mark as temporary to restore after step",
      "clear_env": "Clear all env vars except step-specific"
    },
    "working_directory": {
      "description": "Per-command working directory control",
      "syntax": "working_dir: './frontend'",
      "interpolation": "working_dir: '${env.DEPLOY_DIR}'"
    }
  },
  "advanced_features": {
    "conditional_execution": {
      "when_clause": "Execute step only if condition is true",
      "syntax": "when: '${build.success} == true'",
      "expressions": [
        "Equality: ${var} == 'value'",
        "Comparison: ${count} >= 10",
        "Boolean: ${enabled} == true",
        "Existence: ${output} != ''"
      ]
    },
    "output_capture": {
      "formats": {
        "string": "Raw string output (default)",
        "json": "Parse as JSON object",
        "lines": "Split into array of lines",
        "number": "Parse as numeric value",
        "boolean": "Parse as boolean or use exit code"
      },
      "configuration": {
        "capture_output": "true | 'variable_name'",
        "capture_format": "string | json | lines | number | boolean",
        "capture_streams": "stdout, stderr, both",
        "output_file": "Redirect output to file"
      },
      "access_patterns": {
        "simple": "${var}",
        "nested": "${var.field.subfield}",
        "metadata": "${var.exit_code}, ${var.success}, ${var.duration}"
      }
    },
    "nested_handlers": {
      "on_failure": "Commands to run if step fails",
      "on_success": "Commands to run if step succeeds",
      "on_exit_code": "Map specific exit codes to actions (future)",
      "nesting": "Handlers can contain goal_seek, foreach, etc."
    },
    "timeout_control": {
      "command_level": "timeout: 300 (seconds)",
      "workflow_level": "Overall workflow timeout",
      "phase_level": "Setup, map, reduce timeouts",
      "agent_timeout": "Per-agent timeout in MapReduce"
    }
  },
  "error_handling": {
    "workflow_level": {
      "on_item_failure": {
        "dlq": "Send failed items to Dead Letter Queue (default)",
        "retry": "Retry item with backoff",
        "skip": "Skip item and continue",
        "stop": "Stop entire workflow",
        "custom": "Use custom failure handler"
      },
      "error_collection": {
        "aggregate": "Collect all errors before reporting (default)",
        "immediate": "Report errors as they occur",
        "batched": "Report errors in batches of N"
      },
      "circuit_breaker": {
        "failure_threshold": "Failures to trigger open state",
        "success_threshold": "Successes to close circuit",
        "timeout": "Time before attempting to close",
        "half_open_requests": "Test requests in half-open state"
      },
      "thresholds": {
        "max_failures": "Stop after N failures",
        "failure_threshold": "Stop if failure rate exceeds X%",
        "continue_on_failure": "Continue or stop on first failure"
      }
    },
    "command_level": {
      "on_failure": {
        "syntax": "on_failure: { claude: '...', max_attempts: 3 }",
        "fields": ["claude", "shell", "max_attempts", "fail_workflow", "commit_required"],
        "use_case": "Automatic debugging and recovery"
      },
      "retry_config": {
        "max_attempts": "Maximum retry attempts",
        "backoff": "fixed | linear | exponential | fibonacci",
        "parameters": {
          "fixed": "delay duration",
          "linear": "initial + increment",
          "exponential": "initial * multiplier",
          "fibonacci": "initial"
        }
      },
      "continue_on_error": "Whether to continue workflow on command failure"
    }
  },
  "best_practices": {
    "workflow_design": [
      "Keep workflows simple and focused on one main goal",
      "Use validation commands to verify quality gates",
      "Handle errors gracefully with on_failure handlers",
      "Capture important outputs for downstream commands",
      "Use descriptive names for clarity",
      "Break complex workflows into smaller, composable pieces"
    ],
    "mapreduce": [
      "Set appropriate max_parallel based on system resources",
      "Use DLQ for failed items to enable retry later",
      "Monitor execution with events and checkpoints",
      "Design idempotent work items (can safely retry)",
      "Use filter and sort to optimize processing order",
      "Consider using distinct to eliminate duplicates",
      "Set agent_timeout to prevent hung processes"
    ],
    "environment": [
      "Use profiles for environment-specific configs",
      "Mark sensitive data as secrets",
      "Load common config from .env files",
      "Use step-level env for temporary overrides",
      "Document required environment variables"
    ],
    "variables": [
      "Use ${} syntax for clarity and consistency",
      "Leverage nested field access for JSON data",
      "Set defaults for optional variables",
      "Capture outputs with descriptive names",
      "Use standard variable names for consistency"
    ],
    "error_handling": [
      "Set appropriate max_failures for workflow",
      "Use circuit breaker for unstable external services",
      "Enable DLQ for MapReduce workflows",
      "Add on_failure handlers for critical steps",
      "Monitor error metrics and patterns"
    ],
    "testing": [
      "Include test steps in workflows",
      "Use goal_seek for test-driven development",
      "Add on_failure handlers to debug test failures",
      "Validate before deploying or merging",
      "Use validation commands to check completeness"
    ]
  },
  "common_patterns": [
    {
      "name": "Build and Test",
      "description": "Standard CI workflow with error recovery",
      "structure": [
        "shell: cargo build",
        "shell: cargo test with on_failure debug handler",
        "shell: cargo clippy"
      ],
      "use_case": "Continuous integration"
    },
    {
      "name": "Parallel Processing",
      "description": "MapReduce for independent work items",
      "structure": [
        "setup: Generate work items",
        "map: Process items in parallel",
        "reduce: Aggregate results and report"
      ],
      "use_case": "Large-scale code transformations, batch processing"
    },
    {
      "name": "Goal Seeking",
      "description": "Iterative improvement to quality threshold",
      "structure": [
        "goal_seek with claude command",
        "validate command returning score",
        "threshold and max_attempts"
      ],
      "use_case": "Test coverage, performance optimization, spec implementation"
    },
    {
      "name": "Multi-Stage Pipeline",
      "description": "Sequential stages with validation gates",
      "structure": [
        "Analyze and plan",
        "Implement with validation",
        "Test and verify",
        "Deploy or merge"
      ],
      "use_case": "Feature implementation, documentation maintenance"
    },
    {
      "name": "Conditional Workflow",
      "description": "Different paths based on conditions",
      "structure": [
        "Initial assessment step",
        "Conditional steps with when clauses",
        "Different actions per environment"
      ],
      "use_case": "Environment-specific deployments, feature flags"
    },
    {
      "name": "Error Recovery",
      "description": "Automatic debugging on failure",
      "structure": [
        "Primary command",
        "on_failure: debugging command",
        "Retry or escalate"
      ],
      "use_case": "Test debugging, build failures, deployment issues"
    }
  ],
  "troubleshooting": {
    "common_issues": [
      {
        "issue": "Variables not interpolating correctly",
        "symptoms": [
          "Literal ${var} appearing in output",
          "Empty strings instead of values"
        ],
        "solutions": [
          "Check ${} syntax is correct",
          "Verify variable is available in current context",
          "Use ${var|default:fallback} for optional variables",
          "Check scope (workflow vs step vs map/reduce)"
        ]
      },
      {
        "issue": "MapReduce items not found",
        "symptoms": [
          "No items processed",
          "JSONPath returns empty"
        ],
        "solutions": [
          "Verify JSONPath expression with jq or online tool",
          "Check input file exists and is valid JSON",
          "Ensure json_path starts with $ (e.g., '$.items[*]')",
          "Use json_path: '' for root-level arrays"
        ]
      },
      {
        "issue": "Commands timing out",
        "symptoms": [
          "Workflow hangs",
          "Timeout errors"
        ],
        "solutions": [
          "Increase timeout at command, phase, or workflow level",
          "Optimize long-running commands",
          "Check for blocking I/O or infinite loops",
          "Use agent_timeout_secs in MapReduce"
        ]
      },
      {
        "issue": "Environment variables not set",
        "symptoms": [
          "Commands fail with missing variables",
          "Empty values in interpolation"
        ],
        "solutions": [
          "Check env defined at workflow or profile level",
          "Verify .env files are loaded",
          "Use system env vars as fallback",
          "Check spelling and case sensitivity"
        ]
      },
      {
        "issue": "Goal seek not converging",
        "symptoms": [
          "Max attempts reached without success",
          "Score not improving"
        ],
        "solutions": [
          "Verify validate command outputs 'score: N' format",
          "Check threshold is achievable",
          "Increase max_attempts if needed",
          "Review Claude command for clarity",
          "Check for determinism in validation"
        ]
      },
      {
        "issue": "Parallel execution too slow",
        "symptoms": [
          "MapReduce not utilizing cores",
          "Sequential-like performance"
        ],
        "solutions": [
          "Increase max_parallel (default: 10)",
          "Check system resources (CPU, memory)",
          "Ensure work items are truly independent",
          "Monitor with -v for agent activity"
        ]
      },
      {
        "issue": "DLQ items not retryable",
        "symptoms": [
          "prodigy dlq retry fails",
          "Items fail again on retry"
        ],
        "solutions": [
          "Fix underlying issue causing failures",
          "Check reprocess_eligible flag",
          "Review failure_history for error patterns",
          "Ensure idempotent work item design"
        ]
      },
      {
        "issue": "Merge conflicts after MapReduce",
        "symptoms": [
          "Cannot merge worktree results",
          "Git conflicts"
        ],
        "solutions": [
          "Use custom merge workflow",
          "Ensure work items don't overlap",
          "Add conflict resolution in merge phase",
          "Consider reducing max_parallel"
        ]
      }
    ]
  },
  "dlq_features": {
    "description": "Dead Letter Queue for failed work items",
    "capabilities": {
      "automatic_dlq": "Failed items sent to DLQ automatically",
      "retry_mechanism": "Retry failed items with prodigy dlq retry <job_id>",
      "failure_tracking": "Full history of attempts and errors",
      "correlation_ids": "Track items across retries",
      "streaming": "Process large DLQs without memory issues"
    },
    "storage": {
      "location": "~/.prodigy/dlq/{repo_name}/{job_id}/",
      "format": "JSONL for efficient streaming",
      "shared": "Centralized across worktrees"
    },
    "retry_options": {
      "max_parallel": "Control parallelism during retry",
      "dry_run": "See what would be retried",
      "filtering": "Future: retry specific items"
    }
  },
  "custom_merge_workflow": {
    "description": "Configurable merge process for worktree integration",
    "configuration": {
      "merge.commands": "Array of commands to execute",
      "merge.timeout": "Timeout for entire merge phase (default: 600s)"
    },
    "available_variables": [
      "${merge.worktree}",
      "${merge.source_branch}",
      "${merge.target_branch}",
      "${merge.session_id}"
    ],
    "use_cases": [
      "Pre-merge validation (tests, linting)",
      "Conflict resolution strategies",
      "Post-merge hooks and notifications",
      "Integration with CI/CD systems"
    ],
    "streaming_output": {
      "default": "Clean output (verbosity = 0)",
      "verbose": "JSON streaming with -v flag",
      "env_override": "PRODIGY_CLAUDE_CONSOLE_OUTPUT=true"
    }
  },
  "version_info": {
    "analyzed_version": "0.2.0+",
    "analysis_date": "2025-01-04",
    "project_name": "Prodigy"
  }
}
